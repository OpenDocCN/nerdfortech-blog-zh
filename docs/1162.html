<html>
<head>
<title>Review — CB Loss: Class-Balanced Loss Based on Effective Number of Samples (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾— CB损失:基于有效样本数的类别平衡损失(图像分类)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-cb-loss-class-balanced-loss-based-on-effective-number-of-samples-image-classification-3056a1a1a001?source=collection_archive---------3-----------------------#2021-03-07">https://medium.com/nerd-for-tech/review-cb-loss-class-balanced-loss-based-on-effective-number-of-samples-image-classification-3056a1a1a001?source=collection_archive---------3-----------------------#2021-03-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1ea2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用每个类别的有效样本数来重新平衡损失，优于<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank"> RetinaNet </a>中的<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank">聚焦损失</a></h2></div><p id="3d48" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">在</span>这篇论文中，回顾了康奈尔大学、康奈尔理工大学、谷歌大脑和Alphabet Inc .、<strong class="ja hj"> CB损耗</strong>的<strong class="ja hj">基于有效样本数的类平衡损耗。在本文中:</strong></p><ul class=""><li id="8223" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">设计了一种重新加权方案，即<strong class="ja hj">使用每一类的有效样本数来重新平衡损失</strong>，称为类平衡损失。</li></ul><p id="ed61" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这是一篇在<strong class="ja hj"> 2019 CVPR </strong>超过<strong class="ja hj"> 200 </strong> <strong class="ja hj">引文</strong>的论文。(<a class="km kn ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----3056a1a1a001--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="8cf7" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">概述</h1><ol class=""><li id="4efa" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ls kj kk kl bi translated"><strong class="ja hj">阶层失衡问题</strong></li><li id="91ef" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ls kj kk kl bi translated"><strong class="ja hj">有效样本数</strong></li><li id="d192" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ls kj kk kl bi translated"><strong class="ja hj">类别平衡损失(CB损失)</strong></li><li id="e352" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ls kj kk kl bi translated"><strong class="ja hj">实验结果</strong></li></ol></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="4525" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated"><strong class="ak"> 1。阶层失衡问题</strong></h1><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ly"><img src="../Images/66a8902b5af68003c982b936fb807c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*EaerUegse4nPm_BqRgq70w.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">两个类，一个来自长尾数据集的头部，一个来自尾部(iNaturalist 2017 Dataset) </strong></figcaption></figure><ul class=""><li id="771b" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">假设有一些职业像上面一样不平衡。</li><li id="640f" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj"> Head </strong>:指数小的类，这些类的样本数多。</li><li id="4fea" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">尾</strong>:指数大的类，这些类的样本数较少。</li><li id="3893" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">黑色实线</strong>:直接在这些样本上训练的模型<strong class="ja hj">偏向优势类。</strong></li><li id="dacd" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">红色虚线</strong> : <strong class="ja hj">通过逆类别频率对损失重新加权</strong>可能会在具有高类别不平衡的真实世界数据上产生不良性能。</li><li id="5f9c" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">蓝色虚线</strong>:设计了一个类平衡项<strong class="ja hj">通过反有效样本数重新加权损失。</strong></li></ul></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="b658" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">2.有效样本数</h1><h2 id="23dc" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">2.1.定义</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es my"><img src="../Images/d31d3d5bcc63edefded7c8a5c00216c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7Ap1ZFwKRJxIGgk0e6BBA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">数据间信息重叠，左:特征空间S，中:1个样本数据的单位体积，右:数据间信息重叠</strong></figcaption></figure><blockquote class="nd ne nf"><p id="69db" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">直觉上，数据越多越好。然而，由于<strong class="ja hj">数据中存在信息重叠，随着样本数量的增加，模型从数据中提取的边际收益会减少。</strong></p></blockquote><ul class=""><li id="1bb7" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">左</strong>:给定一个类，将该类的<strong class="ja hj">特征空间中所有可能数据的集合表示为<strong class="ja hj"> <em class="ng"> S </em> </strong>。假设<em class="ng"> S </em>的体积为<em class="ng"> N </em>且<em class="ng"> N </em> ≥ 1。</strong></li><li id="b267" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">中间</strong>:<strong class="ja hj"><em class="ng">S</em>子集中的每个样品</strong>的<strong class="ja hj">单位体积为1 </strong>，<strong class="ja hj">可能与其他样品</strong>重叠。</li><li id="6c4c" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">右</strong>:从<em class="ng"> S </em>中随机抽取每个子集，覆盖整个<em class="ng"> S </em>集合。采样的数据越多，<em class="ng"> S </em>的覆盖范围就越好。</li><li id="465b" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">采样数据的预期总量随着样本数量的增加而增加，并受到<em class="ng"> N </em>的限制。</li></ul><blockquote class="nd ne nf"><p id="67fb" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">因此，有效样本数被定义为预期的样本量。</p></blockquote><ul class=""><li id="138e" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">这个想法是通过使用一个类的更多数据点来捕捉边际收益递减。</li><li id="6085" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">由于真实世界数据之间的内在相似性，随着样本数量的增长，新添加的样本很可能是现有样本的近似副本。</li><li id="8b19" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">此外，用大量数据扩充来训练CNN，所有扩充的例子也被认为与原始例子相同。</li><li id="9372" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">对于一个类，<strong class="ja hj"> <em class="ng"> N </em> </strong>可以看作是<strong class="ja hj">唯一原型</strong>的数量。</li></ul><h2 id="6d85" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">2.2.数学公式</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nk"><img src="../Images/7a60ca870478e39c7ff401f6a03db7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*5cCz1laZOczdtGx-KxxHiA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">新采样数据与先前采样数据重叠或不重叠</strong></figcaption></figure><ul class=""><li id="08de" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">将<strong class="ja hj">样品的有效数量(预期体积)</strong>表示为<strong class="ja hj"> <em class="ng"> En </em> </strong>。</li><li id="5a34" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">为了简化问题，不考虑部分重叠的情况。</li><li id="d90c" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">也就是说，<strong class="ja hj">一个新采样的数据点</strong>只能以两种方式与先前采样的数据相互作用:要么<strong class="ja hj">完全在先前采样的数据集</strong>内，概率为<strong class="ja hj">p</strong>，要么<strong class="ja hj">完全在</strong>外，概率为1- <em class="ng"> p </em> 。</li><li id="f0de" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">命题(有效数)</strong>:<em class="ng">en</em>=(1<em class="ng">β^n</em>)/(1<em class="ng">β</em>，其中<br/><em class="ng">β</em>=(<em class="ng">n</em>—1)/<em class="ng">n</em>。这个命题用<strong class="ja hj">数学归纳法</strong>证明。</li><li id="b8aa" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">当<em class="ng"> E </em> 1 = 1时，没有重叠。<em class="ng">e</em>1 =(1<em class="ng">β^</em>1)/(1<em class="ng">β</em>)= 1保持。</li><li id="7cda" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">考虑有先前采样的第<em class="ng">n</em>1个样本，并且将要采样第<em class="ng"> n </em>个样本。现在<strong class="ja hj">先前采样数据的预期量是<em class="ng">En</em>1</strong>和<strong class="ja hj">新采样的数据点有概率<em class="ng">p</em>=<em class="ng">E</em>(<em class="ng">N</em>1)/<em class="ng">N</em>与先前的样本</strong>重叠。因此，<strong class="ja hj">抽样后的期望体积<em class="ng">第n个</em>例子是:</strong></li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nl"><img src="../Images/e48865c7273b89ff5be648aa6f56819f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*rPF7o7Wg8IiVY-pgDorrWg.png"/></div></figure><ul class=""><li id="eee1" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">使用:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nm"><img src="../Images/26cef8f58b964a26f034a5bb23dc0a57.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Qe-wyDM65GsT62F27JQn_g.png"/></div></figure><ul class=""><li id="26ff" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">我们有:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nn"><img src="../Images/bb333586a1563d66d53292fe4d19f570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*ywACVzEPv-BkiRKXOaPPpQ.png"/></div></figure><ul class=""><li id="d63c" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">上述命题表明<strong class="ja hj">有效样本数是<em class="ng"> n </em>的指数函数。</strong></li><li id="ed41" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">超参数<em class="ng">β</em>∈【0，1】控制<em class="ng"> En </em>随着<em class="ng"> n </em>增加的速度。</strong></li></ul></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="a435" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated"><strong class="ak"> 3。类别平衡损失(CB损失)</strong></h1><ul class=""><li id="baef" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ki kj kk kl bi translated"><strong class="ja hj">等级平衡(CB)损失</strong>写为:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es no"><img src="../Images/5b4894cd0ea0e165b4a74328b44c2c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*hzIvplyYnWQJLqJk6gg24Q.png"/></div></figure><ul class=""><li id="21a5" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">其中<em class="ng"> ny </em>是地面实况类<em class="ng"> y </em>中的样本数。</li><li id="c7d0" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><em class="ng"> β </em> = 0对应不重新加权，<em class="ng"> β </em> → 1对应按逆类频率重新加权。</li></ul><blockquote class="nd ne nf"><p id="5c3c" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">所提出的有效样本数的新概念使我们能够使用<strong class="ja hj">超参数<em class="hi"> β </em>在不重新加权和通过逆类频率重新加权之间平滑地调整类平衡项。</strong></p></blockquote><ul class=""><li id="8c1c" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">所提出的类平衡项是<strong class="ja hj">模型不可知的</strong>和<strong class="ja hj">损失不可知的</strong>，因为它独立于损失函数<em class="ng"> L </em>和预测的类概率<em class="ng"> p </em>的选择。</li></ul><h2 id="9f4a" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">3.1.类平衡Softmax交叉熵损失</h2><ul class=""><li id="e564" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ki kj kk kl bi translated">给定一个分类标签为<em class="ng"> y </em>的样本，该样本的softmax交叉熵(CE)损失可写为:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es np"><img src="../Images/e690d161382b8669e255ab54a3cd12f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*pBwONQrzbVDqfJXVY71uDQ.png"/></div></figure><ul class=""><li id="2382" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">假设类<em class="ng"> y </em>有<em class="ng"> ny </em>个训练样本，<strong class="ja hj">类平衡(CB) softmax交叉熵损失</strong>为:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nq"><img src="../Images/0b108c8005c301ae79b1758aee700133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*xOzPnYEuUembcGwvQGwG3g.png"/></div></figure><h2 id="990c" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">3.2.类平衡Sigmoid交叉熵损失</h2><ul class=""><li id="3f37" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ki kj kk kl bi translated">当使用sigmoid函数处理多类问题时，网络的每个输出ode都在执行一个<strong class="ja hj">一对一分类</strong>来预测目标类相对于其余类的概率。</li><li id="b28b" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">在这种情况下，<strong class="ja hj"> Sigmoid并没有假设类之间的互斥性。</strong></li><li id="b293" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">由于每个类都被认为是独立的，并且有自己的预测器，因此sigmoid将单标签分类与多标签预测统一起来。这是一个很好的属性，因为真实世界的数据通常有不止一个语义标签。</li><li id="8e59" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">sigmoid交叉熵(CE)损失可写为:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nr"><img src="../Images/c08d1483d42508c679ff62ae1ad24fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*jIoWVyYGAuT_gK9TmKX5Nw.png"/></div></figure><ul class=""><li id="9b2f" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">类平衡(CB) sigmoid交叉熵损失</strong>为:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ns"><img src="../Images/98530e6764b3bc844a53c840ca01a7b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*rwEgUzCWkWt8VDueWb9_0A.png"/></div></figure><h2 id="95cc" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">3.3.类平衡<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank">局灶性丢失</a></h2><ul class=""><li id="6b3e" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ki kj kk kl bi translated"><a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank">视网膜</a>提出的<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank">局灶性丢失(FL) </a>，降低了分类良好样本的相对丢失，聚焦于疑难样本:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nt"><img src="../Images/06dbc10c060fcf585b79930fa8b5ffc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*OLP00E7OF8Gqyoi40DbGaQ.png"/></div></figure><ul class=""><li id="1894" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">(如需更多详细信息，如有兴趣，请随意阅读<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank">视网膜</a>。)</li><li id="6bcf" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">类平衡(CB) </strong> <a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank"> <strong class="ja hj">局灶性丢失</strong> </a> <strong class="ja hj"> </strong>为:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nu"><img src="../Images/356c79202615bd3c9eef33bec5d8f442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*fFm7erySt65HYnd1UH5sSg.png"/></div></figure></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="d0c2" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">4.实验结果</h1><h2 id="428c" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">4.1.数据集</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nv"><img src="../Images/b6554a59387e896cbd91f94d90de6bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*kDe-ZE36F3EE3Mn3zz3sLw.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">用于评估类平衡损失有效性的数据集</strong></figcaption></figure><ul class=""><li id="8250" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">尝试了5种分别具有10、20、50、100和200失衡因子的长尾版本的CIFS-10和CIFS-100。</li><li id="3c06" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">非理性主义者和ILSVRC本质上是阶级失衡。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nw"><img src="../Images/b0c4a35f53ee83fed961a71e9e1786c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*_oq7Q9nqal5yAzwE02FBEw.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">在不同失衡因子的人工创建的长尾CIFS-100数据集上，每类训练样本数。</strong></figcaption></figure><ul class=""><li id="044f" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">上图显示了具有不同不平衡因子的每类图像的数量。</li></ul><h2 id="d864" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">4.2.CIFAR数据集</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nx"><img src="../Images/3ada85c616b54ff5cbbbb960bde5c098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8r4n0-tb_0UmLm7ncdr7bw.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx"/><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd kx">RES net</strong></a><strong class="bd kx">分类错误率-32在长尾CIFAR-10和CIFAR-100测试集上接受了不同损失函数的训练</strong></figcaption></figure><ul class=""><li id="ee0e" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">超参数的搜索空间为{softmax，sigmoid，focal}为损耗类型，<em class="ng">β</em>∞{ 0.9，0.99，0.999，0.9999}，<em class="ng">γ</em>∞{ 0.5，1.0，2.0}为<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank">焦损</a>。</li><li id="baaf" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">最佳<em class="ng"> β </em>一致为0.9999 on cipal-10</strong>。</li><li id="f780" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">但在CIFS ar-100上，不同失衡因子的数据集往往有不同且较小的最优<em class="ng"> β </em>。</strong></li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es ny"><img src="../Images/94c8f75e9bad53c09a862747a406c6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yRzakYWf0BWt7IiCL_hu3w.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">有无类平衡项训练时的分类错误率。</strong></figcaption></figure><ul class=""><li id="93ba" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在<strong class="ja hj"> CIFAR-10 </strong>上，根据<em class="ng"> β </em> = 0.9999重新加权时，有效样本数接近样本数。这意味着<strong class="ja hj">CIFAR-10上的最佳重新加权策略类似于通过逆类频率进行重新加权。</strong></li><li id="715d" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">在<strong class="ja hj"> CIFAR-100 </strong>上，使用更大的<em class="ng"> β </em>表现不佳，说明按逆类频率重新加权不是明智的选择。<strong class="ja hj">需要一个更小的<em class="ng"> β </em>,它具有跨类的更平滑的权重。</strong></li><li id="46c1" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">例如，特定鸟类的独特原型的数量应该少于一般鸟类的独特原型的数量。<strong class="ja hj">由于CIFAR-100中的类比CIFAR-10更细粒度，因此与CIFAR-10相比，CIFAR-100的<em class="ng"> N </em>更小。</strong></li></ul><h2 id="60aa" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">4.3.大规模数据集</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nz"><img src="../Images/0f91d43068367ac33c0a10c79e6f8409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G2a5EqrUXA5zLg2AiOwAhw.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">在所有数据集的验证集上使用不同损失函数训练的大规模数据集上的前1和前5分类错误率</strong></figcaption></figure><ul class=""><li id="3163" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">使用类平衡的</strong> <a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank"> <strong class="ja hj">焦损</strong> </a>，因为它更具有灵活性，并且发现<strong class="ja hj"> <em class="ng"> β </em> = 0.999 </strong>和<strong class="ja hj"> <em class="ng"> γ </em> = 0.5 </strong>在所有数据集上都产生合理的<strong class="ja hj">良好性能</strong>。</li><li id="a2b2" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">值得注意的是，<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="ja hj">【ResNet】</strong></a><strong class="ja hj">-50在使用类平衡</strong> <a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank"> <strong class="ja hj"><strong class="ja hj">ResNet</strong></strong></a><strong class="ja hj"><strong class="ja hj">-152和</strong><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="ja hj">ResNet</strong></a><strong class="ja hj">-101在ILSVRC 2012上使用类平衡</strong><a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank"><strong class="ja hj">焦损</strong> </a>代替softmax交叉熵时，能够达到与它们相当的</strong></li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es oa"><img src="../Images/ebc86f99352c02985b3e773ccc0c7b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uae-3mixOges_QaIg2gUew.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd kx">RES net-50在ILSVRC 2012(左)和iNaturalist 2018(右)上的训练曲线。</strong></figcaption></figure><ul class=""><li id="c8e6" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">上图显示<strong class="ja hj">职业平衡的</strong> <a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank"> <strong class="ja hj">焦损</strong> </a> <strong class="ja hj">经过60个历元的训练后开始显现优势。</strong></li></ul></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h2 id="7578" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">参考</h2><p id="28b3" class="pw-post-body-paragraph iy iz hi ja b jb ln ij jd je lo im jg jh ob jj jk jl oc jn jo jp od jr js jt hb bi translated">【2019 CVPR】【CB损耗】<br/> <a class="ae ix" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">基于有效样本数的类平衡损耗</a></p><h2 id="3870" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated">图像分类</h2><p id="83e9" class="pw-post-body-paragraph iy iz hi ja b jb ln ij jd je lo im jg jh ob jj jk jl oc jn jo jp od jr js jt hb bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(什)(么)(情)(感)(呢)(?)(我)(们)(都)(不)(知)(道)(了)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(什)(么)(情)(感)(呢)(?)(我)(们)(还)(没)(有)(什)(么)(好)(感)(,)(我)(们)(就)(没)(有)(什)(么)(情)(感)(,)(我)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(强)(的)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)( )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(都)(不)(想)(要)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(了)(,)(我)(们)(还)(没)(想)(到)(这)(个)(人)(,)(我)(们)(还)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(都)(不)(想)(到)(这)(里)(去)(了)(。 )(我)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。 )(我)(们)(都)(不)(想)(到)(这)(里)(来)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(不)(想)(要)(到)(这)(里)(去)(,)(我)(们)(都)(不)(想)(到)(这)(里)(去)(了)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(。</p><h2 id="03a1" class="mk kw hi bd kx ml mm mn lb mo mp mq lf jh mr ms lh jl mt mu lj jp mv mw ll mx bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>