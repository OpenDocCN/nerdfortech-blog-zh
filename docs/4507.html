<html>
<head>
<title>What are Transformers models- part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是变形金刚模型-第 3 部分</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/what-are-transformers-models-part-3-7638f41d7377?source=collection_archive---------24-----------------------#2021-07-23">https://medium.com/nerd-for-tech/what-are-transformers-models-part-3-7638f41d7377?source=collection_archive---------24-----------------------#2021-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f94c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在之前的故事中，我们讨论了<a class="ae jd" rel="noopener" href="/nerd-for-tech/what-are-transformers-models-part-1-cf7ec6e8b3e8">变压器型号</a>及其应用，并详细讨论了<a class="ae jd" rel="noopener" href="/nerd-for-tech/what-are-transformers-models-part-2-83ddc20c038e">编码器模块</a>架构。在本文中，我们将更多地了解解码器模块，这是变压器的另一个主要构建模块。</p><h2 id="aec3" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">解码器模块</h2><p id="db84" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi ke translated"><span class="l kf kg kh bm ki kj kk kl km di">解码器的架构类似于我们之前讨论的编码器模型。它由一堆结构相同的解码器组成。<em class="kn">编码器的输出将把它作为序列输入传递给解码器，这个过程将继续，直到到达一个特定的符号，表示输出完成</em>例如:当我们解码句子<strong class="ih hj">“欢迎来到纽约市。”</strong>使用解码器时，每个单词将有一个数字表示或特征向量作为解码器模型的输出，并且当"."符号传递到解码器，表示输出已完成。</span></p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ko"><img src="../Images/4336e8334a34ba69a8a8d9ad47e92967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8jTqC2HMbBVVgVM63UOWyQ.gif"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">编码器-解码器插图(提供:Jay Alammar 的<a class="ae jd" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">博客</a></figcaption></figure><h2 id="e64c" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">解码器模块中的自我关注层</h2><p id="32e8" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">正如我们之前讨论的，编码器和解码器的架构没有区别。自我注意在解码器中的运作方式有所不同。解码器使用<strong class="ih hj">屏蔽自我关注</strong> <strong class="ih hj">机制</strong>屏蔽输入的未来位置，例如:如果我们取同一句话“欢迎来到纽约”当解码单词“to”时，解码器将根据配置只访问左边的单词:“Welcome”或右边的单词:“NYC”。它不能同时访问两个位置。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es le"><img src="../Images/02ec0c339527e4f4bc5ca6e8846333e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*64pVTLA5bvnR7rhfP0Rvtg.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">这里，解码器在解码“to”时只能访问单词“welcome”(礼貌:<a class="ae jd" href="https://huggingface.co/course/chapter1/6?fw=pt" rel="noopener ugc nofollow" target="_blank"> huggingface </a>)</figcaption></figure><p id="a8cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解码器的输出是特征向量，将这些特征向量转换成单词是由最终的线性层和随后的 softmax 层完成的。线性层(完全连接的神经网络)将特征向量转换成更大的 logits 向量(借助于训练数据的词汇),然后传递到 softmax 层，soft max 层转换成概率以识别最终的输出单词。</p><h2 id="2859" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">解码器型号</h2><p id="3a18" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">解码器模型仅使用变压器架构的解码器模块。解码器模型通常被称为自回归模型，因为它在预测下一个字时使用解码器的先前预测(输出)作为输入本身。解码器模型的预训练通常围绕预测句子中的下一个单词(屏蔽单词)。</p><blockquote class="lf lg lh"><p id="96f6" class="if ig kn ih b ii ij ik il im in io ip li ir is it lj iv iw ix lk iz ja jb jc hb bi translated">解码器模型广泛用于文本生成、自然语言生成(NLG)</p></blockquote><p id="66d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一些解码器系列型号包括</p><ul class=""><li id="1ca4" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/ctrl.html" rel="noopener ugc nofollow" target="_blank"> CTRL </a></li><li id="fe51" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/gpt.html" rel="noopener ugc nofollow" target="_blank"> GPT </a></li><li id="481a" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/gpt2.html" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a></li><li id="897a" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/transformerxl.html" rel="noopener ugc nofollow" target="_blank">变压器 XL </a></li></ul><h2 id="56dd" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">编码器-解码器模型</h2><p id="bcda" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">编码器-解码器模型是转换器架构的一部分，也称为<strong class="ih hj">序列-序列模型</strong>。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lz"><img src="../Images/9a85258a3468d98bc155041ce218a15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*E8yAMi49lRMkKnLRG8svmg.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">编码器-解码器模型架构(礼貌:<a class="ae jd" href="https://huggingface.co/course/chapter1/7?fw=pt" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>)</figcaption></figure><p id="615d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ke translated"><span class="l kf kg kh bm ki kj kk kl km di"> T </span>编码器模块生成输入句子的特征向量作为输出，并传递给解码器。在产生输出之前，解码器需要考虑两个输入，一个是编码器的输出，另一个是输入序列或输入序列的开始。如上图所示，在第一个输出(WORD_1)之后，解码器将使用该输出作为下一个输出的输入(上面讨论的自回归方法)。编码器和解码器块通常不互相共享权重。<em class="kn">例如，在翻译任务的情况下，编码器将理解一种语言的完整句子并生成特征向量，解码器负责基于编码器的理解生成目标语言的序列。</em></p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ma"><img src="../Images/2bb067a43b9b2f99f125c662fbe1fdbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*rfIzok8J8C_y8J7iKy1uhA.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">使用序列对序列模型的总结示例:此处输入上下文和输出上下文不同，因此编码器和解码器的权重也不同，此处不需要权重共享(图片来自<a class="ae jd" href="https://huggingface.co/course/chapter1/7?fw=pt" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>)</figcaption></figure><blockquote class="lf lg lh"><p id="b14d" class="if ig kn ih b ii ij ik il im in io ip li ir is it lj iv iw ix lk iz ja jb jc hb bi translated">序列到序列模型最适合于根据给定的输入生成新句子的任务，例如摘要、翻译或生成性问题回答。</p></blockquote><p id="f891" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">属于序列到序列系列的型号有:</p><ul class=""><li id="879f" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/bart.html" rel="noopener ugc nofollow" target="_blank">巴特</a></li><li id="f5e8" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/mbart.html" rel="noopener ugc nofollow" target="_blank"> mBART </a></li><li id="3616" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/marian.html" rel="noopener ugc nofollow" target="_blank">玛丽安</a></li><li id="f279" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/t5.html" rel="noopener ugc nofollow" target="_blank"> T5 </a></li></ul><p id="92ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢你阅读这篇文章，我希望读者能从这个故事中对变形金刚有所了解。</p><p id="2b28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kn">参考文献:</em></p><ol class=""><li id="5357" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc mb lr ls lt bi translated"><a class="ae jd" href="https://huggingface.co/course/chapter1?fw=pt" rel="noopener ugc nofollow" target="_blank"> <em class="kn">变形金刚模型</em></a><em class="kn">—hugging face 教程</em></li><li id="1e5e" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc mb lr ls lt bi translated"><a class="ae jd" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <em class="kn">图解变压器</em> </a> <em class="kn"> —杰·阿拉玛</em></li></ol></div></div>    
</body>
</html>