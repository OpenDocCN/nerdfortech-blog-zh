<html>
<head>
<title>AI Dance based on Human Pose Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于人体姿态估计的人工智能舞蹈</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/ai-dance-based-on-human-pose-estimation-738ac2ff6d1f?source=collection_archive---------1-----------------------#2020-11-21">https://medium.com/nerd-for-tech/ai-dance-based-on-human-pose-estimation-738ac2ff6d1f?source=collection_archive---------1-----------------------#2020-11-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="5a53" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">人体姿态骨架以图形格式表示人的方向。本质上，它是一组可以连接起来描述人的姿态的坐标。骨架中的每个坐标称为一个部分(或一个关节，或一个关键点)。两个部分之间的有效连接称为一对(或一个分支)。下面显示了一个人体姿势骨架示例。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es jh"><img src="../Images/554a5452db43c427f874ff4f342d8f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/0*v630pX96woI2qrhh.gif"/></div></figure><p id="2162" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated">因此，在本文中，我们将研究如何使用深度神经网络模型在OpenCV中执行人体姿态估计。</p><h1 id="6212" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">目录</h1><ol class=""><li id="a936" class="kq kr hi il b im ks iq kt jp ku jq kv jr kw jg kx ky kz la bi translated">数据集</li><li id="f858" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated">模型架构</li><li id="b00c" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated">实验和结果</li></ol><h2 id="8d16" class="lg jt hi bd ju lh li lj jy lk ll lm kc jp ln lo kg jq lp lq kk jr lr ls ko lt bi translated">数据集</h2><p id="7a16" class="pw-post-body-paragraph ii ij hi il b im ks io ip iq kt is it jp lu iw ix jq lv ja jb jr lw je jf jg hb bi translated">迄今为止，由于缺乏高质量的数据集，人体姿态估计仍是一个具有挑战性的问题。现在，每一个人工智能挑战都需要一个好的数据集。在过去几年中，具有挑战性的数据集已经发布，这使得研究人员更容易有效地解决问题。</p><p id="64a4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated">一些数据集包括:</p><ol class=""><li id="063b" class="kq kr hi il b im in iq ir jp lx jq ly jr lz jg kx ky kz la bi translated"><a class="ae ma" href="http://cocodataset.org/#keypoints-2018" rel="noopener ugc nofollow" target="_blank">可可关键点挑战</a></li><li id="2e06" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated"><a class="ae ma" href="http://human-pose.mpi-inf.mpg.de/" rel="noopener ugc nofollow" target="_blank"> MPII人体姿态数据集</a></li><li id="f7ca" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated"><a class="ae ma" href="http://www.robots.ox.ac.uk/~vgg/data/pose_evaluation/" rel="noopener ugc nofollow" target="_blank"> VGG姿态数据集</a></li><li id="e301" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated"><a class="ae ma" href="https://www.di.ens.fr/willow/research/surreal/" rel="noopener ugc nofollow" target="_blank">超现实</a>(执行真实任务的合成人)</li><li id="2b4f" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated"><a class="ae ma" href="http://files.is.tuebingen.mpg.de/classner/up/" rel="noopener ugc nofollow" target="_blank"> UP-3D </a></li></ol><p id="c8d8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated">对于本文，我们使用COCO数据集进行人体姿态估计。</p><h2 id="cadc" class="lg jt hi bd ju lh li lj jy lk ll lm kc jp ln lo kg jq lp lq kk jr lr ls ko lt bi translated">模型架构</h2><p id="e5e9" class="pw-post-body-paragraph ii ij hi il b im ks io ip iq kt is it jp lu iw ix jq lv ja jb jr lw je jf jg hb bi translated">OpenPose首先检测属于图像中每个人的部分(关键点)，然后将部分分配给不同的个人。下面显示的是OpenPose模型的架构。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mb"><img src="../Images/9f0c977484ec3ee5a023d207885be80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/0*Vg1_dnqkChTmvIca.png"/></div><figcaption class="mc md et er es me mf bd b be z dx translated">OpenPose架构的流程图。</figcaption></figure><p id="7461" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated">该模型将尺寸为w × h的彩色图像作为输入，并产生图像中每个人的关键点的2D位置作为输出。检测分三个阶段进行:</p><blockquote class="if ig ih"><p id="df8d" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><strong class="il hj">阶段0:</strong>VGGNet的前10层用于创建输入图像的特征图。</p><p id="79b3" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><strong class="il hj">阶段1: </strong>使用2分支多阶段CNN，其中第一分支预测一组身体部位(例如，肘、膝等)的2D置信图。).下面给出了关键点的置信度图和相似度图。第二个分支预测一组零件亲缘关系的2D矢量场(L ),其编码零件之间的关联程度。</p><p id="d438" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><strong class="il hj">阶段2: </strong>通过贪婪推理来解析置信度和相似度图，以产生图像中所有人的2D关键点。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mg"><img src="../Images/b813b28afddbda25787e3925b1718d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-xFdsnjkmviKSMaR.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">使用OpenPose进行人体姿态估计的步骤。(<a class="ae ma" href="https://arxiv.org/pdf/1812.08008.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h2 id="4518" class="lg jt hi bd ju lh li lj jy lk ll lm kc jp ln lo kg jq lp lq kk jr lr ls ko lt bi translated">实验和结果</h2><p id="fe8b" class="pw-post-body-paragraph ii ij hi il b im ks io ip iq kt is it jp lu iw ix jq lv ja jb jr lw je jf jg hb bi translated">在这一节中，为了简单起见，我们将在单个人上加载用于理解人体姿势估计的训练模型。以下是步骤:</p><p id="ff4e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated"><em class="ik">从</em> <a class="ae ma" href="https://drive.google.com/file/d/1WYWwZR_mtUSfRCR-Rwi0mDGNlL_Uvbei/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <em class="ik">这里</em> </a> <em class="ik">下载</em> <strong class="il hj"> <em class="ik">模型权重</em> </strong> <em class="ik">。</em></p><p id="3f7a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated"><strong class="il hj"> <em class="ik">加载网络</em> </strong> <em class="ik"> : </em>我们使用的是在Caffe深度学习框架上训练的模型。Caffe型号有两个文件——</p><ol class=""><li id="68e4" class="kq kr hi il b im in iq ir jp lx jq ly jr lz jg kx ky kz la bi translated">。指定神经网络架构的prototxt文件。</li><li id="f507" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated">。存储训练模型权重的caffemodel文件</li></ol><p id="a9a2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated"><strong class="il hj"> <em class="ik">读取图像并准备对网络的输入</em> </strong>:我们使用OpenCV读取的输入帧应该被转换成一个输入blob(像Caffe)以便它能被馈送到网络。这是使用blobFromImage函数完成的，该函数将图像从OpenCV格式转换为Caffe blob格式。首先，我们将像素值归一化为(0，1)。然后我们指定图像的尺寸。接下来，要减去的平均值，即(0，0，0)。</p><p id="1c5d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated"><strong class="il hj"> <em class="ik">进行预测并解析关键点</em> </strong> <em class="ik"> : </em>一旦将图像传递给模型，就可以进行预测了。输出是4D矩阵:</p><ol class=""><li id="2097" class="kq kr hi il b im in iq ir jp lx jq ly jr lz jg kx ky kz la bi translated">第一维是图像ID(如果您向网络传递多个图像)。</li><li id="14b2" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated">第二维表示关键点的索引。该模型产生置信度图和零件相似性图，它们都被连接起来。对于COCO模型，它由57个部分组成——18个关键点置信度图+ 1个背景+ 19*2个部分相似性图。</li><li id="b2c7" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated">第三个维度是输出地图的高度。</li><li id="abb6" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated">第四维是输出地图的宽度。</li></ol><p id="17a2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated"><strong class="il hj"> <em class="ik">画骨架</em> </strong> <em class="ik"> : </em>我们只要把这些对连起来，有了关键点就可以画出骨架了。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="16fa" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated">上述代码的输出是:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mn"><img src="../Images/832bd150a57c1e1f1f6054e81d337cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pr77sYCo2xTeNN4GA0osXw.png"/></div></div></figure><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mo mm l"/></div></figure><p id="4307" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jp iv iw ix jq iz ja jb jr jd je jf jg hb bi translated">代码可从github . com/Devashi-choudh ary/AI-Dance-based-on-Human-Pose-Estimation获得。如有任何问题或疑问，请直接联系我，电话:<a class="ae ma" href="https://github.com/Devashi-Choudhary" rel="noopener ugc nofollow" target="_blank"><strong class="il hj">github.com/Devashi-Choudhary</strong>。</a></p><h1 id="51f2" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">参考</h1><p id="c38d" class="pw-post-body-paragraph ii ij hi il b im ks io ip iq kt is it jp lu iw ix jq lv ja jb jr lw je jf jg hb bi translated">提供参考总是好的</p><ol class=""><li id="ade2" class="kq kr hi il b im in iq ir jp lx jq ly jr lz jg kx ky kz la bi translated"><a class="ae ma" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">打开姿势</a></li><li id="4e49" class="kq kr hi il b im lb iq lc jp ld jq le jr lf jg kx ky kz la bi translated"><a class="ae ma" href="https://github.com/keshavoct98/DANCING-AI" rel="noopener ugc nofollow" target="_blank">跳舞——艾</a></li></ol></div></div>    
</body>
</html>