<html>
<head>
<title>Tackling Exploration-Exploitation Dilemma in K-armed Bandits</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">破解《K武装匪徒》中的探索与开发困境</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/tackling-exploration-exploitation-dilemma-in-k-armed-bandits-598c0329cf88?source=collection_archive---------25-----------------------#2021-06-11">https://medium.com/nerd-for-tech/tackling-exploration-exploitation-dilemma-in-k-armed-bandits-598c0329cf88?source=collection_archive---------25-----------------------#2021-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="187b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">简要介绍Epsilon Greedies和Thompson采样这两种方法，以及在python中的实现</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/0b5cdb392d437eeb2c0fa4108a6f796f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5dltx4BcI8rRmCK2Sq_kw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions . html</a></figcaption></figure><p id="1f92" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在现实世界中，我们遇到了许多情况，我们面临着众所周知的<strong class="jq hj">勘探和开发</strong>的权衡。但是这些术语是什么意思呢？</p><blockquote class="kk kl km"><p id="ef65" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">开发是从我们已知的事物中获取利益的过程，而探索是获取我们未知的知识。</p></blockquote><p id="c995" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我将试着用一个例子来解释这些术语，假设你喜欢数学，并且总是优先考虑它的主题。现在在这种情况下，你总是喜欢(利用)数学，因为它在过去为你提供了巨大的回报。但是只选择数学不允许你探索其他学科。所以只是为了实验，你随机选了化学(探索)，发现了一些有趣的话题。因此，你的选择让你知道更多关于其他的事情。但代价是你不能同时学习数学和化学。这就是我们正在讨论的探索-开发的困境。</p><p id="427d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了看到这一点，我们考虑强化学习中最简单的设置，即多臂强盗。这就像一个吃角子老虎机，每次你拉手臂，它都会产生一个奖励，描述特定的行动有多有利，并移动到一个终端状态。</p><p id="0535" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在进入实际的方法之前，让我简要地讨论一些在RL和土匪的文献中广泛使用的术语。</p><h1 id="e367" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">定义</h1><ol class=""><li id="c366" class="lj lk hi jq b jr ll ju lm jx ln kb lo kf lp kj lq lr ls lt bi translated"><strong class="jq hj">奖励- </strong>它可以定义为利益或舒适。</li><li id="3456" class="lj lk hi jq b jr lu ju lv jx lw kb lx kf ly kj lq lr ls lt bi translated"><strong class="jq hj">动作</strong> -在土匪问题的情况下，拉手臂被定义为动作。</li><li id="4947" class="lj lk hi jq b jr lu ju lv jx lw kb lx kf ly kj lq lr ls lt bi translated"><strong class="jq hj"> Action-Values </strong> -它定义了代理在状态<strong class="jq hj"> s </strong>中采取的动作<strong class="jq hj"> a </strong>的质量。但是对于强盗来说，由于每一个动作都导致一个终结状态，所以它可以被认为是一个单一状态问题，并且只依赖于动作。数学上，</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/8929d62d2d987a142ee2ca80d128def8.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*buPdLJlS39nqgqlqZXLynw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">多臂土匪问题的q值</figcaption></figure><p id="cd7a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果我们知道每个行动的最优q值，这是因祸得福。但实际上，我们对此并不知情，而是根据我们通过连续拉臂收集的经验来估计这些值。一种这样的方法是<strong class="jq hj">样本平均</strong>定义为:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/b43fb211de69662f613039e16827efbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*IFhqass3wa0vKU666Nqhzw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">样本平均估计</figcaption></figure><p id="20e1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于足够大的样本，它接近真正的Q*(通过中心极限定理)。尽管如此，我们仍然需要平衡探索和开发，以保证每个动作收敛到其最佳值。我们将在下一节讨论两种方法。</p><h1 id="9ebb" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">为什么是这两种方法，而不是其他方法？</h1><p id="2132" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx mb jz ka kb mc kd ke kf md kh ki kj hb bi translated">在进一步讨论之前，可能会出现一个问题，即有不同的其他方法来解决这一困境，如Greedies、置信上限、乐观初始值，那么为什么选择这两种。答案是，我试图提供一个下限和上限，因为ε通常是处理这些权衡的起点，而Thompson采样最近被称为这方面的最新技术之一。</p><h1 id="301a" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">环境设置</h1><p id="416e" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx mb jz ka kb mc kd ke kf md kh ki kj hb bi translated">为了比较这种解决方案，我模拟了与萨顿和巴尔托书中所述相同的10臂测试床，即通过从标准正态分布中采样Q*,然后用平均值Q*和方差1奖励高斯采样。真实的奖励分配如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/004493137b18d74af12ebfe831ad72e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ATr4tIfFuLNmLf0sUh4i6w.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">真实报酬分配(最优行动=2)</figcaption></figure><h1 id="d347" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">Greedies类</h1><p id="ff51" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx mb jz ka kb mc kd ke kf md kh ki kj hb bi translated">Epsilon Greedies是一类算法，仅改变参数Epsilon来解释探索。如果ε= 0意味着纯粹的贪婪，任何介于0和1之间的ε都意味着一些探索。这些算法的主要思想是，不是每次都采取贪婪的行动，而是以概率ε从其他行动中均匀地随机选择一个行动。</p><h1 id="b8cc" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">汤普森取样</h1><p id="1398" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx mb jz ka kb mc kd ke kf md kh ki kj hb bi translated">Thompson抽样基于贝叶斯推理的概念，在这种推理中，我们用我们观察到的数据(称为<strong class="jq hj">后验</strong>概率)来更新我们对参数的信念(称为<strong class="jq hj">先验</strong>)。要了解更多关于这些术语的知识，请查看下面的博文。</p><div class="mf mg ez fb mh mi"><a href="https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9" rel="noopener follow" target="_blank"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hj fi z dy mn ea eb mo ed ef hh bi translated">最大似然估计、映射和贝叶斯推理</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">通过关注与最大似然法和映射的区别，掌握贝叶斯推理的思想</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">towardsdatascience.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw jh mi"/></div></div></a></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mx"><img src="../Images/478cebfe700a3a33ed9d4e91c04be534.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*hsXHdKUN25C9W8YJHF8VDQ.jpeg"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">连接先验和后验的贝叶斯规则</figcaption></figure><p id="cbfb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，在知道后验分布之后，汤普森采样所做的就是从这些后验分布中对动作进行采样。因此，如果一个行为被探索的越少，它被选择的机会就越多。在足够的时间步长之后，即，当每个动作被采样足够多次时，它将分析最终分布以获得最佳动作，并在之后开始利用它。在这样做的时候，它利用了一种完美的方法<a class="ae jn" href="https://en.wikipedia.org/wiki/Conjugate_prior" rel="noopener ugc nofollow" target="_blank">共轭先验</a>来降低复杂度。我在这里关注核心实现，因此为了了解详细的理论，请访问<a class="ae jn" href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf" rel="noopener ugc nofollow" target="_blank"> TS </a>上的参考资料</p><h1 id="61d1" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">算法和实现</h1><p id="d6ea" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx mb jz ka kb mc kd ke kf md kh ki kj hb bi translated">为了比较Eps-Greedy和TS，我修改了常规epsilon greedy的算法，以创建相同的条件集，这样整个焦点就集中在实际的核心部分，即从模型中采样(在TS中)到从模型中进行估计(Eps-Greedy中的模式/平均值)之间的差异。我还在latex中自己制作了算法，用于假设(方差= 1)的更真实的高斯采样。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/30481647d3be73dc16de1fcc06aabd0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*v3nJUY_omE5w0OT-y18X3Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">ε贪婪算法</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/10b57644960bc2e159b8d005fdb591a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*5x-9aK4cLHeJYtt9P7J2Ew.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">汤普森取样</figcaption></figure><p id="5c30" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后两行显示了利用共轭先验属性的参数更新。这些算法将从代码中清晰可见。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="na nb l"/></div></figure><p id="65a5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在上面的代码中，我创建了一个类bandit来模拟单个Bandit的属性，并创建了一个类来跟踪随时间收集的日志。现在让我们来看两种算法的实现。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="na nb l"/></div></figure><p id="6624" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在上面的代码中，我们使用了前面讨论过的相同的方法。除了algos中描述的_choose_bandit函数之外，这两种情况下的所有事情都是相同的。</p><h1 id="7b68" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">结果</h1><p id="182f" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx mb jz ka kb mc kd ke kf md kh ki kj hb bi translated">我们在2500次独立模拟运行中绘制了每种方法1500个时间步长的Excepted_Rewards，并取其平均值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/ed03dee28133a8a8934f351a916c806e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1LGxzKwbOOazlQrzpieDpQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">超过2500次独立运行的平均预期回报</figcaption></figure><p id="1ecf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从上图可以推断出，在所有的eps-Greedies中，eps= 0.01可以被认为是最好的，因为它在增加，而不是平稳，但即使是这样，它对Thompson采样来说也是非常低的。现在，为了进一步研究，我们绘制了eps = 0.01和TS的所有独立运行的最优操作的平均百分比，以查看TS的优越性。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/21bc7e6e406ad19128d0dfb566788d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bsOS9FHhABZcI8MWN6TWGw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">最佳行动百分比的比较</figcaption></figure><p id="0576" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">不错！！从上面的图中，我们可以得出结论，即使两种方法都支持行动-2，但eps = 0.01，即使在后期也很难达到80%，但TS即使在早期也能达到100 %的最佳行动。这表明TS迅速探索了所有的行动，然后利用了最佳的一个。</p><h1 id="f1cb" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">结论</h1><p id="3357" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx mb jz ka kb mc kd ke kf md kh ki kj hb bi translated">通读这篇文章后，人们心中会有两个问题。首先，从高斯采样的真实回报和作为高斯的先验是好的吗？其次，考虑的方差是固定的。答案是，我使用了高斯TS的最小实现，方差是固定的，但这很容易做到，不需要知道任何关于方差的假设。假设高斯为先验并没有害处，因为我们知道大多数自然现象遵循它，并且对于大样本，每个分布都接近高斯(中心极限定理)，因此不缺乏一般性，并且我们可以看到TS优于Greedies。<strong class="jq hj">如果你喜欢我的文章，请随意投票支持这篇文章，你可以在这里找到我的回购协议的完整代码-</strong><a class="ae jn" href="https://github.com/Amshra267/Thompson-Greedy-Comparison-for-MultiArmed-Bandits" rel="noopener ugc nofollow" target="_blank">https://github . com/am shra 267/Thompson-Greedy-Comparison-for-multi armed-Bandits</a></p><h1 id="af22" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">参考</h1><ol class=""><li id="3635" class="lj lk hi jq b jr ll ju lm jx ln kb lo kf lp kj lq lr ls lt bi translated">萨顿和巴尔托:强化学习</li><li id="c1bc" class="lj lk hi jq b jr lu ju lv jx lw kb lx kf ly kj lq lr ls lt bi translated"><a class="ae jn" href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf" rel="noopener ugc nofollow" target="_blank">汤普森取样</a></li></ol></div></div>    
</body>
</html>