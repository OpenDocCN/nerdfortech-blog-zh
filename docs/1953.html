<html>
<head>
<title>Live Speech Emotion Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">现场语音情感识别</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/live-speech-emotion-categorization-7692933037ba?source=collection_archive---------4-----------------------#2021-04-14">https://medium.com/nerd-for-tech/live-speech-emotion-categorization-7692933037ba?source=collection_archive---------4-----------------------#2021-04-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/828bf8bfc7e99f6813e72f75db5bd0eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpbbXPumvxAqzNOBKO6dpg.jpeg"/></div></div></figure><p id="8549" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你在对话中听到以下句子，你可能会知道它们属于哪种情绪类别——平静、快乐、悲伤、愤怒、恐惧、惊讶和厌恶。</p><blockquote class="jo jp jq"><p id="cb99" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">“哎！生日快乐！”</p><p id="2934" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">“我刚刚失业了。我不知道该怎么办。”</p><p id="a01f" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">“哇！我没想到会有惊喜派对。”</p></blockquote><p id="27fa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算机能以同样的方式对声音进行分类吗？我和<a class="ae jv" href="https://www.linkedin.com/in/devinder-sarai/" rel="noopener ugc nofollow" target="_blank">德文德·萨莱</a>都想知道，仅在美国就有超过<strong class="is hj"> 32 00万</strong>的人因<strong class="is hj">自闭症</strong>或一种叫做<strong class="is hj">述情障碍</strong>的状况而难以识别他人的感受。</p><p id="d560" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有这些情况的人知道如何对不同的情绪做出反应；然而，他们不能解读视觉肢体语言信号来理解与他们互动的人的情绪。</p><p id="ffb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们开始建立一个模型，可以对情绪进行分类，如平静、快乐、悲伤、愤怒、恐惧、惊讶和厌恶。Devinder负责该项目的实时面部表情识别部分——你可以在这里阅读他的文章<a class="ae jv" href="https://devindersarai.medium.com/live-facial-expression-recognition-c417f388aa2f" rel="noopener"/>——我负责实时音频情感分类部分。</p><p id="787f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我开始研究如何为这个项目执行现场音频情感分类，并发现文本情感分析通常在第一天学习任何介绍性的NLP课程时教授。然而，<strong class="is hj">通过音频进行情感分类</strong>并不常见，尤其是实时音频情感分类。</p><p id="9b4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不得不自己想出如何执行现场音频情感分类，因为我是一个机器学习初学者，在模型工作之前花了几个漫长的夜晚。我想我会分享我是如何让实时音频情感分析发挥作用的，以及我下一步要做的是让这个模型更加准确。</p><h1 id="6713" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">使用的库和数据集</h1><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/9ee10d34aec537faf5a52cc8a4455ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyT2xZPFl98ymTOZjLQE9A.jpeg"/></div></div></figure><h2 id="1754" class="kz jx hi bd jy la lb lc kc ld le lf kg jb lg lh kk jf li lj ko jj lk ll ks lm bi translated"><strong class="ak">模型库</strong></h2><ul class=""><li id="34e3" class="ln lo hi is b it lp ix lq jb lr jf ls jj lt jn lu lv lw lx bi translated"><a class="ae jv" href="https://librosa.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Librosa </strong> </a>:用于音乐和音频分析的Python包。</li><li id="a01a" class="ln lo hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated"><a class="ae jv" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Matplotlib </strong> </a>:这允许我们绘制<strong class="is hj">光谱图。</strong></li><li id="9194" class="ln lo hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated"><a class="ae jv" href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">scipy . io . WAV file</strong></a>:从WAV文件中返回采样率(以采样/秒为单位)和数据。</li><li id="be61" class="ln lo hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated"><a class="ae jv" href="https://docs.python.org/3/library/glob.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Glob </strong> </a>:用于返回所有符合特定模式的文件路径。</li><li id="e71e" class="ln lo hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated"><a class="ae jv" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Numpy </strong> </a>:用于处理数组。</li><li id="d918" class="ln lo hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated"><a class="ae jv" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Fastai </strong> </a> : Fastai是一个深度学习库，为从业者提供高级组件，可以快速轻松地<strong class="is hj">在标准深度学习领域提供</strong>最先进的结果。我们将使用fastai.vision。</li></ul><h2 id="e31c" class="kz jx hi bd jy la lb lc kc ld le lf kg jb lg lh kk jf li lj ko jj lk ll ks lm bi translated"><strong class="ak">用于使其实时工作的库</strong></h2><ul class=""><li id="47ee" class="ln lo hi is b it lp ix lq jb lr jf ls jj lt jn lu lv lw lx bi translated"><a class="ae jv" href="https://docs.python.org/3/library/struct.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Struct </strong> </a>:用于将音频数据解包成整数</li><li id="3f4f" class="ln lo hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated"><a class="ae jv" href="https://docs.python.org/3/library/tkinter.html" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">Tkinter</strong></a>:Tkinter是Python的标准GUI库。Python与Tkinter相结合，提供了一种快速而简单的方法来创建GUI应用程序。</li><li id="9cdf" class="ln lo hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated"><a class="ae jv" href="https://python-sounddevice.readthedocs.io/en/0.4.1/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Sounddevice </strong> </a>:这个<a class="ae jv" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>模块提供了对<a class="ae jv" href="http://www.portaudio.com/" rel="noopener ugc nofollow" target="_blank"> PortAudio </a>库的绑定和一些方便的函数来播放和录制包含音频信号的<a class="ae jv" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> NumPy </a>数组。</li></ul><h2 id="4bc5" class="kz jx hi bd jy la lb lc kc ld le lf kg jb lg lh kk jf li lj ko jj lk ll ks lm bi translated"><strong class="ak">数据集— </strong> RAVDESS <strong class="ak">数据集</strong></h2><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/31cebbc37feeaf095ad6c6817fed9bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A5w5qBvISboAEmoe"/></div></div></figure><p id="0686" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用<a class="ae jv" href="https://zenodo.org/record/1188976#.YGtIDEhKgY0" rel="noopener ugc nofollow" target="_blank">瑞尔森情感语音和歌曲视听数据库(RAVDESS </a>)数据集来训练该模型。该数据库包含24名职业演员——12名女性和12名男性——用中性北美口音发出两个词汇匹配的语句。</p><p id="f36a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该数据集包括标记为平静、快乐、悲伤、愤怒、恐惧、惊讶和厌恶的语音。每个表情都带有正常的、强烈的情绪强度，外加一个中性的表情。</p><h1 id="c11b" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">训练模型如何工作</h1><p id="b962" class="pw-post-body-paragraph iq ir hi is b it lp iv iw ix lq iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">链接到<a class="ae jv" href="https://colab.research.google.com/drive/17JebI-YhfJHjBaLN9Y_aJss8SK0o_eyD?usp=sharing" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><h2 id="dd97" class="kz jx hi bd jy la lb lc kc ld le lf kg jb lg lh kk jf li lj ko jj lk ll ks lm bi translated"><strong class="ak">处理数据:</strong></h2><p id="84df" class="pw-post-body-paragraph iq ir hi is b it lp iv iw ix lq iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">我们要做的第一件事是使用<a class="ae jv" href="https://librosa.org/" rel="noopener ugc nofollow" target="_blank"> librosa </a>库将声音剪辑转换成<strong class="is hj">图形声谱图</strong>。这样，它们可以用来训练卷积神经网络(CNN)。</p><p id="b8ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们用<a class="ae jv" href="https://en.wikipedia.org/wiki/Mel_scale" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">梅尔标度</strong> </a>将声音数据转换成声谱图。这种比例是为了使音频的图像表现更容易理解，以便让CNN在数据中找到模式。</p><figure class="kv kw kx ky fd ij"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="db67" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然我们正在分析音频，但我们将使用<strong class="is hj">计算机视觉</strong>对情绪进行分类。我们将这样做，因为模型将被训练来从音频的声谱图表示中识别和解释与情感类别相关的模式。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/2d9b7ff326eca49f0db5fba4be543068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/0*lLJn6btbEjtOCy51.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">MEL标度的光谱图</figcaption></figure><p id="df61" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">光谱图将被放入你电脑上的目录系统，分类在一个子目录下，该子目录的名称为情绪— <em class="jr">愤怒、厌恶、恐惧、快乐、惊讶、悲伤和中性。</em></p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/3e7a87c839a786ebb5da7493b30a1802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*0e1S3an9l5BHE2rvjhZDzQ.gif"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">如何构建目录</figcaption></figure><p id="fcdf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，您必须手动创建目录和子目录。然后代码将把相应的光谱图放在相应的目录中。</p><h2 id="b9ef" class="kz jx hi bd jy la lb lc kc ld le lf kg jb lg lh kk jf li lj ko jj lk ll ks lm bi translated"><strong class="ak">训练卷积神经网络:</strong></h2><p id="48b5" class="pw-post-body-paragraph iq ir hi is b it lp iv iw ix lq iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">现在我们可以开始训练CNN通过观察声音片段产生的频谱图来识别情绪。我们将使用<strong class="is hj"> fastai库</strong>来训练CNN通过查看声音片段生成的频谱图来识别情绪。</p><figure class="kv kw kx ky fd ij"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="f3cc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">预训练的CNN (resnet34)将在我们刚刚生成并标记的声谱图图像上进行训练。</p><h2 id="3913" class="kz jx hi bd jy la lb lc kc ld le lf kg jb lg lh kk jf li lj ko jj lk ll ks lm bi translated"><strong class="ak">让模型在现场音频上工作</strong></h2><p id="2a54" class="pw-post-body-paragraph iq ir hi is b it lp iv iw ix lq iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">首先以<a class="ae jv" href="https://docs.python.org/3/library/pickle.html" rel="noopener ugc nofollow" target="_blank"> pickle文件</a>的形式加载模型。然后，我们将在变量<em class="jr">秒</em>的时间范围内录制音频，在本例中设置为3秒。然后，音频将作为wav文件导出到您计算机上的一个目录中。</p><figure class="kv kw kx ky fd ij"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="5be3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">完成后，我们将使用wav文件，并将其转换成光谱图图像，然后输出到您计算机上的另一个目录中。这一步是可选的，但是如果您想稍后分析光谱图中的差异，我建议您这样做。最后，使用<em class="jr"> model.predict(img) </em>预测图像的情感。</p><p id="eb2d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">恭喜你！如果你按照本教程，你现在可以执行现场音频情感分类！如果你的最终目标是帮助美国32，000，000人，他们在用ML识别他人的感受方面有困难，那么你就离你的目标更近了一步！</p><p id="9495" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，如果这不是你的最终目标，有一个现场音频情感分类的广泛的其他应用。下面我只谈了其中的几个。</p><h1 id="34ac" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">音频情感检测还能用来做什么？</h1><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/0687c0e29530f4517555887c68379403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9rWRz_goxNL41-eIqnT27A.jpeg"/></div></div></figure><p id="533b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">早在2008年，在亚马逊的Alexa AI部门，一个团队就用机器学习来检测快乐、悲伤和愤怒等情绪。他们这样做是为了帮助保护患有PTSD的退伍军人，通过人工智能从他们的声音中理解退伍军人的<strong class="is hj">心理健康</strong>。</p><p id="fa56" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">医院里的情感识别</strong></p><p id="2167" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">医疗保健行业正在利用这项技术。人工智能识别软件可以帮助医疗保健工作者决定患者何时需要药物，或者帮助医生决定先帮助哪个患者。</p><p id="7559" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">营销</strong></p><p id="914d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">情绪识别已经被不同的公司广泛用于评估消费者对他们产品或品牌的情绪。有几家语音情感分析公司可以处理包含人类语音的音频文件，并解释音频中描绘的人类情感。然后这些公司根据这些信息提供分析报告或者其他服务。这里只是这类公司的几个例子:<a class="ae jv" href="https://www.audeering.com" rel="noopener ugc nofollow" target="_blank">audering</a>、<a class="ae jv" href="http://www.beyondverbal.com" rel="noopener ugc nofollow" target="_blank"> Beyond Verbal </a>和<a class="ae jv" href="https://www.affectiva.com" rel="noopener ugc nofollow" target="_blank"> Affectiva </a>。</p><h1 id="23a0" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">后续步骤</h1><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/020f7f4420da6d89e2cf23f7da26c679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kz77FG4Q5qsjOUMvAS7IeA.jpeg"/></div></div></figure><p id="f6c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从事这个项目非常有趣，并且教会了我很多关于自然语言处理的知识！我很高兴在这个项目上迭代，因为这个模型的训练方式有几个错误。</p><p id="83b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，训练模型的数据集在<strong class="is hj">范围内受到</strong>限制。它只包含几个相同的句子，由不同的演员一遍又一遍地重复。</p><p id="9733" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这让我不得不说<em class="jr">“孩子们在门边说话”</em>凌晨2点，模型终于起作用了。我可以向你保证，我的演技受到了考验。</p><p id="5503" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，找到一个具有广泛种族和短语范围的数据集将是提高该模型准确性的关键。此外，我将在<em class="jr">的情感分析中添加</em>正在说什么，而不是<em class="jr">如何说</em>，以进一步提高其准确性。</p><p id="456d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对情感进行分类的下一步将是实现<a class="ae jv" href="https://www.aclweb.org/anthology/W18-5429.pdf" rel="noopener ugc nofollow" target="_blank">自我关注网络(SANet) </a>，这是一种灵活且可解释的文本分类架构。</p><p id="7f43" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，为了提高情感分类中的模型准确性，我将把这个实时音频情感检测模型与Devinder的实时面部情感检测模型合并。</p><p id="984c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您对如何改进该模型有任何想法，请随时联系: )</p></div></div>    
</body>
</html>