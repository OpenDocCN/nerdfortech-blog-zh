<html>
<head>
<title>Building a Personal AI Assistant: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建个人人工智能助手:第1部分</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/building-a-personal-ai-assistant-part-1-b73974f80c1?source=collection_archive---------1-----------------------#2021-08-31">https://medium.com/nerd-for-tech/building-a-personal-ai-assistant-part-1-b73974f80c1?source=collection_archive---------1-----------------------#2021-08-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="e5ed" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">利用意图分类和实体抽取理解自然语言</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/707d20bedcb65e1ea16abcf36c09bf70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oe6tojQwtgmcwhTh5A6WLg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">托马斯·科尔诺斯基在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片，经过编辑</figcaption></figure><p id="84bb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Siri是第一项让我真正“惊叹”的技术。能够自然地与计算机互动真的令人印象深刻。我想这种魅力在过去的十年里一直伴随着我。自然语言技术一直给我留下深刻的印象，不管它有效与否。这就是为什么我决定在过去的一个月里建立我自己的虚拟助手，名为<a class="ae jn" href="https://github.com/Robert-MacWha/Project-Aurras" rel="noopener ugc nofollow" target="_blank">奥拉计划</a>。</p><p id="47d1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在接下来的三篇文章中，我将记录Aurras的核心组件是如何工作的。本文将关注数据准备。接下来的两个将分别关注意图分类和实体提取。这些系统放在一起，可以为大多数个人用例提供足够的交互。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="6724" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">虚拟助理的结构</h1><p id="82a2" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">这个虚拟助手，在基本层面上，将依赖于两个互锁的组件。意图分类以理解上下文，实体提取以定位含义。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lo"><img src="../Images/faef5d41dfb51ed806358de6a00aeb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57XGlDKtyTmpMNbwKFpNOQ.png"/></div></div></figure><h2 id="e4bb" class="lp ks hi bd kt lq lr ls kx lt lu lv lb jx lw lx ld kb ly lz lf kf ma mb lh mc bi translated">意图分类</h2><p id="b024" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">意图分类是<strong class="jq hj">句子级别的</strong>过程，它确定用户想要通过说出给定的句子来完成什么。举例来说，如果你要告诉你的虚拟助理“<em class="md">请关灯，</em>”意图分类将分析该提示，并确定你打算发出“<em class="md"> lights_off </em>”命令。</p><h2 id="4ef2" class="lp ks hi bd kt lq lr ls kx lt lu lv lb jx lw lx ld kb ly lz lf kf ma mb lh mc bi translated">实体提取</h2><p id="10e9" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">实体提取是<strong class="jq hj">标记级</strong>过程，定位与您的意图相关的关键词。例如，如果您发出提示“<em class="md">打开楼下的灯</em>”，那么实体提取会将关键字<strong class="jq hj">楼下</strong>检测为<strong class="jq hj">位置</strong>。</p><blockquote class="me mf mg"><p id="53ee" class="jo jp md jq b jr js ij jt ju jv im jw mh jy jz ka mi kc kd ke mj kg kh ki kj hb bi translated">开始前还有最后一件事。我准备了一个Colab笔记本，里面有这篇文章的所有代码，可以在这里找到。如果你对代码有问题，那么我推荐你看看Colab笔记本。如果做不到，请随时联系我，我会尽力帮助你。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/9bc5b66ec7d31fa19217c5623a6135b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OV8eQzbVVq_6Q6WBM1EBcw.png"/></div></div></figure><h1 id="6887" class="kr ks hi bd kt ku ml kw kx ky mm la lb io mn ip ld ir mo is lf iu mp iv lh li bi translated">数据集准备</h1><p id="5d58" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">在开始虚拟助手的工作之前，需要准备一个数据集。获取这个数据集的一个简单方法是寻找公共意图分类数据集，比如rikhuijzer的Github中的<a class="ae jn" href="https://github.com/rikhuijzer/nlu_datasets/blob/master/data/chatbot/original/ChatbotCorpus.json" rel="noopener ugc nofollow" target="_blank">这个</a>。然而，通过依赖预先制作的数据集，虚拟助手将被限制于仅理解存在于所述数据集中的命令。因此，我决定程序化地创建我自己的数据集。</p><h2 id="f053" class="lp ks hi bd kt lq lr ls kx lt lu lv lb jx lw lx ld kb ly lz lf kf ma mb lh mc bi translated">程序数据集生成过程</h2><p id="4b7b" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">创建自然语言数据集很困难，因为大多数时候，每个句子都需要手写。这对于我们的用例来说是不合适的。至少需要1000个数据点，而且要花很长时间才能写出这么多独特的句子。相反，可以利用句子的结构来促进程序生成。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/2f7015512fbad65be76d729330df938b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fsPpU1i_C9dSs5J7YSvICw.jpeg"/></div></div></figure><p id="4e14" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了生成数据集，需要找到产生句子的可靠方式。幸运的是，这个问题已经被mad libs的创造者解决了。Mad libs是一种儿童游戏，玩家可以看到一组部分完成的句子。玩游戏的人在空白处填入正确类型的单词，如名字或动作。这就产生了个性化的故事。通过采用这种方法，可以从句子模板和词组的集合中生成数据集。</p><p id="7e55" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用这种创建句子模板，然后填充缺失单词的方法，将极大地提高我们的数据生产率。例如，通过使用这三个句子模板，我能够生成超过40万个独特的句子。</p><ul class=""><li id="f9dd" class="mr ms hi jq b jr js ju jv jx mt kb mu kf mv kj mw mx my mz bi translated">什么是{ number } { math _ function } { number }</li><li id="c9ea" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj mw mx my mz bi translated">{prepend_request}计算{ number } { math _ function } { number }</li><li id="2784" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj mw mx my mz bi translated">{prepend_request}告诉我什么是{ number } { math _ function } { number }</li></ul><p id="25d6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了增加多样性，大多数样本应该被丢弃，但是这种方法可以很容易地从这三个模板中生成一百个独特的句子。</p><h2 id="a134" class="lp ks hi bd kt lq lr ls kx lt lu lv lb jx lw lx ld kb ly lz lf kf ma mb lh mc bi translated">用于程序数据集生成的代码</h2><p id="51a9" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">我已经写了代码，可以将句子模板转换成数据集。这段代码可以在<strong class="jq hj">数据集生成</strong>分支的<a class="ae jn" href="https://github.com/Robert-MacWha/NLP-Intent-Classification/tree/Dataset-Generation" rel="noopener ugc nofollow" target="_blank">这里</a>找到。如果您不想创建自己的数据集，那么下载<strong class="jq hj"> resources </strong>文件夹并使用<a class="ae jn" href="https://github.com/Robert-MacWha/NLP-Intent-Classification/blob/main/resources/data/train.csv" rel="noopener ugc nofollow" target="_blank">预先存在的数据集</a>。这个预先存在的数据集包含以下<strong class="jq hj">五种意向</strong>的样本:</p><ol class=""><li id="4ab8" class="mr ms hi jq b jr js ju jv jx mt kb mu kf mv kj nf mx my mz bi translated">计算</li><li id="27d8" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj nf mx my mz bi translated">光</li><li id="0891" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj nf mx my mz bi translated">TodoAdd</li><li id="3f05" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj nf mx my mz bi translated">托多吉</li><li id="db63" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj nf mx my mz bi translated">天气预报</li></ol><p id="1fe6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">可以按照自述文件中的说明添加更多示例。在继续本文之前，请随意添加更多示例，因为除了最终结果之外，它们不会影响任何东西。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="ac1b" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">意向分类概述</h1><p id="8fcf" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">我知道我说过我们不会在本文中使用意图分类器，但是请耐心听我说。为了开始预处理步骤，首先必须理解预训练模型和记号化器之间的关系。</p><p id="1318" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在NLP领域，预先训练的模型通常是庞大而复杂的。这意味着它们需要非常特定格式的输入数据。此外，由于大多数预训练模型使用不同的数据格式，因此必须编写代码来格式化它们所使用的每个NLP模型的数据集。为了解决这个问题，大多数NLP模型，实际上还有许多其他预先训练的模型，都带有标记化器。记号赋予器是处理所有格式问题以节省时间和减少出错几率的对象。</p><h2 id="6028" class="lp ks hi bd kt lq lr ls kx lt lu lv lb jx lw lx ld kb ly lz lf kf ma mb lh mc bi translated">对数据集进行符号化</h2><p id="a919" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">在使用意图分类数据集来训练模型之前，需要对其进行标记化。标记化是将我们的数据集转换成NLP模型可以理解的格式的过程:<strong class="jq hj">number id</strong>。</p><blockquote class="me mf mg"><p id="59fc" class="jo jp md jq b jr js ij jt ju jv im jw mh jy jz ka mi kc kd ke mj kg kh ki kj hb bi translated">对于<strong class="jq hj">更多关于标记化的信息</strong>，我建议阅读<a class="ae jn" href="https://huggingface.co/course/chapter2/4?fw=pt" rel="noopener ugc nofollow" target="_blank">的这篇</a>文章。它对标记化给出了比我更好的解释。</p></blockquote><p id="f284" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了将原始句子转换成它们的标记形式，使用了一个<strong class="jq hj">标记器</strong>。在这种情况下，标记器来自<a class="ae jn" href="https://huggingface.co/course/chapter2/4?fw=pt" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>的transformer库。由于<em class="md">distilbert-base-uncated</em>变压器将在模型中使用，因此这里也必须使用它的令牌化器。</p><p id="20b4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要对数据集进行令牌化，必须首先将其加载到python中。还必须创建一个<strong class="jq hj">记号赋予器对象</strong>。</p><pre class="iy iz ja jb fd ng nh ni nj aw nk bi"><span id="32de" class="lp ks hi nh b fi nl nm l nn no">from transformers import DistilBertTokenizerFast<br/>from ast import literal_eval<br/>import pandas as pd<br/>import json</span><span id="c1b2" class="lp ks hi nh b fi np nm l nn no">#* create the tokenizer<br/>tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')</span><span id="0e4e" class="lp ks hi nh b fi np nm l nn no">#* load in the dataset<br/>df_train = pd.read_csv('https://raw.githubusercontent.com/Robert-MacWha/NLP-Intent-Classification/Intent-Classification/resources/data/train.csv')</span><span id="7af2" class="lp ks hi nh b fi np nm l nn no"># load json data<br/>resp = requests.get('https://raw.githubusercontent.com/Robert-MacWha/NLP-Intent-Classification/Intent-Classification/resources/data/intent_labels.json')</span><span id="d7a5" class="lp ks hi nh b fi np nm l nn no">intent_labels = json.loads(resp.text)<br/>intent_count = len(intent_labels)</span></pre><p id="4b2c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一旦数据集被加载，标记器就可以用来将文本转换成数字输入id。</p><pre class="iy iz ja jb fd ng nh ni nj aw nk bi"><span id="1267" class="lp ks hi nh b fi nl nm l nn no">inputs = tokenizer(<br/>    list(df_train['words']),      # specify the string[] to tokenize<br/>    max_length=128,               # custom padding<br/>    padding='max_length',         # sets padding to the custom value<br/>    return_attention_mask=True,<br/>    return_token_type_ids=False,<br/>    return_tensors='np'           # flag to return numpy array<br/>)</span><span id="43c8" class="lp ks hi nh b fi np nm l nn no">x_train_ids = inputs['input_ids']<br/>x_train_attention = inputs['attention_mask']</span></pre><p id="691b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">记号赋予器将返回两个数组— <em class="md"> x_train_ids </em>和<em class="md"> x_train_attention </em>。<em class="md"> train_ids </em>变量包含一组<strong class="jq hj">标记化的句子</strong>，可以直接输入到模型中。<em class="md"> train_attention </em>变量用于告诉模型哪些标记是填充的，哪些不是。因为记号赋予器使用了128的填充，所以所有较短的句子都将被附加null记号(0 ),直到它们达到那个长度。</p><pre class="iy iz ja jb fd ng nh ni nj aw nk bi"><span id="4d2c" class="lp ks hi nh b fi nl nm l nn no"># An example of a single sentence after being tokenized</span><span id="e0fd" class="lp ks hi nh b fi np nm l nn no">print(x_train_ids)<br/># the first 7 numbers are text IDs, while the rest are padding<br/># [7993, 170, 11303, 1200, 2443, 1110, 3014, 0, 0, 0, 0, 0, ..., 0]</span><span id="a881" class="lp ks hi nh b fi np nm l nn no">print(x_train_attention)<br/># ones denote parts of the sentence, zeros denote padded tokens/<br/># [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0..., 0]</span></pre><p id="f855" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因为注意力屏蔽告诉模型哪些标记是重要的，所以它允许模型只关注那些信息标记。</p><p id="3455" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，用于标记化句子的y_labels需要被转换成<strong class="jq hj">一个热标签</strong>。这是使用TensorFlow的<em class="md"> one_hot </em>函数完成的，并且是必要的，因为模型的输出是一个热分类。</p><pre class="iy iz ja jb fd ng nh ni nj aw nk bi"><span id="7f26" class="lp ks hi nh b fi nl nm l nn no">y_train_intents = tf.one_hot(df_train['intent_label'].values, intent_count)</span></pre><p id="ad05" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，让我们打印出其中一个数据点，以确保一切都按计划进行。</p><pre class="iy iz ja jb fd ng nh ni nj aw nk bi"><span id="c2c1" class="lp ks hi nh b fi nl nm l nn no"># sample datapoint<br/>print(f'Prompt:         {df_train["words"][0]}')<br/>print(f'Token IDs:      {x_train_ids[0][:12]}...')<br/>print(f'Attention mask: {x_train_attention[0][:12]}...')<br/>print(f'One-hot Label:  {y_train_intents[0]}')</span><span id="1554" class="lp ks hi nh b fi np nm l nn no"># Prompt: can you calculate twelve point five plus two Token<br/># IDs: [101 2064 2017 18422 4376 2391 2274 4606 2048, 102 0 0]... <br/># Attention mask: [1 1 1 1 1 1 1 1 1 1 0 0]... <br/># One-hot Label: [1. 0. 0. 0. 0.]</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="0ce2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">就是这样！提示已被标记和填充，注意掩码与输入id匹配，标签是正确的。接下来的步骤是构建和训练一个意图分类转换器，但这是本系列的第二部分。我将在几天后发布它，所以如果你已经完成了这篇文章，请耐心等待并关注我，这样当我发布第二部分时你会得到通知。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/670e1c620e65375877c07a01a574743d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*NJoApEmFfm1z5xv5hL5DoQ.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">下面先睹为快！</figcaption></figure><blockquote class="me mf mg"><p id="cc50" class="jo jp md jq b jr js ij jt ju jv im jw mh jy jz ka mi kc kd ke mj kg kh ki kj hb bi translated">感谢阅读我的文章！请随意查看我的<a class="ae jn" href="https://tks.life/profile/robert.macwha#portfolio" rel="noopener ugc nofollow" target="_blank">作品集</a>，如果你有什么要说的，请在<a class="ae jn" href="https://www.linkedin.com/in/robert-macwha-0555141b6/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我发消息，或者在Medium上关注我，以便在我发布另一篇文章时得到通知！</p></blockquote><h2 id="bb1c" class="lp ks hi bd kt lq lr ls kx lt lu lv lb jx lw lx ld kb ly lz lf kf ma mb lh mc bi translated">有用的链接</h2><ul class=""><li id="f6e8" class="mr ms hi jq b jr lj ju lk jx nr kb ns kf nt kj mw mx my mz bi translated"><strong class="jq hj"> Colab笔记本</strong>带代码进行数据预处理:<br/><a class="ae jn" href="https://colab.research.google.com/drive/1VpqA35sWtbO5MZ6miw5N8g9mMlLyqO9n?authuser=1#scrollTo=JPsujT9X53-P" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/drive/1 vpqa 35 swt bo 5 mz 6 miw 5n 8g 9 mllyqo 9n？authuser = 1 # scroll to = jpsujt9x 53-P</a></li><li id="dbf1" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj mw mx my mz bi translated"><strong class="jq hj">用于创建自定义意图分类数据集的代码库</strong>包括文档:<br/><a class="ae jn" href="https://github.com/Robert-MacWha/NLP-Intent-Classification/tree/Dataset-Generation" rel="noopener ugc nofollow" target="_blank">https://github . com/Robert-MAC wha/NLP-Intent-Classification/tree/Dataset-Generation</a></li><li id="da88" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj mw mx my mz bi translated"><strong class="jq hj">介绍</strong>变形金刚，由HuggingFace。对于本文，只有“<em class="md">变压器型号</em>”部分是重要的:<br/><a class="ae jn" href="https://huggingface.co/course/chapter1" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/course/chapter1</a></li><li id="8235" class="mr ms hi jq b jr na ju nb jx nc kb nd kf ne kj mw mx my mz bi translated"><strong class="jq hj">介绍</strong>给tokenizers，by hugging face:<br/>T28】https://huggingface.co/course/chapter2/4?fw=pt</li></ul></div></div>    
</body>
</html>