<html>
<head>
<title>Customer Churn Prediction: Machine Learning Project For Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">客户流失预测:面向初学者的机器学习项目</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/customer-churn-prediction-a-machine-learning-kaggle-project-10343af7f394?source=collection_archive---------0-----------------------#2021-11-20">https://medium.com/nerd-for-tech/customer-churn-prediction-a-machine-learning-kaggle-project-10343af7f394?source=collection_archive---------0-----------------------#2021-11-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/28375cb62047ca27081a40244da17c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CQIDKYuqFEmj2fvxwDqvA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:Onur Binay，Unsplash</figcaption></figure><p id="c846" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">本案例研究是一家电信公司预测客户流失的各种机器学习工具和技术的实施。这篇博客是写给任何想学习如何使用python进行客户流失分析的人的。</p><p id="a8c0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">学习:</strong></p><ul class=""><li id="12d0" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">类别不平衡对机器学习模型的影响。</li><li id="c0bd" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">使用SMOTE创建合成样本并对少数类进行上采样。</li><li id="d2d4" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">机器学习模型中的超参数调整</li></ul><p id="e719" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">步骤:</strong></p><ol class=""><li id="4d8e" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr kg jy jz ka bi translated">问题描述:了解电信流失预测问题。</li><li id="12f2" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr kg jy jz ka bi translated">探索性数据分析:使用各种可视化技术来掌握数据。发现模式，观察和分析特征。</li><li id="80a4" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr kg jy jz ka bi translated">训练测试分割:将数据分割成训练集和测试集。</li><li id="537c" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr kg jy jz ka bi translated">特征化:将基于文本的分类特征转换为数字特征，并执行特征标准化。</li><li id="2b76" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr kg jy jz ka bi translated">模型:在训练数据上训练监督学习模型，如SVM、随机森林和XgBOOST，并在测试数据上检查它们的性能。</li><li id="bf67" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr kg jy jz ka bi translated">结论:用精确的要点结束项目。</li></ol><h1 id="be32" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">问题描述:</h1><ul class=""><li id="4683" class="js jt hi iw b ix lf jb lg jf lh jj li jn lj jr jx jy jz ka bi translated">客户流失是当客户决定停止使用企业服务时使用的术语。企业一直在做客户流失分析，因为如果他们知道哪些客户将要离开，这对一个公司非常有帮助。</li><li id="40b6" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">这个项目的目的是在可用数据上训练一个机器学习模型，以训练一个机器学习模型，该模型将高精度地预测哪些客户将流失，这反过来将帮助企业所有者做出有用的营销决策。</li><li id="d25a" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">可以在Kaggle上找到原项目详情:<a class="ae lk" href="https://www.kaggle.com/c/customer-churn-prediction-2020/overview" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/customer-churn-prediction-2020/overview</a></li><li id="fe36" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">点击这里查看该项目的源代码:<a class="ae lk" href="https://github.com/S-G-001/customer_churn_prediction" rel="noopener ugc nofollow" target="_blank">https://github.com/S-G-001/customer_churn_prediction</a></li></ul><p id="755c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">数据概述:</strong></p><p id="efdd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">训练数据集包含4250个样本。每个样本包含19个特征和1个指示样本类别的布尔变量“churn”。19个输入特征和1个目标变量是:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/3f9bb38619a959dc5f7203fee47dec5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5580v9E3uJiVl4VCmKMXg.png"/></div></div></figure><p id="f8bc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">业务限制和要求:</strong></p><ul class=""><li id="6105" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">他们没有严格的延迟要求</li><li id="ef24" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">要求高精度</li><li id="f3a3" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">由于数据是不平衡的，我们还需要跟踪召回和混淆矩阵。</li></ul><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/7a97acfd3b3d077a33607a57f9e8d59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*IK2AMGd_qy4K3niCESLvuA.png"/></div></figure><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/0d350b5aa0915e0b2421c9e801dba5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*8k4Njx1kYtdD8B3AD-eL9A.png"/></div></figure><h1 id="a31f" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">探索性数据分析:</h1><p id="c73b" class="pw-post-body-paragraph iu iv hi iw b ix lf iz ja jb lg jd je jf ls jh ji jj lt jl jm jn lu jp jq jr hb bi translated"><strong class="iw hj">加载数据并浏览高级统计:</strong></p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="6fcb" class="ma ki hi lw b fi mb mc l md me"># Load the Data and take a look at the first three samples<br/>data = pd.read_csv('train.csv')<br/>data.head(3)</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/f8c126de29eb23cd7811d0826822cc10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9uDPTGv0beKicK6R1YIttg.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">前三个样本— GIF</figcaption></figure><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="8f83" class="ma ki hi lw b fi mb mc l md me"># High level statistics of Numerical features<br/>data.describe()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/3e6108d2d43071315e2185e99201f783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*eSGxGACm6_fA7LyqGD26Ow.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">高级统计— GIF</figcaption></figure><p id="3928" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">观察:训练数据中的特征在方差和均值方面有很大不同。如果使用利用距离测量的模型，执行均值居中和方差缩放将是一个好主意。</p><p id="7fb2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">检查班级不平衡:</strong></p><p id="67d2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我通过计算每个类别的样本数来完成，并绘制出搅拌样本与未搅拌样本的百分比。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="dffc" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Checking Imbalance in Data</em><br/>not_churned = data[data['churn']=='no']['churn'].count()<br/>churned = data[data['churn']=='yes']['churn'].count()<br/>print('not_churned: ',not_churned,', churned: ',churned)<br/><br/>fig = plt.figure(figsize=(5,5)) <br/>plt.pie([not_churned,churned], labels=['not_churned','churned'],autopct='<strong class="lw hj">%.2f</strong>')<br/>plt.title('Pie Chart Customers Churned v/s Not Churned')<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/4726c4377518343e301fe0f27844d146.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*VqtiBIVz5WWN4uzoPK5pmw.png"/></div></figure><p id="3ac3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">观察:这两类数据存在明显的不平衡。</em></p><p id="3ea6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">可视化两个类的一些特征的分布:</strong></p><ul class=""><li id="df39" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">帐户_长度</li></ul><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="ee17" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Plotting PDF account length for both the class labels</em><br/>ac_len_churned = data[data['churn']=='yes']['account_length']<br/>ac_len_not_churned =data[data['churn']=='no']['account_length']<br/>sns.distplot(ac_len_churned,label='Churned')<br/>sns.distplot(ac_len_not_churned,label='Not_churned')<br/>plt.title('Distribution of Account Length for Churned and  Not_Churned')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/0ea3c015e09d82b654111fafdb07600c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*w-M4xMkAJZLxVVVKMmGqrA.png"/></div></figure><p id="b665" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">观察:搅动实例的account_length分布和未搅动实例的account_length分布几乎重叠。</em></p><ul class=""><li id="f9f5" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">总计_天_费用</li></ul><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="289a" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Plotting PDF of day charge for both the class labels</em><br/>day_charge_churned = data[data['churn']=='yes']['total_day_charge']<br/>day_charge_not_churned =data[data['churn']=='no']['total_day_charge']<br/>sns.distplot(day_charge_churned,label='Churned')<br/>sns.distplot(day_charge_not_churned,label='Not_churned')<br/>plt.title('Distribution of day charge for Churned and Not_Churned')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/60a5915906d1b78b3c9f07b5779a94dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*h8P-y1VQYub9jumyXmbajQ.png"/></div></figure><p id="a528" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">观察:橙色pdf(针对非搅动类)和蓝色pdf(针对搅动类)没有完全重叠。当day_charge高于40时，更多数量的样本被搅动。</em></p><ul class=""><li id="abd8" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">总费用</li></ul><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="838e" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Plotting PDF of eve_charge for both the class labels</em> eve_charge_churned = data[data['churn']=='yes']['total_eve_charge'] eve_charge_not_churned =data[data['churn']=='no']['total_eve_charge'] sns.distplot(eve_charge_churned,label='Churned') sns.distplot(eve_charge_not_churned,label='Not_churned') plt.title('Distribution of eve_charge for Churned and Not_Churned') plt.legend() <br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/1d4040798288283b3100bbd91b8f6c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*ezh8Xf6A4eoqKk4O4zFNsw.png"/></div></figure><ul class=""><li id="a557" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">总_夜_费</li></ul><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="653d" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Plotting PDF of night_charge for both the class labels</em><br/>night_charge_churned = data[data['churn']=='yes']['total_night_charge']<br/>night_charge_not_churned =data[data['churn']=='no']['total_night_charge']<br/>sns.distplot(night_charge_churned,label='Churned')<br/>sns.distplot(night_charge_not_churned,label='Not_churned')<br/>plt.title('Distribution of night_charge for Churned and Not_Churned')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/5185e3b78c432f18a1484bfff0236e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*ptRgkF_Jd7N_W4HO3XNEeg.png"/></div></figure><ul class=""><li id="a2ec" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">总费用</li></ul><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="6798" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Plotting PDF of intl_charge for both the class labels</em><br/>intl_charge_churned = data[data['churn']=='yes']['total_intl_charge']<br/>intl_charge_not_churned =data[data['churn']=='no']['total_intl_charge']<br/>sns.distplot(intl_charge_churned,label='Churned')<br/>sns.distplot(intl_charge_not_churned,label='Not_churned')<br/>plt.title('Distribution of intl_charge for Churned and Not_Churned')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/599e86640c0b7ef75de4b806aef7da24.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*-qBwG1OoTXgp8pro-PfchA.png"/></div></figure><ul class=""><li id="031c" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">数字邮件消息</li></ul><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="efbf" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Plotting PDF of no_vmail_charge for both the class labels</em><br/>vmail_msgs_churned = data[data['churn']=='yes']['number_vmail_messages']<br/>vmail_msgs_not_churned =data[data['churn']=='no']['number_vmail_messages']<br/>sns.distplot(vmail_msgs_churned,hist=<strong class="lw hj">False</strong>,label='Churned')<br/>sns.distplot(vmail_msgs_not_churned,hist=<strong class="lw hj">False</strong>,label='Not_churned')<br/>plt.title('Distribution of number_vmail_messages for Churned and Not_Churned')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/8352db390592c87a92d24e230ae30cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*_grrObx7exU6t2tmd6LKpA.png"/></div></figure><p id="7e2f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">观察:当语音邮件消息的数量高于15时，可以看到大多数搅动的样本。</em></p><p id="8a1b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">检查配对图:</strong></p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="328c" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Building pair plot</em><br/>sns.pairplot(data,vars=['account_length','number_vmail_messages', 'total_day_charge','total_eve_charge','total_night_charge',            'total_intl_charge','number_customer_service_calls'],hue='churn')</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/30947777a787d216b21c7424464ac2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EX-vWU-0n5ViOh4vbjSa7w.png"/></div></div></figure><p id="b72e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">观察:对于总日费、总夜费和总夜费等特征，这两个类别有明显的区别</em></p><p id="e531" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">特征关联:</strong></p><p id="a4e6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">构建相关矩阵</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="8f8e" class="ma ki hi lw b fi mb mc l md me">sns.set(style="white", font_scale =1.5)<br/>corr = data.corr()<br/><br/>mask = np.zeros_like(corr, dtype=np.bool)<br/>mask[np.triu_indices_from(mask)] = <strong class="lw hj">True</strong><br/><br/>f,ax = plt.subplots(figsize=(14,8))<br/>f.suptitle('Correlation_Matrix')<br/><br/><em class="mg">#cmap = sns.diverging_palette(220, 10, as_cmap=True)</em><br/><br/>sns.heatmap(corr, mask=mask, cmap='YlGnBu', vmax=.3, center=0,<br/>            square=<strong class="lw hj">True</strong>, linewidths=.5, cbar_kws={"shrink": .5})</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/c6d8291a39fc572a479c7a7b201b9bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O_axSMLcU16L6kAHv5bH5w.png"/></div></div></figure><p id="15c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们应该删除高度相关的特征</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="e2ca" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Removing  the highly correlated features</em><br/>data = data.drop(['total_day_minutes','total_eve_minutes','total_night_minutes','total_intl_minutes'], axis=1)</span></pre><h1 id="e0e2" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">列车测试分离:</h1><p id="32bc" class="pw-post-body-paragraph iu iv hi iw b ix lf iz ja jb lg jd je jf ls jh ji jj lt jl jm jn lu jp jq jr hb bi translated">首先将目标变量中的字符串类型类标签从“是”“否”转换为整数1和0。</p><blockquote class="mq"><p id="dc27" class="mr ms hi bd mt mu mv mw mx my mz jr dx translated">为了避免数据泄露，在进行特征工程之前分割训练集和测试集是一个很好的做法。</p></blockquote><p id="a05b" class="pw-post-body-paragraph iu iv hi iw b ix na iz ja jb nb jd je jf nc jh ji jj nd jl jm jn ne jp jq jr hb bi translated">使用sklearn的train_test_split将数据拆分为训练集和测试集。10%的测试规模对于检查模型的性能似乎足够合理了。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="3ac9" class="ma ki hi lw b fi mb mc l md me">data.churn.replace(['yes','no'],[1,0],inplace=<strong class="lw hj">True</strong>)<br/>Y = data['churn']<br/>X = data.drop('churn', axis=1)</span><span id="6366" class="ma ki hi lw b fi nf mc l md me">x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.10,stratify=Y,random_state=11)<br/>print('Shape of x_train and y_train: ',x_train.shape, y_train.shape)<br/>print('Shape of x_test and y_test: ',x_test.shape, y_test.shape)</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/d24694514dec8444e75e5ae97abf17c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*jHwM2gacUI5ufgwcazPX3g.png"/></div></figure><h1 id="c976" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">特色化:</h1><p id="20f2" class="pw-post-body-paragraph iu iv hi iw b ix lf iz ja jb lg jd je jf ls jh ji jj lt jl jm jn lu jp jq jr hb bi translated">使用一个热编码将分类特征转换为数字:</p><p id="b240" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在使用CoutVectorizer()时，我们应该对训练和测试数据分别进行fit_transform，以避免泄漏。</p><p id="0578" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">状态:</strong></p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="bdc2" class="ma ki hi lw b fi mb mc l md me">vectorizer = CountVectorizer()<br/>train_state = vectorizer.fit_transform(x_train['state'].values)<br/>test_state = vectorizer.fit_transform(x_test['state'].values)<br/>train_state.shape</span></pre><p id="5309" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi">(3825, 51)</p><p id="5989" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">区号:</strong></p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="5817" class="ma ki hi lw b fi mb mc l md me">vectorizer = CountVectorizer()<br/>train_area_code = vectorizer.fit_transform(x_train['area_code'].values) <br/>test_area_code = vectorizer.fit_transform(x_test['area_code'].values)<br/>print(vectorizer.get_feature_names())</span></pre><p id="ac87" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">['区域代码_408 '，'区域代码_415 '，'区域代码_510']</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="f48f" class="ma ki hi lw b fi mb mc l md me">train_area_code.shape</span></pre><p id="68b8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi">(3825, 3)</p><p id="cb10" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">国际_计划:</strong></p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="0eaa" class="ma ki hi lw b fi mb mc l md me">vectorizer = CountVectorizer()<br/>train_international_plan = vectorizer.fit_transform(x_train['international_plan'].values)<br/>test_international_plan = vectorizer.fit_transform(x_test['international_plan'].values)<br/>print(vectorizer.get_feature_names())</span></pre><p id="54d1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">['否'，'是']</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="5848" class="ma ki hi lw b fi mb mc l md me"># Creating a separate list of the new and more descriptive feature names. These names will be later used to get feature importance. </span><span id="3dd2" class="ma ki hi lw b fi nf mc l md me">intnl_fea = ['intl_plan_no','intl_plan_yes']<br/>print(intnl_fea)</span></pre><p id="e08e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">['国际计划否'，'国际计划是']</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="1e31" class="ma ki hi lw b fi mb mc l md me">train_international_plan.shape</span></pre><p id="1c87" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi">(3825, 2)</p><p id="0e14" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">语音_邮件_计划:</strong></p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="01e0" class="ma ki hi lw b fi mb mc l md me">vectorizer = CountVectorizer()<br/>train_voice_mail_plan = vectorizer.fit_transform(x_train['voice_mail_plan'].values) <br/>test_voice_mail_plan = vectorizer.fit_transform(x_test['voice_mail_plan'].values) <br/>print(vectorizer.get_feature_names())</span></pre><p id="2ec7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">['否'，'是']</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="143f" class="ma ki hi lw b fi mb mc l md me"># Creating a separate list of the new and more descriptive feature names. These names will be later used to get feature importance.</span><span id="8a7f" class="ma ki hi lw b fi nf mc l md me">voice_mail_plan_fea = ['voice_plan_no','voice_plan_yes']<br/>print(voice_mail_plan_fea)</span></pre><p id="b469" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">['语音_计划_否'，'语音_计划_是']</p><p id="a5e4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">数值特征:</strong></p><p id="9c65" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先手动输入列表中所有数字特征的名称，然后创建仅包含数字特征的新训练和测试数据框。然后应用StandardScaler()来标准化新的数字数据框。</p><p id="68b7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">或者你可以标准化所有的特性(更好的选择)。我只标准化了那些原本是数字的特征。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="f2de" class="ma ki hi lw b fi mb mc l md me">numerical_fea = ['account_length','number_vmail_messages','total_day_calls','total_day_charge', 'total_eve_calls', 'total_eve_charge', 'total_night_calls', 'total_night_charge', 'total_intl_calls', 'total_intl_charge', 'number_customer_service_calls']</span><span id="b49b" class="ma ki hi lw b fi nf mc l md me">train_numerical = x_train[numerical_fea]<br/>test_numerical = x_test[numerical_fea]</span><span id="28ef" class="ma ki hi lw b fi nf mc l md me"><em class="mg"># Scaling the data using sklearn's Standard scaler</em><br/>scaler = StandardScaler()<br/>train_numerical_scaled = scaler.fit_transform(train_numerical)<br/>test_numerical_scaled = scaler.fit_transform(test_numerical)</span></pre><p id="4757" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">合并特征:</strong></p><p id="8e7c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用hstack()水平合并特征。然后制作一个包含所有特性名称的列表，以便稍后用于获取特性重要性。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="ab0b" class="ma ki hi lw b fi mb mc l md me">x_train_merged = hstack((train_numerical_scaled,train_voice_mail_plan,   train_international_plan,train_area_code,train_state))<br/><br/>x_test_merged = hstack((test_numerical_scaled,test_voice_mail_plan,test_international_plan, test_area_code,test_state))</span><span id="0a96" class="ma ki hi lw b fi nf mc l md me"># list of all features<br/>all_features = numerical_fea + voice_mail_plan_fea + intnl_fea + area_code_fea + state_fea</span></pre><h1 id="1cd4" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">型号:</h1><p id="5c5f" class="pw-post-body-paragraph iu iv hi iw b ix lf iz ja jb lg jd je jf ls jh ji jj lt jl jm jn lu jp jq jr hb bi translated">我们将在所有模型中遵循的一般步骤:</p><ul class=""><li id="b258" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">创建超参数字典。</li><li id="60f9" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">实例化分类器。</li><li id="8120" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">定义网格对象。在GridSearchCV中提供超参数字典。保持return_train_score = True，这样我们就可以得到训练精度。</li><li id="3040" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">对训练数据进行网格拟合，得到网格搜索的结果。</li><li id="7873" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">对照交叉验证准确度绘制训练准确度图，并选择超参数的最佳值。</li><li id="6930" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">使用通过网格搜索获得的最佳超参数来训练新的模型。</li><li id="a38b" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">检查测试数据的性能。获取测试数据的准确率、召回率和混淆矩阵。</li><li id="7861" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">在列表中保存精确度和召回，以便稍后在项目结束时创建结论表。</li></ul><ol class=""><li id="3968" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr kg jy jz ka bi translated"><strong class="iw hj">)没有阶级平衡的SVM</strong></li></ol><p id="c389" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">创建一个没有类平衡的模型来创建一个基准模型。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="b546" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Grid Search To Get Best Hyperparameters</em> <br/>parameters = {'C':[0.01,0.1,1,3,5,10]} <br/>svm_clf = SVC(random_state=43) <br/>grid = GridSearchCV(estimator=svm_clf, param_grid=parameters,scoring='accuracy',return_train_score=<strong class="lw hj">True</strong>,verbose=1) <br/>grid.fit(x_train_merged,y_train)  <br/>cv_result = pd.DataFrame(grid.cv_results_)                        plt.scatter(cv_result['param_C'],cv_result['mean_train_score'])   plt.plot(cv_result['param_C'],cv_result['mean_train_score'],label='Train') plt.scatter(cv_result['param_C'],cv_result['mean_test_score']) plt.plot(cv_result['param_C'],cv_result['mean_test_score'],label="CV") <br/>plt.title('Hyperparameter vs accuracy') <br/>plt.legend() <br/>plt.xlabel('C') <br/>plt.ylabel('Accuracy') <br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/90ba321dc96547f8e9fafa7a59c3ae35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*oii9i6scWsn7yI6AIbke8A.png"/></div></figure><blockquote class="mq"><p id="178b" class="mr ms hi bd mt mu ni nj nk nl nm jr dx translated">当训练和测试精度之间存在较大差距，并且测试精度没有显著提高时，模型开始过度拟合。</p></blockquote><pre class="nn no np nq nr lv lw lx ly aw lz bi"><span id="1e96" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Best parameter chosen manually by observation using above plot, that gives the best test accuracy while keeping variance in check</em></span><span id="044f" class="ma ki hi lw b fi nf mc l md me">best_parameter = 3<br/>print("Best value of C: ", best_parameter)</span></pre><p id="d874" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最佳C值:3</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="0c91" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Training the model again using the optimum parameters discovered</em><br/>svm_clf = SVC(C=best_parameter,random_state=43)<br/>svm_clf.fit(x_train_merged,y_train)<br/><br/>result1 = ["1.","SVM","No class balancing"] # saving result in list<br/>y_pred_tr = svm_clf.predict(x_train_merged)<br/>print('Train accuracy SVM: ',accuracy_score(y_train,y_pred_tr))<br/>result1.append(round(accuracy_score(y_train,y_pred_tr),2))<br/><br/>y_pred_test = svm_clf.predict(x_test_merged)<br/>print('Test accuracy SVM: ',accuracy_score(y_test,y_pred_test))<br/>result1.append(round(accuracy_score(y_test,y_pred_test),2))<br/><br/>recall = recall_score(y_test,y_pred_test)<br/>print("Recall Score: ",recall)<br/>result1.append(round(recall,2))<br/>print("-----"*20)<br/><br/>cm = confusion_matrix(y_test,y_pred_test)<br/>ax=plt.subplot();<br/>sns.heatmap(cm, annot=<strong class="lw hj">True</strong>, fmt='d', linewidths=2, linecolor='black', cmap='YlGnBu',ax=ax)<br/>ax.set_xlabel('Predicted')<br/>ax.set_ylabel('Actual')<br/>ax.set_ylim(2.0,0)<br/>ax.set_title('Confusion Matrix')<br/>ax.xaxis.set_ticklabels(['Neg','Pos'])<br/>ax.yaxis.set_ticklabels(['Neg','Pos'])<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/6b59ef2c1247a60cb9939d485ef50bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*kC0QTSiSjp94QArGYmKSbA.png"/></div></figure><p id="9f76" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。)SVM —使用类别权重平衡的类别:</strong></p><p id="4472" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Scikit learn提供了一种方法来平衡SVM分类器内部的数据集，以保持class_weight = 'balanced '。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="eccc" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Grid Search To Get Best Hyperparameters</em><br/>parameters = {'C':[0.01,0.1,1,3,5,10]}<br/>svm_clf = SVC(class_weight='balanced',random_state=43)<br/>grid = GridSearchCV(estimator=svm_clf, param_grid=parameters,scoring='accuracy',return_train_score=<strong class="lw hj">True</strong>,verbose=1)<br/>grid.fit(x_train_merged,y_train)<br/><br/>cv_result = pd.DataFrame(grid.cv_results_)<br/>plt.scatter(cv_result['param_C'],cv_result['mean_train_score'])<br/>plt.plot(cv_result['param_C'],cv_result['mean_train_score'],label='Train')<br/>plt.scatter(cv_result['param_C'],cv_result['mean_test_score'])<br/>plt.plot(cv_result['param_C'],cv_result['mean_test_score'],label="CV")<br/>plt.title('Hyperparameter vs accuracy')<br/>plt.legend()<br/>plt.xlabel('C')<br/>plt.ylabel('Accuracy')<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/3a998abdd4092d435409ea15a0d10fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*PwBGTYqnP7a9pu5CTEY1vA.png"/></div></figure><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="8255" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Best parameter chosen manually by observation.</em><br/>best_parameter = 3<br/>print("Best value of C: ", best_parameter)</span></pre><p id="68f4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最佳C值:3</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="2824" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Training the model again using the optimal parameters discovered</em><br/>svm_clf =  SVC(C=best_parameter,class_weight='balanced', random_state=43)<br/>svm_clf.fit(x_train_merged,y_train)<br/><br/>result2 = ["2.","SVM","Balanced using class weights"]<br/>y_pred_tr = svm_clf.predict(x_train_merged)<br/>print('Train accuracy SVM: ',accuracy_score(y_train,y_pred_tr))<br/>result2.append(round(accuracy_score(y_train,y_pred_tr),2))<br/><br/>y_pred_test = svm_clf.predict(x_test_merged)<br/>print('Test accuracy SVM: ',accuracy_score(y_test,y_pred_test))<br/>result2.append(round(accuracy_score(y_test,y_pred_test),2))<br/><br/>recall = recall_score(y_test,y_pred_test)<br/>print("Recall Score: ",recall)<br/>result2.append(round(recall,2))<br/>print("-----"*20)<br/><br/>cm = confusion_matrix(y_test,y_pred_test)<br/>ax=plt.subplot();<br/>sns.heatmap(cm, annot=<strong class="lw hj">True</strong>, fmt='d', linewidths=2, linecolor='black', cmap='YlGnBu',ax=ax)<br/>ax.set_xlabel('Predicted')<br/>ax.set_ylabel('Actual')<br/>ax.set_ylim(2.0,0)<br/>ax.set_title('Confusion Matrix')<br/>ax.xaxis.set_ticklabels(['Neg','Pos'])<br/>ax.yaxis.set_ticklabels(['Neg','Pos'])<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nu"><img src="../Images/0258e159afbd3fa213d08b0226cb5196.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*_bw13ZSzqE7IR8fcdB1N4g.png"/></div></figure><p id="a143" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">平衡等级后，SVM给出了更好的结果。与没有做任何平衡的模型1相比，准确率更高，召回率也更好。</em></p><p id="37e8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。)随机森林分类器</strong></p><p id="129b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">RF是使用一种称为bagging(升压聚集)的集合技术创建的。在RF中，我们必须调整两个参数max_depth(树的最大深度)和n_estimators(森林中树的数量)</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="9d57" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Grid Search To Get Best Hyperparameters</em><br/>parameters = {'max_depth':[5,10,20,50], 'n_estimators': [100,200,300,400,500]}<br/>clf_rf = RandomForestClassifier(class_weight='balanced', random_state=43)      <br/>grid = GridSearchCV(estimator=clf_rf,param_grid=parameters,scoring= 'accuracy',return_train_score= <strong class="lw hj">True</strong>,verbose=1)<br/>grid.fit(x_train_merged,y_train)<br/><br/>print("-----"*20)<br/>cv_result = pd.DataFrame(grid.cv_results_).sort_values(by= 'mean_train_score', ascending=<strong class="lw hj">True</strong>)params = list(cv_result['params'].astype(str))<br/><br/>plt.figure(figsize=(12,6))<br/>plt.scatter(params,cv_result['mean_train_score'])<br/>plt.plot(params,cv_result['mean_train_score'],label='Train')<br/>plt.scatter(params,cv_result['mean_test_score'])<br/>plt.plot(params,cv_result['mean_test_score'],label="CV")<br/>plt.title('Hyperparameter vs accuracy')<br/>plt.legend()<br/>plt.xlabel('Hyperparametr combination Dict')<br/>plt.xticks(rotation=90)<br/>plt.ylabel('Accuracy')<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/58bf68d479c03aadabc2c859dceb8544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSO5MaBAE5kColb24GcUBw.png"/></div></div></figure><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="30d6" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Best parameter chosen manually by observation that gives the best accuracy and keeps variance in check</em></span><span id="87a8" class="ma ki hi lw b fi nf mc l md me">best_max_depth = 10 <br/>best_n_estimator = 300</span><span id="54ce" class="ma ki hi lw b fi nf mc l md me"># Creat a new model with optimum hyperparametersclf_rf = RandomForestClassifier(max_depth=best_max_depth, n_estimators= best_n_estimator, class_weight ='balanced', random_state=43)<br/>clf_rf.fit(x_train_merged,y_train)<br/><br/>result3 = ["3.","RF","Balanced using class weights"]<br/>y_pred_tr = clf_rf.predict(x_train_merged)<br/>print('Train accuracy RF: ',accuracy_score(y_train,y_pred_tr))<br/>result3.append(round(accuracy_score(y_train,y_pred_tr),2))<br/><br/>y_pred_test = clf_rf.predict(x_test_merged)<br/>print('Test accuracy RF: ',accuracy_score(y_test,y_pred_test))<br/>result3.append(round(accuracy_score(y_test,y_pred_test),2))<br/><br/>recall = recall_score(y_test,y_pred_test)<br/>print("Recall Score: ",recall)<br/>result3.append(round(recall,2))<br/>print("-----"*20)<br/><br/>cm = confusion_matrix(y_test,y_pred_test)<br/>ax=plt.subplot();<br/>sns.heatmap(cm, annot=<strong class="lw hj">True</strong>, fmt='d', linewidths=2, linecolor='black', cmap='YlGnBu',ax=ax)<br/>ax.set_xlabel('Predicted')<br/>ax.set_ylabel('Actual')<br/>ax.set_ylim(2.0,0)<br/>ax.set_title('Confusion Matrix')<br/>ax.xaxis.set_ticklabels(['Neg','Pos'])<br/>ax.yaxis.set_ticklabels(['Neg','Pos'])<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/e914e9bb1dbba07a09fc9266010cc7d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*71U41BN3SBrEQzIUO0VYww.png"/></div></figure><p id="0e6a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">RF的性能比SVM车型好得多。与模型2相比，测试准确度和召回率都更高。</em></p><p id="b1f5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">特征重要性:</strong></p><p id="59f6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">还有一件事可以用RF轻松完成，获得最重要的特性。</p><p id="4a32" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">绘制特征重要性:</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="9b0a" class="ma ki hi lw b fi mb mc l md me">importance = clf_rf.feature_importances_<br/>fig = plt.figure(figsize=(18,5))<br/>plt.bar(all_features, importance)<br/>plt.xlabel('Features')<br/>plt.ylabel('Importance')<br/>plt.xticks(rotation=90)<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nw"><img src="../Images/db54c9c8ea0b8ff91f832a86b8b48ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MyccDJDFveX2mmtkJPmoxg.png"/></div></div></figure><p id="4c13" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">这里我们可以得出结论，对客户流失预测影响最大的因素是:</em></p><p id="e7dd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg"> total_day_charge，numer _ customer _ Service _ calls，International_plan，total_eve_charge和total_night_charge。</em></p><p id="52f1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 4。)XgBoost分类器</strong></p><p id="5305" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">据我所知，XgBoost是目前最流行和最强大的机器学习分类器。</p><p id="922d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，我们需要计算阶级不平衡率:</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="eb99" class="ma ki hi lw b fi mb mc l md me"><strong class="lw hj">import</strong> <strong class="lw hj">math</strong><br/>scale=round(math.sqrt(y_train.value_counts()[0]/y_train.value_counts()[1]),2)</span></pre><p id="9b93" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">XgBoost的网格搜索需要更多的时间，因为它有更多的超参数:</p><p id="769c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">n_estimators(提升轮数)、max_depth(基础学习者的最大树深度)、learning_rate(提升学习率)、colsample_bytree(构建每棵树时列的子样本比率)</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="f11b" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Grid Search To Get Best Hyperparameters</em><br/>parameters = {"learning_rate"    : [0.10,0.20,0.30 ],\<br/>              "max_depth"        : [ 3,5,10,20],\<br/>              "n_estimators" : [ 100, 200, 300, 500],\<br/>              "colsample_bytree" : [ 0.3, 0.5, 0.7 ] }<br/>clf_xgb = XGBClassifier(scale_pos_weight=scale, eval_metric ='mlogloss')<br/>grid = GridSearchCV(estimator=clf_xgb, param_grid=parameters, scoring='accuracy',return_train_score=<strong class="lw hj">True</strong>,verbose=1)<br/>grid.fit(x_train_merged,y_train)<br/><br/>print("-----"*20)<br/><em class="mg"># plotting only the first 70 train scores</em><br/>cv_result = pd.DataFrame(grid.cv_results_).sort_values(by='mean_train_score',ascending=<strong class="lw hj">True</strong>)[:70]<br/>param_list = list(cv_result['params'])<br/>param_index = np.arange(70)<br/>plt.figure(figsize=(18,6))<br/>plt.scatter(param_index,cv_result['mean_train_score'])<br/>plt.plot(param_index,cv_result['mean_train_score'],label='Train')<br/>plt.scatter(param_index,cv_result['mean_test_score'])<br/>plt.plot(param_index,cv_result['mean_test_score'],label="CV")<br/>plt.title('Hyperparameter vs accuracy')<br/>plt.grid()<br/>plt.legend()<br/>plt.xlabel('Hyperparametr combination Dict')<br/><em class="mg">#plt.xticks(rotation=90)</em><br/>plt.ylabel('Accuracy')<br/>plt.show()</span></pre><p id="b596" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我没有提到x轴上的超参数组合字典，那样会很麻烦，而是画出了它们的指数。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nx"><img src="../Images/5083a84cc2e742d22ebaae520497b7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_euMD1QouH8x4qUN3T5DA.png"/></div></div></figure><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="ea64" class="ma ki hi lw b fi mb mc l md me"># Whichever index on x-axis of above graph gives the optimum results, we use that index to get the hyperparameters from param_list: </span><span id="f5fb" class="ma ki hi lw b fi nf mc l md me">best_parameters = param_list[34]<br/>print(best_parameters)</span></pre><p id="1643" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">{'colsample_bytree': 0.7，' learning_rate': 0.2，' max_depth': 5，' n_estimators': 100}</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="6821" class="ma ki hi lw b fi mb mc l md me"># Now train a new XgBoost model using the optimum hyperparameters: </span><span id="74c6" class="ma ki hi lw b fi nf mc l md me">clf_xgb = XGBClassifier(learning_rate= best_parameters['learning_rate'] ,max_depth=best_parameters ['max_depth'], n_estimators=best_parameters['n_estimators'], colsample_bytree=best_parameters['colsample_bytree'],                        eval_metric='mlogloss',scale_pos_weight=scale)<br/>clf_xgb.fit(x_train_merged,y_train)<br/><br/>result4 = ["4.","XGBClassifier","Balanced using scale_pos_weight"]<br/>y_pred_tr = clf_xgb.predict(x_train_merged)<br/>print('Train accuracy XGB: ',accuracy_score(y_train,y_pred_tr))<br/>result4.append(round(accuracy_score(y_train,y_pred_tr),2))<br/><br/>y_pred_test = clf_xgb.predict(x_test_merged)<br/>print('Test accuracy XGB: ',accuracy_score(y_test,y_pred_test))<br/>result4.append(round(accuracy_score(y_test,y_pred_test),2))<br/><br/>recall = recall_score(y_test,y_pred_test)<br/>print("Recall Score: ",recall)<br/>result4.append(round(recall,2))<br/>print("-----"*20)<br/><br/>cm = confusion_matrix(y_test,y_pred_test)<br/>ax=plt.subplot();<br/>sns.heatmap(cm, annot=<strong class="lw hj">True</strong>, fmt='d', linewidths=2, linecolor='black', cmap='YlGnBu',ax=ax)<br/>ax.set_xlabel('Predicted')<br/>ax.set_ylabel('Actual')<br/>ax.set_ylim(2.0,0)<br/>ax.set_title('Confusion Matrix')<br/>ax.xaxis.set_ticklabels(['Neg','Pos'])<br/>ax.yaxis.set_ticklabels(['Neg','Pos'])<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/67eb2aee8027e5c98d7ccb714f415cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*eD64OY5huz4TMmgatwhzeg.png"/></div></figure><p id="7e6b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是迄今为止最好的模型，准确率为95%，召回率为75%。</p><p id="a208" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 5。)XGBClassifier使用SMOTE对数据进行平衡</strong></p><p id="c2b9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们还可以尝试另一种上采样技术，称为SMOTE(合成少数过采样技术)。</p><p id="3ffc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">SMOTE创建新的合成少数样本，而不是复制粘贴已经可用的数据点。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="e224" class="ma ki hi lw b fi mb mc l md me"><em class="mg">#Synthetic oversampling of minority class with smote</em></span><span id="8ca8" class="ma ki hi lw b fi nf mc l md me">print(y_train.value_counts())<br/>print('----'*10)<br/>sm = SMOTE(sampling_strategy ='minority')<br/>x_train_sm,y_train_sm = sm.fit_resample(x_train_merged,y_train)<br/>print(x_train_sm.shape,y_train_sm.shape)<br/>print(y_train_sm.value_counts())</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ny"><img src="../Images/fea2d6c57db41c5cc3162425f3ce14bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*ymizbMmySUOMbtDbFzMXng.png"/></div></figure><p id="6c4a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在应用smote之后，我们现在对于类0和类1都有相同数量的样本。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="fb86" class="ma ki hi lw b fi mb mc l md me"><em class="mg"># Grid Search To Get Best Hyperparameters</em><br/>parameters = {"learning_rate"    : [0.10,0.20,0.30 ],\<br/>              "max_depth"        : [ 3,5,10,20],\<br/>              "n_estimators" : [ 100, 200, 300, 500],\<br/>              "colsample_bytree" : [ 0.3, 0.5, 0.7 ] }<br/>clf_xgb = XGBClassifier(eval_metric='mlogloss')<br/>grid = GridSearchCV(estimator=clf_xgb, param_grid=parameters,scoring='accuracy',return_train_score=<strong class="lw hj">True</strong>,verbose=1)<br/>grid.fit(x_train_sm,y_train_sm)<br/><br/>print("-----"*20)<br/>cv_result = pd.DataFrame(grid.cv_results_).sort_values(by='mean_train_score',ascending=<strong class="lw hj">True</strong>)[:70]<br/>param_list = list(cv_result['params'])<br/>param_index = np.arange(70)<br/>plt.figure(figsize=(18,6))<br/>plt.scatter(param_index,cv_result['mean_train_score'])<br/>plt.plot(param_index,cv_result['mean_train_score'],label='Train')<br/>plt.scatter(param_index,cv_result['mean_test_score'])<br/>plt.plot(param_index,cv_result['mean_test_score'],label="CV")<br/>plt.title('Hyperparameter vs accuracy')<br/>plt.grid()<br/>plt.legend()<br/>plt.xlabel('Hyperparametr combination Dict')<br/><em class="mg">#plt.xticks(rotation=90)</em><br/>plt.ylabel('Accuracy')<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nz"><img src="../Images/b318dd2ebd3fa1db82a6d83c9ebbccc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ZPG_GcQ5rij9A4KqIyITg.png"/></div></div></figure><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="8653" class="ma ki hi lw b fi mb mc l md me">best_parameters = param_list[52] <br/>print(best_parameters)</span></pre><p id="58a8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">{'colsample_bytree': 0.7，' learning_rate': 0.1，' max_depth': 10，' n_estimators': 100}</p><p id="e0f8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用上述超参数值创建一个新的XgBoost模型。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="6257" class="ma ki hi lw b fi mb mc l md me">clf_xgb = XGBClassifier(learning_rate=best_parameters ['learning_rate'],max_depth=best_parameters['max_depth'],<br/>n_estimators=best_parameters['n_estimators'], colsample_bytree=best_parameters['colsample_bytree'], eval_metric='mlogloss')<br/>clf_xgb.fit(x_train_sm,y_train_sm)<br/><br/>result5 = ["5.","XGBClassifier","Balanced using SMOTE"]<br/>y_pred_tr = clf_xgb.predict(x_train_sm)<br/>print('Train accuracy XGB: ',accuracy_score(y_train_sm,y_pred_tr))<br/>result5.append(round(accuracy_score(y_train_sm,y_pred_tr),2))<br/><br/>y_pred_test = clf_xgb.predict(x_test_merged)<br/>print('Test accuracy XGB: ',accuracy_score(y_test,y_pred_test))<br/>result5.append(round(accuracy_score(y_test,y_pred_test),2))<br/><br/>recall = recall_score(y_test,y_pred_test)<br/>print("Recall Score: ",recall)<br/>result5.append(round(recall,2))<br/>print("-----"*20)<br/><br/>cm = confusion_matrix(y_test,y_pred_test)<br/>ax=plt.subplot();<br/>sns.heatmap(cm, annot=<strong class="lw hj">True</strong>, fmt='d', linewidths=2, linecolor='black', cmap='YlGnBu',ax=ax)<br/>ax.set_xlabel('Predicted')<br/>ax.set_ylabel('Actual')<br/>ax.set_ylim(2.0,0)<br/>ax.set_title('Confusion Matrix')<br/>ax.xaxis.set_ticklabels(['Neg','Pos'])<br/>ax.yaxis.set_ticklabels(['Neg','Pos'])<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es oa"><img src="../Images/d119e551bdd961aca834d2c89d1ed0aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*7L5oauxgFcRje2a31TLtMQ.png"/></div></figure><p id="ba0f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mg">使用SMOTE我们得到了最高的召回分数87% </em> <strong class="iw hj"> <em class="mg">但是测试准确率较低</em> </strong> <em class="mg">。训练和测试精度之间的高差距表明模型正遭受</em> <strong class="iw hj"> <em class="mg">高方差</em> </strong> <em class="mg">。我们可以在以后的工作中通过调整SMOTE来解决这个问题。</em></p><h1 id="b6cb" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论:</h1><p id="ce94" class="pw-post-body-paragraph iu iv hi iw b ix lf iz ja jb lg jd je jf ls jh ji jj lt jl jm jn lu jp jq jr hb bi translated">我总是发现在最后以表格形式显示结果更好。PrettyTable是创建这种表格的好方法。</p><pre class="lm ln lo lp fd lv lw lx ly aw lz bi"><span id="93ea" class="ma ki hi lw b fi mb mc l md me">Result_table = PrettyTable(["S.No.","Model","class imbalance status","Train_accuracy","Test_Accuracy","Test_Recall_score"])<br/>Result_table.add_row(result1)<br/>Result_table.add_row(result2)<br/>Result_table.add_row(result3)<br/>Result_table.add_row(result4)<br/>Result_table.add_row(result5)<br/>print(Result_table)</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ob"><img src="../Images/a9c2b8614e9dcee66b9babc7151067ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J40X9a009NT7UtmB0OOllg.png"/></div></div></figure><ul class=""><li id="19d9" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">当使用SMOTE平衡数据时，XGBClassifier(模型5)的测试召回分数最高，但这会导致高方差。</li><li id="bccb" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">当使用scale_pos_weight参数在分类器中平衡数据时，XGBClassifier(模型4)提供了最高的测试准确性和体面的召回分数。</li><li id="ed0e" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">面临的挑战是减少类别不平衡，同时在不过度拟合的情况下获得高召回分数。这可以通过获取更大的数据集并对分类器和SMOTE进行更严格的超参数调整来实现。</li></ul><p id="20e5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">— — — — — — — END — — — —</p><p id="0b88" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">源代码:<a class="ae lk" href="https://github.com/S-G-001/customer_churn_prediction" rel="noopener ugc nofollow" target="_blank">https://github.com/S-G-001/customer_churn_prediction</a></p></div></div>    
</body>
</html>