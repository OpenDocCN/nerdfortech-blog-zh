<html>
<head>
<title>A First look at Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习初探</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/first-look-at-reinforcement-learning-67688f36413d?source=collection_archive---------14-----------------------#2021-06-27">https://medium.com/nerd-for-tech/first-look-at-reinforcement-learning-67688f36413d?source=collection_archive---------14-----------------------#2021-06-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7f4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在机器学习中听到的一种学习类型是强化学习，其中代理通过奖励和惩罚在已知或未知的环境中学习目标。与监督和非监督学习等学习方法不同，强化学习完全不需要数据。在我的课程CS4100中，该课程简要介绍了这种学习方法的实践，所以我想进一步探索。强化学习的许多应用是在游戏或复杂且计算昂贵的现实世界问题中，所以很难找到“有意义地”应用强化学习的东西。尽管如此，这仍然是一个超级有趣的话题，在这个话题中，一个代理可以在不知道其环境的情况下建立一个最大化回报函数的策略。我想探索强化学习的高层次视图，所以我将使用OpenAI的健身房库，其中有预构建的环境，具有定义的观察和行动空间，并为不同的状态分配奖励值。</p><p id="8a71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我首先处理的一个简单任务是FrozenLake游戏。游戏很简单，环境是一个4×4的平铺方块，玩家从S方块开始，导航到G方块。掉进一个洞，或H方块意味着游戏结束。这个游戏在健身房图书馆的实现有一个简单的奖励功能。如果代理达到目标，则为1；如果没有达到目标，则为0。行动空间是四个基本方向，观察空间是玩家可能在的16个方格。这个游戏也有一个陷阱:地板很滑，你有可能每一步都朝一个随机的方向走。然而，为了查看构建的策略是否有效，我将关闭滑动模式。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/a7d9004292a31a926b0e9caa312c5a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*YOvEDDaGFCaBCPpT.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">不滑的冰湖</figcaption></figure><p id="557c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么代理人是如何制定政策的呢？有许多不同的方法来实现强化学习，如价值或政策迭代，解决MDP，TD-learning，但我将探索Q-learning。</p><p id="90a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Q-learning中，代理希望最大化“Q”值，这是从特定状态和动作映射到的值。开始时，Q表将被初始化，其中Q(s，a)的基数是<em class="jp">动作空间</em> × <em class="jp">观察空间</em>。代理人在许多集里玩这个游戏，并在此过程中建立q表。</p><p id="3688" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，代理人选择一步棋。一个策略，我将使用ε贪婪算法，在这个算法中，一个代理会选择一个随机行动。否则，它将选择Q值最高的动作。ε是勘探与开发之间的平衡，通常，存在ε衰减因子，勘探的机会随着迭代次数的增加而下降。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jq"><img src="../Images/40c01bb8b9880a1200fde17a7a3e3335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DzP5kD__wc1hZs1F.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">ε贪婪算法</figcaption></figure><p id="2cc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理选择操作后，我们将使用下面的等式，以便代理可以“学习”。在等式中，max_a Q(S_t+1，a)是代理执行其动作后下一个状态的最佳动作的Q值。Q(S_t，A_t)是当前状态的q值。下一个状态的q值乘以γ，γ是一个折扣因子，因为它在一个时间步长之外，这是一个未来奖励的价值小于即时奖励的概念。R_t+1是你执行动作后的奖励。总的来说，奖励+贴现的未来效用和当前效用之间的差额就是代理人认为当前q值应该是多少。目标值与alpha相乘，alpha是将当前q值向“真实”q值推进的学习速率，代理将随着时间的推移学习“真实”q值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jv"><img src="../Images/e23b227e68b3f98c200e8b43b6e56212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E0bJTTdeJ_5CKEH1.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">更新方程式</figcaption></figure><p id="cdbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在让代理学习了10，000集之后，我们建立了一个为每个状态选择最佳动作的策略。请注意，由于没有光滑地板的FrozenLake是确定性的，所以这可以用动态规划来解决。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jw"><img src="../Images/f6d173af71c864edb547f8cc1948d9fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HIC4snJqIYsgOucU63A_bg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">我们的政策经过10000次反复。每个字母对应一个动作:下、上、左、右。</figcaption></figure><p id="584e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">绘制每集的奖励给了我们一个看起来非常奇怪的图表。因为奖励是二元的，要么你达到目标，要么你没有达到目标。你可以看到这个谜题在4000集左右就被解开了。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/2fa3720631b61c41f3432289d7be4748.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*b60nGyUbE5RukSQB-0w-oA.png"/></div></figure><p id="5d76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然所有这些都很简单，计算成本也很低，但FrozenLake问题只需要一个4*16 = 64大小的q表。如果我们想在更复杂的环境中使用Q-learning呢？让我们以横竿问题为例。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jy"><img src="../Images/d8e4fbf956055ce5b444ff8d082d1354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*SWGYqDqsRSR8w6qS.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">OpenAI健身房的钢管舞-v1</figcaption></figure><p id="7313" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CartPole问题是一个游戏，在这个游戏中，一根杆子被放在一辆可以向左或向右推的车上保持平衡。对于这个问题，可能采取的措施只是施加+1或-1的力。然而，观察空间是连续的。观察空间中的给定状态由小车位置、小车速度、极角和极角速度组成。在这种情况下，q表不起作用。这就是深度Q学习的用武之地。</p><p id="3ac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在深度Q学习中，我们可以将Q(s，a)视为一个函数，而不是查找表Q(s，a)。如果我们回顾一下Q更新方程，它类似于神经网络如何学习。回报+贴现的未来效用这个“目标”就是q值应该是多少。Q(S_t，A_t)是模型认为的当前q值。它们之间的差别就是损失，我们可以用学习率慢慢更新网络的权重。使用神经网络，我们可以通过提供状态和目标q值来拟合网络，当我们使用网络进行预测时，网络将返回每个动作的动作空间和q值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jv"><img src="../Images/e23b227e68b3f98c200e8b43b6e56212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E0bJTTdeJ_5CKEH1.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">更新方程式</figcaption></figure><p id="e03e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">掷骰子游戏的奖励系统很简单。对于柱子直立的每一个时间步，你将得到+1。在尝试为CartPole游戏实现深度Q学习后，尽管运行了100集的算法，我还是得到了平均10分。事实上，一段时间后，ε衰减导致ε接近0，因此Q函数收敛到一个次优策略。问题是，当我们用最新的状态拟合网络时，模型会慢慢“忘记”过去的障碍，并回到起点。虽然模型确实学会了如何越过某个障碍，但后来的状态不会遇到该障碍，因此网络的权重将从解决方案转移到该障碍。</p><p id="95b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对此的解决方案是体验重放。我们保留过去状态和动作的“记忆”,并重放这些记忆以保持模型了解其过去。体验重放的实现将是保持(状态、动作、奖励、新状态、isOver)元组的阵列，其中网络适合随机的一批状态，而不仅仅适合最新的状态。使用这种方法，代理在测试策略时，在50次行动中平均得分为430.26。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/0c583c466c1ac0e6965daa5dfd5c6acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*BNWxeZnnvsQk0yioPSQn6A.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">特工训练超过150集</figcaption></figure><p id="3a6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来对比一下随机播放，平均21.48分。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/5918ab86fcfe53b393c324c946555d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*ryf8SngqXlFUzeSridGSIA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">代理人玩随机行动策略</figcaption></figure><p id="2f90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们把这比作人类的游戏。我在45次游戏中平均得分为30.8，但与随机游戏不同，人类游戏会随着时间的推移而“学习”。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/b018cc5ad01d1c8e59031425a02d917a.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*WfdWB2tFTkt-LQ83aCTIxw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">人类游戏</figcaption></figure><p id="0d43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们对比人类游戏和深度q学习的学习速率，我们可以观察到机器的学习速率更快！很明显，人类游戏的基准只有一个玩家，那就是我，所以这不是对人类表现的准确衡量。让我们看看是否可以改进我们的强化学习算法。我们可以调整的几件事是批量大小，以允许更长的“记忆”，奖励系统，我们神经网络的超参数，可能更多，但这里我们要改变奖励系统。我们将缩放奖励以匹配极角的范围，而不是简单的每个时间步长+1。CartPole-v1在极点角度超过[-0.418，0.418]弧度后终止，因此我们将让reward = 1-|a/0.418|以弧度表示极点的角度，以便为更加垂直的极点角度赋予更大的权重。结果如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/7cc3db06c91a439cf2ce61af6a21ccd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*HSBIBtEl5nWOY6dfVEnrag.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">代理人在玩新的奖励系统</figcaption></figure><p id="ad66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看起来性能好多了。请注意在10集之后，评分从未低于100，相比之下，原始策略的评分在120集之后仍会偶尔暴跌。我假设这可能是因为极点被激励保持在0弧度，所以极点下降的空间更小，而旧的奖励系统仍然给出相同的奖励，如果极点即将下降但在界内。在50场比赛中，我们测试了这一策略，平均得分为489.88。这比我们旧系统的平均值高出整整50分。</p><p id="2c47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，这是对强化学习的有趣的初步观察。在未来的帖子中，我可能会尝试查看一些更复杂的环境，如OpenAI健身房中的Atari游戏，或者尝试自己编写一个环境。当我浏览健身房的环境时，有一个Box2D部分，那里有一堆用Box2D物理库模拟的场景。这是一个惊喜，因为我在创建RPGConcept时曾与Box2D合作过(参见我2018年的帖子)！也许我会用Java来制作我的环境。最终，我希望能够在《我的世界》使用Malmo T1做一些事情，这是微软在《我的世界》上建立的一个人工智能实验平台。</p><p id="dec1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码:<a class="ae ka" href="https://github.com/chengxi600/RLStuff/blob/master/Q%20Learning/QLearning_DQN.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Q % 20 learning/QLearning _ dqn . ipynb</a></p></div></div>    
</body>
</html>