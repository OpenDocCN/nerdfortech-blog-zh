<html>
<head>
<title>NLP Zero to One: LSTM Part(9/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 到 1:LSTM 部分(9/30)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-lstm-part-9-40-98e8cc4c296d?source=collection_archive---------23-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-lstm-part-9-40-98e8cc4c296d?source=collection_archive---------23-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="c216" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">存储单元、单元状态和栅极层</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/13cb4d2d7f08f9a8e08d4207d3a29acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29k1qFmQYy4lGGWxnmDmKw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="a57d" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="f69b" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">在上一篇博客中，我们讨论了香草 RNN 架构，也讨论了它的局限性。消失梯度是一个非常重要的缺点，它限制了 RNN 对较短序列的建模。在相关输入事件和目标信号之间存在大于 5-10 个离散时间步长的时间滞后的情况下，传统的 RNNs 无法学习。这基本上限制了香草 RNN 应用于许多实际问题，特别是 NLP，因为句子中的单词数通常远远超过 10 个。</p><p id="b385" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">长短期记忆(LSTM)是一种流行的递归神经网络结构的更好的变体，专门设计用于解决消失梯度问题。LSTM 调整了 RNN 循环单元的内部结构，为消失的渐变带来了一个解决方案。LSTM 在翻译和序列生成方面获得了巨大的成功。在这篇博客中，我们将讨论 LSTM 的神经结构。如果你不熟悉 RNN，请参考我以前的博客。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/b271e25be66c728067c4d9df079d6e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gG3quYkDsh7g3pWgSDF-xA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">LSTM 神经架构</figcaption></figure><h1 id="c96a" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">神经架构</h1><p id="3277" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">LSTM 和我们所了解的 RNN 有很多相似之处，它有一个类似于循环神经网络的控制流程。在 RNN 的理论中，当进行反向传播时，信息(隐藏状态/梯度)在时间步长上不间断地传递。LSTM 所做的只是利用简单的门来控制递归网络单元中的梯度传播。通过不同的门，LSTM 存储单元处理数据，在信息向前传播时传递信息。让我们看看这个信息是如何在 LSTM 记忆单元中处理的，首先让我们定义细胞状态，然后我们将定义用于处理信息的门。</p><h1 id="39d4" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">细胞状态</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/5327b52d3e1f246857109b1c34d9d0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*Fz5awjOwFQLGdR8_ve8IMA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">细胞状态图解</figcaption></figure><p id="4ba5" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">细胞状态就像网络的流动记忆。你可以把细胞状态想象成一条传送带，在不同的时间点上传送相关的信息。这是我们的记忆单元在每个时间点看到的信息，并且该细胞状态“Ct”可以被视为模型在时间“t”之前看到/学习的内容的总结。它沿着整个序列直线运行，只有一些小的线性相互作用，正如你在图中看到的。这种细胞状态的概念可以使来自较早时间步骤的信息能够进入较晚的时间步骤，从而减少短期记忆的影响。</p><h1 id="b9e3" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">盖茨..</h1><p id="d4ff" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">这些门是不同的神经网络，决定哪些信息被允许遗忘、忽略或保留在记忆细胞中。门控机制本身是神经网络层，因此 LSTM 记忆单元中的门从它看到的数据中学习。让我们仔细讨论每个门。</p><h2 id="5ec2" class="lj jo hi bd jp lk ll lm jt ln lo lp jx ko lq lr jz ks ls lt kb kw lu lv kd lw bi translated">1.忘记栅极层</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/fcd894f224d6e5a84210864e547301a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPsZvLJH4wFsajOkcGutVw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">忘记栅极层</figcaption></figure><p id="d278" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">首先，我们想要确定它在时间步长“t”之前已经学习的信息(“Ct”)在多大程度上是有用的。忘记门层负责这个操作。<br/>遗忘门层接收“ht1”和“xt”，并具有从<strong class="kh hj"> 0 </strong>到<strong class="kh hj"> 1 </strong>的输出。我们可以强制这个输出，因为它是一个 sigmoid 函数。<strong class="kh hj"> 1 </strong>表示“完全保持小区状态 Ct-1”，0 表示“完全摆脱 Ct-1”</p><h2 id="5295" class="lj jo hi bd jp lk ll lm jt ln lo lp jx ko lq lr jz ks ls lt kb kw lu lv kd lw bi translated">2.输入栅极层</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/bc75e11e56cb088a0c567ec9124088b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nGU8OTbHSyT75DKJg7HQQQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">输入栅极层</figcaption></figure><p id="086a" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">接下来，在每个时间步长“t ”,我们的存储单元接收一个输入“xt”。问题是输入“xt”在多大程度上被存储到单元状态？。这个输入门层决定了我们在单元状态中保留多少新信息“xt”。这分两部分发生。</p><p id="860c" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated"><strong class="kh hj"> 1。</strong>双曲正切层，采用“ht1”和“xt”并创建候选值“C̃ t”，可添加到单元状态。<br/> <strong class="kh hj"> 2。</strong>一个 sigmoid 层，接受“ht1”和“xt”并输出从 0 到 1 的值，决定我们允许多少候选值，“C̃ t”。<br/>在这样的 2 个门之后，旧的单元状态“ct1”现在更新为新的单元状态“Ct”。</p><h2 id="17c3" class="lj jo hi bd jp lk ll lm jt ln lo lp jx ko lq lr jz ks ls lt kb kw lu lv kd lw bi translated">3.输出栅极层</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/1128db7fd9d508c1bbe3d0296d849ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-6anon1HXbOyKTaz2y3tg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">输出栅极层</figcaption></figure><p id="ad71" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">最后，我们需要决定我们在时间 t，“ht”的隐藏状态。该输出将是更新的单元状态“Ct”的函数。这也分两步进行</p><p id="d39c" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated"><strong class="kh hj"> 1。</strong>一个 tanh 层，以“Ct”为输入，输出一个矢量“V”<br/><strong class="kh hj">2。</strong>一个 sigmoid 层，它接受“ht1”和“xt”并输出从 0 到 1 的值，这些值是向量“V”的倍数，以获得输出“ot”或隐藏状态“ht”。</p><h1 id="60ea" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">完整存储单元</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/b271e25be66c728067c4d9df079d6e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gG3quYkDsh7g3pWgSDF-xA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">LSTM 神经架构</figcaption></figure><p id="95d8" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">LSTM 的训练过程与 RNN 的训练过程非常相似。需要再次强调的是，LSTM 存储单元中的门也是参数化的，因此这些参数也是用梯度下降优化方法调整/更新的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="a340" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">接下来:<a class="ae mb" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-bi-directional-lstm-part-10-30-cab0eab65533?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零比一:双向 LSTM 部分(10/30) </strong> </a> <br/>上一篇:<a class="ae mb" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-recurrent-neural-networks-basics-part-8-30-ca77af9d47ff?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零比一:递归神经网络基础部分(8/30) </strong> </a></p></div></div>    
</body>
</html>