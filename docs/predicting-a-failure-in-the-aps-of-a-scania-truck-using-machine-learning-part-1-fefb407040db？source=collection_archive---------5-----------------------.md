# 使用机器学习预测斯堪尼亚卡车 APS 的故障—第 1 部分

> 原文：<https://medium.com/nerd-for-tech/predicting-a-failure-in-the-aps-of-a-scania-truck-using-machine-learning-part-1-fefb407040db?source=collection_archive---------5----------------------->

![](img/3b2ba269d9de39a4e4d7117f8722a380.png)

里卡多·戈麦斯·安吉尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

一辆卡车有许多子系统。一个这样的子系统被称为 APS 或*气压系统(APS)。*APS 负责为卡车内部提供必要的空气压力。现代重型卡车使用空气制动器，而不是其他轻型车辆中使用的传统液压制动器。这些制动器需要持续供应压缩空气来保持分离，这样车辆才能继续行驶。如果由于任何情况，这种压缩空气的供应受到损害，制动器将不再分离，卡车将停止。当这种情况发生时，车主必须派出维修车辆来诊断和修理故障。然而，APS 广泛分布在卡车和拖车上，由大量的管道组成，这些管道实际上提供空气。这使得很难诊断问题是否与接入点有关。这花费了车队所有者大量可以节省的时间和金钱。

因此，拥有能够预测卡车中的故障是否是由 APS 引起的机器学习算法/系统将对相关人员非常有帮助，因为它可以在一定程度上减少停机时间和在故障中花费的总金钱。

这个问题的端到端解决方案将由两部分组成。在第一部分，我们将讨论问题的基础，如问题陈述、约束、性能指标等，并执行 EDA 和数据集的数据预处理。 *在第二部分中，我们将在处理后的数据上训练实际的机器学习模型，然后选择最佳模型，以 webapp 的形式部署在 AWS EC2 实例上，使用*[*Streamlit*](https://docs.streamlit.io/en/stable/)*制作。读完第二部* [*这里*](https://amansavaria.medium.com/predicting-a-failure-in-the-aps-of-a-scania-truck-using-machine-learning-part-2-271814dfebdd) *。*

# 内容:

1.  机器学习的问题公式化
2.  业务限制
3.  数据集描述和获取数据
4.  性能指标
5.  入门指南
6.  探索性数据分析
7.  缺失值插补
8.  数据标准化

# ML 的问题公式化

这个问题可以被视为一个简单的二进制分类模型，其中*正类意味着卡车中的问题是由于 APS* 中的故障造成的，而*负类则意味着其他原因。*因此，给定一个涉及特定故障的新数据点，我们需要对故障是否是 APS 故障的结果进行分类。

# 业务限制

*   **延迟:**在获得数据后进行预测所需的时间必须相当短，以避免任何不必要的维护时间和成本的增加。
*   **错误分类的成本:**错误分类的成本非常高，尤其是错误地将一个正类数据点分类，因为这可能导致卡车完全损坏，并导致一些严重的成本。在这里进行错误的分类只会浪费修理工的时间，因为他们会因为相信模型的分类而在错误的地方寻找问题。

# 数据集描述

该数据集是作为 **IDA-2016 工业挑战**的一部分提供的，包含从日常使用的斯堪尼亚卡车上获取的读数，由斯堪尼亚自己收集和提供。由于专有原因，所有功能的名称都是匿名的。

此案例研究的数据集可在此处找到:

[https://archive . ics . UCI . edu/ml/datasets/APS+Failure+at+Scania+Trucks](https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks)

数据集分为两部分，一个**训练集** 和一个**测试集**。列车组包含**60000 行**，而测试组包含**16000 行。**数据集中有 171 列，其中一列是数据点的类标签，导致每个数据点有 170 个特征。这些特征是数字特征和从直方图箱构建的特征的组合。所有的特征本质上都是数字的。仅通过查看数据集，就可以看出这是一个高度不平衡的数据集***，其中有**59000 个**属于**负类**，只有**1000 个**属于**正类**。另一件要注意的事情是，数据点包含许多缺失值。有些特性甚至有**超过一半**的值在其中缺失。训练和测试数据集中的许多数据点至少包含一个缺失值。***

# ***性能指标***

***使用以下性能指标:***

*   *****错误分类成本:**该指标是竞赛的一部分。它由下式给出:***

> *****误分类成本:(cost_2 x FN) + (cost_1 x FP)*****
> 
> *****其中，成本 _1 = 10，成本 _2 = 500*****

***此指标表示由于模型的错误分类，车队所有者将承担的成本。在这种情况下，成本 _1 指的是需要在车间由机械师进行不必要的检查的成本，而成本 _2 指的是错过故障卡车的成本，这可能是未来发生故障的成本。***

*   *****混淆、精确和召回矩阵:**混淆、精确和召回度量将告诉我们基于每个类的模型的性能，这是非常重要的，因为数据集如此偏向于负类。***
*   *****F-1 分数:**单个指标中的 F-1 分数可以根据模型的精确度和召回率告诉我们模型的整体性能，使用单个数字比矩阵更容易解释。我们将使用**宏观平均 F-1 分数**，因为它考虑了两个级别的 f1 分数，因此可以告诉我们模型在两个级别的整体性能。***

# ***入门指南***

***首先，让我们从导入库和加载数据开始。***

***导入库和加载数据***

# ***探索性数据分析***

***首先，我们将分析关于完整数据集的非常基本的东西，例如数据的类别分布等。然后再讨论特性本身的实际 EDA。***

## *****首先来看看数据的阶级分布:*****

***数据的分类分布。***

***我们可以看到，在数据集中的**60000 个**数据点中，**59000 个(~98.3%)** 属于**负类**，只有**1000 个(~1.7%)** 属于**正类。**因此，数据集非常不平衡，因此我们将需要使用技术来抵消这种不平衡，否则我们将在正类上获得非常糟糕的性能。***

*****现在，让我们看看有多少特征是数值特征，有多少是基于直方图的特征:*****

***数据集中的要素分布。***

***我们可以看到，有 7 个直方图，从这些直方图中已经构建了不同的直方图特征，并且对于每个直方图特征，我们有 10 个箱，因此在 170 个特征中总共有 70 个特征是直方图特征，而剩余的 100 个特征是数值特征。***

## *****现在，我们来分析一下数据集中缺失的值:*****

***在查看数据时，我们意识到数据集中有相当多的缺失值。现在，让我们实际分析这些缺失的数据，看看数据集中到底有多少缺失值。因为我们知道要素中有很多缺失值，所以我们将分析每个数据点和每个要素的缺失值。***

***2.**每个特性的缺失值分析:*****

***首先，让我们看看有多少特性没有缺失值及其名称。***

***没有缺失值的要素。***

***我们可以看到，只有一个特征没有缺失值，即 *aa_000。****

***现在，让我们通过获得任何特征中缺失值的最小和最大数量以及特征中缺失值数量的分布来做一些进一步的分析。***

***对于要素 bt_000，我们可以找到的最少缺失值数是 167，而对于要素 br_000，最大缺失值数是 49264，这是数据集的大部分。由于该值较大，每个特征的平均值约有 5000 个缺失值，几乎是中值 688 的 9 倍。这可能表明有些要素缺少大量的值。
进一步的分析证实了这一假设，我们发现只有 28 个特征的缺失值大于平均值，其中大多数特征在 60000 个训练数据点中至少有 9550 个缺失值，这是一个很大的百分比。***

***现在，让我们通过绘制图表来查看每个特性的缺失值的实际数量。***

***每个要素中缺失值的数量。***

***大多数要素的缺失值不到 5%,其他一些要素的缺失值在 5-25%之间，少数在 25-50%之间。但是，有些要素有超过 70%的缺失值。基于这个缺失值的百分比，我们将制定我们的插补策略。***

## ***3.EDA 上的特性***

***现在，让我们对实际功能进行 EDA。由于有两种类型的特征，即数字和直方图特征，我们将对这两种特征分别执行 EDA。然而，我们有 100 个数字特征和 70 个直方图特征。因此，在所有这些方面进行 EDA 是不可能的。因此，我们将从这两种类型中选择前 15 个特性，然后对它们执行 EDA。我们将使用 RandomForestClassifier 给出的*特征重要性来获取重要特征。****

****功能选择* **:*****

```
*def top_important_features(data, labels, top_x=15, verbose=10):
    '''
        This function uses random forests to get feature importances of the features and returns top_x important features
        and their feature importances.
        Returns:
            tuple of (features, importances).
    '''
    # training a random forest
    rf = RandomForestClassifier(n_estimators=200, n_jobs=-1, verbose=verbose, random_state=42)
    rf.fit(data, labels)

    # get the feature importances
    feat_imp = rf.feature_importances_
    imp_ind = np.argsort(feat_imp)[::-1] # getting the indices in decreasing order of importance
    top15_ft = data.columns[imp_ind][:15]
    top15_imp = feat_imp[imp_ind][:15]

    return(top15_ft, top15_imp)*
```

****对给定特征进行单变量分析的功能:****

```
*def univariate_analysis(features):
    """
        This function takes a list of features and performs univariate analysis on them by plotting CDF, PDF, Boxplots and 
        printing mean, standard deviation and median for that feature.
    """
    for ft in features:
        print('---UNIVARIATE ANALYSIS OF', ft, '---')
        values = train_eda[ft]
        values0 = train_eda[train['class']=='neg'][ft]
        values1 = train_eda[train['class']=='pos'][ft]
        desc_neg = values0.describe() # for printing the mean and standard deviation values of the feature for individual classes
        desc_pos = values1.describe()print("FOR NEGATIVE CLASS:- 1\. Mean:", round(desc_neg['mean'], 3), '2\. Standard Deviation:', round(desc_neg['std'], 3), '3\. Median:', round(desc_neg['50%'], 3))
        print("FOR POSITIVE CLASS:- 1\. Mean:", round(desc_pos['mean'], 3), '2\. Standard Deviation:', round(desc_pos['std'], 3), '3\. Median:', round(desc_pos['50%'], 3))# plots
        fig, ax = plt.subplots(ncols=3, figsize=(18,6))
        sns.kdeplot(values0, ax=ax[0], shade=True, label='Neg')
        sns.kdeplot(values1, ax=ax[0], shade=True, label='Pos')
        sns.kdeplot(values0, ax=ax[1], cumulative=True, label='Neg')
        sns.kdeplot(values1, ax=ax[1], cumulative=True, label='Pos')
        sns.boxplot(data=train, x=Y, y=ft, ax=ax[2])ax[0].set_title('PDF of '+ft+' for Class 0')
        ax[1].set_title('CDF of '+ft)
        ax[2].set_title('Boxplot of '+ft)
        ax[0].legend()
        ax[1].legend()
        plt.show()
        print('-'*100)*
```

*   *****EDA 上的数字特性:*****

***首先，让我们选出前 15 个数字特征。***

***现在我们已经得到了前 15 个特征，让我们对它们进行单变量分析。***

> ***15 个特征中的大部分都有一个总的趋势。与负类相比，正类值的分布更广。负类具有相对较小的分布，如大多数要素的密集 PDF 和非常陡峭的 CDF 所示***
> 
> ***平均值、标准偏差和中值都遵循上述趋势，所有这些特征的所有三个值都非常大，大约是正类的负类的 10 或 100 或有时甚至 1000 或更多倍。***
> 
> ***特征 **ci_000、aq_000、bj_000、ck_000、aa_000、dn_000、cq_000、ap_000、by_000、bx_000、bt_000、bb_000** 具有正类和负类的独立 iqr，如方框图所示。与负类相比，正类的 CDF 和 pdf 也更加分散。***
> 
> ***对于特征， **ai_000、al_000 和 am_0** ，超过 95%的数据点具有等于或非常接近 0 的负类值。***

***现在，让我们看看所选特性与类别标签的相关性。***

***重要特征与类别标签的相关性分析。***

> ***特征 **ci_000、aq_000、bj_000、ck_000、aa_000、dn_000、cq_000、ap_000、by_000、bx_000、bt_000、bb_000** 在+0.5 附近具有更高的相关性，这解释了为什么正类数据点比负类数据点具有更高的正值。***
> 
> ***从它们的单变量分析来看，具有大约 0 的大部分值的特征 **am_0、al_000 和 ai_000** 具有小于 0.4 的较小相关值。特征 **ai_000** 在 0.11–0.13 附近的所有特征中相关性最小。***

***让我们通过对特性 **ai_000** 和其他顶级特性进行二元分析，来获得更多关于特性**的信息。*****

***特征的二元分析 *ai_000****

> ***对于大多数数据点，特征 **ai_000** 的值等于 0。有一些数据点对于两个类都有非零值。***
> 
> ***该特征可以与其他重要特征一起用于预测。其他特征试图在其自身的轴上将两个类别分开，因为可以看出，对于大多数图来说，与负类别点相比，正类别点(橙色)在 x 轴上具有更高的值。***

*   *****直方图上的 EDA 特征*****

***首先，让我们选择直方图特征的前 15 个特征。***

***直方图特征中的特征选择***

***现在我们已经得到了前 15 个特征，让我们对它们进行单变量分析。***

***直方图特征的单变量分析***

> ***直方图特征也遵循同样的趋势。一般来说，与负类数据点相比，正类数据点具有更大的平均值、中间值和标准偏差，并且具有更高的分布。***
> 
> ***对于特征**，ag_002、ag_001、cn_000、cn_001、az_000 和 ay_005** 有超过 95%的特征具有非常小的值(相对而言)为负类。***
> 
> ***此外，特征 **ag_002、ag_003、ag_001、cn_000、cn_001 和 ay_005** 的中值为 0，表明对于负类，至少 50%的值等于 0。***
> 
> ***从重要度图中可以看出，最重要的特征小于或等于 0.03，只有两个特征，即 **ag_002 和 ee_005** 的值分别大于 0.06 和 0.05。***

***现在，让我们看看所选特性与类别标签的相关性。***

***直方图特征与类别标签的相关性分析。***

> ***特征 **ee_005，ag_003，ba_000，cn_001，ee_000，ba_003，cs_004，ba_004** 相关性大于等于 0.4。***
> 
> *****ee_005** 相关系数值最高，在 0.49 左右。***
> 
> *****ag_001 和 az_000** 相关系数最低，分别为 0.18 和 0.2 左右。***

***由于 *ag_001 和 az_000* 相关性最小，让我们使用散点图对这两个特征进行双变量分析，看看使用两个特征是否能改善什么。首先，让我们做一下 *ag_001* 的二元分析。***

> ***对于所有数据点来说， **ag_001** 的大多数值都相对较小。***
> 
> ***这个特性的较大值都是针对正类的。***

***现在，是时候为特性 *az_000 做同样的事情了。****

***特征 az_000 的二元分析***

> ***特性 **az_000** 对于这两个类来说有很多交集。***
> 
> ***对于此功能，大多数值都很小。***

*   *****所有特征的多元分析。*****

***为了进行*多变量分析，*我们将执行 ***t_SNE*** 并绘制结果。***

***所有特征的 t-SNE 图***

> ***从 t_SNE 图中，我们可以看到负类点到处都是。然而，它们是在定义明确的集群中。***
> 
> ***然而，正如我们在图中看到的，正类数据点也在聚类中，但是这些聚类与负类聚类相交很多。***
> 
> ***在 2D，有很多的混合，但是考虑到这些点存在于清晰可见的集群中，在更高的维度中，很有可能在两个类的集群之间得到分离。***

# ***缺失数据插补***

***对于输入缺失数据，我们将使用以下策略:***

*   ***或者缺失值少于 5%的特征，我们将进行均值插补。***
*   ***对于缺失值在 5%和 15%之间的特征，我们将使用中位数插补，因为它对异常值具有鲁棒性。***
*   ***对于缺失值在 15-70%之间的特征，我们将使用基于模型的插补。为此，我们将使用两种不同的技术***

> *****MICE** -通过链式方程进行多重插补是一种处理数据集中缺失数据的可靠、信息丰富的方法。该过程通过一系列迭代预测模型来估算数据集中的缺失数据。要了解更多关于老鼠的知识，请参考[这个](https://www.youtube.com/watch?v=WPiYOS3qK70&ab_channel=RachitToshniwal)。***
> 
> ***基于 KNN 的插补***

*   ***对于缺失值超过 70%的要素，我们会将其从数据集中完全删除。***

***现在让我们执行缺失值插补。首先，我们将丢弃缺失值超过 70%的特征，执行均值插补和中值插补。***

*****注意:** *strategy_list* 只是一个嵌套列表，包含 4 个不同的列表，包含基于上面定义的策略的特性名称。***

***现在，让我们对具有 15-70%缺失值的特征执行 MICE *插补*和基于 KNN *的插补*，插补所有数据，然后保存由此形成的数据帧和对象。***

***然后，我们从磁盘中加载这些对象，对测试数据集进行插补并保存它们。***

# ***数据标准化***

***既然我们已经成功地估算了所有的数据并构建了所有的要素，那么是时候执行要素缩放了。对于特征缩放，我们将执行归一化。我们将对两个数据集进行归一化处理***

***标准化数据集***

***这篇由两部分组成的博客的第一部分*到此结束。在第二篇博客中，我们将讨论建模部分和 AWS EC2 实例中最佳模型的部署。最后，总结这个解决方案。在此阅读第 2 部分。****