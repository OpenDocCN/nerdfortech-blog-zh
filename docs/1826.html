<html>
<head>
<title>Word Embedding : Text Analysis : NLP : Part-2 : Intuition behind Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入:文本分析:NLP:第2部分:Word2Vec背后的直觉</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/word-embedding-text-analysis-nlp-part-2-intuition-behind-word2vec-3057031710da?source=collection_archive---------9-----------------------#2021-04-09">https://medium.com/nerd-for-tech/word-embedding-text-analysis-nlp-part-2-intuition-behind-word2vec-3057031710da?source=collection_archive---------9-----------------------#2021-04-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b700" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大型非结构化文本数据高级技术</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/089c5143da84957c8a81fe4fa42a34de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/0*sJwIvetMjQNEX5OQ"/></div></figure><p id="f244" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的上一篇文章<a class="ae jl" rel="noopener" href="/analytics-vidhya/word-embedding-text-analysis-nlp-part-1-c6c3f161e69f">Word Embedding:Text Analysis:NLP:Part-1</a>中，我们讨论了像OneHotEncoding和TF-IDF这样的将文本数据转换成数字数据的技术。通过这种方法，我们可以很容易地将文本数据转换成机器可以理解的矢量格式，但这种方法的局限性。</p><ol class=""><li id="3c8e" class="jm jn hi ih b ii ij im in iq jo iu jp iy jq jc jr js jt ju bi translated">句子中单词的语义信息不能被存储，因为它占用了文档中单词的权重。</li><li id="0fd9" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">由于这种方法根据文档计算任何单词的权重，所以不能确定该单词在特定类别中的相关性。</li></ol><p id="bff0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果我们正在处理语音、图像或大型文本数据序列，传统方法(如具有一个热编码值的单词包或TF-IDF)将导致大量稀疏度量，这将导致ML模型的过度拟合。而且，它不能存储单词之间的语义信息。为了解决这些问题并处理长序列，我们将讨论更高级的单词嵌入方法，如基于深度学习技术的<strong class="ih hj"> Word2Vec </strong>、<strong class="ih hj"> GloVe </strong>和<strong class="ih hj"> FastText </strong>。让我们来看看上面提到的嵌入技术。</p><h1 id="2e57" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak"> Word2Vec </strong></h1><p id="1043" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">Word2Vec是谷歌在2013年发布的，用深度学习技术以密集的矢量形式表示单词。这是一种无监督的网络，它对从大量文本数据中创建为词汇表的单词进行训练，并将在向量空间中为每个单词生成嵌入。在这里，向量的维度是可以控制的，这是BOW和TF-IDF在将文本转换为高维度量时遇到的问题。</p><p id="14e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2Vec是一种使用两种不同深度学习架构生成嵌入的方法，这两种架构是:<strong class="ih hj"> Skip Gram和Continuous Bag Of Words (CBOW) </strong></p><p id="eb33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">连续单词包(CBOW)模型</strong></p><p id="ba2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法将单词的上下文作为输入，并试图根据上下文来预测单词。我们举个例子<em class="ld">“男生在学NLP”。</em>我们可以借助context_window来预测单词target _ word“boy”。如果我们取2的上下文窗口，组合将是这样的[the，is] ad在神经网络的帮助下，可以预测目标单词。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es le"><img src="../Images/4a246b5b86b00e2b92cb34776ab55f11.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/0*TKVT0vEru1Ibzq9M.png"/></div></figure><p id="5a9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该架构是单层的，输入和目标是一个热编码。输入-隐藏层和隐藏-输出层之间有两个权重。输入乘以输入隐藏权重，隐藏乘以隐藏输出权重，得到输出。计算目标值和预测值之间的误差，并在反向传播中更新权重。线性激活函数用于</p><h1 id="08be" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">跳格模型</h1><p id="52c4" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">Skip gram模型实际上以与CBOW模型相反的模式工作，它试图根据给定的目标单词来预测上下文单词。skip-gram模型的输入向量类似于CBOW模型。区别在于目标变量，因为我们要预测两边的上下文向量。关于目标变量，分别计算误差向量，并从中获得最终误差，该最终误差将被反向传播用于权重更新。为了表示单词的向量，在训练之后取输入和隐藏层之间的权重。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lf"><img src="../Images/88dd104ebb2fa684ecc146d0ce7b57b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/0*luwveYkXgqphCzMi.png"/></div></figure><p id="9e4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是跳格模型的体系结构。输入向量的维数为1xV，隐藏层的维数为VxN，输出层的维数为1xV。我们有输入向量、隐藏激活函数和隐藏层权重，在此帮助下，我们将获得2个上下文单词的输出(因为我们预测2个上下文单词)。该输出被发送到softmax层以转换成概率。误差是用实际的目标字减去我们从输出中得到的softmax值来计算的。最后，对所有向量进行逐元素求和以获得最终的误差向量，该误差向量将被传播回去以更新权重。</p><p id="7a07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跳过文法模式是在n-grams上训练的，允许跳过令牌，如下图所示。单词的上下文通过一对<code class="du lg lh li lj b">(target_word, context_word)</code>来表示，其中<code class="du lg lh li lj b">context_word</code>出现在<code class="du lg lh li lj b">target_word</code>的相邻上下文中。</p><p id="9634" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑8个单词的句子</p><blockquote class="lk ll lm"><p id="6194" class="if ig ld ih b ii ij ik il im in io ip ln ir is it lo iv iw ix lp iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">宽阔的马路在烈日下闪闪发光。</em> </strong></p></blockquote><p id="8b24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上下文可以由句子的所有8个单词的窗口大小来定义，并且窗口大小表示被认为是上下文单词的目标单词任一侧的单词跨度。在下面的例子中，我们可以看到窗口大小为2。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/faae41c2fd47336421d9010b408d1fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wb0NVwP2LV8alTG9.png"/></div></div></figure><p id="2f17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跳格模型也使用了负抽样，我们可以在上面的图中看到。在第一行中，窗口大小为2的句子中没有出现[宽，闪烁]对。</p><h1 id="2b67" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">用Gensim实现Word2Vec模型</h1><p id="15ae" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">借助pytorch、Tesnsorflow以及预训练的库，我们可以实现Word2Vec模型。在这里，我们将尝试使用Gensim库实现Word2Vec。Gensim是高效的和可扩展的，易于实现Word2Vec模型。我们将采用一个小语料库，然后我们将标记数据，并使用给定的参数，我们将建立Word2Vec模型。</p><pre class="je jf jg jh fd lv lj lw lx aw ly bi"><span id="e1ff" class="lz kb hi lj b fi ma mb l mc md">from gensim.models import word2vec<br/>import nltk</span><span id="0989" class="lz kb hi lj b fi me mb l mc md">raw_text = """Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The result is a computer capable of "understanding" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.""".split()</span><span id="b9a4" class="lz kb hi lj b fi me mb l mc md"># tokenize sentences in corpus<br/>wpt = nltk.WordPunctTokenizer()<br/>tokenized_corpus = [wpt.tokenize(document) for document in raw_text]</span><span id="18cc" class="lz kb hi lj b fi me mb l mc md"># Set values for various parameters<br/>feature_size = 100    # Word vector dimensionality  <br/>window_context = 30          # Context window size                                                                                    <br/>min_word_count = 1   # Minimum word count                        <br/>sample = 1e-3   # Downsample setting for frequent words</span><span id="d070" class="lz kb hi lj b fi me mb l mc md">w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, <br/>                          window=window_context, min_count=min_word_count,<br/>                          sample=sample, iter=100)<br/># view similar words based on gensim's model<br/>similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]<br/>                  for search_term in ['extract','information','insights']}<br/>similar_words</span><span id="9d70" class="lz kb hi lj b fi me mb l mc md"><br/>##Output </span><span id="3a25" class="lz kb hi lj b fi me mb l mc md">{'extract': ['linguistics', '"', 'of', 'including', 'a'],<br/> 'information': ['program', 'within', 'subfield', 'result', 'contained'],<br/> 'insights': ['well', ')', 'how', 'categorize', 'particular']}</span></pre><p id="c331" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们可以看到我们使用了如下参数</p><p id="5f78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特征大小:嵌入向量的维数</p><p id="e8bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">窗口:上下文窗口大小</p><p id="5693" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">min_count:最小字数</p><p id="c160" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">样本:向下采样设置</p><p id="f1b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练Word2Vec模型后我们可以看到相似的单词。在这里，我们已经采取了一个小语料库，所以没有给出完美的结果。让我们把这些相似的单词画出来，以便更好地理解。</p><pre class="je jf jg jh fd lv lj lw lx aw ly bi"><span id="0c6b" class="lz kb hi lj b fi ma mb l mc md">import numpy as np<br/>from sklearn.manifold import TSNE<br/>import matplotlib.pyplot as plt</span><span id="b9ea" class="lz kb hi lj b fi me mb l mc md">words = sum([[k] + v for k, v in similar_words.items()], [])<br/>wvs = w2v_model.wv[words]</span><span id="1408" class="lz kb hi lj b fi me mb l mc md">tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)<br/>np.set_printoptions(suppress=True)<br/>T = tsne.fit_transform(wvs)<br/>labels = words</span><span id="522a" class="lz kb hi lj b fi me mb l mc md">plt.figure(figsize=(14, 8))<br/>plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')<br/>for label, x, y in zip(labels, T[:, 0], T[:, 1]):<br/>    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mf"><img src="../Images/2e95149ba2e68c9888b2ad54cd28aa12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4B4MnYXn7XtpSYB0mCDIoQ.png"/></div></div></figure><p id="2968" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以检查任何单词的矢量表示。这里，我们将100作为一个维度，因此我们可以在100维向量中看到“自然”单词。</p><pre class="je jf jg jh fd lv lj lw lx aw ly bi"><span id="d4d0" class="lz kb hi lj b fi ma mb l mc md">w2v_model.wv['Natural']</span><span id="78fe" class="lz kb hi lj b fi me mb l mc md">##Output<br/>array([-0.00181083,  0.00448997,  0.00354504, -0.00115317, -0.0043177 ,<br/>        0.00021277,  0.00230537, -0.00092127,  0.00368612,  0.00396339,<br/>       -0.00432571,  0.00493624,  0.00061124, -0.0022428 , -0.0034374 ,<br/>        0.00289598, -0.00166694,  0.00477126, -0.00183866,  0.00382926,<br/>       -0.00392748, -0.00271961, -0.0046588 , -0.00226145,  0.00373776,<br/>       -0.00416674, -0.00012754, -0.00381866,  0.00321343, -0.00240004,<br/>        0.00363028, -0.00328013, -0.00178409, -0.00235526, -0.0009552 ,<br/>        0.0038814 ,  0.00266416, -0.00086921,  0.00155356,  0.00076463,<br/>        0.00114544,  0.00430724, -0.00231419, -0.00074246,  0.00282576,<br/>        0.00001543,  0.00400576,  0.00251022,  0.00396486, -0.00001084,<br/>       -0.00405879,  0.00177121,  0.00315285, -0.00270734, -0.00044774,<br/>       -0.00409922,  0.00101802, -0.00142405,  0.00274875,  0.00204634,<br/>        0.00324002, -0.00225011, -0.00113855, -0.00219695,  0.00319166,<br/>        0.00071592, -0.00315284,  0.00185551,  0.00097711, -0.00070271,<br/>       -0.00369302, -0.00356723, -0.00135671,  0.00045015, -0.00433108,<br/>        0.00392874,  0.00140838, -0.00117412,  0.00413575,  0.00170295,<br/>       -0.00394475,  0.00001962, -0.00357439, -0.00057754, -0.00097144,<br/>        0.0035095 , -0.00366997, -0.00320746,  0.00418892, -0.0023406 ,<br/>        0.00005771, -0.00132968,  0.00197431, -0.00197416,  0.00487064,<br/>       -0.00363371, -0.00282901, -0.00457178,  0.00202645, -0.00152074],<br/>      dtype=float32)</span></pre><h1 id="768a" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结论:</h1><p id="9f42" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">在这篇博客中，我们已经深入了解了Word2Vec，接下来我们将学习其他高级方法，如GloV2和FastText。</p><p id="dd34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强烈推荐建议。</p></div></div>    
</body>
</html>