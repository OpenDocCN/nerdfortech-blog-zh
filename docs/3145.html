<html>
<head>
<title>Key Feature extraction from classified summary of a Text file using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BERT的文本分类摘要关键特征提取</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/key-feature-extraction-from-classified-summary-of-a-text-file-using-bert-c1472f7b493?source=collection_archive---------0-----------------------#2021-05-31">https://medium.com/nerd-for-tech/key-feature-extraction-from-classified-summary-of-a-text-file-using-bert-c1472f7b493?source=collection_archive---------0-----------------------#2021-05-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="dbf7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">利用BERT嵌入的强大功能</h1><p id="da0e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在本文中，我将向您展示BERT如何解决一个基本的文本摘要和分类问题。</p><blockquote class="kb"><p id="5654" class="kc kd hi bd ke kf kg kh ki kj kk ka dx translated"><strong class="ak">关于伯特(</strong>来自变压器的双向编码器表示)</p></blockquote><blockquote class="kl km kn"><p id="8424" class="jd je ko jf b jg kp ji jj jk kq jm jn kr ks jq jr kt ku ju jv kv kw jy jz ka hb bi translated">简而言之，BERT是一个理解如何表示文本的模型。你给它输入一个序列，它会左右扫描多次，然后为每个单词生成一个向量表示作为输出。</p><p id="f785" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">BERT和其他Transformer编码器架构在NLP(自然语言处理)中的各种任务上取得了巨大成功。</p></blockquote><h1 id="eb72" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">伯特的结构</strong></h1><h2 id="716e" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated"><strong class="ak"> 1。伯特总结者</strong></h2><ul class=""><li id="e75f" class="lq lr hi jf b jg jh jk jl jo ls js lt jw lu ka lv lw lx ly bi translated">它有两个部分:一个BERT编码器和一个摘要分类器。</li><li id="c1fc" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">在编码器中，我们学习文档中标记之间的交互，而在摘要分类器中，我们学习句子之间的交互。</li></ul><p id="4849" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">为了给每个句子分配一个标签，我们需要在每个句子前添加一个标记<code class="du me mf mg mh b">[CLS]</code>，表示该句子是否应该包含在最终的摘要中。</p><figure class="mj mk ml mm fd mn er es paragraph-image"><div class="er es mi"><img src="../Images/ce100782b73c25b5a3fe99a93fdcd2b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/0*4dWSd7wQgBTPQegM.png"/></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">用于摘要的BERT结构</figcaption></figure><h2 id="ba5e" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated"><strong class="ak"> 2。伯特分类器</strong></h2><p id="19aa" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">输入——在每个序列的开始有一个<code class="du me mf mg mh b">[CLS]</code>标记(分类),还有一个特殊的<code class="du me mf mg mh b">[SEP]</code>标记将输入分成两部分。</p><p id="1ebb" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">输出—对于分类，我们使用第一个令牌的输出(<code class="du me mf mg mh b">[CLS]</code>令牌)。对于更复杂的输出，我们可以使用所有其他的令牌输出。</p><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es mu"><img src="../Images/dc22dbaec9743b5babfc76e2a6fe4edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9j7r1HGOAJ-k7WVa.JPEG"/></div></div></figure><h1 id="5392" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">比较BERT与</strong> XLNet &amp; GPT-2、<strong class="ak">基于性能的文本摘要</strong></h1><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es mz"><img src="../Images/9e38f0e91fd04aec067c91593e90b2f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-5OmHMcNGG17Kt5v2tZNA.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">安装Bert-extract-summarizer后的比较，变压器==2.2.0，空间</figcaption></figure><p id="3e64" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated"><strong class="jf hj">结果:</strong></p><ul class=""><li id="689c" class="lq lr hi jf b jg kx jk ky jo na js nb jw nc ka lv lw lx ly bi translated">性能方面——GPT-2-中等是最好的</li><li id="969c" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">耗时— XLNet (11秒)GPT-2中等(35秒)Bert(30秒)</li><li id="66ec" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">易用性条款— BERT</li></ul><h2 id="1ec3" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated"><strong class="ak">第一步:选择BERT模型</strong></h2><p id="a6ef" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">有多种BERT模型可供选择。</p><ul class=""><li id="00d0" class="lq lr hi jf b jg kx jk ky jo na js nb jw nc ka lv lw lx ly bi translated"><a class="ae nd" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3" rel="noopener ugc nofollow" target="_blank">伯特-基</a>，</li><li id="86a0" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated"><a class="ae nd" href="https://tfhub.dev/google/collections/bert/1" rel="noopener ugc nofollow" target="_blank">小铺位</a></li><li id="abc7" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">艾伯特</li><li id="eeac" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated"><a class="ae nd" href="https://tfhub.dev/google/collections/experts/bert/1" rel="noopener ugc nofollow" target="_blank">伯特专家</a></li><li id="b510" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated"><a class="ae nd" href="https://tfhub.dev/google/collections/electra/1" rel="noopener ugc nofollow" target="_blank">伊莱克特</a></li></ul><blockquote class="kl km kn"><p id="2b44" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated"><strong class="jf hj">最终使用的型号:DistilBERT </strong></p><p id="7b83" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">它是一个小型、快速、廉价、轻便的变压器模型，通过提取BERT base训练而成。</p><p id="240a" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">根据GLUE语言理解基准测试，它比bert-base-uncased少40%的参数，运行速度快60%，同时保留了超过95%的bert性能。</p></blockquote><h2 id="c265" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated"><strong class="ak">第二步:使用BERT进行文本分类</strong></h2><blockquote class="kb"><p id="3b2a" class="kc kd hi bd ke kf kg kh ki kj kk ka dx translated">你的大脑一定在和伯特开启的所有可能性赛跑。对于我们的NLP应用程序，我们可以在无数的上下文中使用BERT庞大的知识库！</p></blockquote><h2 id="34f3" class="lc ig hi bd ih ld ne lf il lg nf li ip jo ng lk it js nh lm ix jw ni lo jb lp bi translated"><strong class="ak"> 1。让我们开始吧！</strong></h2><p id="ee77" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我使用了来自<a class="ae nd" href="https://github.com/tensorflow/models" rel="noopener ugc nofollow" target="_blank"> tensorflow/models </a>的AdamW优化器。</p><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nj"><img src="../Images/8035b29b3dc0d6872bdb9e532bedd8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mGSTKmOZaBoEMMuYX3dWvg.png"/></div></div></figure><h2 id="b5c7" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated"><strong class="ak"> 2。</strong>导入并预处理数据集</h2><p id="795c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">来源:<a class="ae nd" href="https://www.kaggle.com/cfpb/us-consumer-finance-complaints" rel="noopener ugc nofollow" target="_blank">卡格尔</a></p><blockquote class="kl km kn"><p id="f793" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">数据集由CFPB向公司发送的消费者对金融产品和服务的投诉组成，以便公司做出回应，帮助改善金融市场。</p></blockquote><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nk"><img src="../Images/29466133fb38a192ef1763f283a88c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6Ft2cFzafuJCaB5eJTlPg.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">加载数据集</figcaption></figure><p id="7533" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated"><strong class="jf hj"> 2.1。功能选择</strong></p><p id="ef72" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">我选择了与重新解决问题直接相关的列，并将它们分类到产品类别中</p><p id="12d9" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">下面的输出显示我们的数据集有555，957行和18列。</p><figure class="mj mk ml mm fd mn er es paragraph-image"><div class="er es nl"><img src="../Images/21ae17a35a9f5cd73868406784479de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*HJT3W_Nbmbv-SopGyd345g.png"/></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">从18个功能中选择2个。</figcaption></figure><figure class="mj mk ml mm fd mn er es paragraph-image"><div class="er es nm"><img src="../Images/533689e14d3aaee9c6cd1fc2f4948b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*3U3hLhFGoUr3Fnp91uVrbw.png"/></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">问题分为10个产品类别</figcaption></figure><p id="da62" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated"><strong class="jf hj"> 2.2。标签编码</strong></p><p id="c3e0" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">我已经对<strong class="jf hj"> <em class="ko">产品</em> </strong>列进行了标签编码，以使用<code class="du me mf mg mh b"><strong class="jf hj">LabelEncoder</strong></code>将文本格式转换为标签格式。</p><blockquote class="kl km kn"><p id="889d" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated"><code class="du me mf mg mh b"><strong class="jf hj">LabelEncoder</strong></code>:允许给分类数据分配序数级。</p><p id="bab7" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated"><code class="du me mf mg mh b"><strong class="jf hj">fit_transform</strong></code> (y):安装标签编码器，返回编码后的标签。</p></blockquote><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nn"><img src="../Images/17fa321c90bac2287459039af0bb2aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sy2ttWETCKrUqyrA_szxEQ.png"/></div></div></figure><h2 id="7050" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated">3.创建BERT记号赋予器</h2><blockquote class="kl km kn"><p id="e270" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">在输入到BERT之前，文本输入需要转换为<strong class="jf hj">数字标记id</strong>并排列在几个<strong class="jf hj">张量</strong>中。</p><p id="f07c" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">标记化是指把一个句子分成单个的单词。为了对我们的文本进行分词，我们将使用BERT分词器。</p></blockquote><p id="6e2a" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated"><strong class="jf hj">导入BERT专用的预训练模型和标记器</strong></p><ul class=""><li id="7777" class="lq lr hi jf b jg kx jk ky jo na js nb jw nc ka lv lw lx ly bi translated">从<code class="du me mf mg mh b">hub.KerasLayer</code>导入BERT模型，创建一个BERT嵌入层</li><li id="f17e" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">以一个<strong class="jf hj"> <em class="ko"> numpy数组的形式检索BERT <strong class="jf hj"> <em class="ko">词汇文件</em> </strong>。</em> </strong></li><li id="8ec9" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">将文本设置为小写，并将我们的<code class="du me mf mg mh b">vocab_file</code>和<code class="du me mf mg mh b">do_lower</code>变量传递给<code class="du me mf mg mh b">BertTokenizer</code>对象。</li><li id="5847" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">初始化tokenizer_for_bert。</li></ul><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es no"><img src="../Images/ae67903b3f54838839761238b54aee09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-8Vpfb4Ojf_GaRtqfXo14A.png"/></div></div></figure><h2 id="0093" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated">4.为文本预处理定义辅助函数</h2><ul class=""><li id="f215" class="lq lr hi jf b jg jh jk jl jo ls js lt jw lu ka lv lw lx ly bi translated"><strong class="jf hj"> <em class="ko"> encode_text </em> </strong>函数将原始文本数据转换为编码文本(' CLS '+令牌+ 'SEP ')，该编码文本被拟合并转换为令牌</li><li id="8343" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">为了创建长度相等的句子，我填充了<strong class="jf hj"> <em class="ko"> token_ids、mask_ids、segment_ids </em> </strong>来截断具有所提供的批量大小的记号。</li></ul><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es np"><img src="../Images/636a703d5eadc20fec67c84d961277e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eg0neut_OntPtzyolxdE6g.png"/></div></div></figure><ul class=""><li id="7033" class="lq lr hi jf b jg kx jk ky jo na js nb jw nc ka lv lw lx ly bi translated">该模型将字符串作为输入，并返回适当格式的对象，这些对象可以传递给BERT。</li></ul><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nq"><img src="../Images/0f20df9038e4107eeded12c41c01acfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OxLZFHar-nXzd93RDFVsg.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">将<strong class="bd ih"> test_text </strong>中的文本传递给<strong class="bd ih"> encode_text </strong>函数</figcaption></figure><blockquote class="kl km kn"><p id="4de5" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">由于这个文本预处理器是一个TensorFlow模型，它可以直接包含在任何模型中。</p></blockquote><h2 id="6498" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated">5.定义模型</h2><ul class=""><li id="050d" class="lq lr hi jf b jg jh jk jl jo ls js lt jw lu ka lv lw lx ly bi translated">创建一个非常简单的微调模型，包括预处理模型、选定的BERT模型、一个密集层和一个用于正则化的下降层。</li><li id="2de0" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">如您所见，BERT模型将使用预处理的3个输出(<code class="du me mf mg mh b">input_words_id</code>、<code class="du me mf mg mh b">input_mask</code>和<code class="du me mf mg mh b">segment_ids</code>)。</li></ul><p id="e323" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated"><em class="ko">批处理大小= 40意味着如果输入是&gt;大于40，它将被截断为40个令牌，如果输入是&lt; 40，它将填充为40个令牌。</em></p><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nr"><img src="../Images/9ec8905f9a0703b9241340dc30beed56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wv0i_svqzgw1ZOsp24ut4A.png"/></div></div></figure><h2 id="83e7" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated"><strong class="ak"> 6。将列车文本转换为编码格式</strong></h2><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es ns"><img src="../Images/b0df442ba6c770c024098f24cc39dcdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2YLH0Bh8Eo1VCxlm6t0_A.png"/></div></div></figure><h2 id="1077" class="lc ig hi bd ih ld le lf il lg lh li ip jo lj lk it js ll lm ix jw ln lo jb lp bi translated">7.文本分类模型的微调</h2><p id="1bb1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">微调遵循BERT预训练中的优化器设置:它使用<strong class="jf hj"> <em class="ko"> AdamW </em> </strong>优化器</p><blockquote class="kl km kn"><p id="2f8f" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">伯特最初接受的训练是:“适应时刻”(亚当)。该优化器最小化预测损失，并通过权重衰减进行正则化。</p></blockquote><p id="3e41" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">要提高精确度，请增加历元的数量</p><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nt"><img src="../Images/dbcb8bf04972030289c02789b32fce61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6c91OCs2HnnRIajRKEx7A.png"/></div></div></figure><h1 id="ccbd" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">建筑管道</h1><blockquote class="kb"><p id="40c6" class="kc kd hi bd ke kf kg kh ki kj kk ka dx translated"><strong class="ak">管道流量:</strong></p><p id="8be3" class="kc kd hi bd ke kf kg kh ki kj kk ka dx translated"><strong class="ak">使用BERT的文本摘要&gt;使用BERT的文本分类&gt;使用spaCy的名称实体识别</strong></p></blockquote><p id="f8fd" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo ks jq jr js ku ju jv jw kw jy jz ka hb bi translated"><strong class="jf hj">文本摘要:</strong></p><p id="820e" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">提取、抽象和混合总结是最常用的策略。</p><ul class=""><li id="d9a3" class="lq lr hi jf b jg kx jk ky jo na js nb jw nc ka lv lw lx ly bi translated"><em class="ko">提取策略</em> —它选择最能代表文章重要主题的前N个句子。</li><li id="7154" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka lv lw lx ly bi translated">抽象的摘要——它试图用新词来重新表述文章的主要思想。</li></ul><ol class=""><li id="3848" class="lq lr hi jf b jg kx jk ky jo na js nb jw nc ka nu lw lx ly bi translated"><strong class="jf hj">安装Bert-extract-summarizer:</strong></li><li id="558c" class="lq lr hi jf b jg lz jk ma jo mb js mc jw md ka nu lw lx ly bi translated"><strong class="jf hj">安装空间:</strong>最小的英语语言模型只需要一会儿就可以下载，因为它大约有11MB</li></ol><blockquote class="kl km kn"><p id="8515" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated">该工具利用<strong class="jf hj">hugging face</strong>py torch transformers库来运行提取摘要。</p><p id="430c" class="jd je ko jf b jg kx ji jj jk ky jm jn kr kz jq jr kt la ju jv kv lb jy jz ka hb bi translated"><strong class="jf hj">首先嵌入句子，然后运行聚类算法，找到最接近聚类中心的句子</strong></p></blockquote><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nv"><img src="../Images/0643a6f57b212607cf28fabbb5d54d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TNqoWUnkXhuNcMJIpb-AdA.png"/></div></div></figure><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nw"><img src="../Images/edf1fdac1f4cd99ef5923850d75982ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1vdwALleFfgQ0QDVxpkBzw.png"/></div></div></figure><p id="32c1" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated"><strong class="jf hj"> 3。定义管道功能</strong></p><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nx"><img src="../Images/603e1c4dabd97ec21dcaf4cfbf45c485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odj9MmB7-nzl0-ZvX7gbMg.png"/></div></div></figure><h1 id="1921" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">测试模型</h1><p id="1aeb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">将输入传递给已训练的模型以对文本进行总结和分类。</p><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es nv"><img src="../Images/05b18e05f34e4053ed4f0759c84a17d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnKTffLGxR7p2PQ9gz3AiA.png"/></div></div></figure><h1 id="43ec" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">基于空间聚类的关键特征提取</h1><blockquote class="kb"><p id="a96d" class="kc kd hi bd ke kf kg kh ki kj kk ka dx translated">关于空间命名实体识别</p><p id="d5d5" class="kc kd hi bd ke kf kg kh ki kj kk ka dx translated">spaCy的命名实体识别(NER)将非结构化文本中的命名实体定位并识别为标准类别，如人名、位置、组织、时间表达式、数量、货币值、百分比、代码等。</p></blockquote><figure class="nz oa ob oc od mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es ny"><img src="../Images/9ce2a65a886129761971da1c04c11c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A1Qc27lKofPVmtL2xpqc5A.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">空间NER的结构</figcaption></figure><p id="4e4a" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated"><strong class="jf hj">访问生成的文本摘要上的实体注释</strong></p><figure class="mj mk ml mm fd mn er es paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="er es oe"><img src="../Images/4c841180bbcb149c249884bc570e449e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s_W6tzMmAWdvCh30p34Vkw.png"/></div></div></figure><p id="09ea" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">是具有自己的注释集的标记范围</p><figure class="mj mk ml mm fd mn er es paragraph-image"><div class="er es of"><img src="../Images/ae76ef90551126348f275ee07e71443f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HQ7H0sbUoiVgmX6RjtroWg.png"/></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">实体注释</figcaption></figure><h1 id="faf8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">进一步的想法</h1><p id="0fac" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于一个更快的方法，我可以通过使用spaCy从生成的文本摘要中提取<strong class="jf hj">名词短语</strong>来直接提取关键特征。</p><p id="c78c" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">这将有助于通过统计文本文件中所有标记的频率来获得最常见的名词、动词、副词等。</p><p id="236f" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">请随意使用spaCy，因为它有更多的内置功能可用。我会在我的下一篇博客中这样做。保持联系！</p></div></div>    
</body>
</html>