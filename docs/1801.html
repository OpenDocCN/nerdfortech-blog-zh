<html>
<head>
<title>Regularization techniques in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的正则化技术</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/regularization-techniques-in-machine-learning-a31daf2acc3e?source=collection_archive---------8-----------------------#2021-04-07">https://medium.com/nerd-for-tech/regularization-techniques-in-machine-learning-a31daf2acc3e?source=collection_archive---------8-----------------------#2021-04-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4a52b32c9ae5b12529370d40ca7fde4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g4ZCJ3-yKRhCYurm.jpg"/></div></div></figure><h2 id="1837" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">什么是正规化？</strong></h2><p id="649d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">正则化是 ML 中一种常用的技术，它解决了我们模型的过拟合问题。好的..有人可能会问，我们还可以降低模型复杂性来解决这个问题。是的，这个问题是真实的，但是，如果我们降低模型的复杂性，我们可能会以拟合不足而告终，这是完全不可取的。因此，我们需要一种更可靠的技术，确保我们不会丢失任何数据信息，同时能够自信地预测我们的目标变量。这就是正规化发挥作用的地方。</p><p id="b88f" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">这是一种回归形式，它将系数估计值约束或缩小到零。换句话说，<strong class="jq hj"> <em class="ko">这种技术不鼓励学习更复杂或更灵活的模型，以避免过度拟合的风险。</em> </strong></p><h2 id="e018" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">它是如何工作的？</h2><p id="8547" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">在进行正规化工作之前，让我们快速浏览一下 OLS 是如何工作的。我们知道，在 OLS，模型参数是通过考虑残差平方和(RSS)的最佳值来确定的。OLS 的拟合过程涉及一个损失函数，称为残差平方和或 RSS。选择系数，使得它们最小化这个损失函数。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/6b1f3c9fd9bf2bd3a400f4ae751b9a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/0*O0-jnVmp1rS0vr7I"/></div></figure><p id="dcd4" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">其中:</p><ul class=""><li id="271b" class="ku kv hi jq b jr kj jv kk jb kw jf kx jj ky ki kz la lb lc bi translated">n 是<strong class="jq hj">观察值(数据)</strong>的总数。</li><li id="8f69" class="ku kv hi jq b jr ld jv le jb lf jf lg jj lh ki kz la lb lc bi translated">yi 是<strong class="jq hj">观测值(数据)</strong>的实际输出值。</li><li id="2a1a" class="ku kv hi jq b jr ld jv le jb lf jf lg jj lh ki kz la lb lc bi translated">p 是<strong class="jq hj">特征总数</strong>。</li><li id="5f53" class="ku kv hi jq b jr ld jv le jb lf jf lg jj lh ki kz la lb lc bi translated">βj 是一个<strong class="jq hj">模型的系数</strong>。</li><li id="99b8" class="ku kv hi jq b jr ld jv le jb lf jf lg jj lh ki kz la lb lc bi translated">xij 是第<strong class="jq hj">个观察值，</strong>第<strong class="jq hj">个特征值</strong>。</li></ul><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/620d502819f99a148da9b9b96ced6414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S-JcKW6VB7v2Nt-8.png"/></div></div></figure><p id="ee9e" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">在 OLS 的例子中，我们通过梯度下降达到最优，梯度下降从一个随机的起点(0，0)开始。</p><p id="0a2b" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj"> <em class="ko">正则化线性回归模型非常类似于最小二乘，除了通过最小化一个稍微不同的目标函数来估计系数。我们最小化 RSS 和一个惩罚系数大小</em> </strong> <em class="ko">的“惩罚项”之和。</em></p><p id="1d04" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj"> <em class="ko">正则化对优化算法施加了约束。</em>T29】</strong></p><p id="e3c7" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">在 ML 中主要使用了 3 种正则化技术，让我们分别讨论它们。</p><h2 id="9b2d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">套索回归:</strong></h2><p id="66cc" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">在 lasso 中，参数的绝对值之和乘以一个收缩因子被添加到 OLS 的损失函数中。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/d8ce0e780d05365a187024cd0296819d.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/0*XMdFjhVpGYLEBfXl"/></div></figure><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/e5a36198b3b6399796bc10d5c1972166.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/0*6yTVHquJZGkdd5PL"/></div></figure><p id="ed32" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">其中:<br/> λ是一个调谐参数(收缩因子)，用于在模型与数据的拟合度和模型系数的大小之间寻求平衡:</p><ul class=""><li id="2eb3" class="ku kv hi jq b jr kj jv kk jb kw jf kx jj ky ki kz la lb lc bi translated">微小的λ对系数大小没有影响，相当于正常的线性回归。</li><li id="4d10" class="ku kv hi jq b jr ld jv le jb lf jf lg jj lh ki kz la lb lc bi translated">增加λ会使系数恶化，从而使系数向零收缩。</li></ul><p id="d57b" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">上图显示了套索回归，其中<strong class="jq hj"> <em class="ko"> RSS 通过添加收缩量进行了修改。</em> </strong>现在，通过最小化这个函数来估计系数。这里，<strong class="jq hj"> <em class="ko"> λ是调节参数，它决定了我们想要对模型的灵活性进行多少惩罚。</em> </strong>一个模型的灵活性的增加是通过它的系数的增加来表示的，如果我们想最小化上面的函数，那么这些系数就需要很小。这就是套索回归技术如何防止系数升得太高。此外，请注意，我们缩小了每个变量与响应的估计关联，除了截距β0，此截距是 xi1 = xi2 = …= xip = 0 时响应平均值的度量。</p><p id="c06d" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">让我们通过数学方法改变损失函数来理解系数是如何降低的。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/468d66237494b2604097816592e7731f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OWOmdVJ8P3RJgJeWS_g0UA.png"/></div></div></figure><p id="c384" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">这些椭圆边界是为特定值误差函数绘制的。简单地说，如果我们沿着这些边界移动，我们的 RSS 函数值不会改变，但我们会遇到各种系数的组合。</p><p id="e0fe" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj"> <em class="ko">对于套索，方程变成，|β1|+|β2|≤ s </em> </strong>。这意味着<em class="ko">拉索系数对于位于由|β1|+|β2|≤ s 给出的菱形内的所有点具有最小的 RSS(损失函数)</em></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/840975163a11558582034b5d7fbb1adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*FN1h5_SnX_bsrJcRQjXNJw.png"/></div></figure><p id="af37" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">套索回归只是放了一个约束。</p><p id="5ab3" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">由于绝对函数在 0 处不可微，所以我们不能在这里使用梯度下降，但我们使用其他优化方法，如<em class="ko">坐标下降</em>等。</p><p id="4bfc" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj"> <em class="ko">套索约束在每个轴上都有角，所以椭圆通常会在一个轴上与约束区域相交。当这种情况发生时，其中一个 coeﬃcients 将等于零。</em> </strong>我们在这种情况下只考虑了两个参数，但在多个参数的情况下，可能会将多个参数减少到零。所以，<strong class="jq hj">用于<em class="ko">特征选择</em>也。</strong></p><h2 id="18ac" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.<strong class="ak">岭回归:</strong></h2><p id="4fd2" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">在岭回归中，我们添加一个罚项，它等于系数的平方和乘以收缩因子。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/12b34934346ed29618ccb2b49e668132.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/0*4mrmsqDJrfPVGDwH"/></div></figure><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/85597c862e339cfbb60b21ef39ec65d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/0*x4sXu2VWV7x59pxJ"/></div></figure><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/9da4b7a4ef2117bf41bebf95bc1f2788.png" data-original-src="https://miro.medium.com/v2/format:webp/1*1pHwPfuhgTDFH8elIh_B2g.png"/></div></figure><p id="d5ed" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj"> <em class="ko">岭回归用β + β ≤ s </em> </strong>表示。这意味着<em class="ko">岭回归系数对于位于由β + β ≤ s 给出的圆内的所有点具有最小的 RSS(损失函数)</em></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/081211a19196133d2e62fffb5c34be5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Aa9Qur1UvcbL-TR5ttW_w.png"/></div></div></figure><p id="ddee" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj"> </strong></p><h2 id="2060" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">3.弹性净回归；</h2><p id="1372" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">这是一种回归，是套索和山脊的结合。</p><p id="a943" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">约束区域是圆形和菱形的组合。除了形状，一切都一样。</p><p id="2d09" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">弹性网回归=a *拉索回归(L1 范数)+b *岭回归(L2 范数)</strong></p><p id="a57c" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">L1 _ 诺姆=a/(a + b) </strong></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/0ba1d8c7e5e172703300a50d33553c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OAIRgLWB1-FFXlag.png"/></div></div></figure><p id="dab5" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj"> <em class="ko">不同 L1 比下弹性网的行为:</em> </strong></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/845fcb2d473585965f70aff41b9b6dd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TEGOal8mPVc3qbWcHfEfqA.png"/></div></div></figure><h1 id="9143" class="lt ir hi bd is lu lv lw iw lx ly lz ja ma mb mc je md me mf ji mg mh mi jm mj bi translated">正规化实现了什么？</h1><p id="055c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated"><em class="ko">正则化，显著降低了模型的方差，而没有显著增加其偏差</em>。因此，在上述正则化技术中使用的调谐参数λ控制对偏差和方差的影响。随着λ值的上升，它会降低系数的值，从而降低方差。<strong class="jq hj">因此，应该仔细选择λ的值。</strong></p><p id="2750" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">实现这些算法的一个流行库是 Scikit-learn。它有一个很棒的 API，只需几行 python 代码就能让你的模型开始运行。</p></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="3ad3" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">如果你喜欢这篇文章，<strong class="jq hj">请在下面为这篇文章鼓掌以示支持，</strong>请发表评论。</p><p id="3237" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">你也可以在<a class="ae mr" href="https://twitter.com/MuduliSumanta" rel="noopener ugc nofollow" target="_blank">推特</a>、<a class="ae mr" href="http://sumanta.skm97@gmail.com" rel="noopener ugc nofollow" target="_blank">、<strong class="jq hj">直接给我发邮件</strong>、</a>或者<a class="ae mr" href="https://www.linkedin.com/in/sumanta-kumar-muduli-08a303198/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我。我很乐意收到你的来信。</p><p id="ac81" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">乡亲们，祝你们有美好的一天:)</p></div></div>    
</body>
</html>