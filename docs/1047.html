<html>
<head>
<title>NLP Zero to One: BERT (Part 14/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 零到一:BERT(第 14/30 部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-bert-part-14-40-691ef069712f?source=collection_archive---------20-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-bert-part-14-40-691ef069712f?source=collection_archive---------20-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="cd35" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">来自变压器的双向编码器表示</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ca6944203c193e959b80d5e2274eb180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLBnlJCST7GPwam3y0J29g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="4e7b" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="da40" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">BERT(来自变压器的双向编码器表示)是一种语言表示模型。这是最近在自然语言处理中取得的成功，它在许多自然语言处理任务中被证明优于许多现有的先进模型。这些预先训练好的模型然后可以被<strong class="kh hj"> <em class="lb">微调</em> </strong>用于许多 NLP 问题，如问题&amp;回答和情感分析。</p><p id="45b1" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">预先训练的语言表示可以是<strong class="kh hj"> <em class="lb">与上下文无关的</em> </strong>或<strong class="kh hj"> <em class="lb">基于上下文的</em> </strong>。像 word2vec 这样的上下文无关模型为词汇表中的每个单词生成一个单词嵌入表示(一个数字向量)。像 Word2Vec 等其他密集表示模型一样，Bert 也采用了无监督学习设置，因此消除了对标记数据的需要。BERT 试图根据上下文(周围的单词)来预测目标单词。在这篇博客中，我们将试图更详细地了解 BERT。</p><h1 id="f94f" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">想法..</h1><p id="e7cc" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">让我们首先尝试修改 Wor2Vec 架构，其中我们基于上下文预测目标。上下文被定义为"<strong class="kh hj"> n" </strong>词两边的目标词。为了更好的理解，请浏览我在这个系列中关于 Wor2Vec 架构的博客。与 Word2Vec 不同，BERT 使用了一种叫做<strong class="kh hj"> Masked LM </strong> (MLM)的新技术，其中 model 随机屏蔽句子中的单词，然后试图预测它们。顾名思义,“来自变压器的双向编码器表示”暗示模型在两个方向上训练，并且它使用来自两个方向的句子的完整上下文来预测被屏蔽的单词。</p><p id="45e5" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">BERT 是基于上下文的嵌入，它基本上意味着单词的表示依赖于上下文。“学校”这个词在句子中的嵌入会有所不同“他要去<strong class="kh hj"> <em class="lb">学校</em> </strong>离得很远”和“这个新来的<strong class="kh hj"> <em class="lb">学校</em> </strong>的思想很激进”作为“学校”这个词在两个句子中的意思完全不同。</p><h1 id="ee7a" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">体系结构..</h1><h2 id="1caf" class="lh jo hi bd jp li lj lk jt ll lm ln jx ko lo lp jz ks lq lr kb kw ls lt kd lu bi translated">掩饰</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lv"><img src="../Images/23f0fa1524f6dccb5396c79c8e3ae446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Md-6sidoTqfCuYDenrzCoQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">伯特建筑，由作者生成</figcaption></figure><p id="5ac4" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">BERT 基于 Transformer 模型架构，这是一种自我关注机制，可以学习文本中单词之间的上下文关系。请浏览我在变形金刚系列中的博客，更好地理解这些概念，因为理解 BERT 非常重要。Transformers 使用编码器-解码器设置。但是 BERT 是单词嵌入生成模型，所以 BERT 只包含编码器网络。</p><p id="7968" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">我们随机屏蔽输入中的单词，然后通过基于 BERT 注意力的编码器运行整个序列，然后根据序列中未被屏蔽的单词所提供的上下文，仅预测被屏蔽的单词。我们用[MASK]替换被屏蔽的单词，并且模型仅尝试预测[MASK]标记何时出现在输入中。这种掩蔽方法有一个缺点。我们希望模型能够预测正确的单词，而不管输入是什么，为了克服这个缺点，10%的[掩码]被替换为随机单词。</p><h2 id="fabb" class="lh jo hi bd jp li lj lk jt ll lm ln jx ko lo lp jz ks lq lr kb kw ls lt kd lu bi translated">下一句预测..</h2><p id="366b" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">除了预测单词，模型还接收两个句子作为输入。BERT 的任务是预测语料库中第二个句子是否在第一个句子之后。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lv"><img src="../Images/6947473a3dc5445d6cddcd1b41376c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g96NzZbjXYL3VFhLejOhiw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">伯特建筑，由作者生成</figcaption></figure><p id="c598" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">BERT 损失函数只考虑屏蔽值的预测，而忽略非屏蔽字的预测。在训练 BERT 模型时，屏蔽 LM 和下一句预测一起训练，目标是最小化两种策略的组合损失函数。</p><p id="c519" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">BERT 论文中介绍了两种不同的体系结构。请注意参数的数量。512 个令牌是输入的最大序列长度。任何不在词汇表中出现的单词都被贪婪地分解成子单词。</p><ul class=""><li id="e147" class="lw lx hi kh b ki lc kl ld ko ly ks lz kw ma la mb mc md me bi translated">BERT base — 12 层(变压器模块)、12 个注意力头和 1.1 亿个参数。</li><li id="16bf" class="lw lx hi kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">BERT Large — 24 层，16 个注意力头，3.4 亿个参数。</li></ul><h1 id="9ff8" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">应用程序..</h1><p id="a5c0" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">BERT 技术已经在各种各样具有挑战性的自然语言任务上取得了最先进的结果。在句子分类任务中，可以通过在 transformer 之上应用分类层来直接应用 BERT。在像“<strong class="kh hj">问答</strong>这样的任务中，任务是标记序列的答案，可以通过学习两个标记答案开始和结束的额外向量来训练 BERT。它还可以执行命名实体识别(NER)任务。值得注意的是，在微调训练中，大多数超参数保持与 BERT 训练中相同。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="7efa" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">上一部:<a class="ae ml" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-transformers-part-13-30-5cd5a3ddd93b?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零比一:变形金刚(第 13/30 部)</strong> </a></p><p id="1804" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">接下来:<a class="ae ml" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-knowledge-graphs-part-15-40-df278d91c635?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零对一:知识图谱部分(15/30) </strong> </a></p></div></div>    
</body>
</html>