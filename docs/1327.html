<html>
<head>
<title>Simple ETL using Apache Spark &amp; PostgreSQL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Spark和PostgreSQL的简单ETL</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/simple-etl-using-apache-spark-postgresql-9f759351872b?source=collection_archive---------2-----------------------#2021-03-14">https://medium.com/nerd-for-tech/simple-etl-using-apache-spark-postgresql-9f759351872b?source=collection_archive---------2-----------------------#2021-03-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f4454a44ce4e2c0f7b9edc06ee14540d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*7QvE1_mmpBEfebn9FSpHEA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Spark &amp; PostgreSQL</figcaption></figure><p id="a11b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Apache Spark是一种快如闪电的集群计算技术，专为快速计算而设计。它基于Hadoop MapReduce，并扩展了MapReduce模型，以有效地将其用于更多类型的计算，包括交互式查询和流处理。</p><p id="8daa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">通过这次练习，我想证明apache spark有多棒。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><ol class=""><li id="47c6" class="jz ka hi iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated"><strong class="iw hj">导入我们需要使用的所有库</strong>。</li></ol><p id="6f0b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="ki">从pyspark.sql导入SparkSession <br/>从pyspark.sql导入SQLContext <br/>导入psycopg2 </em></p><p id="86ee" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。创建到PostgreSQL的连接。</strong></p><p id="2d50" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="ki">conn = psycopg 2 . connect(host = ' localhost '，database='postgres '，user ='postgres '，password = ' postgres ')<br/>cur = conn . cursor()</em></p><p id="0ff5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。加载数据。</strong></p><p id="0f53" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="ki">sqlctx = SQLContext(sc)<br/>pop _ data = sqlctx . read . CSV(' ratings . CSV ')</em></p><p id="a8ce" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 4。在PostgreSQL中创建表。</strong></p><p id="4e2f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="ki"> cur.execute(" "创建表格电影<br/> (rating numeric不为空，<br/> count_movie integer不为空<br/>)；"”)</em></p><p id="acbb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 5。做一些转换，在这种情况下，我只是使用数据与_c0不等于'用户ID '和计数电影组的评级。</strong></p><p id="d673" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="ki">clean = pop _ data[pop _ data[' _ c0 ']！= ' userId ']<br/>a =[tuple(x)for x in clean . group by(" _ C2 ")。计数()。collect()] <br/> b = '，'。join(['%s'] * len(a)) </em></p><p id="fe6e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">6。最后，将数据插入表中，不要忘记提交。</p><p id="d834" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="ki"> q = "插入电影(分级，计数_电影)值{} "。format(b) <br/> cur.execute(q，a) <br/> conn.commit() </em></p><p id="852a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这种情况下，我使用清理后的25000095行数据。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/71d58d135372defb26db7569268f3baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*thM62TqC4zXV3eMfTBmCyg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="c626" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Spark将作业拆分为2个作业，作业id = 0的作业仅创建1个阶段和1个任务，作业id = 1的作业创建2个阶段和208个任务。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/c72ba869567880527e489cfabccf8f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yppGT3lHab97O66y2ict8A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="2868" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作业id = 0的汇总指标。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/124bf6a5310895d658d26658b43301af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AIyCCzA8tFBFai33Ac5ngA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="18be" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作业id = 1的阶段1有8个任务。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/a66d3adbbf827604b1d9fe360f2dc232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmQhSnaIoJuMGvmlFaDWDA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="5dc6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">具有208个任务的作业id = 1的阶段2。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/75864497583e8b9a3f65f6f1a6a8280a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*De_aWVVulsu0oWHonu4Mlw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="95c5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于作业id = 0(加载csv文件)，需要0，3s。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/8890fdaf0dbee3a5347ffa203f0f2677.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*hTbAVG5BIbnmY0MpF6h6Uw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="f7f4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于job id = 1，进行一些转换并将数据插入PostgreSQL需要8s。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/8579e4392442159de68ed9a245127b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MkJY0ho5ynGXtKfbw2Vp7w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="fc01" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果为PostgreSQL。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/25ea82780d67a819ddbf1a375abe5bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*e_k3ehUIPRqJh9p84oXufw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片(作者)</figcaption></figure><p id="307e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">结论</strong></p><p id="8a28" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">本练习概述了使用PySpark从单一数据源到数据库的基本Apache Spark ETL过程。根据我的经验，与SSIS或ADF或使用。RedHat中的sh文件可能需要8，3 s以上的时间，但是与一个复杂的情况相比是非常有趣的。在这个实践中，我只使用了2个代码(1个代码用于清理，1个代码用于转换)，并且我很有兴趣尝试另一个转换。</p><p id="6ee4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">参考</strong></p><p id="1150" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae kv" href="https://hevodata.com/learn/create-an-apache-spark-etl/" rel="noopener ugc nofollow" target="_blank">创建Apache Spark ETL:简单指南—学习| Hevo(hevodata.com)</a></p><p id="4137" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae kv" href="https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm" rel="noopener ugc nofollow" target="_blank"> Apache Spark —简介—教程要点</a></p></div></div>    
</body>
</html>