<html>
<head>
<title>NLP Zero to One: Training Embeddings using Gensim and Visualisation (Part 7/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP零到一:使用Gensim和可视化训练嵌入(第7/30部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-training-embeddings-using-gensim-and-visualisation-part-7-30-f0540e976568?source=collection_archive---------15-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-training-embeddings-using-gensim-and-visualisation-part-7-30-f0540e976568?source=collection_archive---------15-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="9dd0" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">t分布随机邻居嵌入，嵌入偏差</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ac1c0376d2e430609e9ed7c0d5b251ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ToVLd2QjMOV-fwLijk5qRg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="0148" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="03b2" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">单词嵌入使得许多NLP任务的显著改进成为可能。它对word schematics的理解和用固定向量表示不同长度文本的能力使它在许多复杂的NLP任务中非常受欢迎。由于向量的长度是固定的，大多数机器学习算法可以直接应用于用于分类和回归任务的单词嵌入。在这篇博客中，我们将尝试看看帮助我们使用两种流行的方法CBOW和Skip-Gram实现Word2Vec的包。我们也将看看嵌入的一些属性和可视化。</p><h1 id="028b" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">培训CBOW和跳过程序..</h1><p id="bd23" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">我们可以只把上面的一小段作为单词嵌入的文本。我们将会看到如何在密集空间中编写代码来表示上述文本中的单词。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lb"><img src="../Images/6b175f3ab4087ec9c0c72a9c2b15fb47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*1FvxLoEP-zSd-83kt0AaCg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">代码可在此处获得</figcaption></figure><p id="baf7" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">正如在之前的博客中所解释的，首先我们需要使用NLTK进行标记化，然后在gensim库中使用Word2Vec。参数<strong class="kh hj">“SG”</strong>指定了训练算法CBOW (0)，Skip-Gram (1)。<br/>她我们可以清楚地看到单词<strong class="kh hj">“the”的密集矢量表示。</strong>这是一个50维向量，再次作为参数提供给gensim库Word2Vec函数。</p><h1 id="8519" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">(英)可视化(= visualization)..</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lh"><img src="../Images/1ec2f942530ffec98c34358eedfb078c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*39I1q7awlEzEsohZ7jW5yQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">代码可在此处获得</figcaption></figure><p id="f8d3" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">最常见的观想方法是将一个单词的100个维度投射成2个维度。<br/>可将PCA和TSNE等降维技术应用于密集矢量，以创建二维或三维矢量。让我们简单讨论一下TSNE的概念，并理解为什么它在可视化单词嵌入中如此流行。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es li"><img src="../Images/d20256a5abf5a1c7b7e53525137f6cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*T_6IWlNuKYd0XMPE6K_HVQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">代码输出:50维向量简化为2d，并使用matplotlib可视化</figcaption></figure><h2 id="6ea1" class="lj jo hi bd jp lk ll lm jt ln lo lp jx ko lq lr jz ks ls lt kb kw lu lv kd lw bi translated">TSNE (t分布随机邻居嵌入)</h2><p id="2202" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">t-分布式随机邻居嵌入(t-SNE)主要用于数据探索和可视化高维数据。它帮助我们将高维数据分解成2维或3维数据，这使得我们很容易对这些高维数据点进行绘图并获得一种直觉。</p><p id="5853" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">t-SNE算法计算高维空间和低维空间中的实例对之间的相似性度量。它试图保持从高维空间到低维空间的相似性。但是我们如何量化更高和更低维空间中的相似性，因为相似性的一些尺度不变度量将帮助我们在更高和更低维空间中保持相似性。</p><p id="007e" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><strong class="kh hj">更高维度中的相似性度量</strong>(联合概率)<strong class="kh hj"> : </strong>对于每个数据点，我们将在该点上集中高斯分布。然后我们测量所有其他点的密度。原始或空间中的相似性由高斯联合概率表示。</p><p id="69fb" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><strong class="kh hj">较低维度的相似性度量</strong>(联合概率)<strong class="kh hj"> : </strong>不使用高斯分布，而是使用一个自由度的学生t分布。因此嵌入空间中的相似性由学生的t分布联合概率来表示。</p><p id="c97f" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><strong class="kh hj">成本函数:</strong>为了保持从高维到低维的相似性度量，<strong class="kh hj"> </strong>我们将需要找到一个度量/成本函数，该函数找到联合概率之间的距离。<br/> Kullback-Liebler散度(KL)是我们的选择，因为它是计算两个概率分布之间距离的非常流行的指标。我们可以使用梯度下降来最小化我们的KL成本函数。</p><p id="8bb7" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">TNSE是用于可视化单词嵌入的流行技术，因为它保留小的成对距离或局部相似性的能力不同于其他维度技术，如PCA，其涉及保留大的成对距离以最大化方差。</p><p id="3e47" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><strong class="kh hj">层次聚类可视化</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/12b138d9c1feac502d7af988b7da5615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xP0OrHS83joYuGR4TDN3aw.png"/></div></div></figure><p id="f321" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">另一种流行的可视化方法是使用聚类算法来显示嵌入空间中哪些单词与其他单词相似的分层表示。<br/>这种聚类层次结构被表示为一棵树(或树状图)。树根是收集所有样本的唯一聚类，树叶是只有一个样本的聚类。</p><p id="6f58" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">生成上图的代码:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/5f54a37be6b95422896dd9228f8e9c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mb7LWllOYqHX7-2n2AEeYQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">代码可在此处获得</figcaption></figure><h1 id="0867" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">注意..</h1><p id="0b66" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated"><strong class="kh hj">分配伤害:</strong>嵌入类比还表现出文本中隐含的性别刻板印象和其他偏见。比如:“医生”职业接近“男人”，“护士”职业接近“女人”。当使用嵌入建模时，NLP工程师必须记住嵌入的这种固有偏差。嵌入的去偏处理从这些嵌入中移除偏置。</p><p id="57f9" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><strong class="kh hj">一阶和二阶共现:</strong>如果两个词通常彼此相邻，则称它们具有一阶共现。如果两个词有相似的邻居，它们就有二阶。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="9c03" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">接下来:<a class="ae lz" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-recurrent-neural-networks-basics-part-8-30-ca77af9d47ff?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP零比一:递归神经网络基础部分(8/30) </strong> </a> <br/>上一篇:<a class="ae lz" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-count-based-embeddings-glove-part-6-40-c5bb3ebfd081?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP零比一:基于计数的嵌入，手套(6/40) </strong> </a></p></div></div>    
</body>
</html>