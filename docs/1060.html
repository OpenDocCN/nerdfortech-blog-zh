<html>
<head>
<title>Adversarial Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗性机器学习</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/adversarial-machine-learning-2a3b96134b42?source=collection_archive---------33-----------------------#2021-03-02">https://medium.com/nerd-for-tech/adversarial-machine-learning-2a3b96134b42?source=collection_archive---------33-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/866ba03f6f490e093353a2d4d59c9550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VNmbEXyTaNqZ-elm.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:媒体</figcaption></figure><p id="93e8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">主要问题源于这样一个本质，即机器学习算法主要不是为了对付雄辩和动态的对手而开发的，因此，在概念上，通过精心篡改原始数据来利用掌握算法的相关安全问题，可以突破整个安全级别。</p><p id="2803" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这里，对抗性学习可以进入画面！！</p><p id="48c8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi js translated"><span class="l jt ju jv bm jw jx jy jz ka di"> A </span> <strong class="iw hj">分布式学习</strong>是一个新颖的研究领域，主要在于机器学习和网络安全的<strong class="iw hj">融合。它专注于允许在不利环境下安全实施机器学习方法，如垃圾邮件过滤数据保护和生物识别。</strong></p><h2 id="4993" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jf km kn ko jj kp kq kr jn ks kt ku kv bi translated">什么是对抗性机器学习？</h2><p id="6d16" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">对抗性机器学习是机器学习中使用的一种方法，用于欺骗或混淆欺骗性输入模型。虽然对抗性机器学习可以在许多实施中使用<strong class="iw hj">，但这种方法最广泛地用于在机器学习框架中执行入侵或触发故障。攻击的同一个方面将很容易被定制为在各种数据库或框架的几个版本上操作。</strong></p><p id="6939" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">应使用对抗性机器学习，而不是白盒或黑盒攻击。</strong>在白盒攻击中，入侵者了解正在使用的模型的内部，而在黑盒攻击中，黑客只识别原型的结果。</p><h2 id="9e5f" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jf km kn ko jj kp kq kr jn ks kt ku kv bi translated"><strong class="ak">对抗性机器学习攻击的类型:</strong></h2><p id="4ab1" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">对抗性机器学习攻击可以分为<strong class="iw hj">曲解输入或数据污染</strong>。错误分类的输入是攻击者在机器学习算法管道中隐藏恶意材料的最常见类型。这种策略的目的是让设备误识别特定的数据集。<strong class="iw hj">部署设备后，可利用后门特洛伊木马攻击来实现此目的。</strong></p><p id="b431" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">数据污染是指攻击者试图通过将错误信息放入存储库中来改变机器学习过程，从而降低结果的可靠性。这种入侵形式的目的是削弱机器学习机制，使算法的效用最小化。</p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/3c919714405b8c846a811afa68c1a569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-L2E8mM1p06b6Lxp.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:pluribus-one.it</figcaption></figure><h2 id="8a03" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jf km kn ko jj kp kq kr jn ks kt ku kv bi translated">对抗机器学习的预防:</h2><p id="84f3" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">这些策略包括<strong class="iw hj">对抗性训练，防守升华。</strong></p><p id="4321" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对抗性训练是一种机制，在这种机制中，对抗性事件的表现被添加到范式中，并被归类为具有挑战性。这种方法可以有效地减轻更具对抗性的机器学习攻击的发生，但需要相当程度的小心。</p><p id="c82c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">防御性蒸馏通过使一个模型预测先前学习的其他模型的性能，有助于使机器学习方法更加通用。这种技术可用于对未知威胁进行分类。这类似于对生成对抗网络(GANs)的思考，它一起建立了两个计算模型来加速机器学习过程——这是两个神经网络同时使用的概念。</p><h2 id="3eab" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jf km kn ko jj kp kq kr jn ks kt ku kv bi translated"><strong class="ak">对抗性学习的现实案例:</strong></h2><p id="eb4f" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated"><strong class="iw hj">2019 年，IBM Research、亚马逊和德克萨斯大学的研究人员生成了对抗性的插图，可以欺骗内容分类器机器学习模型，如垃圾邮件检测和情感检测系统。基于文本的对抗性实例，也称为“释义攻击”，改变一条信息中的特征向量，导致机器学习过程中的误诊失败，同时为特定用户保留一致的上下文。</strong></p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/1b6d7ff42ecb354185e6f6dd2a291cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2Dqy9UbDkR8WNYnK.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Keen 安全实验室</figcaption></figure><p id="9a8e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">腾讯敏锐的安全中心展示了他们是如何智胜特斯拉 Model S 进入变速车道，从而让它直接驶入迎面而来的车辆。他们需要做的是在车道上放置三个符号，创造一条线的假象。车辆的自动驾驶功能依赖于图像识别，它感应到这些符号，并感知它们以指示道路正在转向。所以汽车转向了那个方向。</strong></p></div></div>    
</body>
</html>