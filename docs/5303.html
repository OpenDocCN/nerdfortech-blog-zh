<html>
<head>
<title>Auto Differentiation with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流自动微分</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/auto-differentiation-with-tensorflow-790edd34a50a?source=collection_archive---------0-----------------------#2021-09-17">https://medium.com/nerd-for-tech/auto-differentiation-with-tensorflow-790edd34a50a?source=collection_archive---------0-----------------------#2021-09-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="cb3e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">让我们来解开自动分化的机制！</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/6c75f5b1d80ad0a74ab8fd1278ce0a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eieINarLoIsxjYNs6-Ur4Q.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">卢卡斯·法夫尔在<a class="ae jn" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="5edc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果读这篇文章的人是神经网络和其他东西的新手，那么这篇博客中的大部分内容将会很难理解。所以我建议在开始之前先阅读一下<a class="ae jn" rel="noopener" href="/artificialis/neural-networks-101-88d4b1f5d854">神经网络101 </a>。</p><p id="babf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们知道，在深度学习中，我们往往会有一个损失函数，它会根据我们的模型参数(权重和偏差)进行微分，这种情况会持续下去，直到参数找到最佳值。但是驱动所有这些计算的潜在机制是什么呢？</p><p id="8b3d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对你来说是微积分。</p><p id="4d7d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但这不是一个关于微积分101的博客，相反，我们将研究<strong class="jq hj">自动微分</strong>，它是深度学习框架的基础。因为我们都知道深度学习模型通常基于梯度技术工作，自动微分使我们很容易获得复杂或深度模型的梯度。</p><h1 id="5e64" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">为什么我们需要汽车差异化？</h1><p id="74ee" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">例如，对于下面的函数，我们可以很容易地计算偏导数，即计算<strong class="jq hj"> w1 </strong>和<strong class="jq hj"> w2 </strong>的梯度，并且方程是偏导数。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="fab7" class="lm kl hi li b fi ln lo l lp lq"><strong class="li hj"><br/>def </strong>f(w1, w2): <br/>  <strong class="li hj">return </strong>3* w1**2 + 2*w1*w2</span></pre><p id="d33c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里我们的函数看起来很简单，但是当我们构建一个深度神经网络的时候，情况就不一样了，我们会有一个10倍复杂的函数。更精确地说，用数学术语来说，我们要计算链式法则成千上万次。</p><p id="84aa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">此外，我们将在向前传递的过程中计算偏导数，对于一个完整的神经网络，这将是一个巨大的数字，几乎不可能跟踪这些导数，在计算梯度时，我们将需要这些偏导数。</p><p id="39bd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">好了，说了这么多术语，让我们花点时间看看这些术语对我们到底意味着什么。</p><h2 id="2d59" class="lm kl hi bd km lr ls lt kq lu lv lw ku jx lx ly kw kb lz ma ky kf mb mc la md bi translated">解构术语</h2><p id="8826" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">当我们第一次用参数计算损失函数时，它会返回偏导数，我们称这个过程为向前传递。</p><p id="03a6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正向传递负责用我们的参数计算损失函数。但我们知道，神经网络必须优化其参数，以实现最佳结果，即获得最小的损失误差。</p><p id="913a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但是，我们如何找到有助于神经网络找到最佳参数的值，以使损失最小化呢？</p><p id="a4e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">渐变。</p><p id="f371" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们必须通过激活<strong class="jq hj">反向传播</strong>(或)反向通道来获取梯度。首先，我们执行一个正向传递，得到我们的偏导数，通过激活反向传播，使用链式法则来计算梯度。</p><p id="e419" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">但是所有这些与汽车差异化有什么关系呢？</strong></p><p id="1e0c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">自动微分有助于我们跟踪这些计算，在反向传播过程中，它只需使用这些参数来计算梯度。我们知道，在偏导数的帮助下，我们能够计算可训练变量(权重和偏差)的梯度，并且仍然能够记录成千上万的导数和梯度。</p><p id="26fe" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">好了，理论到此为止，让我们进入一些代码，把这个东西包起来。让我们用上面的等式来做实验，我们要用张量流来做这个。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="edc4" class="lm kl hi li b fi ln lo l lp lq">w1 , w2 = 5 ,3  # params<br/>eps = 1e-6 # learning rate to step the params</span><span id="a404" class="lm kl hi li b fi me lo l lp lq"># Doing the computation by hand <br/>(f(w1 + eps, w2) - f(w1, w2)) / eps</span></pre><p id="cc1f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">需要为每个参数调用上述<strong class="jq hj">函数f() </strong>，对于大型神经网络而言，这将是一个繁琐的过程。因此，让我们通过使用TensorFlow的<strong class="jq hj"> GradientTape </strong>来美化它，它使用自动微分机制。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mf mg l"/></div></figure><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="e68f" class="lm kl hi li b fi ln lo l lp lq">[ &lt;tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0&gt;,&lt;tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0&gt; ]</span></pre><p id="2a55" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是如何工作的？</p><ul class=""><li id="4309" class="mh mi hi jq b jr js ju jv jx mj kb mk kf ml kj mm mn mo mp bi translated">首先，我们定义两个变量<code class="du mq mr ms li b">w1</code>和<code class="du mq mr ms li b">w2</code>。</li><li id="21c1" class="mh mi hi jq b jr mt ju mu jx mv kb mw kf mx kj mm mn mo mp bi translated"><code class="du mq mr ms li b">tf.GradientTape()</code>将自动记录每一个涉及变量的操作(只有可训练变量，tf.constants)</li><li id="2086" class="mh mi hi jq b jr mt ju mu jx mv kb mw kf mx kj mm mn mo mp bi translated">我们要求磁带计算结果(损失)z关于变量[ <code class="du mq mr ms li b">w1</code>和<code class="du mq mr ms li b">w2</code> ]的梯度。</li></ul><p id="a9ba" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">不仅结果准确，而且精度仅限于浮点误差。我们必须记住的一件重要事情是，<code class="du mq mr ms li b">gradient()</code>无论有多少个变量，该方法只遍历一次记录的计算(反向传递-反向顺序)。</p><p id="9f5c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们也可以通过在<code class="du mq mr ms li b">tf.GradientTape()</code>块中创建<code class="du mq mr ms li b">tape.stop_recording()</code>来暂停记录。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="75e1" class="lm kl hi li b fi ln lo l lp lq"># Returns the gradient and tape is erased<br/>dz_dw1 = tape.gradient(z, w1) </span><span id="00a9" class="lm kl hi li b fi me lo l lp lq"># Returns error RUNTIMEERROR<br/>dz_dw2 = tape.gradient(z , w2)</span></pre><p id="a943" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><code class="du mq mr ms li b">tape</code>在我们调用它的<code class="du mq mr ms li b">tape.gradient()</code>后会立即自动擦除。但是如果我们需要多次调用<code class="du mq mr ms li b">gradient()</code>，我们可以使用持久参数。</p><p id="70e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">磁带持久调用<code class="du mq mr ms li b">gradient()</code>并在每次执行后删除它。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="e97b" class="lm kl hi li b fi ln lo l lp lq"># Setting persistent to True </span><span id="6842" class="lm kl hi li b fi me lo l lp lq">with tf.GradientTape(persistent = True) as tape:<br/>   tape.watch(x)</span></pre><p id="6ac8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当我们实现一个定制的正则化损失时，可以使用<code class="du mq mr ms li b">tape.watch()</code>，当输入变化很小时，惩罚变化很大的激活，但是这个损失不会基于<code class="du mq mr ms li b">tf.Variables</code>，最有可能是基于<code class="du mq mr ms li b">tf.constant</code>张量。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mf mg l"/></div></figure><p id="a672" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你可以在TensorFlow文档中了解更多，我也会在下面附上一些链接。如果有任何问题，欢迎在评论中指出来。</p><p id="3cfe" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在那之前，</p><p id="3f08" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下次吧。</p></div><div class="ab cl my mz gp na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="hb hc hd he hf"><ul class=""><li id="29e8" class="mh mi hi jq b jr js ju jv jx mj kb mk kf ml kj mm mn mo mp bi translated">汽车差异化简介:<a class="ae jn" href="https://www.tensorflow.org/guide/autodiff" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/autodiff</a></li><li id="add7" class="mh mi hi jq b jr mt ju mu jx mv kb mw kf mx kj mm mn mo mp bi translated">反向模式自动差异从零开始:<a class="ae jn" href="https://sidsite.com/posts/autodiff/" rel="noopener ugc nofollow" target="_blank">https://sidsite.com/posts/autodiff/</a></li></ul></div></div>    
</body>
</html>