<html>
<head>
<title>PCA for Dimensionality Reduction and Visualization with Streamlit web-app</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Streamlit web应用程序进行PCA降维和可视化</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/pca-for-dimentionality-reduction-and-visualization-with-with-streamlit-web-app-931c7049d399?source=collection_archive---------12-----------------------#2021-05-10">https://medium.com/nerd-for-tech/pca-for-dimentionality-reduction-and-visualization-with-with-streamlit-web-app-931c7049d399?source=collection_archive---------12-----------------------#2021-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="db08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主成分分析(PCA)是一种维度缩减和可视化技术，其中包含大量维度(大于3)的数据集可以被缩减以用于在2D尺度上绘图，或者简单地缩减数据集中的维度数量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/ce04528aaf000b96851d411a1508e8cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vWo80cLPL_6jN4sD"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">艾萨克·史密斯在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="5cdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，让我们考虑虹膜数据集，这里我们有4列(加上一个目标变量—“<em class="ju">物种</em>”)，即<em class="ju">萼片_长度、萼片_宽度、花瓣_长度、花瓣_宽度</em>。这是一个四维数据集。现在的问题是我们如何在2D尺度上想象这个？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jv"><img src="../Images/cb550926ed6f9f9b36f69773abc7e3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-49VbIFD2htthT-iEOUIlw.png"/></div></div></figure><p id="3725" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是使用PCA等技术的地方。使用PCA，可以在2D尺度上可视化这个4维数据集，以提供对数据的更多洞察。</p><p id="0142" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PCA之后:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jw"><img src="../Images/678e3288a45cf584a51c69f99fafec8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Gr0V8xbyJUgTitqEugavw.png"/></div></div></figure><p id="b460" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里使用主成分分析将整个4D数据绘制到2D标度上。我们可以看到存在清晰的聚类，这提供了对数据的更多洞察。</p><p id="1cc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以在这里找到我在PCA上的web应用程序:</p><p id="b044" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://share.streamlit.io/kssubhodh/pca-streamlit/main/pca-app.py" rel="noopener ugc nofollow" target="_blank">https://share . streamlit . io/kssubhodh/PCA-streamlit/main/PCA-app . py</a></p></div><div class="ab cl jx jy gp jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="hb hc hd he hf"><h2 id="899b" class="ke kf hi bd kg kh ki kj kk kl km kn ko iq kp kq kr iu ks kt ku iy kv kw kx ky bi translated">更深入的探讨:</h2><p id="dbe8" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">PCA旨在找到方差最大的方向，并将数据点投影到该向量上以获得主分量。这些主要组件可以用于<em class="ju">可视化</em>，这将是这篇博客的主要焦点。</p><p id="d010" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">进行PCA主要有两种方法<em class="ju">方差最大化</em>和<em class="ju">距离缩减</em>。我们将通过相关的例子更深入地研究方差最大化方法</p><h2 id="f452" class="ke kf hi bd kg kh ki kj kk kl km kn ko iq kp kq kr iu ks kt ku iy kv kw kx ky bi translated">但是首先:为什么是方差？</h2><p id="6f6d" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">方差给我们提供了<strong class="ih hj"> <em class="ju">价差</em> </strong>的数据。差异越大，包含的信息就越多。用最大的<em class="ju">传播/方差</em>保持方向给了我们最多的信息</p><p id="9848" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:让我们考虑需要简化为1D(线)的2D数据</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/56340f2238163705addd2c8cea4f41c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kGeQ4nqmEuZivu2VCZA8kg.png"/></div></div></figure><p id="02ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，沿X轴(<em class="ju"> Vx </em>)的方差(扩散/信息)远大于沿Y轴(<em class="ju"> Vy </em>)的扩散。因此，点被投影到X轴上以获得主分量(方差最大的方向)。这里，本质上，我们保留了最大方差的方向。关于如何获得主成分的详细步骤将在博客中进一步解释。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/196b24d2e40866d728e2c1769a4806a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A70Y6w8k5XmZ865g_UnQOg.png"/></div></div></figure><p id="d0df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的数据从2D规模减少到1D线。</p></div><div class="ab cl jx jy gp jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="hb hc hd he hf"><h1 id="7354" class="lf kf hi bd kg lg lh li kk lj lk ll ko lm ln lo kr lp lq lr ku ls lt lu kx lv bi translated">执行PCA的步骤</h1><h2 id="22fc" class="ke kf hi bd kg kh ki kj kk kl km kn ko iq kp kq kr iu ks kt ku iy kv kw kx ky bi translated">1 .列标准化</h2><p id="5134" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated"><strong class="ih hj">数据标准化</strong>是重新调整一个或多个属性的过程，使它们的<strong class="ih hj">平均值为0，标准差为1 </strong>。这样做是为了消除每个列的缩放比例的影响。</p><p id="3322" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">示例:如果您有两个特征，一个范围从0到1000，另一个范围从-1到+1，在计算PCA时，第一个特征是首选的，因为它具有较高的方差(扩散)。为了防止这种情况发生，我们对每一列进行了标准化。</p><p id="b2e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准化后，每个列/特征对分析的贡献相等，并将转换为相同的比例(平均值= 0，标准差= 1)</p><p id="4f61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准化公式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/fc1d7c74e28e9b700add2b5953c8c81e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5n67QPoNtYW-Iwr9UOW03g.png"/></div></div></figure><p id="e140" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iris数据集的列标准化后:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/e29ce4f7e1ae34b9402925c56ff4b38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*RsnCm6HfEA1JOvaHWixzWg.png"/></div></figure><h2 id="8f16" class="ke kf hi bd kg kh ki kj kk kl km kn ko iq kp kq kr iu ks kt ku iy kv kw kx ky bi translated">2.协方差矩阵</h2><p id="9b82" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">下一步是创建协方差矩阵。这告诉我们不同输入变量之间的关系。协方差矩阵包含每对输入变量的协方差得分。</p><p id="c96c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里协方差分数的符号很重要:</p><ul class=""><li id="af60" class="ly lz hi ih b ii ij im in iq ma iu mb iy mc jc md me mf mg bi translated"><strong class="ih hj">如果为正，则</strong>:两个变量同时增加或减少(正相关)</li><li id="f417" class="ly lz hi ih b ii mh im mi iq mj iu mk iy ml jc md me mf mg bi translated"><strong class="ih hj">如果为负，则</strong>:一个增加，另一个减少(反向相关)</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/0c1b1b48dbcbcac37cef7b9541c58455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*M-x1LztcAAHxInsa04XPkA.png"/></div></figure><p id="95b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">沿着对角线的协方差分数包含方差值，即Cov(x，x) = var(x)等</p><h2 id="f19e" class="ke kf hi bd kg kh ki kj kk kl km kn ko iq kp kq kr iu ks kt ku iy kv kw kx ky bi translated">3.特征向量，特征值</h2><p id="5be9" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">下一步是计算协方差矩阵的特征向量和特征值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/4454a18569c8310c92e316adf559b8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*TWvhJDcrfvIvm76CV5actg.png"/></div></figure><ul class=""><li id="e981" class="ly lz hi ih b ii ij im in iq ma iu mb iy mc jc md me mf mg bi translated">特征向量给出了最大方差的方向，因此给出了主分量。特征值给我们提供了解释方差的百分比或每个主成分中包含的方差量的信息。</li><li id="079f" class="ly lz hi ih b ii mh im mi iq mj iu mk iy ml jc md me mf mg bi translated">特征向量和特征值总是成对出现。因此，对于一个4维数据集，有4个特征向量和值对。当协方差矩阵的相应特征对基于它们的值以降序排序时，第一特征向量(最高特征值)对应于第一主分量或具有最高方差的方向。</li></ul><h2 id="a4f9" class="ke kf hi bd kg kh ki kj kk kl km kn ko iq kp kq kr iu ks kt ku iy kv kw kx ky bi translated">4.将点投影到顶部特征向量上</h2><p id="d6f1" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">现在我们已经计算出了特征对，并且找到了方差最大的方向，那么我们如何使用它来降低维数或者绘图呢？</p><p id="bcb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须将现有的点投影到新的向量(特征向量)上，以获得主分量。在点的投影之后，我们获得最终的数据集(具有主成分),其可以用于可视化/维度缩减的目的。</p><p id="b260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ju">示例</em> </strong>:在iris数据集(4-D)中，执行步骤1-4后，我们现在有了顶部主成分(总共4个)。我们的目标是在2D表面上可视化这个4维数据集。因此，我们仅取前2个主成分(它们具有最高的方差，因此给我们最多的信息)并投影这些点以获得2D可视化。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mo"><img src="../Images/2d090e30fcc9ad93780663c8334c1761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gl8z2-NhYPBHzDb__OmQsA.png"/></div></div></figure><p id="6e6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ju">在执行步骤1-4并获得新矢量和原始数据的点积(投影)后，获得最终数据集</em> </strong>。这里我们得到了最终的主成分(PCA_1-PCA_4 ),它既可以用于可视化，也可以用于降维。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mp"><img src="../Images/d20a31c13b8c46d184c1902552a4dd88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CY-3034Yj52Bxx7IBcQRFQ.png"/></div></div></figure><p id="1d2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用PCA_1和PCA_2后的2D可视化(具有前2个最高方差/扩散的主成分) :</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jw"><img src="../Images/678e3288a45cf584a51c69f99fafec8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Gr0V8xbyJUgTitqEugavw.png"/></div></div></figure></div><div class="ab cl jx jy gp jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="hb hc hd he hf"><h1 id="f890" class="lf kf hi bd kg lg lh li kk lj lk ll ko lm ln lo kr lp lq lr ku ls lt lu kx lv bi translated">PCA的缺点</h1><ol class=""><li id="c43f" class="ly lz hi ih b ii kz im la iq mq iu mr iy ms jc mt me mf mg bi translated"><strong class="ih hj">自变量变得更难解释</strong>:对数据集实施PCA后，你原来的特征会变成主成分。主成分是原始特征的线性组合。主要成分不像原始特征那样可读和可解释。</li><li id="4fce" class="ly lz hi ih b ii mh im mi iq mj iu mk iy ml jc mt me mf mg bi translated"><strong class="ih hj">信息损失:</strong>虽然主成分试图覆盖数据集中特征之间的最大方差，但如果我们不仔细选择主成分的数量，与原始特征列表相比，它可能会丢失一些信息。</li><li id="420a" class="ly lz hi ih b ii mh im mi iq mj iu mk iy ml jc mt me mf mg bi translated"><strong class="ih hj">数据标准化是PCA之前必须的:</strong>你必须在实现PCA之前将你的数据标准化，否则PCA将无法找到最优的主成分。</li><li id="d500" class="ly lz hi ih b ii mh im mi iq mj iu mk iy ml jc mt me mf mg bi translated">PCA不能很好地处理高度聚集的数据，并且不能保留数据中的现有模式。</li></ol></div><div class="ab cl jx jy gp jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="hb hc hd he hf"><p id="9649" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ju">你可以在这里参考我的web-app主成分分析演示:</em> </strong></p><p id="06ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://share.streamlit.io/kssubhodh/pca-streamlit/main/pca-app.py" rel="noopener ugc nofollow" target="_blank">https://share . streamlit . io/kssubhodh/PCA-streamlit/main/PCA-app . py</a></p><p id="ac2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ju"> Github对app的回购:</em> </strong></p><div class="mu mv ez fb mw mx"><a href="https://github.com/kssubhodh/PCA-streamlit" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">kssubhodh/PCA-streamlit</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">简化演示主成分分析的网络应用程序链接到应用程序…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">github.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl jn mx"/></div></div></a></div><p id="e384" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ju"> Python为PCA提供了一个易于实现的模块，同样可以在这里找到:</em> </strong></p><div class="mu mv ez fb mw mx"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">sk learn . decomposition . PCA-sci kit-learn 0 . 24 . 2文档</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">主成分分析。使用数据的奇异值分解进行线性降维…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">scikit-learn.org</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl jn mx"/></div></div></a></div><p id="ccec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。希望这篇博客对你有所帮助。</p><p id="4df0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在linkedin上联系我:</p><div class="mu mv ez fb mw mx"><a href="https://www.linkedin.com/in/subhodh-ks-8b993b177/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">印度卡纳塔克邦本加卢鲁工程学院</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">在世界上最大的职业社区LinkedIn上查看Subhodh KS的个人资料。Subhodh有一个工作列在他们的…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">www.linkedin.com</p></div></div><div class="ng l"><div class="nn l ni nj nk ng nl jn mx"/></div></div></a></div></div></div>    
</body>
</html>