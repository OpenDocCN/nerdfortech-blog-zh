<html>
<head>
<title>K-Means Clustering: Python Implementation from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means 集群:Python 从头实现</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/k-means-python-implementation-from-scratch-8400f30b8e5c?source=collection_archive---------0-----------------------#2021-07-02">https://medium.com/nerd-for-tech/k-means-python-implementation-from-scratch-8400f30b8e5c?source=collection_archive---------0-----------------------#2021-07-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/45d7569406a02732f7d088b3efe3eac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLh15KDN1bRT93UcfH4ovA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:<a class="ae iu" href="https://towardsai.net/p/machine-learning/a-simple-and-scalable-clustering-algorithm-for-data-summarization" rel="noopener ugc nofollow" target="_blank">朝向 AI </a></figcaption></figure><p id="6a28" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">聚类</strong>是根据数据中的模式将整个数据分成组(称为<strong class="ix hj">聚类</strong>)的过程。这是一个无监督的机器学习问题，因为这里我们没有目标变量，我们只根据它们的相似性对数据点进行分组。</p><p id="b4fa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">集群的属性</strong></p><ol class=""><li id="7893" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">一个聚类中的所有数据点彼此相似。</li><li id="3b01" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">来自不同聚类的数据点尽可能不同。</li></ol><p id="9e76" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">聚类的应用</strong></p><ol class=""><li id="7ac1" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">客户细分</li><li id="bb1e" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">推荐系统</li><li id="c9fb" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">图象分割法</li><li id="8f6b" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">文档聚类</li></ol><p id="b371" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> K 均值算法</strong></p><blockquote class="kh ki kj"><p id="3004" class="iv iw kk ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated"><em class="hi">K-Means 算法的主要目标是最小化数据点与其各自聚类质心之间的距离总和。</em></p></blockquote><p id="34eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文的范围只是使用 python 从头开始实现 k-means。如果你是 k-means 聚类的新手，想了解更多，可以参考<a class="ae iu" href="https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/#k-means-clustering-python-code" rel="noopener ugc nofollow" target="_blank">这篇</a>惊人的文章。</p><p id="7f19" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们直接进入实现。您也可以从<a class="ae iu" href="https://colab.research.google.com/drive/1xEmSwgKk_VXbI2GBwi56aPbRGk7Uy7Wb?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>访问下面给出的代码。点击从<a class="ae iu" href="https://drive.google.com/file/d/16kLeQ2F8Dwj7Oiv3C1Bz7KXUfvXlsKtO/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">下载数据集。</a></p><p id="224a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">导入库</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="c11a" class="kx ky hi kt b fi kz la l lb lc">import numpy as np</span><span id="874a" class="kx ky hi kt b fi ld la l lb lc">import pandas as pd</span><span id="b9e3" class="kx ky hi kt b fi ld la l lb lc">import matplotlib.pyplot as plt</span><span id="8c29" class="kx ky hi kt b fi ld la l lb lc">import seaborn as sns</span><span id="34b1" class="kx ky hi kt b fi ld la l lb lc">import warnings</span><span id="b8cb" class="kx ky hi kt b fi ld la l lb lc">warnings.filterwarnings('ignore')</span></pre><p id="b423" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">导入数据集</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="9b51" class="kx ky hi kt b fi kz la l lb lc">data = pd.read_csv('clustering.csv')</span><span id="7395" class="kx ky hi kt b fi ld la l lb lc">data.head()</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/01df76fd553f22c7a8139687569e9526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AzP8rl5N0Tc649dsLH-8fg.png"/></div></div></figure><p id="c121" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，我们将只使用两个特征['ApplicantIncome '，' LoanAmount']，以便我们可以在 2D 平面上可视化集群。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="2a9f" class="kx ky hi kt b fi kz la l lb lc">data = data.loc[:, ['ApplicantIncome', 'LoanAmount']]</span><span id="8951" class="kx ky hi kt b fi ld la l lb lc">data.head(2)</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/72c168678dbde974fe099acfc644b0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*7PlnNqi9JZ6cIJD7V5tYjw.png"/></div></figure><p id="734c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">转换为 numpy 数组</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="4364" class="kx ky hi kt b fi kz la l lb lc">X = data.values</span></pre><p id="e4fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">可视化数据点</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="4c51" class="kx ky hi kt b fi kz la l lb lc">sns.scatterplot(X[:,0], X[:, 1])</span><span id="9f31" class="kx ky hi kt b fi ld la l lb lc">plt.xlabel('Income')</span><span id="0e36" class="kx ky hi kt b fi ld la l lb lc">plt.ylabel('Loan')</span><span id="5fa7" class="kx ky hi kt b fi ld la l lb lc">plt.show()</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/01cd45334c59b52e375cd49b11edbbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*V0cJRAPYv0MOjzlbZkCfFg.png"/></div></figure><p id="e45c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">计算 WCSS </strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="9c44" class="kx ky hi kt b fi kz la l lb lc">def calculate_cost(X, centroids, cluster):</span><span id="4dee" class="kx ky hi kt b fi ld la l lb lc">  sum = 0</span><span id="4b07" class="kx ky hi kt b fi ld la l lb lc">  for i, val in enumerate(X):</span><span id="cdba" class="kx ky hi kt b fi ld la l lb lc">    sum += np.sqrt((centroids[int(cluster[i]), 0]-val[0])**2 +(centroids[int(cluster[i]), 1]-val[1])**2)</span><span id="0b3f" class="kx ky hi kt b fi ld la l lb lc">  return sum</span></pre><p id="28bf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">执行 K-Means </strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="ee60" class="kx ky hi kt b fi kz la l lb lc">def kmeans(X, k):</span><span id="4f01" class="kx ky hi kt b fi ld la l lb lc">  diff = 1</span><span id="8174" class="kx ky hi kt b fi ld la l lb lc">  cluster = np.zeros(X.shape[0])</span><span id="f0d3" class="kx ky hi kt b fi ld la l lb lc">  centroids = data.sample(n=k).values</span><span id="bbaa" class="kx ky hi kt b fi ld la l lb lc">  while diff:</span><span id="2309" class="kx ky hi kt b fi ld la l lb lc">     # for each observation</span><span id="1241" class="kx ky hi kt b fi ld la l lb lc">     for i, row in enumerate(X):</span><span id="6bfb" class="kx ky hi kt b fi ld la l lb lc">         mn_dist = float('inf')</span><span id="07e1" class="kx ky hi kt b fi ld la l lb lc">        # dist of the point from all centroids</span><span id="81bc" class="kx ky hi kt b fi ld la l lb lc">        for idx, centroid in enumerate(centroids):</span><span id="807e" class="kx ky hi kt b fi ld la l lb lc">            d = np.sqrt((centroid[0]-row[0])**2 + (centroid[1]-row[1])**2)</span><span id="ce2f" class="kx ky hi kt b fi ld la l lb lc">            # store closest centroid</span><span id="2165" class="kx ky hi kt b fi ld la l lb lc">            if mn_dist &gt; d:</span><span id="bcf9" class="kx ky hi kt b fi ld la l lb lc">               mn_dist = d</span><span id="64ef" class="kx ky hi kt b fi ld la l lb lc">               cluster[i] = idx</span><span id="1207" class="kx ky hi kt b fi ld la l lb lc">     new_centroids = pd.DataFrame(X).groupby(by=cluster).mean().values</span><span id="e81c" class="kx ky hi kt b fi ld la l lb lc">     # if centroids are same then leave</span><span id="416a" class="kx ky hi kt b fi ld la l lb lc">     if np.count_nonzero(centroids-new_centroids) == 0:</span><span id="80b9" class="kx ky hi kt b fi ld la l lb lc">        diff = 0</span><span id="d513" class="kx ky hi kt b fi ld la l lb lc">      else:</span><span id="01f0" class="kx ky hi kt b fi ld la l lb lc">        centroids = new_centroids</span><span id="414c" class="kx ky hi kt b fi ld la l lb lc">  return centroids, cluster</span></pre><p id="f0dd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">用肘法求 K 值</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="ee75" class="kx ky hi kt b fi kz la l lb lc">cost_list = []</span><span id="8976" class="kx ky hi kt b fi ld la l lb lc">for k in range(1, 10):</span><span id="b7f7" class="kx ky hi kt b fi ld la l lb lc">    centroids, cluster = kmeans(X, k)</span><span id="10c8" class="kx ky hi kt b fi ld la l lb lc">    # WCSS (Within cluster sum of square)</span><span id="fbef" class="kx ky hi kt b fi ld la l lb lc">    cost = calculate_cost(X, centroids, cluster)</span><span id="d26d" class="kx ky hi kt b fi ld la l lb lc">    cost_list.append(cost)</span></pre><p id="9178" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在 WCSS 和 k 之间画一条线</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="9587" class="kx ky hi kt b fi kz la l lb lc">sns.lineplot(x=range(1,10), y=cost_list, marker='o')</span><span id="c17e" class="kx ky hi kt b fi ld la l lb lc">plt.xlabel('k')</span><span id="ac3b" class="kx ky hi kt b fi ld la l lb lc">plt.ylabel('WCSS')</span><span id="78f9" class="kx ky hi kt b fi ld la l lb lc">plt.show()</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/51cc592cfd8e5a5849084a0d3217aeb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*yKnJDkyb0hbflh1PiyoPPQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">WCSS 在 k=4 之后没有减少很多，所以形成 4 个集群</figcaption></figure><p id="95c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">制作集群</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="7eeb" class="kx ky hi kt b fi kz la l lb lc">k = 4</span><span id="ce0d" class="kx ky hi kt b fi ld la l lb lc">centroids, cluster = kmeans(X, k)</span></pre><p id="3d7e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">可视化形成的集群</strong></p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="be75" class="kx ky hi kt b fi kz la l lb lc">sns.scatterplot(X[:,0], X[:, 1], hue=cluster)</span><span id="4eec" class="kx ky hi kt b fi ld la l lb lc">sns.scatterplot(centroids[:,0], centroids[:, 1], s=100, color='y')</span><span id="7a91" class="kx ky hi kt b fi ld la l lb lc">plt.xlabel('Income')</span><span id="495d" class="kx ky hi kt b fi ld la l lb lc">plt.ylabel('Loan')</span><span id="7417" class="kx ky hi kt b fi ld la l lb lc">plt.show()</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/70c2d2aac2a1824cbcf9f149225de654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*X2m9l6xJrYtrEG43DVBapw.png"/></div></div></figure><p id="ee1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以清楚地看到四个星团的形成。绿点代表每个聚类的质心。这样我们就完成了 K-Means 算法的实现。这不是很简单吗？</p><p id="36af" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望你觉得这个博客有用。如果你喜欢它，那么<strong class="ix hj">给它一个掌声，并跟随</strong> <strong class="ix hj">我</strong>阅读我即将发布的博客。也看看我以前的一些博客:</p><ol class=""><li id="a8d5" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><a class="ae iu" href="https://khushijain2810.medium.com/naive-bayes-algorithm-implementation-from-scratch-f9a2a12789b5" rel="noopener">朴素贝叶斯算法</a></li><li id="d4ff" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" rel="noopener" href="/nerd-for-tech/k-nearest-neighbors-aac72032aaea?source=friends_link&amp;sk=84ab45f698fe6bb81494f79f1aa53c7f"> KNN 算法</a></li><li id="bcb1" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://khushijain2810.medium.com/linear-regression-9fd219098405" rel="noopener">线性回归</a></li><li id="576f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://khushijain2810.medium.com/introduction-to-opencv-586e38d536fd" rel="noopener"> OpenCV </a></li><li id="60ee" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://khushijain2810.medium.com/seaborn-data-visualization-library-142ac64d5560" rel="noopener"> Seaborn </a></li><li id="ad81" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">熊猫</li><li id="e1ce" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://khushijain2810.medium.com/numpy-day-3-at-internity-foundation-efcef826e549" rel="noopener">数字价格</a></li></ol><p id="2fa4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">快乐学习！</p></div></div>    
</body>
</html>