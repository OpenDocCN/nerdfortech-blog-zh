<html>
<head>
<title>Target Prediction using Single-layer Perceptron and Multilayer Perceptron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用单层感知器和多层感知器的目标预测</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/flux-prediction-using-single-layer-perceptron-and-multilayer-perceptron-cf82c1341c33?source=collection_archive---------1-----------------------#2021-04-10">https://medium.com/nerd-for-tech/flux-prediction-using-single-layer-perceptron-and-multilayer-perceptron-cf82c1341c33?source=collection_archive---------1-----------------------#2021-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="959b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文主要是为那些想要开始数据科学之旅或者愿意用python编码从头开始学习神经网络的初学者而写的。</p><p id="5e9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">目标:</strong></p><ul class=""><li id="dca6" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">基本感知器</li><li id="bc8f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">多层感知器</li><li id="a9e3" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">用一个小数据集深入编码</li><li id="32c6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">优化者</li><li id="3808" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">管道和应用部署</li><li id="970b" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">结论</li></ul><h1 id="6d5a" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">基本感知器</h1><p id="e676" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">感知器是一种神经网络单元(人工神经元)，它执行某些计算来检测输入数据中的功能或商业智能。</p><p id="6f5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知器是弗兰克·罗森布拉特在1957年提出的。他提出了基于原始MCP神经元的感知器学习规则。</p><p id="78a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知器是用于二进制分类器的监督学习的算法。这种算法使神经元能够一次一个地学习和处理训练集中的元素。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ku"><img src="../Images/0b5fc44bddadd6e0e325f8191be4eccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*_Epn1FopggsgvwgyDA4o8w.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">来源:<a class="ae lg" href="https://www.javatpoint.com/single-layer-perceptron-in-tensorflow" rel="noopener ugc nofollow" target="_blank">https://www . Java point . com/single-layer-感知器-in-tensorflow </a></figcaption></figure><p id="f33d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单层感知器只能学习线性可分模式。对于分类，我们用激活函数作为阈值来预测类别。对于回归，我们不需要激活函数(阈值)，或者我们可以使用线性函数来预测连续值。</p><p id="ab3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入通常是一个<strong class="ih hj">特征</strong>向量<code class="du lh li lj lk b">x</code>乘以<strong class="ih hj">权重</strong> <code class="du lh li lj lk b">w</code>并加到一个<strong class="ih hj">偏差</strong> <code class="du lh li lj lk b">b</code> : <code class="du lh li lj lk b">y = w * x + b</code></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ll"><img src="../Images/9373ba9e9fd481e8478c89d052851b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*uVujGMbb3ezk7s_DyccNSQ.png"/></div></figure><p id="6857" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> w </strong>表示权重向量，<strong class="ih hj"> x </strong>是输入向量，<strong class="ih hj"> b </strong>是偏差，<strong class="ih hj"> φ </strong>是非线性激活函数。</p><p id="1a58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于通过反向传播进行权重更新或感知器学习。我们将在下一节中详细了解这一点。</p><h1 id="39df" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">多层感知器</h1><p id="6503" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">单层感知器不能计算出数据的非线性或复杂性。因此，研究人员利用单层感知器的思想开发了多层感知器。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lm"><img src="../Images/6d32e04da972052c0779738d5c0d28b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*OMxLA9gMGiV73jmi.jpg"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">来源:<a class="ae lg" href="https://www.tutorialspoint.com/tensorflow/tensorflow_multi_layer_perceptron_learning.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/tensor flow/tensor flow _ multi _ layer _ perceptron _ learning . htm</a></figcaption></figure><p id="c128" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用多个隐藏层来发现数据的非线性。这种指令也称为前馈网络。</p><p id="ca2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多层感知器在一组输入-输出对上进行训练，并学习对这些输入和输出之间的相关性(或依赖性)进行建模。<strong class="ih hj"> <em class="ln">训练包括调整模型的参数或权重和偏差，以最小化误差。</em> </strong> <em class="ln">反向传播用于使那些权重和偏差相对于误差进行调整。</em></p><p id="bb2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，模型如何从错误中学习？让我们看看学习过程:</p><p id="69be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习有两种方式，向前传播和向后传播</p><ol class=""><li id="f583" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc lo jj jk jl bi translated"><strong class="ih hj">正向传播</strong>:在<em class="ln">正向传递</em>中，信号流从输入层通过隐藏层移动到输出层，输出层的决策根据地面真实标签进行测量。然后将发现错误与地面真相和真实标签相对照。</li><li id="fd4d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc lo jj jk jl bi translated"><strong class="ih hj">反向传播</strong>:下面是提及反向支柱如何工作的步骤。</li></ol><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lp"><img src="../Images/313d914da46cd42569404efd0151c9ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*JNZQMuqkdODnBwUkKr7RpA.png"/></div></figure><p id="63a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的例子中，我使用SGD作为权重更新的优化算法。但是有各种优化算法，我将在本文中使用<strong class="ih hj"> SGD、Momentum、</strong>和<strong class="ih hj"> Adam </strong></p><h1 id="356f" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">优化算法:</h1><p id="cb9b" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">优化器是用来改变神经网络属性的算法或方法，如权重和学习速率，以减少损失。关于优化者的更多信息，请查看这篇<a class="ae lg" href="https://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="ln">文章</em> </strong> </a></p><ol class=""><li id="110a" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc lo jj jk jl bi translated"><strong class="ih hj">新币</strong></li></ol><p id="0801" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是梯度下降的一个变种。它试图更频繁地更新模型的参数。在这种情况下，在计算每个训练样本的损失之后，更新模型参数。因此，如果数据集包含1000行，SGD将在数据集的一个周期内更新模型参数1000次，而不是像梯度下降那样更新一次。</p><p id="60cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着每个数据点的权重/参数都会更新。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/86e223a5edc7bd190252a973bde43f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*NaiPO1gUT4gdwLoTcSTZNA.png"/></div></div></figure><p id="e3d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">势头:</strong></p><p id="4515" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">动量是为了减少SGD中的高方差和软化收敛而发明的。它加速了向相关方向的收敛，减少了向无关方向的波动。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lv"><img src="../Images/cd878c59e5c88c54e50b6c937e99fbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*mDu8LigOgSEjhWG8_Vqtug.png"/></div></figure><p id="cafa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.亚当:</p><p id="75bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自适应矩估计(Adam)适用于一阶和二阶动量。Adam背后的直觉是，我们不希望滚动得太快，因为我们可以跳过最小值，我们希望稍微降低速度，以便仔细搜索。除了存储像<strong class="ih hj"> AdaDelta </strong>，<strong class="ih hj"><em class="ln">Adam</em></strong><em class="ln"/>这样的过去平方梯度的指数衰减平均值之外，还保存过去梯度的指数衰减平均值</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lm"><img src="../Images/dcb4af47cfaf2a971dae5dfc2e0be940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*WpB7MQPN-pfcVBUQzj1pzw.png"/></div></figure><p id="1911" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，这里将总结感知器和多层感知的历史和介绍，包括各种优化算法。现在，我们将通过一个例子(使用小数据集)和python从头开始编码来说明这一切是如何工作的。</p><h1 id="b1c5" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">数据集</strong></h1><p id="5bd2" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">为了训练单层感知和多层感知，我使用了一个小数据集。让我们看看数据是怎样的，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lw"><img src="../Images/5d4eb550abfb6a6b7fb5c034ef5a2184.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*rh1w0g3c6Xu9gVo140dBOQ.png"/></div></figure><p id="082c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有6个特征，<strong class="ih hj">目标</strong>是我们必须预测的因变量。这显然是一个回归问题，因为输出变量是一个连续的数值。</p><p id="0600" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述数据组具有它们自己的方程，这些方程非常特定于领域，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lx"><img src="../Images/b5c13a835b43e684fb1b4092dfa668cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*wBtrECxCT2h7mS7pnYB41w.png"/></div></figure><p id="9aa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中a，b，c，d，e，f是未知参数。</p><p id="4150" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了简化上述方程，我们将两边的对数都取为，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ly"><img src="../Images/d258018588373febc698211b6fc2127f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*e3naFNiigrccn63040ZtlQ.png"/></div></figure><p id="0e06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lz"><img src="../Images/60b0007ae485b87553710ea1232ec596.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*_9NqKZE1j4Mu8frHahtGIQ.png"/></div></figure><p id="af96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ma"><img src="../Images/fec482d7eea9b28950411241d5bb6258.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*MagmS03T-OT42BO9qzeBAQ.png"/></div></figure><p id="9829" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们需要优化上面的方程，需要找到像a，b，c，d，e，f这样的优化参数。</p><h1 id="5a71" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">单层感知器:</strong></h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mb"><img src="../Images/c6daf0ef87d012681b2eb35c12052425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*m69uyFcPf36Lx5cj9FTQhQ.jpeg"/></div></figure><p id="f960" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个问题，我使用MSE作为损失函数，它可以为单个点定义为，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mc"><img src="../Images/39504b66e543a62f4c092626f7bbe5ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*THGqfGoclLEOVMZc0HTO4Q.png"/></div></figure><p id="0467" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，除了梯度之外，所有方程都已定义，</p><p id="1077" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们需要计算梯度来更新权重，以最小化损失，从而获得最优解。这是我们在反向传播中看到的。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es md"><img src="../Images/069ce2cac160de0e3f4551a762189884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*I0eSFJUkvCFSXvHBZaEI9Q.jpeg"/></div></figure><p id="b25b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，我们正在计算“a”的梯度。我们将按照箭头方向计算导数，我们将使用链式法则。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es me"><img src="../Images/f644b0a15009468af4afef300b0aae71.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*lUV55kti-NSmH1zIB2RA_g.png"/></div></figure><p id="c383" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，我们要找到所有参数的导数</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mf"><img src="../Images/5475ab0a7f5edae8eb33ea81a3d51e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*Pov86ixDjPE73KX7d36OMw.png"/></div></figure><p id="d0a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们得到如下所示的梯度矩阵，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mg"><img src="../Images/769aacc34911b35acd403d18aa9e3bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*Inn1nNUAURdwG-ITm6Y7lQ.png"/></div></figure><p id="d29d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是时候更新权重了，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mh"><img src="../Images/58c32efbe4848ccc03477db9e2eaa53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*gWndChFeaBzDu-4HLtdwhw.png"/></div></figure><p id="b7b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将使用python实现所有方程，</p><p id="69f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们将看到正向传播，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="19e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码中，np.dot(X，w)将进行矩阵乘法，就像我们在上面的等式中看到的那样</p><p id="e245" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将看到反向传播，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="6a87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是参数/权重更新的时候了，以减少误差/损失。首先，我们将使用一个<strong class="ih hj"> SGD </strong>优化器，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mk"><img src="../Images/c244997c452e3d566f01c5ca64cbe7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*6hAkLl6REEc95pFIkXSo_w.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">使用SGD优化器的损失图</figcaption></figure><p id="3aa9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以从损失-时期图中清楚地看到，模型过拟合。与测试损失相比，训练损失非常低，这意味着模型在减少训练损失方面做得很好，但未能减少测试损失。此外，训练和测试损失之间的差异很大，这清楚地表明模型过度拟合。</p><p id="8efc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将看到当我们使用<strong class="ih hj">动量</strong>优化器时会发生什么，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ml"><img src="../Images/f3f1348352f76c51ee95827c42f0ca14.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*F0SgpWiJWcCD8TBNf_1ufA.png"/></div></figure><p id="1f72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里同样的故事再次重复，因为我们看到与SGD优化。</p><p id="fc85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将看到当我们使用<strong class="ih hj"> Adam </strong>优化器时会发生什么，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mm"><img src="../Images/3f7186ba37a59de820b2fc164d631987.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*gUPCqx_4AXkEBJpCwL1hhg.png"/></div></figure><p id="0896" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这也是训练和测试损失之间的巨大差距。一般情况下<em class="ln">单层感知无法将测试损失降至最低</em>。所以我们会看到如果我们使用多层感知会发生什么。</p><h1 id="a523" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">多层感知:</h1><p id="e957" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">针对这个问题，我们设计多层感知如下图所示:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mn"><img src="../Images/89b313fd7fa0ed1eb460b5fc348fa2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*y_TDJf_jVXa9-zApQqZJsQ.jpeg"/></div></figure><p id="d632" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我只使用了一个隐藏层之间的输入和输出的sigmoid激活函数。</p><p id="478a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重矩阵W1(在输入层和隐藏层之间)和W2(隐藏层和输出层)是，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mo"><img src="../Images/93d565d4766fb3d8cac36a7fa09ed03a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*A60sdLhXmbUio1zqRZFLuQ.png"/></div></figure><p id="c0d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个权重/参数的梯度计算如下:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mp"><img src="../Images/cbaa6338c635848178e4f2d5aacde079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*-wigXHHnyvd2RajOHtX2AA.jpeg"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mq"><img src="../Images/52db50b8a0925a95cd58ca034dee6f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*Lsqa8-Usq6GGtj-7uFjI-w.png"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mr"><img src="../Images/478d35b17b2789b989f8817351a67657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*kVVZ5blUNpsFUwYu0tHzNw.jpeg"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ms"><img src="../Images/057b6e1d720cc307cfb983cb5773a7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*YwICN05YdCDJwN9-zxVhiQ.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">Sigmoid函数及其导数</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mt"><img src="../Images/61fc9b845c3cd4c1b3a8958dfc832007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*b-AjNbjZrQhbuBmx1R-yXw.png"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mu"><img src="../Images/a6fa2f8fe3b43410e1a11dff9e4b75a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*f8_uKu5Jm5qa8fgjR7u0IQ.png"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mv"><img src="../Images/e8427ed253a8f9189a183fddecdea5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*R5lUSa3OkxWPwjUXAWsuuQ.png"/></div></figure><p id="c277" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们得到梯度矩阵，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mw"><img src="../Images/8b53d558837b7bdd2a1536870859d090.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*GlLdODAx9_-3AuAhj8YXUw.png"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mg"><img src="../Images/70145b861868de8257cbf225dc601c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*YkPVs6aTRwA8-4No5bYAEg.png"/></div></figure><p id="b4ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在此之前，我们计算了所有参数的导数，现在是使用python实现的时候了。首先，我们看到正向传递函数，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="a8b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码中使用的所有具有变量表示的方程与我们在上面理论上看到的相同。</p><p id="ffd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将看到反向传播函数，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="5812" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们使用<strong class="ih hj"> SGD </strong>优化器来更新权重，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mm"><img src="../Images/435900221428c97db3b8e7bf2b567c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*eUwu5lMCayZGQ1z1rdtQAA.png"/></div></figure><p id="d027" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图中我们可以清楚地看到，MLP做得很好，训练和测试损失彼此非常接近，因此我们可以说模型没有过度拟合。</p><p id="0c20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们看看如果我们使用<strong class="ih hj">动量</strong>优化器，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mx"><img src="../Images/d4f1b0c7b02b32d8a63e4635f920ed47.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*fXlRBPPUwE2lQa3gZSDOoQ.png"/></div></figure><p id="bd8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图中我们可以看出SGD和Momentum的工作原理几乎相似。</p><p id="6979" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们看看如果我们使用<strong class="ih hj">亚当</strong>优化器，</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mi mj l"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es my"><img src="../Images/922c0c74ea34eb6df8ce83ecf6182d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*L0m5Zirmy04g1tpPl3sP5A.png"/></div></figure><p id="a05a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与新币和动量相比，亚当在很大程度上减少了损失。所以相比SGD和动量优化器，Adam表现不错。</p><p id="fc2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，与单层感知器相比，多层感知器工作得非常好。</p><h1 id="c045" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">管道和App部署</strong></h1><p id="4196" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我已经建立了端到端的管道，并使用Stramlit和Heroku部署了web应用程序。整个管道和部署代码是<strong class="ih hj"> </strong> <a class="ae lg" href="https://github.com/Ananda-Hange/heroku_stramlit_app/blob/main/app.py" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="ln">这里的</em> </strong> </a> <strong class="ih hj"> <em class="ln">。</em> </strong> <em class="ln">你可以在我下面提到的GitHub回购链接中查看全部代码。</em></p><div class="mz na ez fb nb nc"><a href="https://github.com/Ananda-Hange/heroku_stramlit_app" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hj fi z dy nh ea eb ni ed ef hh bi translated">Ananda-Hange/heroku _ stramlit _ app</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">在GitHub上创建一个帐户，为Ananda-Hange/heroku _ stram lit _ app开发做出贡献。</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">github.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq la nc"/></div></div></a></div><p id="530d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Web App演示，可以在这里  查看<a class="ae lg" href="https://target-prediction.herokuapp.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="ln"/></strong></a></p><h1 id="f9dc" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">结论</h1><p id="6008" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">正如我们从损失-时期图中看到的，多层感知器明显优于单层感知器。正如我们已经看到的，多层感知从数据中捕捉非线性。我们还没有使用任何像TensorFlow和Keras这样为解决深度学习问题而开发的库。但是当你开发一个更深的网络(不止一个隐藏层)时，我们必须使用这些库，因为它们已经被很好地优化了。</p><p id="2f95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐学习！！！</p></div></div>    
</body>
</html>