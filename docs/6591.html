<html>
<head>
<title>YOLO V2 Configuration file Explained!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO V2 配置文件解释！！</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/yolo-v2-configuration-file-explained-879e2219191?source=collection_archive---------0-----------------------#2022-03-29">https://medium.com/nerd-for-tech/yolo-v2-configuration-file-explained-879e2219191?source=collection_archive---------0-----------------------#2022-03-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4161" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLOv2 配置文件可以在<strong class="ih hj"> darknet/cfg/yolov2.cfg </strong>中找到。这里给出了配置文件的链接:<a class="ae jd" href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov2.cfg" rel="noopener ugc nofollow" target="_blank">https://github . com/pjreddie/darknet/blob/master/CFG/yolov 2 . CFG</a></p><p id="8654" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们一步一步地理解这个配置文件。首先，让我们深入到<strong class="ih hj">网络层:</strong></p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="fab2" class="jn jo hi jj b fi jp jq l jr js"><strong class="jj hj">[net] layer</strong><br/><strong class="jj hj"># Testing</strong><br/>#batch=1<br/>#subdivisions=1<br/><strong class="jj hj"># Training</strong><br/>batch=64<br/>subdivisions=8<br/>width=608<br/>height=608<br/>channels=3<br/>momentum=0.9<br/>decay=0.0005<br/>angle=0<br/>saturation = 1.5<br/>exposure = 1.5<br/>hue=.1<br/>learning_rate=0.001<br/>burn_in=1000<br/>max_batches = 500200<br/>policy=steps<br/>steps=400000,450000<br/>scales=.1,.1</span></pre><ol class=""><li id="712f" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj">批量</strong> <br/>批量是指批量大小。它表示在一次迭代中使用的训练示例的数量。当我们将数据集(在我们的例子中是图像)加载到内存中时，我们有两种选择:<br/> a)要么立即将整个数据集加载到内存中，要么<br/> b)您可以将样本数据集加载到内存中。现在，如果数据的规模非常大，那么模型的训练速度将非常慢，因为你正在使用 CPU 中的大量内存，这是非常<br/>低效的。因此，我们将数据集分成几批。使用批处理的另一个原因是，当我们在没有分成批处理的情况下训练我们的深度学习模型时，我们的神经网络必须在内存中存储所有这些图像的错误值，这将导致训练速度大大降低。正如我们所知，模型仅在通过整个数据集后才更新权重和偏差，因此，如果我们通过大数据集，那么训练将会非常慢。如果我们把数据分成小批会更好。</li><li id="579c" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">细分</strong> <br/>批次被细分成多个“块”。它基本上就像你把你的批次分成多少个小批次一样。例如:Batch=64 - &gt;加载本次<strong class="ih hj">迭代</strong>的 64 幅图像。Subdivision=8 - &gt;将批分割成 8 个“小批”，因此每个“<strong class="ih hj">小批</strong>”有 64/8 = 8 个图像，这些图像被发送到 GPU 进行处理。这将重复 8 次，直到该批完成，新的迭代将从 64 个新图像开始。块的图像在 GPU 上并行运行。如果您的 GPU 有足够的内存，您可以减少细分，以加载更多的图像到 GPU 来同时处理。关于批处理，最重要的一点是它必须能被细分整除，因为代码使用小批量的<br/>批处理/细分，正如你在<strong class="ih hj">parcer . c:</strong><br/><strong class="ih hj">net-&gt;batch/= subdivs</strong>中看到的；<br/>同样，在 detector.c 中定义了每一步的图像数，像<br/><strong class="ih hj">int imgs = net . batch * net . subdivisions * ngpus；</strong></li><li id="6239" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">宽度、高度和通道</strong> <br/>接下来的三个参数分别是宽度、高度和通道。<br/><strong class="ih hj">Width = 608-网络大小(width) </strong>，表示在训练和检测过程中，每张图像都会被调整到网络大小。<br/> <strong class="ih hj"> height=608-网络大小(height) </strong>，表示在训练和检测过程中，每幅图像都会被调整到网络大小。<br/></li><li id="8a7c" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">动量</strong> <br/>动量只不过是运动的累积，即历史对权重的进一步变化有多大影响(优化器)。它是梯度下降优化算法的扩展，允许搜索在搜索空间的一个方向上建立惯性，并克服噪声梯度的振荡和搜索空间平坦点的滑行。默认情况下，动量的值为 0.9。</li><li id="2118" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">衰变</strong> <br/>众所周知，现实世界的数据是复杂的，为了解决复杂的问题，我们需要复杂的解决方案。拥有更少的参数只是防止我们的模型变得过于复杂的一种方式。但这实际上是一个非常有限的策略。更多的参数意味着我们神经网络各部分之间更多的相互作用。更多的相互作用意味着更多的非线性。这种非线性帮助我们解决复杂的问题。然而，我们不希望这些交互失去控制。因此，如果我们<strong class="ih hj">惩罚复杂性</strong>会怎么样。我们仍然会使用很多参数，但是我们会防止我们的模型变得太复杂。体重衰减的想法就是这样产生的。惩罚复杂性的一种方法是，我们将所有参数的平方加到损失函数中。然而，它可能会导致我们的损失变得如此巨大，以至于最好的模型是将所有参数设置为 0。为了防止这种情况发生，我们将平方和乘以另一个更小的数。这个数叫做<strong class="ih hj">重量衰减 wd </strong>。<br/>我们的损失函数现在看起来如下:<br/> <strong class="ih hj"> Loss = MSE(y_hat，y) + wd * sum(w ) </strong> <br/>当我们使用梯度下降来更新权重时，我们执行以下操作:<br/><strong class="ih hj">w(t)= w(t-1)—lr * dLoss/dw</strong><br/>现在由于我们的损失函数中有 2 项， 第二项 w.r.t <strong class="ih hj"> w </strong>的导数是:<br/> <strong class="ih hj"> d(wd * w ) / dw = 2 * wd * w(类似于 d(x )/dx = 2x) </strong> <br/>也就是说，从现在开始，我们不仅要从权重中减去学习率* <br/>梯度，还要从权重中减去<strong class="ih hj"> 2 * wd * w </strong>。 我们从原始重量中减去<strong class="ih hj">常数乘以</strong>重量。这就是它被称为<strong class="ih hj">重量衰减</strong>的原因。</li><li id="17aa" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">角度</strong> <br/>它在训练时随机旋转图像(仅分类)。默认情况下，角度为 0 度。</li><li id="389f" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">饱和度</strong> <br/>它在训练时随机改变图像的饱和度。默认情况下，饱和度的值为 1.5。</li><li id="4e0c" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">曝光</strong>T37】它在训练时随机改变曝光(亮度)。默认情况下，其值为 1.5。</li><li id="1e32" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">色相</strong> <br/>它在训练时随机改变色相(颜色)。默认情况下，其值为 0.1。</li><li id="9d4a" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">学习率</strong> <br/>学习率是一个超参数，它控制每次更新模型权重时，响应于估计误差而改变模型的程度。在配置你的神经网络时，学习率可能是最重要的超参数。学习率控制着模型适应问题的速度。<strong class="ih hj">较小的</strong>学习率需要<strong class="ih hj">更多的</strong>训练次数，因为每次更新时对权重的改变较小，而较大的学习率会导致快速改变，并需要较少的训练次数。如果学习率太大会导致模型过快地收敛到次优解，那么学习率太小会导致过程停滞。训练深度学习神经网络的挑战包括仔细选择学习速率。</li><li id="a67b" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">Burn In</strong>T9】第一次<strong class="ih hj"> 1000 次迭代</strong>，<br/><strong class="ih hj">current _ learning rate = learning _ rate * pow(iterations/<br/>Burn _ In，power)</strong>= 0.001 * pow(iterations/1000，4)其中<br/> power 默认为=4。</li><li id="764b" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">Max Batches</strong><br/>Max _ Batches = 500200—将针对此数量的<br/>迭代(批次)处理训练。</li><li id="f071" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">策略</strong>默认情况下<br/>，策略不变。用于改变学习速率，例如<br/>常量(默认)、sgdr、steps、step、sig、exp、poly、<br/> random(即如果<strong class="ih hj"> policy=random </strong> —那么当前学习速率将这样改变= learning _ rate * pow(rand _ uniform(0，1)，power))。<br/> power=4 —如果 poly = poly—学习率将= learning _ rate *<br/>pow(1—current _ iteration/max _ batches，power)<br/><strong class="ih hj">sgdr _ cycle = 1000</strong>—if<strong class="ih hj">policy = sgdr</strong>—余弦循环的初始迭代次数<br/><strong class="ih hj">sgdr _ mult = 2</strong>—if<strong class="ih hj">policy = sgdr</strong>—余弦乘数</li><li id="84a0" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">步数</strong> <br/>步数是一个检查点(迭代次数),在此处将应用<br/>比例。步骤=400000，450000-如果策略=步骤-它们代表迭代次数<br/>,学习率将乘以比例因子。</li><li id="d2fc" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">Scales</strong>Scales<br/>Scales 是一个系数，learning_rate 将在该检查点乘以该系数。<strong class="ih hj"> scales=.1，. 1，. 1 </strong> —如果<strong class="ih hj"> policy=steps </strong> —即如果<strong class="ih hj"> steps=8000，9000，12000 </strong>，scales=.1，.. 1，当前迭代次数为 10000，则<br/><strong class="ih hj">current _ learning _ rate = learning _ rate * scales[0]*<br/>scales[1]= 0.001 * 0.1 * 0.1 = 0.1</strong></li></ol><p id="73c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们来谈谈卷积层。正如我们所知，我们在 YOLOv2 中使用了 Darknet-19 模型，它有 19 个卷积层和 5 个最大池层。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="f579" class="jn jo hi jj b fi jp jq l jr js">[convolutional] layer<br/>batch_normalize=1<br/>filters=64<br/>size=1<br/>stride=1<br/>pad=1<br/>activation=leaky</span></pre><ol class=""><li id="d9a1" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj">批量正常化</strong> <br/>批量正常化或批量正常化是一种用于训练深度神经网络的技术，它将每个小批量的输入标准化到一个层。这具有稳定学习过程和显著减少训练深度网络所需的训练时期的效果。<strong class="ih hj"> batch_normalize=1 </strong>表示模型将<br/>使用批处理规范化，0 表示不使用。<strong class="ih hj">默认为 0</strong>。</li><li id="bdb3" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">滤镜</strong> <br/>滤镜用于通过检测图像亮度值的变化来检测图像中的边缘等空间图案。默认情况下，过滤器大小为 1。</li><li id="ccb5" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">大小</strong>T13】大小无非就是内核大小。</li><li id="6e71" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">步幅</strong> <br/>是内核过滤器的偏移步长。stride 的默认值为 1。</li><li id="6de6" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">Pad</strong>T19】Pad 是填充的意思。它指的是卷积神经网络的内核处理图像时添加到图像中的像素数量。例如，如果 CNN 中的填充设置为零，则添加的每个像素值将为零值。默认情况下，传递的值为 0</li><li id="2f16" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">激活</strong> <br/>激活无非是 CNN 的激活功能。常用的激活功能之一是<strong class="ih hj"> Leaky Relu </strong>。还有各种其他激活功能，如线性(默认)，loggy，relu，elu，卢瑟，救济，plse，hardtan，lhtan，线性，斜坡，漏，tanh，楼梯。<br/>现在让我们看看最大池层:</li></ol><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="9744" class="jn jo hi jj b fi jp jq l jr js">[maxpool]<br/>size=2<br/>stride=2</span></pre><p id="5f3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最大池层</strong>:</p><p id="d89c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">池图层用于减少要素地图的维度。因此，它减少了要学习的参数数量和网络中执行的计算量。汇集图层汇总了由卷积图层生成的要素地图区域中的要素。最大池化是一种池化操作，它计算要素图面片的最大值，并使用它来创建缩减采样(池化)的要素图。它通常用在卷积层之后。在 max pool 层，Size 是 max-pooling 内核，stride 是 max-pooling 内核的偏移步长。</p><p id="3684" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">路由层</strong> <br/> <strong class="ih hj">【路由】- </strong>它不过是一个串联层。路由层就像一个路由<br/>标志，指向我们想要连接的层。layers = -1，4-表示<br/>将连接成两层，相对索引为-1 和-4。输出:<strong class="ih hj">W x H x C _ layer _ 1+C _ layer _ 2</strong><br/>如果<strong class="ih hj">索引&lt; 0 </strong>，则为相对层数(-1 表示上一层)<br/>如果<strong class="ih hj">索引&gt; = 0 </strong>，则为绝对层数</p><p id="ab34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">重组层</strong></p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="647b" class="jn jo hi jj b fi jp jq l jr js"><strong class="jj hj"><br/>[reorg]</strong><br/>stride=2</span></pre><p id="d3e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重组层通过促进来自不同层的特征连接来提高 YOLOv2 对象检测网络的性能。它重新组织较低层特征地图的维度，以便它可以与较高层特征地图连接。这里，reorg layer 对输出张量进行整形，以便张量的 H 和 W 与下游的其他输出张量匹配，并且这两个张量输出<br/>可以连接在一起。</p><p id="3c63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> stride=2 </strong>表示宽度和高度会减少 2 倍，通道数会增加 2x2 = 4 倍，所以元素总数还是一样:<br/><strong class="ih hj">width _ old * height _ old * channels _ old = width _ new * height _ new * channels _ new</strong></p><p id="cc45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看区域层:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="e77b" class="jn jo hi jj b fi jp jq l jr js">[region]<br/>anchors = 0.57273, 0.677385, 1.87446, 2.06253, 3.33843,<br/>5.47434, 7.88282, 3.52778, 9.77052, 9.16828<br/>bias_match=1classes=80<br/>coords=4<br/>num=5<br/>softmax=1<br/>jitter=.3<br/>rescore=1<br/>object_scale=5<br/>noobject_scale=1<br/>class_scale=1<br/>coord_scale=1<br/>absolute=1<br/>thresh = .6<br/>random=1</span></pre><p id="5051" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<strong class="ih hj">锚点</strong> <br/> YOLO 可以很好地处理多个对象，其中每个对象都与一个网格单元相关联。但是在重叠的情况下，其中一个网格单元实际上包含两个不同对象的中心点，我们可以使用称为锚框的东西来允许一个网格单元检测多个对象。通过定义锚盒，我们可以创建一个更长的网格单元向量，并将多个类与每个网格单元相关联。锚定框有一个定义好的长宽比，它们可以检测出符合这个比例的对象。</p><p id="becb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">偏差匹配</strong></p><p id="f6ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">bias_match 仅用于训练。如果 bias_match=1，则检测到的对象将与锚之一中的对象相同，否则，如果 bias_match=0，则锚的将被神经网络提炼。</p><p id="49a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。类是我们数据集中的类的数量。</strong></p><p id="202f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。Num </strong> <br/> Num 是主播总数。在我们的例子中，num=9。</p><p id="f1c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。YOLO 应用了一个 Softmax 函数，将分数转换成概率，其总和为 1。</strong></p><p id="b262" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。抖动</strong> <br/>它随机裁剪图像并调整其大小，纵横比从 x<strong class="ih hj">(1-<br/>2 *抖动)到 x(1+2 *抖动)</strong>。抖动值越大，<br/>神经网络对物体的大小和长宽比的变化越不变性。</p><p id="6c06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7 .<strong class="ih hj">。Rescore </strong> <br/>它决定了将使用什么损失(delta，cost，…)函数。</p><p id="cda7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 8。</strong>object Scale<br/>object _ Scale 用于对物体的损失(delta，cost，…)函数。</p><p id="1367" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 9。noobject_scale </strong> <br/>用于物体和背景的损失函数。</p><p id="074b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 10。类别比例</strong> <br/>它被用作 delta_region_class()中的比例:</p><p id="0515" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 11。坐标比例</strong> <br/>它也用作 delta_region_box()中的比例。</p><p id="989c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 12。阈值</strong> <br/> YOLO 使用<strong class="ih hj">非最大抑制(NMS) </strong>只保留最佳包围盒。NMS 的第一步是移除所有检测概率小于给定 NMS 阈值的预测边界框。</p><p id="7587" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">13。随机 <br/>它每 10 次迭代随机调整网络大小，从 1/1.4 到 1.4。</p><p id="6024" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考资料:</strong><br/><a class="ae jd" href="https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-different-" rel="noopener ugc nofollow" target="_blank">https://github . com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-different-</a>layers # CFG-Parameters-in-the-different-layers<br/><a class="ae jd" href="https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab" rel="noopener" target="_blank">https://towards data science . com/this-this-thing-called-weight-decay-a 7 CD 4 BCF cab</a></p></div></div>    
</body>
</html>