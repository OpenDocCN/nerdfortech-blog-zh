<html>
<head>
<title>Data Engineering: A Feature Selection Example with the Iris Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据工程:Iris 数据集的特征选择示例</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/data-engineering-a-feature-selection-example-with-the-iris-dataset-11f0554e4b00?source=collection_archive---------2-----------------------#2021-04-16">https://medium.com/nerd-for-tech/data-engineering-a-feature-selection-example-with-the-iris-dataset-11f0554e4b00?source=collection_archive---------2-----------------------#2021-04-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="620b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="69b9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">至于数据工程师和数据科学家成员的最佳比例，8:2 是一个非常普遍的比例。当然，没有固定的“最佳”比例，这完全取决于公司的设置、开发人员的可用性等。但是从这个比例中，我们可以大致看到工作量落在这两个类别上:数据工程与机器学习算法研究。事实上，更好的数据工程工作可以极大地有利于机器学习算法，并最终获得响应反馈和成本节约。</p><p id="a17e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在这篇文章中，我们给出了一个演示来展示特征选择是如何使整个机器学习过程受益的。</p><p id="e685" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下内容仅限于监督、分类任务。该思想可以应用于其他类别机器学习任务，但是它们可能需要不同的数据工程和特征分析过程。</p><h1 id="193a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">数据工程</h1><p id="149b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">从机器学习的角度来看，数据工程包括数据集收集、数据集清洗/转换、特征选择、特征转换。在这里，我们将重点放在特征选择上，以显示它如何有利于机器学习过程。</p><h1 id="cc98" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">特征分析</h1><p id="2e99" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在我们的例子中，我们使用了著名的 iris 数据集。它是结构良好的，干净的，平衡的。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="0812" class="kp ig hi kl b fi kq kr l ks kt">from sklearn import datasets<br/><br/># load data to dict derived class Bunch<br/>iris = datasets.load_iris()<br/>target = datasets.load_iris().target<br/><br/><br/># convert to dataframe for processing <br/>iris = pd.DataFrame(iris.data, columns = iris.feature_names)</span></pre><p id="ff91" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以确保数据平衡。在我们的例子中，每个类都有同样的 50 个样本。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="7dc0" class="kp ig hi kl b fi kq kr l ks kt">import seaborn as sns</span><span id="be87" class="kp ig hi kl b fi ku kr l ks kt">plt.hist(target)</span></pre><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es kv"><img src="../Images/912c0afcc93abbf8970d2ce03c2e27d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/0*wRRR11uzIA3PQLx4"/></div></figure><p id="0451" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">检查它的最小值、最大值和其他基本信息，以确保我们没有异常值</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="a5a3" class="kp ig hi kl b fi kq kr l ks kt">iris.describe()</span></pre><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es kz"><img src="../Images/84e5bce8b529b1803d47424ad33329c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/0*_iKIYLJbADudSJAm"/></div></figure><p id="138c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在，让我们将它规范化，并将特性与类的相关性可视化</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="cafa" class="kp ig hi kl b fi kq kr l ks kt">#to normalize dataset, we use this handy MinMaxScaler<br/>from sklearn.preprocessing import MinMaxScaler</span><span id="6993" class="kp ig hi kl b fi ku kr l ks kt">scaler = MinMaxScaler()<br/>scaler.fit(iris)</span><span id="bad7" class="kp ig hi kl b fi ku kr l ks kt">iris_norm=scaler.transform(iris)</span><span id="0c8e" class="kp ig hi kl b fi ku kr l ks kt"># visualizing features and target<br/>iris_norm = pd.DataFrame(iris_norm, columns = iris.columns)<br/>iris_norm_ = pd.DataFrame(np.hstack((iris_norm, target[:, np.newaxis])), columns = iris.columns.tolist() + ['class'])<br/>sns.pairplot(iris_norm_, hue = 'class', diag_kind='hist')</span></pre><figure class="kg kh ki kj fd kw er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es la"><img src="../Images/c8e078dce13638ef03cc51f36c3acc94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_UWop8gqbNleI72j"/></div></div></figure><p id="c7f8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">正如我们所看到的，除了萼片宽度/萼片长度对之外，所有其他对都可以非常清楚地将 3 类分开。1 班和 2 班在图表中纠结。</p><p id="b1e3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">此外，让我们检查它们在特性和类之间的相关性:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="6b4e" class="kp ig hi kl b fi kq kr l ks kt"># manually verify the correlation among features and classes<br/>iris_cov = iris_norm_.cov()<br/>sns.heatmap(iris_cov, annot = True, cbar = False)</span></pre><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es lf"><img src="../Images/26c96435889a0b03826f1bcae5a6e6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/0*jU3fZppXBtA0DcRO"/></div></figure><h1 id="2476" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">特征选择</h1><p id="c808" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">理想情况下，我们想要一个 a)与类更相关，b)与其他特征不太相关的特征。a)是最重要的因素，因为完全不相关的话是无法贡献一个算法的。b)可以使过程更有效果，但这是超出本文的另一个话题。</p><p id="bb7d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">从协方差热图中，我们可以看到“萼片宽度”与类别的相关性最小。这可以解释为什么类 1 和类 2 在上一节的 pairplot 图中纠缠在一起。</p><p id="1605" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们用 sklearn SelectKBest 模型选出最好的 3 个特征。因为所有 4 个特征都是连续的，我们用 f 检验来做这个。我们的目标是移除“萼片宽度”功能。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="0a3a" class="kp ig hi kl b fi kq kr l ks kt">from sklearn.feature_selection import SelectKBest, f_classif</span><span id="0a7d" class="kp ig hi kl b fi ku kr l ks kt">bestfeatures = SelectKBest(score_func=f_classif, k=3)<br/>iris_trim = bestfeatures.fit_transform(iris_norm, target)</span><span id="31af" class="kp ig hi kl b fi ku kr l ks kt">print(bestfeatures.scores_)<br/>print(bestfeatures.pvalues_)<br/>print(iris_trim.shape)</span></pre><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es lg"><img src="../Images/ab6a7bbf8d9c2af3a683d649b461b1de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/0*GYeqkww9vYOVa2XO"/></div></figure><p id="b28f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如您所见，第二个要素得分最低，p 值最大。并且得到的数据集的形状是 150×3，则第二特征(萼片宽度)被移除。</p><p id="2666" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们再看一遍配对图:</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es lh"><img src="../Images/99818a7c199be23bf2c120e0db18a4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/0*05YaFfkx9rLENY_n"/></div></figure><p id="23bf" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们现在可以为这 3 个特征绘制一个 3D 图表，以获得更直观的视图。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="30a8" class="kp ig hi kl b fi kq kr l ks kt">from mpl_toolkits import mplot3d<br/>fig = plt.figure(figsize=(8,8))<br/>ax = plt.axes(projection='3d')<br/>ax.scatter3D(iris_trim[:, 0], iris_trim[:, 1], iris_trim[:, 2], c = target, cmap='Accent', marker = '&gt;')</span></pre><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es li"><img src="../Images/d349854f303ff27980719f56fdac5795.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/0*jxf0LhCU5xS00VB8"/></div></figure><h1 id="2587" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">确认</h1><p id="73ac" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，让我们比较 4 个特征的情况和 3 个特征的情况。首先定义一个训练和验证函数，然后准备两个数据集。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="7159" class="kp ig hi kl b fi kq kr l ks kt">def train_and_validate(X_train, X_test, y_train, y_test):<br/>    mode = GaussianNB()    <br/>    mode.fit(X_train, y_train);<br/>    y_calc = mode.predict(X_test)<br/>    y_prob = mode.predict_proba(X_test)<br/>    #print(y_prob)<br/>    mat = confusion_matrix(y_test, y_calc)<br/>    sns.heatmap(mat.T, annot=True, cbar = False)</span><span id="4632" class="kp ig hi kl b fi ku kr l ks kt">X_train4, X_test4, y_train, y_test = train_test_split(iris_norm, target, test_size = 0.10, stratify = None, random_state=0)<br/>X_train3, X_test3 = X_train4.drop(['sepal width (cm)'], axis=1), X_test4.drop(['sepal width (cm)'], axis=1)</span></pre><p id="5e4b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">运行并比较</p><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es lj"><img src="../Images/41997ef675fbb1ad5dbd2f66e0565378.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*rMzDRSE7cCa0Hcae"/></div></figure><figure class="kg kh ki kj fd kw er es paragraph-image"><div class="er es lk"><img src="../Images/a6eadda3a6ab230178ed45a3ee07f6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/0*7ykf6U9ZLumExpzW"/></div></figure><p id="685e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">正如我们所看到的，减少的特征集有一个更好的结果。在混淆矩阵中，3 特征数据集产生 100%的准确度，而 4 特征集模型遗漏了一个样本。</p><p id="d2ea" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我更改了 random_state 以生成不同的数据集来重复这个过程，我可以看到 3 个要素的数据集表现得更好，或者至少与 4 个要素的数据集一样好。</p><h1 id="1179" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><p id="f63f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">更好准备的数据集可以有益于机器学习过程。正确选择特征集不仅可以节省模型训练时间和存储空间，而且可以得到更准确的结果。</p></div></div>    
</body>
</html>