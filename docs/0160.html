<html>
<head>
<title>Indian Accent Speech Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">印度口音语音识别</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/indian-accent-speech-recognition-2d433eb7edac?source=collection_archive---------0-----------------------#2020-05-31">https://medium.com/nerd-for-tech/indian-accent-speech-recognition-2d433eb7edac?source=collection_archive---------0-----------------------#2020-05-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="62e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">传统ASR(信号分析，MFCC，DTW，HMM &amp;语言建模)和DNNs(自定义模型&amp;百度DeepSpeech模型)对印度口音语音</em></p><p id="7183" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">礼遇</em> </strong> <em class="jd"> : </em> <a class="ae je" href="https://www.iitm.ac.in/donlab/tts/index.php" rel="noopener ugc nofollow" target="_blank"> <em class="jd">演讲与音乐技术实验室，IIT马德拉斯</em> </a></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/d0aaaaac32d2a6ce541e80e6b2b3d076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*BpRw4aHmB40oV_IKpnr1Cg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><a class="ae je" href="http://icosa.hkbu.edu.hk/listening/english-accents/indian-accent/index.htm" rel="noopener ugc nofollow" target="_blank">图片由</a>提供</figcaption></figure><p id="bc56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管印度英语口音讲话得到认可，但不带口音的发音仍是一个神话。不管种族偏见如何，我们的语言自然是由我们所说的方言形成的，而且印度方言数不胜数！那么，一台计算机如何破译来自印度不同邦的讲话，即使是来自其他邦的印度人也觉得难以理解？</p><p id="a1ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ASR(自动语音识别)</strong>获取任何连续音频语音并输出等效文本。在这篇博客中，我们将从理论和实践两方面探讨语音识别中的一些挑战，重点是与说话人无关的识别。</p><p id="8ffc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ASR的<strong class="ih hj">挑战</strong>包括</p><ul class=""><li id="d50f" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">音量的可变性</strong></li><li id="3dae" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">文字速度的可变性</strong></li><li id="0e80" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">扬声器的可变性</strong></li><li id="1fcc" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">螺距的可变性</strong></li><li id="39e4" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">单词边界:我们说话时没有停顿。</li><li id="c2e4" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">背景声音、观众说话声等噪音。</li></ul><p id="8384" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们在下面讨论的章节中解决上述问题。</p><p id="5880" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上研究的完整源代码可以在<a class="ae je" href="https://github.com/AdroitAnandAI/Indian-Accent-Speech-Recognition" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里</strong> </a> <strong class="ih hj">找到。</strong></p><p id="6963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">语音识别中的模型</strong>在概念上可以分为:</p><ul class=""><li id="ac43" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">声学模型:</strong> <strong class="ih hj">把声音信号变成某种</strong>音标<strong class="ih hj">表示法。</strong></li><li id="10f4" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">语言模型:</strong>为<strong class="ih hj">语言</strong>存储了单词、语法和句子结构的<strong class="ih hj">领域知识。</strong></li></ul><h1 id="fb79" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">信号分析</strong></h1><p id="80e3" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">当我们说话时，我们在空气中产生正弦振动。音高越高，振动越快，频率越高。麦克风将振动中声能转换成电能。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/e4d4e531293145b3fda6acae90c050a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*3VgL3yHz4Vx7ypFLjTQocg.png"/></div></figure><p id="c59e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们说“<strong class="ih hj"> Hello World </strong>”，那么相应的信号将包含2个斑点</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lj"><img src="../Images/502078e8dc7515e9088f3f1d94ce1505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*ojpaAo33GTIKpsf60dL8og.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">信号中的一些振动具有较高的振幅。振幅告诉我们声音中有多少声能</figcaption></figure><p id="ebbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的语音同时由许多频率组成，也就是说，它是所有这些频率的总和。为了分析信号，我们使用分量频率作为特征。<strong class="ih hj">傅立叶变换</strong>用于将信号分解成这些分量。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lk"><img src="../Images/66f27f081ba5b031202f7d5e11e6feb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*HyUTXK5cZ4nl6veOHhTv8g.png"/></div></figure><p id="9ede" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用这种分离技术将声音转换成<strong class="ih hj">声谱图</strong>，其中垂直轴上的<strong class="ih hj">频率</strong>被绘制为<strong class="ih hj">对时间</strong>。阴影的强度表示信号的幅度。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ll"><img src="../Images/67013a770d8a9e7f1be793d86826b7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*rxMrSP3dnglm058cL1-VLQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><em class="lm">hello world短语的声谱图</em></figcaption></figure><p id="860c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了创建<strong class="ih hj">谱图</strong>，</p><ol class=""><li id="69b1" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc ln jx jy jz bi translated"><strong class="ih hj">将信号</strong>分成时间帧。</li><li id="8f9b" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc ln jx jy jz bi translated">用FFT将每个帧信号分成<strong class="ih hj">个频率分量</strong>。</li><li id="479d" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc ln jx jy jz bi translated">每个时间帧现在用每个频率的振幅矢量表示。</li></ol><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/d7c0fe1cf515521fb997b953c2af043b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*sS42TRVeby4NMlX1BblMmQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><em class="lm">一个时间帧的一维向量</em></figcaption></figure><p id="c7f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们<strong class="ih hj">按照向量的时间序列顺序</strong>再次排列向量，我们就可以<strong class="ih hj">得到声音成分的视觉图像，即声谱图</strong>。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lo"><img src="../Images/55c00d4715060e37fc1dd2f17880b6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*uBUMdbpNPcbr0VZ1r3ke9Q.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">声谱图可以与原始音频信号在时间上对齐</figcaption></figure><p id="5fe0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将着眼于<strong class="ih hj">特征提取</strong>技术，它将<strong class="ih hj">减少我们数据的噪声和维度</strong>。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/ac634a18a9976010e82c23ab52e00155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*AjnXTbjWeyrZT2QHtwtTeg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">摄谱仪中编码了不必要的信息</figcaption></figure><h1 id="625a" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">用MFCC提取特征</strong></h1><p id="1a5e" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated"><strong class="ih hj">梅尔频率倒谱系数分析</strong>是使用<strong class="ih hj">梅尔频率分析和倒谱分析将音频信号还原为基本语音成分特征。</strong>频率范围缩小，并分成人类可以分辨的频率组。信号被进一步分离为源和滤波器，从而可以滤除与发音无关的扬声器之间的变化。</p><p id="73bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">a)</strong>T38】梅尔频率分析</p><p id="16a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">只有人类能够听到的频率对于识别语音才是重要的。我们可以将声谱图的频率分成与我们耳朵相关的频段，然后<strong class="ih hj">过滤掉我们听不到的声音。</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lp"><img src="../Images/3c16b76f2e20e55a35545c6872222dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*N5f1DXpH-emsoWYjLkgBRQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><em class="lm">黑线以上的频率将被过滤掉</em></figcaption></figure><p id="eb51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b) <strong class="ih hj">倒谱分析</strong></p><p id="fcbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还需要分离与说话者无关的声音元素。我们可以将人类声音产生模型想象为源和过滤器的组合，其中T2源是个人独有的，而T4过滤器是我们说话时使用的单词的发音。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/094948aba2792158207f0ba8cda4ce48.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*9nPROtqY-MlaiSuLIyScuQ.png"/></div></div></figure><p id="cb04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">倒谱分析</strong>依靠这个模型来分离两者。可以用算法从信号中提取倒谱。因此，我们<strong class="ih hj">去掉了个别</strong>声带独有的语音成分，而<strong class="ih hj">保留了声道发出的声音</strong>的形状。</p><p id="87e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">倒谱分析结合梅尔频率分析可以得到12或13个与语音相关的MFCC特征。<strong class="ih hj"> Delta和Delta-Delta MFCC特性</strong>可以选择性地附加到特性集，有效地将特性数量增加一倍(或三倍)，最多可达<strong class="ih hj"> 39个特性</strong>，但在ASR中提供更好的结果。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es li"><img src="../Images/c5c4ef7d4693bda782aa52d06c720451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*hwu49Zdk--lGc1R1ZDfPyg.png"/></div></div></figure><p id="3524" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从而<strong class="ih hj">MFCC(Mel-频率倒谱系数)</strong>特征提取、</p><ul class=""><li id="bbf6" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">降低了数据的维度</strong>,并且</li><li id="8bb6" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">我们从系统的中挤出噪声</li></ul><p id="2b05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以语音识别有<strong class="ih hj"> 2个声学特征</strong>:</p><ul class=""><li id="ca46" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">光谱图</strong></li><li id="dfb9" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">梅尔频率倒谱系数(MFCC):</strong></li></ul><p id="36b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">构建管道时，您可以选择使用声谱图或MFCC特征。接下来，我们将从语言的角度来看声音，即我们听到的单词的语音。</p><p id="eb5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">语音学</strong></p><p id="e263" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语音学是对人类语言中声音的研究。语言分析用于将人类的单词分解成最小的声音片段。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/051d8410b054ab9fcbacfe465853706b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*tRFZkpVlboP0yUA0SUE5sQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><em class="lm">音素定义不同的声音</em></figcaption></figure><ul class=""><li id="782a" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><a class="ae je" href="https://en.wikipedia.org/wiki/Phoneme" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">音素</strong> </a>是<strong class="ih hj">最小的音段</strong>，可以用来区分一个单词和另一个单词。</li><li id="95b5" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">字形</strong>相反，是一种语言中书写的最小独特单位<strong class="ih hj">。英语有26个字母加上一个空格(<strong class="ih hj"> 27个字素</strong>)。</strong></li></ul><p id="c14f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，我们<strong class="ih hj">无法将音素映射到字形</strong>，因为一些字母映射到多个音素&amp;一些音素映射到许多字母。比如C字母在猫，聊天，圈子里听起来就不一样。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lv"><img src="../Images/aa7819cbfcf842f50cce20ae6e17db98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*S3HmjgHqKgbQ3303P3l2Uw.png"/></div></figure><p id="86ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">音素通常是语音和文本之间的有用媒介。如果我们能够成功地制作一个声学模型，将声音信号解码成音素，剩下的任务就是将这些音素映射到它们匹配的单词。这一步被称为<strong class="ih hj">词汇解码，因为它基于数据集的词典或字典而得名。</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lw"><img src="../Images/600c58d7765bca1dcdf40edb72812fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*aKV1qMIUtYhHU8UmSUIf2Q.png"/></div></figure><p id="25dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们想训练有限的词汇，我们可以跳过音素。如果我们有一个很大的词汇量，那么首先转换成较小的单位，可以减少比较的总次数。</p><h1 id="ca63" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">声学模型和时间带来的麻烦</strong></h1><p id="1be8" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">通过特征提取，我们解决了噪音问题以及说话者的可变性。但是我们仍然没有解决匹配同一个单词的可变长度的问题。</p><p id="2758" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动态时间弯曲(DTW)计算两个信号之间的相似性，即使它们的时间长度不同。</strong>这可用于将新单词的序列数据与单词示例词典中最相似的对应词对齐。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lx"><img src="../Images/175d47db1b8a34360b92b613f7ab9d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*UUBoKdH89LqcgkAZ7BSC7A.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">用动态时间弯曲映射的2个信号</figcaption></figure><h1 id="ae83" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">语音中的隐马尔可夫模型</strong></h1><p id="a2e1" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">hmm<strong class="ih hj">可用于检测随时间变化的模式</strong>。HMMs可以解决时间可变性的问题，即同一单词以不同的速度说出。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ly"><img src="../Images/8afeeffdc0e10f15be5a2940e9dbca70.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*4nGtdzK1-w9OrJeH2PCxuA.png"/></div></figure><p id="0282" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们可以用带标签的时间序列序列训练HMM，为每个特定的声音单元创建单独的HMM模型</strong>。这些单位可以是音素、音节、单词，甚至是词组。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es li"><img src="../Images/aa9237247b0f9ccd3361e374e1503cd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*bsIIKOQjzR6IVNgIddlkSA.png"/></div></div></figure><p id="f7f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们得到每个单词的模型，那么对单个单词的识别就归结为<strong class="ih hj">对每个模型的新观察可能性</strong>进行评分。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/3952a81e0d087e37a67dcbd55f57fc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*HL-69ZgvXVA1p7SXbV8oIQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><em class="lm">单词“brick”在九种不同的发音组合中连续连接</em></figcaption></figure><p id="4055" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了训练连续的话语，<strong class="ih hj">hmm可以被建模成对</strong>。她的砖。这将增加维度。我们不仅需要每个单词的HMM，还需要每个可能的工作联系的HMM。</p><p id="add7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是如果我们使用音素，对于大量的词汇来说，维度的增加并不像单词那样深刻。对于40个音素，我们只需要1600个hmm来考虑过渡。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/f76c1e620aac331aef9cbea3197c4079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*6SLs1s20JgwxhS2j9hWLoQ.png"/></div></figure><h1 id="a1eb" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">语言模型</strong></h1><p id="0939" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated"><strong class="ih hj">语言模型</strong>将语言知识注入语音识别中的文字转文本步骤，以解决拼写和上下文中的歧义。即哪些单词组合是最合理的。</p><p id="9ffd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，由于声学模型是基于声音的，因此我们无法区分类似的发音单词，例如，HERE或HEAR。由声学模型产生的单词可以被认为是许多不同单词的概率分布。每个可能的序列可以被计算为特定单词序列可能已经由音频信号产生的<strong class="ih hj">可能性。</strong></p><p id="e42d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们有<strong class="ih hj">声学模型和语言模型，</strong>那么最可能的序列将是具有最大似然得分的所有这些可能性的组合。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/6e19c828c02e8d66cd2f71c992cfadda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*_Rhgqd11KHHK1yxPivhRxg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><em class="lm">来自信号的声学模型*来自语言信息的统计语言模型</em></figcaption></figure><p id="0fc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们想要计算一个特定的句子在文本语料库中出现的概率。我们已经看到，一系列单词的概率可以从其历史的连锁概率中计算出来。<strong class="ih hj">对于N元文法，我们用链式法则来近似序列概率。</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lz"><img src="../Images/a1b277ac1c129a34015e697a3908c57f.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*IA2Ypitl9UWiXZDlFleoBA.png"/></div></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/d52f2f9a40d4edc70057cefebce54656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*qxChzNHUjqm8NQDk7ZOTHg.png"/></div></figure><p id="d131" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决计算量过大的问题，我们使用<a class="ae je" href="https://en.wikipedia.org/wiki/Markov_property" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">马尔可夫假设</strong> </a> <strong class="ih hj"> </strong>来用更短的序列近似一个序列概率。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/878dc9072f4af6a32fd9eef66d33ce4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*w038LfFhJSnSCpr21qAXEA.png"/></div></figure><p id="1c91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以通过使用二元模型和单个令牌的<strong class="ih hj">计数</strong>来计算概率。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ma"><img src="../Images/c56200e0345912a103def3360371d35e.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*7Ho-4QTt6tj-vwUwKvTRrQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">c是计数函数</figcaption></figure><p id="f0ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以对这些概率以及来自声学模型的概率进行评分，以<strong class="ih hj">从序列选项中移除语言歧义</strong>。</p><p id="e7c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为了总结上述语音到文本(STT)的过程，</strong></p><p id="ea32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.我们用MFCC 从音频语音信号<strong class="ih hj">中<strong class="ih hj">提取特征</strong>。</strong></p><p id="7a40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.使用一个<strong class="ih hj"> HMM声学模型</strong>产生声音单元、音素、单词。</p><p id="6464" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.使用<strong class="ih hj">统计语言模型，如N-grams </strong>来理顺<strong class="ih hj">语言</strong> <strong class="ih hj">歧义</strong>并创建最终的文本序列。<strong class="ih hj">使用对大量文本进行训练的神经语言模型</strong>，可以对拼写和上下文的概率进行评分。</p><h1 id="e9c4" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">传统与最先进的</strong> ASR</h1><p id="c1f5" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">传统的ASR解决方案使用特征提取hmm和语言模型。由于RNNs还可以通过内存跟踪时间序列数据，<strong class="ih hj">声学模型可以用RNN和连接主义者时间分类(CTC)层的组合来代替。</strong></p><p id="0953" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> CTC层解决排序问题</strong>由于任意长度的音频信号，需要转换成文本。如果我们使用DNNs，我们可能根本不需要特征提取或单独的语言模型。</p><p id="8c73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据百度的<strong class="ih hj"> Adam Coates </strong>的说法，传统ASR的额外训练在某种程度上达到了峰值。与此同时，<strong class="ih hj"> DNN解决方案在我们增加数据和模型规模时大放异彩，</strong>尽管它们在小数据集上表现平平。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es mb"><img src="../Images/034a15aa381eba5575a25b031835c570.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*UUzgtEjSXtcrGIdQsCnb6g.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">DNNs在大数据和深度模型方面优于传统方法</figcaption></figure><p id="d725" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们探索<strong class="ih hj">如何用深度神经网络</strong>设计声学模型，并比较它们的性能。</p><h1 id="811e" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">带有自定义模型的语音识别</h1><p id="bf7e" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">下面是设计语音识别深度学习模型时架构考虑的<strong class="ih hj">要点</strong>。</p><ul class=""><li id="db96" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj"> RNN单位:</strong>由于其在建模时序数据方面的有效性</li><li id="79f4" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj"> GRU单位:</strong>使用简单RNN解决爆炸渐变问题</li><li id="f8a5" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">批量归一化:</strong>减少训练次数。</li><li id="4768" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">时间分布层:</strong>寻找更复杂的模式</li><li id="1204" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj"> CNN层:</strong> 1D卷积层增加了额外的复杂度</li><li id="06c1" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj">双向RNNs: </strong>利用未来上下文，在两个方向上处理数据</li></ul><p id="cef3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模式一:CNN + RNN +时间分布密集</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mc"><img src="../Images/0bb939421b18b851a24f259fe5b97d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XgdvS6etSrTtcTp6iz3Qw.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">CNN + RNN +时间分布密集</figcaption></figure><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="md me l"/></div></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mf"><img src="../Images/b81b637d292a4b4138d9a02068b821e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPdm6D1jOI_7SQvtA-pDCQ.png"/></div></div></figure><p id="6b2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型2:较深的RNN +时间分布的密集度</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mg"><img src="../Images/b23b2ed58e2c88f2f0fb015931938367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xUoG_X_aVRNr9jLMDVIug.png"/></div></div></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mh"><img src="../Images/86cc3bdbf064a1d367ed0d813d85d11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAkwF01bsY_Yd6HUgy3rzA.png"/></div></div></figure><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="d0bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您将GRU单位改为SimpleRNN单元，那么由于<strong class="ih hj">爆炸梯度问题</strong>，损失可能变得不确定(NAN)。要解决这个问题，请使用<strong class="ih hj">渐变剪辑</strong>(将SGD优化器中的<strong class="ih hj"> ' </strong> <em class="jd">剪辑规范</em> <strong class="ih hj"> ' </strong>参数设置为较低值)</p><h2 id="5b9b" class="mi kg hi bd kh mj mk ml kl mm mn mo kp iq mp mq kt iu mr ms kx iy mt mu lb mv bi translated">模型1和模型2的比较</h2><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mw"><img src="../Images/8eaf362877053d58b1ee3e77cb661f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uT7a52jFCE7Xn2B3nbBzhQ.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">模型1 (CNN)和模型2 (RNN)的培训损失和验证损失</figcaption></figure><p id="1f59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CNN模型具有较低的训练损失，但具有较高的验证损失，这表明过拟合。<strong class="ih hj">更深的RNN在验证损失方面表现更好，因为它们有助于更好地对序列数据建模。由于顺序输入的长度不多，双向RNN可能帮助不大。不过我们可以试一试。</strong></p><p id="1f16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型3:池化CNN+深度双向RNN+时间分布密集</strong></p><p id="1047" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们结合从上述两个模型中学习时，这里我们将CNN与<strong class="ih hj">更深的双向RNN联系起来，并添加最大池以避免过度拟合。</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mf"><img src="../Images/b32d7c36b5b294318a825c07f890db4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IKa1TilEbY7CdmyIVfP2sA.png"/></div></div></figure><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="md me l"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><em class="lm">由于我们引入了最大池层，对于CTC损失计算，CNN输出长度减少了50%</em></figcaption></figure><p id="0833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您在<a class="ae je" href="https://github.com/AdroitAnandAI/Indian-Accent-Speech-Recognition/blob/master/vui_notebook.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>的分析中所看到的，<strong class="ih hj">型号2(深RNN)在所有3种型号中表现最好</strong>。因此，从逻辑上讲，一个更好的模型架构应该包含更深层次的rnn。</p><p id="f433" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看我们的定制模型在训练1-2小时后的<strong class="ih hj">输出:</strong></p><blockquote class="mx my mz"><p id="b019" class="if ig jd ih b ii ij ik il im in io ip na ir is it nb iv iw ix nc iz ja jb jc hb bi translated"><strong class="ih hj">真实转录:</strong> <br/>当然我有我的期望，她有她的<br/> <strong class="ih hj">预测转录:</strong> <br/>安做af cors我有moixitations，她有她的</p><p id="ea77" class="if ig jd ih b ii ij ik il im in io ip na ir is it nb iv iw ix nc iz ja jb jc hb bi translated"><strong class="ih hj">真实转录:</strong> <br/>三十六人的伪立法机关<br/> <strong class="ih hj">预测转录:</strong><br/>bo OS legeclejur nober thertysexemers</p></blockquote><p id="8f2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在进行词汇解码(音素到单词)和语言建模之后，转录的文本可以变得更有意义。但是为了得到最先进的模型，我们需要训练更大、更深的模型，这在单个GPU上需要花费T2 3-6周的时间。因此，谨慎的做法是<strong class="ih hj">采取预先训练的模式&amp;转移学习破译多种口音</strong>，这是这篇博客的主要焦点。</p><h1 id="7b07" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">印度口音语音识别</h1><p id="6b23" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">不同州的印度人说英语的口音不同。为了让模型识别这种口音变化，我们可以在具有来自许多州的<strong class="ih hj">英语口语记录的语音数据集上训练预训练的语音模型。</strong>在这里，我们转移学习<strong class="ih hj">百度的Deepspeech模型</strong>，并使用测试数据集分析识别改进。</p><ul class=""><li id="e990" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">从<strong class="ih hj"> IITM语音实验室</strong>下载了50+ GB的<strong class="ih hj">印度文TTS </strong>语音数据库，其中包含来自20+个州的10000+个口语句子(男女母语者均可)</li><li id="ad6b" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">数据集包含音频及其描述。但是要将数据加载到深度语音模型，我们需要<strong class="ih hj">生成包含音频文件路径、其转录和文件大小的CSV </strong>。</li><li id="5c30" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">将CSV文件分割成<strong class="ih hj"> 3个部分</strong> : test.csv、train.csv和valid.csv。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es nd"><img src="../Images/9dbf05bdd1bf583906dd6ac1a563a600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*qNOoEg_GXefb0aZXfCcwvA.png"/></div></figure><ul class=""><li id="bef3" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">编写一个<strong class="ih hj"> python程序</strong> <strong class="ih hj">将所有音频文件的帧率</strong>设置为12000hz(深度语音模型要求)</li></ul><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="md me l"/></div></figure><ul class=""><li id="e757" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">从<a class="ae je" href="https://github.com/mozilla/DeepSpeech" rel="noopener ugc nofollow" target="_blank">这里</a>克隆<strong class="ih hj">百度DeepSpeech项目</strong> 0.6.1</li><li id="2694" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">使用适当的参数执行<strong class="ih hj"> DeepSpeech.py </strong>。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ne"><img src="../Images/0cb9c966f2b2500aae37a1b3fc521df4.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*XFY7_fEeO7-xjTJrD9MUxg.png"/></div></figure><ul class=""><li id="15d5" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">Export_dir将包含您在deepspeech.model()函数中加载的<strong class="ih hj"> output_graph.pbmm </strong>。</li><li id="11f2" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><strong class="ih hj"> KenLM ToolKit </strong>用于生成Trie文件。它需要传递给深层语音解码器功能。</li><li id="f0eb" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">模型。<strong class="ih hj">enableDecoderWithLM</strong>(lm _ file，trie，0.75，1.85): lm_file是。训练后的pbmm和trie是KenLM工具包的输出。</li><li id="66dc" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">使用深度语音解码功能做STT。</li></ul><p id="170c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">比较印度口音英语模型和深度语音模型</strong></p><p id="b30d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了检查准确性，我们使用了3个指标:<strong class="ih hj"> WER </strong>、<strong class="ih hj"> WACC </strong>和<strong class="ih hj">蓝分</strong>。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es nf"><img src="../Images/1cdaa5bc4a21a87162b7f923452f63ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*LnpU8zQeZJZ-nd1-SHsaow.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">度量显示训练的模型对于印度口音英语表现得更好</figcaption></figure><p id="68fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们绘制上述指标，将印度口音语音数据(测试集)提供给DeepSpeech预训练模型和我们训练的模型进行比较。下图中的3个箱从左到右分别代表低、中、高精度。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es ng"><img src="../Images/fac6d903c51ef3da3adc72509f33f093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zc_Gfr44RnT584AwFctprA.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><strong class="bd kh"> DeepSpeech基本模型</strong>:在所有3个指标中，大多数数据点被归类为<strong class="bd kh">【低准确度】</strong></figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es nh"><img src="../Images/58044d495fa46a95062d7b58f81642c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twhcTBb8AAdtHv2jf8rUWA.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><strong class="bd kh">训练模型</strong>:在所有3个指标中，大多数数据点被归类为<strong class="bd kh">“中等&amp;高精度”</strong></figcaption></figure><p id="bfff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上描述证明了与DeepSpeech模型相比，<strong class="ih hj">训练模型对于印度口音语音识别</strong>表现得更好。</p><ul class=""><li id="8d17" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><em class="jd"> Indic TTS项目由GOI通信和信息技术部资助。</em></li><li id="eec2" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><em class="jd">学分:使用印度语音数据集训练DeepSpeech模型是由我的实习生</em><a class="ae je" href="https://github.com/GiridharKannappan/Voice-Based-Search-Engine" rel="noopener ugc nofollow" target="_blank"><em class="jd">Giridhar Kannappan</em></a>完成的</li></ul><h1 id="a76b" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">结论</h1><p id="33a0" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">我们已经看到<strong class="ih hj">“倒谱分析”</strong>分离出语音信号中的重音成分，同时在传统的ASR中进行特征提取(MFCC)。在最先进的<strong class="ih hj">深度神经网络</strong>中，特征是内在学习的。因此，我们可以转移学习一个预先训练好的具有多种口音的模型，让该模型自己学习口音特性。</p><p id="cb77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经通过对来自多个州的印度英语语音数据进行迁移学习百度的DeepSpeech预训练模型，证明了案例<strong class="ih hj">。您也可以很容易地将这种方法<strong class="ih hj">扩展到任何根语言或地区口音</strong>。</strong></p><p id="c3b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上研究的完整源代码可以在<a class="ae je" href="https://github.com/AdroitAnandAI/Indian-Accent-Speech-Recognition" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里</strong> </a> <strong class="ih hj">找到。</strong></p><p id="2770" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">如有任何疑问或建议，可在此</em> </strong> 联系我  <a class="ae je" href="https://www.linkedin.com/in/ananduthaman/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd"/></strong></a></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ni"><img src="../Images/0da442d72e8bbc6a36ff9e9817a062f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*M78ik9Y6diXcrLv1"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><a class="ae je" href="http://www.bodhisutra.com/ideas/how-mother-tongue-influence-kills-your-communication-confidence/" rel="noopener ugc nofollow" target="_blank">图片由</a>提供</figcaption></figure><h1 id="d580" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">参考</h1><p id="1b16" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated"><a class="ae je" href="https://www.iitm.ac.in/donlab/tts/database.php" rel="noopener ugc nofollow" target="_blank">【1】https://www.iitm.ac.in/donlab/tts/database.php</a></p><p id="9603" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae je" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">【2】https://www . uda city . com/course/natural-language-processing-nano degree-nd 892</a></p></div></div>    
</body>
</html>