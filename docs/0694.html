<html>
<head>
<title>What is LSTM , peephole LSTM and GRU?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是LSTM，窥视孔LSTM和GRU？</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/what-is-lstm-peephole-lstm-and-gru-77470d84954b?source=collection_archive---------0-----------------------#2021-02-05">https://medium.com/nerd-for-tech/what-is-lstm-peephole-lstm-and-gru-77470d84954b?source=collection_archive---------0-----------------------#2021-02-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dc77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">长短期记忆(LSTM)是由hoch Reiter(1997)提出的，并被许多研究者完善。LSTM是一种特殊的RNN，它能记住长期的依赖关系。LSTM是专门为避免RNN面临的问题而设计的。你可以在我之前的文章<a class="ae jd" href="https://jaimin-ml2001.medium.com/understanding-rnn-91d548c86ac9?source=friends_link&amp;sk=b7458cecd44b469c88e1887516cabf69" rel="noopener">了解RNN </a>中了解RNN。架构行为使得它很难记住长期的依赖关系。在简单的RNN网络中，它具有简单的重复神经网络，例如简单的tanh或relu网络，如下图所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/abe99ccee39a807521de0c438c72f37b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xZG7ROxaj6hWTSBU.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ju">标准RNN中的重复模块包含一个单层。图片取自</strong><a class="ae jd" href="https://colah.github.io/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/</a></figcaption></figure><p id="ea3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTM也有同样的链式结构，但是重复模块没有简单的神经网络结构。这些重复模块包含4个神经网络。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jv"><img src="../Images/c0aec59214636cb55cd7a8c28cd4cf34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T_-9HypnVL09rnuo.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">Gates盖茨</figcaption></figure><p id="3611" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图中的符号可以这样解释。在这个符号中，黄色方框代表神经网络。粉红色的点是点的操作，不管是矢量乘法还是加法。线的合并是连接，线的分离意味着相同的内容在不同的位置。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jw"><img src="../Images/0711fe690657684ce78727c99557c273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_3xg4RRA8LKNbCrB.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ju">图片取自</strong><a class="ae jd" href="https://colah.github.io/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/</a></figcaption></figure><p id="e822" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTM建筑中有三个门，它们是</p><ol class=""><li id="649b" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">忘记大门</li><li id="ca22" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">输入门</li><li id="1f08" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">输出门</li></ol><h1 id="4101" class="kl km hi bd ju kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">LSTM背后的核心理念</h1><p id="8a52" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">图的顶部包含单元状态，这是一条信息可以通过一些小的线性操作轻松传递的路径。门可以添加或删除单元状态中的信息。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/52c50d983a26cb613f402534739fb1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ANl-y5NYU9_EUlmX.png"/></div></div></figure><p id="5f9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该门由sigmoid神经网络和逐点乘法运算组成。因为sigmoid给出了输出0 &amp; 1，这意味着如果它是0，那么什么都不会被传递，如果是1，那么所有的东西都会被传递。</p><p id="b2a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">忘记门</strong></p><p id="c9ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是LSTM网络中的第一步，它决定哪些信息将通过信元状态传递，这个决定由遗忘门做出。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/6336a19679973111f0618ab3277f753e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wutOX28jS3eDOmJQ.png"/></div></div></figure><p id="0b66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它采用ht-1和xt，输入和输出为0和1，然后逐点乘以Ct-1，最后它将决定哪些信息将被传递。如果我们正在处理一些基于上下文的数据，如果上下文改变了，这个遗忘单元将丢弃与上下文无关的信息。</p><p id="21fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输入门</strong></p><p id="f8d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在此步骤中，将决定哪些信息将存储在单元状态中。该操作分两步进行。首先，一个sigmoid神经网络决定我们将更新哪些值，一个tanh层创建一个新的候选值Ct的向量，该向量可以添加到状态中。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/aad9e5930c6898a3be001cf682aaaadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xVJnPxHwTiXw6iIj.png"/></div></div></figure><p id="7d40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们必须将在Ct-1获得的细胞状态更新到Ct。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/9ec5309dcc489cc6a51af32c91990cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2228SoOemtO3fELM.png"/></div></div></figure><p id="a1de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将旧状态Ct-1乘以ft，并忘记不需要的东西。然后我们添加它*Ct来更新我们需要记住的上下文。在这里，我们在要记住的上下文中添加新信息，并传递到下一阶段。</p><p id="7d1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出门</strong></p><p id="3f2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个阶段，我们必须决定在输出中发送什么。这将基于我们的细胞状态，但是我们运行一个sigmoid神经网络来决定我们将输出细胞状态的哪一部分。然后，我们将单元状态通过tanh(将值推到1和1之间)并乘以sigmoid门的输出，这样我们就可以决定输出数据。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/7623a1a0ec6162a04ee86b5c62381538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2K-E1PZSbg6kgrw3.png"/></div></div></figure><p id="7b90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们可以看到一个关于LSTM的gif的整体想法。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lo"><img src="../Images/35cd09a45ce18b1006b8e091282c6a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-Gsbo9tO04t9OGML.gif"/></div></div></figure><p id="7f3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">窥视孔建筑</strong></p><p id="b30e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经看到了简单的LSTM网络，但在每一篇研究论文中，这种架构都随着时间的推移而修改。由Gers &amp; Schmidhuber (2000)提出的一个流行的LSTM变体，是增加“窥视孔连接”这意味着我们让栅极层查看单元状态。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/a10b43bef5f23b17727434bd860c3087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9ofTZTHKBGuMwTxd.png"/></div></div></figure><p id="aa1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个窥视孔连接中，我们可以看到所有的门都有一个输入和细胞状态。</p><p id="c34d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> GRU </strong></p><p id="1a9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTM的另一个变体是门控循环单元，或GRU，由<a class="ae jd" href="http://arxiv.org/pdf/1406.1078v3.pdf" rel="noopener ugc nofollow" target="_blank"> Cho等人(2014) </a>引入。它将遗忘门和输入门结合成新加入的更新门。它还合并了单元格状态和隐藏状态。由此产生的模型比传统的LSMT更简单，而且越来越受欢迎。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/99df307ec161e100ef15a38f79f389b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2-WfwwaYKksIIF6a.png"/></div></div></figure><p id="f794" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">整个GRU行动可以被视为。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lp"><img src="../Images/99569b3fbfadde5d8d024e2f9b9cd43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ti14BJTROmgvPjnK.gif"/></div></div></figure><p id="2ca4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们检查了一些LSTM变种，但也有其他变种。RNN的所有缺点都已经在LSTM实现了，但研究人员仍然要求采取另一个步骤，即所谓的关注。</p><h1 id="4498" class="kl km hi bd ju kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">参考</h1><h1 id="cfe5" class="kl km hi bd ju kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated"><a class="ae jd" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">理解LSTM网络——colah的博客这些循环使得递归神经网络看起来有点神秘。然而，如果你想得更多一点，原来… </a></h1><h2 id="7b9b" class="lq km hi bd ju lr ls lt kq lu lv lw ku iq lx ly ky iu lz ma lc iy mb mc lg md bi translated"><a class="ae jd" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> colah.github.io </a></h2><p id="ca3c" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated"><a class="ae jd" href="https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45" rel="noopener" target="_blank">https://towards data science . com/animated-rnn-lstm-and-gru-ef 124d 06 cf 45</a></p></div></div>    
</body>
</html>