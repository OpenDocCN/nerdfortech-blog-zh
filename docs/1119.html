<html>
<head>
<title>All the math you need to know about Simple Linear Regression for ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于ML的简单线性回归，你需要知道的所有数学知识</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/all-the-math-you-need-to-know-about-simple-linear-regression-for-ml-378d77995f3b?source=collection_archive---------8-----------------------#2021-03-05">https://medium.com/nerd-for-tech/all-the-math-you-need-to-know-about-simple-linear-regression-for-ml-378d77995f3b?source=collection_archive---------8-----------------------#2021-03-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="acc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我开始学习数据科学的旅程时，我意识到我对数学太生疏了，无法完全理解算法是如何工作的，例如<em class="jd"> k </em> -NN、线性回归算法等等。就好像我对它有一个模糊的概念，但是作为一个优等生，我需要理解每个概念到底是如何运作的！这就是为什么我决定写这篇文章，我要邀请你和我一起重温那些数学公式！</p><h1 id="91e2" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">从八年级的数学开始</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kc"><img src="../Images/fbc30c360f20af422e0087f78f066099.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/0*axM8Plwl0dkIkzrT"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">资源:<a class="ae ko" href="https://www.texasgateway.org/resource/23-position-vs-time-graphs" rel="noopener ugc nofollow" target="_blank">德州网关</a></figcaption></figure><p id="8db3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">没错，这就是我有多认真，我们要用这个从0到100！如果我有两点，那么我可以计算斜率，截距，得到这条线的一个方程，格式为<code class="du kp kq kr ks b"><strong class="ih hj">y = mX + b</strong></code> <strong class="ih hj">，</strong>也就是<strong class="ih hj">线性度</strong>。</p><p id="75bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这时，预测的想法就出现了。我们刚刚做的是在给定的信息中找到模式，我们得到了方程。有了这个等式，我们可以用任何给定的<code class="du kp kq kr ks b"><strong class="ih hj">x</strong></code>值来计算<code class="du kp kq kr ks b"><strong class="ih hj">y</strong></code>的值。考虑到这一点，我们已经为大学水平的数学做好了准备！</p><h1 id="e772" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">什么是简单线性回归？</h1><p id="4db2" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">我们说x是自变量，y是因变量，因为<code class="du kp kq kr ks b">x</code>会影响<code class="du kp kq kr ks b">y</code>的结果。例如，我们可以说你学习的时间越多，你可能得到的分数越高。或者地段越大，房价就越大。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/c1fecb4ec28bc0ed4daf615b9840db9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9aLDtq55opTH8DzpkVSkog.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx translated">学习时间和百分比分数的散点图</figcaption></figure><p id="7a87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，这并不保证这两个变量之间会有因果关系。我们只能说它们是相关的，因为<code class="du kp kq kr ks b">x</code>可能不是导致结果<code class="du kp kq kr ks b">y</code>的唯一原因。这也解释了为什么当你看散点图时，这些点没有完全落在一条线上。正如你从上面的图表中看到的，在点和线之间有一些空间，我们称这些空间为残差或误差。</p><h1 id="68b3" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">最适合</strong>的线</h1><p id="1e32" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">因此，我们的下一个问题将是:我们如何找到一条可以最小化所有点的残差的线？现在是时候介绍著名的线性回归函数了！</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ld"><img src="../Images/a89b3cc99b9e435cc1252e04de071017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2czAjOdqZnByRdNi.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx"><a class="ae ko" href="https://chih-sheng-huang821.medium.com/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8-linear-regression-3a271a7453e" rel="noopener">resource: 線性回歸(Linear Regression) by Tommy Huang</a></figcaption></figure><p id="bfb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kp kq kr ks b"><em class="jd">ŷi =β0+β1xi，i=1, …,n</em></code></p><p id="2493" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请不要被方程式吓倒，因为它只是成人版的<code class="du kp kq kr ks b">y=mx+b</code>。让我为你脱下它的伪装。<code class="du kp kq kr ks b">β0</code>是截距，<code class="du kp kq kr ks b">β1</code>是回归系数，你可以把它想成斜率，<code class="du kp kq kr ks b"><em class="jd">ŷ</em></code> <em class="jd"> </em>是y的预测值，是你从方程<code class="du kp kq kr ks b"><em class="jd">β0+β1x</em></code>中得到的y的值，我们称之为“<em class="jd"> y帽”很简单，对吧？戴帽子的y！</em></p><p id="ef59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回到问题，我们想知道如何最小化残差，我们可以把它表示为方程为<code class="du kp kq kr ks b">εi<em class="jd"> = yi</em> — <em class="jd">ŷi</em></code> <em class="jd">，</em> y是实际值。</p><p id="14e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了获得最小值<code class="du kp kq kr ks b">εi</code>，我们将使用<strong class="ih hj">普通最小二乘法、</strong>和<strong class="ih hj">T21，它涉及最小化残差平方和。因为残差可以是正的，也可以是负的，为了避免求和时出现的问题，我们对残差求平方。</strong></p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es le"><img src="../Images/1623a14c533197822a64aa4631593528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/0*MxUe7EdTl0SqZdDA.png"/></div></figure><p id="d078" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了得到<code class="du kp kq kr ks b">β0</code>和<code class="du kp kq kr ks b"><em class="jd">β1</em></code>的值，我们对<code class="du kp kq kr ks b">B̂0</code>和<code class="du kp kq kr ks b">B̂1</code>进行偏导数。接下来我们设置偏导数等于0，然后就可以开始求解了！要看如何一步一步解方程，可以看下面的视频。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="lf lg l"/></div></figure><h1 id="8fe1" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">让我们进入正题</h1><p id="7537" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">既然我们已经对什么是线性回归有了一个概念，我们就可以开始动手了！我们要用<code class="du kp kq kr ks b">scikit-learn</code>库实现一个线性回归机器学习模型。</p><h2 id="cf84" class="lh jf hi bd jg li lj lk jk ll lm ln jo iq lo lp js iu lq lr jw iy ls lt ka lu bi translated">根据盐度数据预测水温</h2><p id="c20a" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在这个项目中，我将使用线性回归算法用<a class="ae ko" href="https://www.kaggle.com/sohier/calcofi?select=cast.csv" rel="noopener ugc nofollow" target="_blank"> CalCOFI数据集</a>训练一个模型。如果你想查看完整的代码，这里是我的Github repo:<a class="ae ko" href="https://github.com/yinnyC/CalCOFI-Regression-ML" rel="noopener ugc nofollow" target="_blank">CalCOFI-Regression-ML</a></p><pre class="kd ke kf kg fd lv ks lw lx aw ly bi"><span id="6ecc" class="lh jf hi ks b fi lz ma l mb mc"><em class="jd">import</em> pandas <em class="jd">as</em> pd<br/><em class="jd">import</em> numpy <em class="jd">as</em> np<br/><em class="jd">import</em> seaborn <em class="jd">as</em> sns<br/><em class="jd">import</em> matplotlib.pyplot <em class="jd">as</em> plt</span><span id="35f5" class="lh jf hi ks b fi md ma l mb mc">df = pd.read_csv('bottle.csv') # Load in data<br/>df.describe() </span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es me"><img src="../Images/df7c1bad70fcbc9e5635e8a0fbec5f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQomHNIcxy4HNxC6tX17TA.png"/></div></div></figure><p id="d0bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在定型模型之前，我们可以先浏览数据，以便更好地了解对此数据集使用线性回归算法是否是个好主意。</p><pre class="kd ke kf kg fd lv ks lw lx aw ly bi"><span id="0df3" class="lh jf hi ks b fi lz ma l mb mc"><em class="jd"># Data Cleaning<br/></em>df = df[['Salnty', 'T_degC']]<br/>df.dropna(axis = 0,inplace = True)<br/>df.shape <em class="jd"># (814247, 2) too many data, we just take 500 rows<br/></em>df = df[:][:500]<br/>sns.regplot(x="Salnty", y="T_degC", data=df);<br/>df.corrwith(df['T_degC']) <em class="jd"># -0.921586</em></span></pre><p id="c856" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从下面的图表中，我们可以看到盐分和温度之间的关系。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mf"><img src="../Images/f77a8d7613e2328ad20d2fdc98e7a7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jD5ynbZ0Sk4oc6qt8iTXrQ.png"/></div></div></figure><p id="8c2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们需要准备用于训练和测试的数据。请注意，我们将X设置为salty的原因是，我们希望根据salty来预测温度。</p><pre class="kd ke kf kg fd lv ks lw lx aw ly bi"><span id="672c" class="lh jf hi ks b fi lz ma l mb mc"><em class="jd">from</em> sklearn.model_selection <em class="jd">import</em> train_test_split</span><span id="a1db" class="lh jf hi ks b fi md ma l mb mc">X = df['Salnty'].values.reshape(-1,1)<br/>y = df['T_degC'].values<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span></pre><p id="6099" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，请击鼓！我们正式开始训练模特了！</p><pre class="kd ke kf kg fd lv ks lw lx aw ly bi"><span id="005e" class="lh jf hi ks b fi lz ma l mb mc"><em class="jd">from</em> sklearn.linear_model <em class="jd">import</em>  LinearRegression</span><span id="a28c" class="lh jf hi ks b fi md ma l mb mc">regessor = LinearRegression()<br/>regessor.fit(X_train, y_train) # Here is where the magic happens<br/>print(f"y = {regessor.coef_[0]:2f}x+ {regessor.intercept_:2f}")</span></pre><p id="f808" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们得到的输出是基于我们上面讨论的线性回归算法的最佳拟合线。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mg"><img src="../Images/511439c89f12beaa09d76c460ea19e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*inOc65dCUSmzvOcKKyy05w.png"/></div></div></figure><p id="a06b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将使用我们之前准备的测试数据集来测试该模型，以便我们可以了解该模型可以做出多准确的预测。</p><pre class="kd ke kf kg fd lv ks lw lx aw ly bi"><span id="be44" class="lh jf hi ks b fi lz ma l mb mc">y_pred = regessor.predict(X_test.reshape(-1,1))</span><span id="8085" class="lh jf hi ks b fi md ma l mb mc">x = np.arange(1, 101, 1)<br/>plt.plot(x,y_test,label='Actual Value')<br/>plt.plot(x,y_pred,label='Predicted Value')<br/>regessor.score(X_test, y_test) <em class="jd"># Accuracy Score = 80.36</em></span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es me"><img src="../Images/71789019711bdb6c4953deeca673211c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cbis11z6DMpjBEPfIlbS0g.png"/></div></div></figure><p id="dfa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面得到的结果来看，我们可以说这个模型做得不错。然而，这只是一个演示，展示了我们如何用线性回归算法来训练一个模型。希望这篇文章对你有所帮助！</p><h1 id="bf4a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参考</h1><ol class=""><li id="fb65" class="mh mi hi ih b ii kt im ku iq mj iu mk iy ml jc mm mn mo mp bi"><a class="ae ko" href="https://chih-sheng-huang821.medium.com/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8-linear-regression-3a271a7453e" rel="noopener">線性回歸(Linear Regression) by Tommy Huang</a></li><li id="9fa2" class="mh mi hi ih b ii mq im mr iq ms iu mt iy mu jc mm mn mo mp bi translated">线性方程由<a class="ae ko" href="https://www.texasgateway.org/resource/23-position-vs-time-graphs" rel="noopener ugc nofollow" target="_blank">德克萨斯网关</a></li><li id="a249" class="mh mi hi ih b ii mq im mr iq ms iu mt iy mu jc mm mn mo mp bi translated"><a class="ae ko" href="https://www.kaggle.com/sohier/calcofi?select=cast.csv" rel="noopener ugc nofollow" target="_blank">卡尔科菲数据集</a></li></ol></div></div>    
</body>
</html>