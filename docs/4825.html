<html>
<head>
<title>Review — Unsupervised Visual Representation Learning by Context Prediction (Self-Supervised)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾-通过上下文预测的无监督视觉表示学习(自我监督)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff?source=collection_archive---------3-----------------------#2021-08-09">https://medium.com/nerd-for-tech/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff?source=collection_archive---------3-----------------------#2021-08-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a0dc" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">自我监督学习:不使用基础事实标签的上下文预测</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/69ea583bee3d92ac489237d8e514b6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bVC6ZZucjP8ksg3wPhpwQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">学习小块表示的任务包括随机抽样一个小块(蓝色)和八个可能的邻居(红色)中的一个</strong></figcaption></figure><p id="4995" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi kk translated"><span class="l kl km kn bm ko kp kq kr ks di">在</span>这个故事中，回顾了卡耐基梅隆大学和加州大学的<strong class="jq hj">通过上下文预测</strong>(Context Prediction)进行的无监督视觉表示学习。</p><blockquote class="kt ku kv"><p id="eb85" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">在标准的监督学习中，网络用基本事实标签进行训练。然而，标注标签通常既昂贵又耗时，尤其是当数据集很大时，例如ImageNet。</p></blockquote><p id="65aa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在本文中:</p><ul class=""><li id="5f05" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><strong class="jq hj">使用图像内上下文</strong>学习的特征表示确实捕捉了图像间的视觉相似性。</li><li id="c598" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">例如:在上面，你能猜出两对补片的空间结构吗？请注意，一旦您识别了对象，任务就容易多了！</li><li id="faf7" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">这是一种<strong class="jq hj">自我监督学习</strong>。</li></ul><p id="8179" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是一篇发表在<strong class="jq hj"> 2015 ICCV </strong>的论文，引用超过<strong class="jq hj"> 1300次</strong>。(<a class="lo lp ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----51a1d7ce6aff--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="6313" class="lx ly hi bd jn lz ma mb mc md me mf mg io mh ip mi ir mj is mk iu ml iv mm mn bi translated">概述</h1><ol class=""><li id="c7e9" class="la lb hi jq b jr mo ju mp jx mq kb mr kf ms kj mt lg lh li bi translated"><strong class="jq hj">动机&amp;概念想法</strong></li><li id="c9d6" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj mt lg lh li bi translated"><strong class="jq hj">学习视觉上下文预测</strong></li><li id="8c06" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj mt lg lh li bi translated"><strong class="jq hj">实施细则</strong></li><li id="174c" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj mt lg lh li bi translated"><strong class="jq hj">实验结果</strong></li></ol></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="9554" class="lx ly hi bd jn lz ma mb mc md me mf mg io mh ip mi ir mj is mk iu ml iv mm mn bi translated"><strong class="ak"> 1。动机&amp;概念想法</strong></h1><ul class=""><li id="2bd4" class="la lb hi jq b jr mo ju mp jx mq kb mr kf ms kj lf lg lh li bi translated">互联网规模的数据集(即数以千亿计的图像)受到所需的人工注释费用的阻碍。</li><li id="0106" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">解决这一困难的自然方法是采用<strong class="jq hj">无监督学习</strong>。不幸的是，无人监管的方法<strong class="jq hj">还没有被证明能够从大量全尺寸的真实图像中提取有用的信息</strong>。</li></ul><blockquote class="kt ku kv"><p id="baf9" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">如果没有一个物体被标记，如何编写一个目标函数来鼓励一个表示去捕捉，例如，物体？</p></blockquote><ul class=""><li id="5bf7" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">在<strong class="jq hj">文本领域</strong>中，给定大的文本语料库，想法是训练将每个单词映射到特征向量的模型，使得在给定向量的情况下<strong class="jq hj">容易预测上下文中的单词(即，前面和/或后面的几个单词)</strong>。</li><li id="c748" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">这将一个明显无监督的问题</strong>(寻找单词之间良好的相似性度量)<strong class="jq hj">转化为一个“自我监督的”问题</strong>:从一个给定的单词到它周围的单词学习一个函数。</li><li id="f7d7" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">本文旨在为图像数据提供一个类似的“自我监督”公式。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/2212a1a121a2f2ddce21fbc77bd2d1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*P3q8MpmTpNUmsSGd5iLx_g.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">该算法接收这八种可能的空间排列之一的两个小块，没有任何上下文，然后必须分类哪个配置被采样</strong></figcaption></figure><blockquote class="kt ku kv"><p id="b315" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated"><strong class="jq hj">随机成对的小块</strong>以八种空间配置中的一种进行采样。然后，算法必须<strong class="jq hj">猜测一个补丁相对于另一个补丁的位置。</strong></p><p id="f53b" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">潜在的假设是，做好<strong class="jq hj">这项任务需要理解场景和物体，即良好的视觉表现。</strong></p></blockquote></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="4c59" class="lx ly hi bd jn lz ma mb mc md me mf mg io mh ip mi ir mj is mk iu ml iv mm mn bi translated">2.学习视觉上下文预测</h1><h2 id="6384" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">2.1.一对<a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------">Alex net</a>T10】般的网络</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nk"><img src="../Images/2aa6e07c9e815ad2b8c645d0c3d8e802.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*ehoyrfwfBEQ5RWh-jKakgA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">一对</strong><a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"><strong class="bd jn">Alex net</strong></a><strong class="bd jn">类网络用于对分类</strong></figcaption></figure><ul class=""><li id="274a" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><strong class="jq hj">卷积神经网络(CNN) </strong>用于学习我们的托词任务的图像表示，即预测图像内小块的相对位置。</li><li id="7a79" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">网络必须<strong class="jq hj">通过几个卷积层馈送两个输入面片</strong>，并且<strong class="jq hj">产生一个输出，该输出为八个空间配置中的每一个分配一个概率。</strong></li></ul><blockquote class="kt ku kv"><p id="62a0" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">但是要注意的是，<strong class="jq hj">最终目标</strong>是<strong class="jq hj">学习针对各个小块</strong>的特征嵌入，使得视觉上相似(跨不同图像)的<strong class="jq hj">小块将在嵌入空间中接近。</strong></p></blockquote><ul class=""><li id="4c2f" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><strong class="jq hj">一对</strong><a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"><strong class="jq hj">Alex net</strong></a><strong class="jq hj">风格的架构</strong>被用来分别处理每个补丁，直到一个深度类似于<a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"> AlexNet </a>中的<strong class="jq hj"> fc6 </strong>，在该点之后<strong class="jq hj">表示被融合。</strong></li><li id="0942" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">权重在网络</strong>的两端绑定/共享(用虚线表示),以便为两个补片计算相同的fc6级嵌入函数。</li></ul><h2 id="b4df" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">2.2.训练样本</h2><ul class=""><li id="bffe" class="la lb hi jq b jr mo ju mp jx mq kb mr kf ms kj lf lg lh li bi translated">为了获得给定图像的训练样本，<strong class="jq hj">在不参考图像内容的情况下，对第一个小块进行均匀采样</strong>。</li><li id="3fc4" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">给定第一个小块的位置，<strong class="jq hj">第二个小块是从八个可能的邻近位置随机抽样的。</strong></li></ul><h2 id="477d" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">2.3.避免“琐碎”的解决方案</h2><h2 id="1e20" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">2.3.1.低级线索</h2><ul class=""><li id="b274" class="la lb hi jq b jr mo ju mp jx mq kb mr kf ms kj lf lg lh li bi translated">必须小心确保任务迫使网络提取所需的信息(高级语义)，而不走“琐碎”的捷径。</li><li id="caaf" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">像<strong class="jq hj">边界图案</strong>或<strong class="jq hj">纹理在面片之间延续</strong>这样的低级线索</strong>可以被当作快捷方式。</li></ul><blockquote class="kt ku kv"><p id="f7d1" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">在补片之间留出间隙(大约是补片宽度的一半)是很重要的。<strong class="jq hj">每个补丁位置最多随机抖动7个像素。</strong></p></blockquote><h2 id="ae8e" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">2.3.2.C <strong class="ak">色差</strong></h2><ul class=""><li id="d0ac" class="la lb hi jq b jr mo ju mp jx mq kb mr kf ms kj lf lg lh li bi translated">另一个问题是<strong class="jq hj">色差</strong>。</li><li id="e339" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">透镜聚焦不同波长的光。在一些相机中，一个颜色通道(通常是绿色)相对于其他通道向图像中心收缩。</li><li id="de1d" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">网络可以通过检测绿色和品红色(红色+蓝色)的分离来学习平凡解，这是一种我们不希望网络学习的捷径。</strong></li><li id="d4f3" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">为了处理这个问题，<strong class="jq hj">尝试了两种类型的预处理</strong>。</li><li id="ebc0" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">一种是将绿色和品红色向灰色转移(“投影”)。</li><li id="ecb7" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">具体来说，假设<em class="kw">a</em>=[1，2，1](RGB空间中的“绿-洋红色轴”)。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/d2099111f799b5fdd4be991e24e780d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*xGNCnfZyHDrQIJmIOjgppQ.png"/></div></figure><ul class=""><li id="59b5" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><em class="kw"> B </em>是<strong class="jq hj">减去一种颜色在绿-品红颜色轴上的投影</strong>的矩阵。每个像素值由<em class="kw"> B </em>相乘。</li><li id="4bf0" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">另一种方法是<strong class="jq hj">从每个补丁中随机丢弃3个颜色通道中的2个(“颜色丢弃”)</strong>，用高斯噪声替换丢弃的颜色(标准偏差为剩余通道标准偏差的1/100)。</li></ul></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="14da" class="lx ly hi bd jn lz ma mb mc md me mf mg io mh ip mi ir mj is mk iu ml iv mm mn bi translated"><strong class="ak"> 3。实施细节</strong></h1><ul class=""><li id="6edf" class="la lb hi jq b jr mo ju mp jx mq kb mr kf ms kj lf lg lh li bi translated"><strong class="jq hj"> ImageNet 2012训练集</strong> (1.3M图片)，但是<strong class="jq hj">丢弃标签</strong>。</li><li id="774c" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">每张图像都被调整到150K到450K的总像素数，保持宽高比。</li><li id="3782" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">面片</strong>以分辨率<strong class="jq hj"> 96×96 </strong>采样，呈<strong class="jq hj">网格状。</strong></li><li id="3b11" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">网格中采样的小块之间有48个像素</strong>的间隙，而且<strong class="jq hj">抖动</strong>网格中每个小块的位置在每个方向上有<strong class="jq hj">7到7个像素</strong>。</li><li id="9be4" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">通过(1)均值减法(2)投影或丢弃颜色，以及(3)随机地将一些小块向下采样到总共100个像素，然后向上采样，对小块进行预处理，以建立对像素化的鲁棒性。</li><li id="eee1" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><a class="ae nj" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">批量归一化</a>用于那些不使用LRN的conv图层。(据本<a class="ae nj" href="https://github.com/abhisheksambyal/Self-supervised-learning-by-context-prediction/blob/master/Self_supervised_learning_by_context_prediction.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>。)</li></ul></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="e518" class="lx ly hi bd jn lz ma mb mc md me mf mg io mh ip mi ir mj is mk iu ml iv mm mn bi translated">4.实验结果</h1><ul class=""><li id="6f94" class="la lb hi jq b jr mo ju mp jx mq kb mr kf ms kj lf lg lh li bi translated">经过训练的网络应用于两个领域。</li><li id="91ca" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">首先，<strong class="jq hj">预训练</strong>，用于只有有限训练数据的标准视觉任务:特别是VOC 2007对象检测。</li><li id="8672" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">第二，<strong class="jq hj">视觉数据挖掘</strong>，目标是从一个未标记的图像集合开始，发现对象类别。</li><li id="8097" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">最后，在布局预测“借口任务”上分析性能，以查看还有多少要从监控信号中学习。</li></ul><h2 id="7d6d" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">4.1.<strong class="ak">最近邻居</strong></h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nm"><img src="../Images/8dd3a82b9e785d70409367e9060cffff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*od_6v7Y6ZqsQRFmlsvod9g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">最近邻居获得的补丁簇示例(fc6 </strong>来自我们架构的<strong class="bd jn">随机初始化</strong>的特征，<a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"><strong class="bd jn">Alex net</strong></a><strong class="bd jn">fc7</strong>在<strong class="bd jn">标记的ImageNet </strong>上训练后，以及<strong class="bd jn"> fc6 </strong>从<strong class="bd jn">提出的方法</strong>中学习到的特征)</figcaption></figure><ul class=""><li id="8fc0" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">目标是了解补丁有多相似。</li><li id="0960" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">fc7和更高层被删除。<strong class="jq hj"> NN被应用在fc6特性上。</strong></li><li id="cd77" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">上面显示了一些补丁程序(从1000个随机查询中选出)的结果。</li><li id="c0d9" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">提出的上下文预测算法表现良好。</strong></li><li id="a1a7" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">对于中间的<a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"> AlexNet </a>，它优于提出的上下文预测方法。因为这个<a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"> AlexNet </a>是监督学习的。</li></ul><blockquote class="kt ku kv"><p id="b025" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">通过可视化由NN分组的小块，预测的上下文预测能够学习没有标签的图像表示。</p></blockquote><h2 id="94cc" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">4.2.采用进入<a class="ae nj" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------"> R-CNN </a>进行目标检测</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/58269c0d6e0b1d532b022e1fa8e5ebbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*l4ipZUfzJsf1dN5BchYEtQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">物体探测网络</strong></figcaption></figure><ul class=""><li id="68af" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><a class="ae nj" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------"> <strong class="jq hj"> R-CNN </strong> </a> <strong class="jq hj">管道使用。</strong></li><li id="d6c0" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">但是，使用的是227×227输入，而不是96×96。网络需要修改。</li><li id="1c1e" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">pool5在空间上执行7×7。</li><li id="bcef" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">新的“conv6”层是通过将fc6层转换为卷积层而创建的。</li><li id="2b49" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">conv6层有4096个通道，每个单元连接到pool5的一个3×3区域。</li><li id="a4f6" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">conv6之后的另一层(称为conv6b)使用1×1内核，可将维度降至1024个通道。</li><li id="665c" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">输出通过一个汇集层馈送到一个全连接层(fc7 ),该层又连接到一个最终的fc8层，该层馈送到softmax。</li><li id="2dc8" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">conv6b、fc7和fc8以随机权重开始。</li><li id="7ff6" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">fc7被用作最终表示。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es no"><img src="../Images/6e85d40a97a60b8a3541264d00d4f2ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dI-Kzw_sRk81nmKC1Kn4iw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">PASCAL VOC-2007的AP和mAP结果(%)</strong></figcaption></figure><ul class=""><li id="c1b4" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><strong class="jq hj"> Scratch-Ours </strong>:从零开始训练(随机初始化)的架构表现略差于从零开始训练的<a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------">AlexNet</a>(<strong class="jq hj">Scratch-</strong><a class="ae nj" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------"><strong class="jq hj">R-CNN</strong></a><strong class="jq hj">)</strong>。</li></ul><blockquote class="kt ku kv"><p id="668f" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated"><strong class="jq hj">Ours-projection/Ours-color-dropping:预训练将从零开始的数量提升6% mAP，并优于在Pascal上从零开始训练的一个</strong><a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"><strong class="jq hj">AlexNet</strong></a><strong class="jq hj">风格的模型5%以上。</strong></p><p id="6d8f" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated"><strong class="jq hj">只比</strong> <strong class="jq hj">落后8% ImageNet-</strong><a class="ae nj" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------"><strong class="jq hj">R-CNN</strong></a><strong class="jq hj">，</strong>即<a class="ae nj" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------"> R-CNN </a>预训练有ImageNet标签。这是<strong class="jq hj">VOC 2007在不使用数据集外标签的情况下(当时)的最佳结果。</strong></p></blockquote><ul class=""><li id="be87" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">Ours-Yahoo100m :随机选择的Yahoo/Flickr 1亿数据集[51]的2M子集被使用，它完全是自动收集的。微调后的性能<strong class="jq hj">略差于Ours-projection/Ours-color-dropping</strong>，但<strong class="jq hj">仍比从头模型有相当大的提升。</strong></li><li id="f390" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated"><strong class="jq hj">Ours-</strong><a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------"><strong class="jq hj">VGG</strong></a>:<a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------">VGG</a>网试，地图关闭ImageNet- <a class="ae nj" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------"> R-CNN </a> one。</li></ul><h2 id="05ce" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">4.3.可视化数据挖掘</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es np"><img src="../Images/d7cf7ac4d51194452219d4662632b80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iucumK13dVYXrdsbdDl3Lw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">发现的物体群</strong></figcaption></figure><ul class=""><li id="b2df" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">视觉数据挖掘或无监督对象发现旨在<strong class="jq hj">使用大型图像集合来发现恰好描述相同语义对象的图像片段。</strong></li><li id="599b" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">首先，<strong class="jq hj">从一幅图像中采样四个相邻片</strong>的星座。</li><li id="40cc" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">然后，<strong class="jq hj">查找对所有四个小块具有最强匹配的前100个图像，忽略空间布局。</strong></li><li id="7eb3" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">(过滤掉四个匹配在几何上不一致的图像。)</li><li id="dde4" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">每个聚类旁边的数字表示其排名，由几何验证的最高匹配分数确定。</li><li id="cce1" class="la lb hi jq b jr lj ju lk jx ll kb lm kf ln kj lf lg lh li bi translated">上面显示了一些生成的补丁簇。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/137de5f26b3d6682ebaf76ff7b4b6b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*46Z0hpRqgd0ShFtsPPpvsg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">从巴黎街景数据集发现的聚类</strong></figcaption></figure><ul class=""><li id="7565" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">提议的表示捕捉场景布局和建筑元素。</li></ul></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><blockquote class="kt ku kv"><p id="e50c" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated"><strong class="jq hj">与有监督的预训练策略相比，使用提出的上下文预测的预训练优于随机初始化，同时不需要地面真实标签。</strong></p></blockquote></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h2 id="26bb" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">参考</h2><p id="df1e" class="pw-post-body-paragraph jo jp hi jq b jr mo ij jt ju mp im jw jx nr jz ka kb ns kd ke kf nt kh ki kj hb bi translated">【2015 ICCV】【上下文预测】<br/> <a class="ae nj" href="https://arxiv.org/abs/1505.05192" rel="noopener ugc nofollow" target="_blank">通过上下文预测的无监督视觉表征学习</a></p><h2 id="950f" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated">自我监督学习</h2><p id="04e3" class="pw-post-body-paragraph jo jp hi jq b jr mo ij jt ju mp im jw jx nr jz ka kb ns kd ke kf nt kh ki kj hb bi translated">[ <a class="ae nj" href="https://sh-tsang.medium.com/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff" rel="noopener">上下文预测</a> ]</p><h2 id="c335" class="mv ly hi bd jn mw mx my mc mz na nb mg jx nc nd mi kb ne nf mk kf ng nh mm ni bi translated"><a class="ae nj" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我之前的其他论文阅读</a></h2></div></div>    
</body>
</html>