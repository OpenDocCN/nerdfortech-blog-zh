<html>
<head>
<title>Illustrated Naive Bayes Implementation from scratch for Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始为情感分析图解朴素贝叶斯实现</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/illustrated-naive-bayes-implementation-from-scratch-for-sentiment-analysis-63c4bcab6053?source=collection_archive---------4-----------------------#2021-07-16">https://medium.com/nerd-for-tech/illustrated-naive-bayes-implementation-from-scratch-for-sentiment-analysis-63c4bcab6053?source=collection_archive---------4-----------------------#2021-07-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="be5a" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">为了更简单和更好的理解，我试图说明如何从头开始实现朴素贝叶斯算法。我用的是DeepLearning.AI提供的自然语言处理与分类和向量空间课程的笔记本，你可以从<a class="ae jh" href="https://www.coursera.org/learn/classification-vector-spaces-in-nlp" rel="noopener ugc nofollow" target="_blank">这里</a>到达课程，从<a class="ae jh" href="https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/tree/master/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%202" rel="noopener ugc nofollow" target="_blank">这个资源库</a>到达笔记本。</p></blockquote><p id="82ff" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">在这项任务中，我们将使用NLTK库中的<em class="ik"> twitter_samples </em>数据集，训练一个朴素贝叶斯模型来预测一条推文是正面还是负面情绪。</p><p id="c6c9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">我们将遵循以下3个步骤:</p><p id="9cc0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">1.对数据进行预处理，建立词频词典</p><p id="663e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">2.使用朴素贝叶斯训练模型</p><p id="da1b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">3.测试模型</p></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h1 id="b9b6" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">1.对数据进行预处理，建立词频词典</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kq"><img src="../Images/03a002c2036ad67fb77a1101674bca9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOM9DvmLTlEhr4SuBPkT3A.png"/></div></div></figure><p id="45d7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj"> count_tweets() </strong>函数将tweets和标签的列表作为输入，使用<strong class="il hj"> process_tweet() </strong>函数清理所有tweets，并返回一个字典，该字典将词干单词的元组及其标签(0，1)作为关键字，将该单词出现的次数作为值。</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="lc ld l"/></div></figure><p id="7702" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj"> process_tweet() </strong>是一个帮助器函数，用于预处理数据集，将文本标记为单独的单词，将单词转换为词干，将所有字母小写，并删除停用词和标点符号。</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="lc ld l"/></div></figure><h1 id="aca6" class="js jt hi bd ju jv le jx jy jz lf kb kc kd lg kf kg kh lh kj kk kl li kn ko kp bi translated">2.使用朴素贝叶斯训练模型</h1><p id="5fb6" class="pw-post-body-paragraph ii ij hi il b im lj io ip iq lk is it ji ll iw ix jj lm ja jb jk ln je jf jg hb bi translated">朴素贝叶斯是一种受监督的机器学习算法，它依赖于词频计数，如逻辑回归。在实现朴素贝叶斯算法之前，我们需要理解两个重要术语，它们是<strong class="il hj">对数似然性</strong>和<strong class="il hj">对数先验。</strong></p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lo"><img src="../Images/a1327ca9f6e011071790c884053c0c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0eancepj-ta_ZLC5n3y78w.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">二元分类的朴素贝叶斯推理条件规则</figcaption></figure><p id="c26d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">可能性</strong>是推文中每个词的正概率与负概率之比的乘积。当可能性值高于1时，我们可以预测这条推文为<strong class="il hj">积极的</strong>，这样推文中的单词共同对应积极的情绪。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lt"><img src="../Images/b7af69fd94a0d09f10efbe66bc694b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VKyqBRTUBARrd3HPhwo6_A.png"/></div></div></figure><p id="c1ff" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">先验</strong>是数据集中正面和负面推文数量的比率。在这种情况下，不需要考虑先验，因为数据集是平衡的，这意味着数据集包括完全相同数量的正面和负面推文。然而，当数据集不平衡时，我们需要将先验与似然相乘。</p><p id="f9d0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">计算概率时，我们将0和1之间的许多数字相乘。为了防止<em class="ik">数值下溢</em>问题的可能性，我们将采用似然和先验的对数。对数将数字的乘积转换为加法运算，公式变为:</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lu"><img src="../Images/c0c3d52760469cf601f7a8514e2b165b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SyYwqtTlZ964D7MqqlZflw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">二元分类的朴素贝叶斯公式</figcaption></figure><p id="f77e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">如果这个公式的结果是正的，则该推文也是正的，否则如果它小于0，则表明该推文是负的。</p><p id="8f7d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">因此，我们可以使用<strong class="il hj"> train_naive_bayes() </strong>开始实现朴素贝叶斯算法，该算法采用三个参数:<strong class="il hj"> freqs </strong>，先前构建的频率字典，<strong class="il hj"> train_x </strong>，先前分割的数据集的%80，<strong class="il hj"> train_y，</strong>标签属于train_x</p><p id="f88d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj"> train_naive_bayes() </strong>返回数据集的<strong class="il hj"> logprior </strong>值和<strong class="il hj"> loglikelihood </strong>作为单词的字典，其loglikelihood作为键值对。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es lv"><img src="../Images/6b88471a1e095aa5612b6c8f783f5aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*6hZ-2CJZjuRy1jKdwKjO0w.png"/></div></figure><p id="cd27" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">当计算对数似然方程中的概率时，我们使用<strong class="il hj">拉普拉斯平滑</strong>。我们在频率值上加1，避免得到概率为0。<strong class="il hj"> V </strong>是所有数据集中唯一单词的总数，并添加到分母中以计算额外的1。</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="lc ld l"/></div></figure><h1 id="ff42" class="js jt hi bd ju jv le jx jy jz lf kb kc kd lg kf kg kh lh kj kk kl li kn ko kp bi translated">3.测试模型</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lw"><img src="../Images/90154ee443c3ebad5d6317c1c1133b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vi6D3bqaGnv6M4ByS4IxpQ.png"/></div></div></figure><p id="38d4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj"> test_naive_bayes() </strong>函数测试朴素贝叶斯模型在<strong class="il hj"> test_x </strong>数据集中的性能，该数据集先前被拆分为数据集的%20。对于test_x中的每个tweet，<strong class="il hj"> naive_bayes_predict() </strong>函数计算属于该tweet中的单词的<em class="ik"> p </em>值、logprior和loglikelihood之和，如果p大于0，则将该tweet标记为正，否则为负。将预测标签与<strong class="il hj"> test_y. </strong>中的标签进行比较计算出的准确度</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="lc ld l"/></div></figure><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="lc ld l"/></div></figure><p id="52a7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">该模型有望达到0.9940的精度。</p></div></div>    
</body>
</html>