<html>
<head>
<title>Review — Zhong ELECGJ’21: A GAN-Based Video Intra Coding (HEVC Intra)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种基于GAN的视频帧内编码(HEVC帧内编码)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-zhong-elecgj21-a-gan-based-video-intra-coding-hevc-intra-9e3486dbca78?source=collection_archive---------9-----------------------#2021-03-06">https://medium.com/nerd-for-tech/review-zhong-elecgj21-a-gan-based-video-intra-coding-hevc-intra-9e3486dbca78?source=collection_archive---------9-----------------------#2021-03-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="618b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">胜过<a class="ae ix" rel="noopener" href="/@sh.tsang/review-ipfcn-intra-prediction-using-fully-connected-network-hevc-intra-prediction-28de33dff3a5"> IPFCN </a>、<a class="ae ix" rel="noopener" href="/@sh.tsang/review-ipcnn-intra-prediction-convolutional-neural-network-hevc-intra-prediction-a8d00409156"> IPCNN </a>和<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-spatial-rnn-cnn-guided-spatial-rnn-for-video-coding-hevc-intra-prediction-a0c1bf96ef30">空间RNN </a>。复杂度低于<a class="ae ix" href="https://sh-tsang.medium.com/review-zhu-tmm20-generative-adversarial-network-based-intra-prediction-for-video-coding-c8a217c564ea" rel="noopener">朱TMM’20</a></h2></div><p id="7a7a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">本故事中，<strong class="ja hj">简要回顾了中山大学、南方海洋科学与工程广东实验室、实验室合作的基于GAN的视频帧内编码</strong>(Zhong elec gj’21)。在本文中:</span></p><ul class=""><li id="12e4" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>用作<strong class="ja hj">相邻重构信号到预测单元的映射，以提高帧内预测精度。</strong></li></ul><p id="2645" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这是一篇发表在<strong class="ja hj">2021 elec gj</strong>《MDPI电子及其应用杂志》上的论文，影响因子<strong class="ja hj">为2.412 </strong> (2019)。(<a class="km kn ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----9e3486dbca78--------------------------------" rel="noopener" target="_blank">曾植和</a> @中)</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="92ef" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">概述</h1><ol class=""><li id="f678" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ls kj kk kl bi translated"><strong class="ja hj">拟</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="ja hj">甘</strong> </a></li><li id="0216" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ls kj kk kl bi translated"><strong class="ja hj">实验结果</strong></li></ol></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="e1e8" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated"><strong class="ak"> 1。拟定</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a></h1><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es ly"><img src="../Images/90915a988903a85a07032fd26b3754dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JulTOqrhrUE8sos0yPqyAg.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">提出生成性对抗网络框架</strong></figcaption></figure><ul class=""><li id="ba54" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">生成器<em class="mo"> G </em>用于预测编码块</strong>而鉴别器<em class="mo"> D </em>是鉴别所生成单元是真的还是假的评论家。</li><li id="7ddd" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">输入是24 × 24的图片，其中右下角的8×8是我们想要预测的块，而其他的是原始像素。</li></ul><h2 id="2136" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated">1.1.发电机</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nd"><img src="../Images/4139eb989803a757d5ffd82db427825f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hV-b1cLeKcB110SDGLfucQ.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">发电机</strong></figcaption></figure><ul class=""><li id="220b" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">使用<strong class="ja hj">二级粗到精发生器</strong>。</li><li id="784f" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">粗略网络与精细网络共享相同的参数。</li><li id="3886" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">与“具有上下文关注的生成式图像修复”相比，<strong class="ja hj">由于输入大小较小，而不是整个图片，因此移除了一些下采样和扩张卷积</strong>。</li><li id="2f8e" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">上下文关注层也被移除。</li><li id="1b16" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">指数线性单位(ELU)用于每个卷积，除了最后一层。</li><li id="d83e" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">在最后一个输出层，它被裁剪为[-1.1]。</li></ul><h2 id="05ce" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated">1.2.<strong class="ak">鉴别器</strong></h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es ne"><img src="../Images/5e8acdba6240d5ab954d7a617e0a8cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_s9COktT3_-UGmAwb4iUdA.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">本地鉴别器</strong></figcaption></figure><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nf"><img src="../Images/fca49f4ba2b572e9fcc1f356a2e67b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Fx6TVCgSa77suRMlHt4og.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">全局鉴别器</strong></figcaption></figure><ul class=""><li id="52f4" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">对于鉴别器，有一个全局鉴别器和一个局部鉴别器。</li><li id="ecec" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">全局鉴别器</strong>采用整个24 × 24图像作为输入给<strong class="ja hj">以确定完整图像</strong>的整体一致性，而<strong class="ja hj">局部鉴别器</strong>仅将待预测的16 × 16块作为输入给<strong class="ja hj">以增强区域一致性</strong>。</li><li id="9dde" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">所有卷积的核大小为5×5，步距为2。</li></ul><h2 id="0be4" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated">1.3.损失函数</h2><ul class=""><li id="a17c" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ki kj kk kl bi translated"><strong class="ja hj">使用逐像素l1损失</strong>代替均方误差(MSE)。</li><li id="84e0" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">考虑到<strong class="ja hj">更近的像素具有更强的空间相关性</strong>，使用权重掩模<em class="mo"> m </em>引入<strong class="ja hj">空间加权l1损失</strong>。</li><li id="64c5" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj"> Wasserstein GAN被认为是为了提高</strong><a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"><strong class="ja hj">GAN</strong></a><strong class="ja hj">稳定性</strong>:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ng"><img src="../Images/7786e6d906c0b1fc73b0058af60d487d.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*SDs1m9SjHH2BjZRBnSQAtg.png"/></div></figure><ul class=""><li id="9d9f" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">更具体地说，使用<strong class="ja hj">wasser stein GAN with Gradient Penalty(WGAN-GP)</strong>，其中WGAN-GP是WGAN的高级版本，具有<strong class="ja hj">梯度惩罚子项</strong>:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nh"><img src="../Images/83645b1fdbe628e234ac9e4212f81a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*XwntA2zerhQOob8yO5nH9A.png"/></div></figure><ul class=""><li id="08ea" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">因为我们只试图预测右下角的编码块；因此，梯度惩罚项<strong class="ja hj">应仅应用于预测块</strong>内的样本:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es ni"><img src="../Images/2b725b20b371746c6d0b5dae1ac585c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*8ANFLujUHCzujM8b-KIWQg.png"/></div></div></figure><ul class=""><li id="f187" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">其中<em class="mo"> m </em>是在右下区域内取值为0的二进制掩码，≥表示逐像素乘法。</li><li id="27b2" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">整体对抗损失</strong>:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nj"><img src="../Images/d8d057386bb49accc653d1499890afee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rj5RgtUIkIi4_BwX0_1htw.png"/></div></div></figure><ul class=""><li id="274d" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">(详情请阅读<a class="ae ix" href="https://arxiv.org/abs/1701.07875" rel="noopener ugc nofollow" target="_blank"> Wasserstein GAN </a>。)</li></ul><h2 id="cd0a" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated">1.4.培训策略</h2><ul class=""><li id="9645" class="kd ke hi ja b jb ln je lo jh lp jl lq jp lr jt ki kj kk kl bi translated">训练数据集是<strong class="ja hj">纽约市图书馆</strong>。该数据集由总共<strong class="ja hj"> 2550张不同大小的图片</strong>组成。</li><li id="7058" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">通过遍历和裁剪，最终获得总共240万幅图像。</li><li id="c173" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">与<a class="ae ix" href="https://sh-tsang.medium.com/review-zhu-tmm20-generative-adversarial-network-based-intra-prediction-for-video-coding-c8a217c564ea" rel="noopener">朱TMM’20</a>不同的是，从地面真实图像中提取的原始像素用于训练。</li><li id="1d94" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">仅使用亮度。</li></ul><h2 id="777d" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated">1.5.融入HEVC</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nk"><img src="../Images/d384a02e49a59732a8a513fea777a188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*HJgUewrs8YjHrUVquHqyJg.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">亮度模式推导图。</strong></figcaption></figure><ul class=""><li id="9d58" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">所提出的模式被视为CU帧内模式内与35帧内预测一起的附加预测<strong class="ja hj">。</strong></li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nl"><img src="../Images/e68f6d0f1dd19ba222173385e9329a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5pbctjJNUFshsee1p9WIg.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">亮度模式的模式信号图。</strong></figcaption></figure><ul class=""><li id="c768" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">一个信令位</strong>用于指示使用传统帧内模式还是使用建议模式。</li></ul></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="07b2" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">2.实验结果</h1><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nm"><img src="../Images/8cd3ed2a5d3f7b1c980428fea3b97807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSpEygYZt7S1K8aO3eOPZA.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx"> BD-Rate (%) </strong></figcaption></figure><ul class=""><li id="e2b2" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">用的是HM-16.15。使用所有帧内配置。</li><li id="82c9" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">在所有测试案例中，提出的stage_2策略优于stage_1策略</strong>。所提出的stage_2策略实现了平均1.6%的BD速率降低，而stage_1策略在亮度分量上实现了平均1.2%的BD速率降低。</li><li id="bdd0" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated"><strong class="ja hj">它证明了两级粗到精发电机网络的有效性。</strong></li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nn"><img src="../Images/c10ddd8a2ce01c3f658b86baba67d939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UilSefrLJeWK7zRgzC413w.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">与SOTA方法的比较</strong></figcaption></figure><ul class=""><li id="a567" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">上述SOTA方法专用于8 × 8块预测。</li><li id="ce3e" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">所提出的方法被重新设计。<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a>还在预测16 × 16块。但是只有8 × 8块可以使用<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>帧内预测。使用时，8 × 8块复制16 × 16块中对应于块位置的像素。</li><li id="a274" class="kd ke hi ja b jb lt je lu jh lv jl lw jp lx jt ki kj kk kl bi translated">如上所示，我们的建议实现了更好的编码增益，并且优于以前的类似工作:<a class="ae ix" rel="noopener" href="/@sh.tsang/review-ipfcn-intra-prediction-using-fully-connected-network-hevc-intra-prediction-28de33dff3a5">IPF cn</a>【15】，<a class="ae ix" rel="noopener" href="/@sh.tsang/review-ipcnn-intra-prediction-convolutional-neural-network-hevc-intra-prediction-a8d00409156">IPCNN</a>【17】，以及<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-spatial-rnn-cnn-guided-spatial-rnn-for-video-coding-hevc-intra-prediction-a0c1bf96ef30">空间RNN</a>【18–19】。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es no"><img src="../Images/0adf55e2188cfd48e3b67ba0103148c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18IydjpNhyW0fDbbC5vS4Q.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd kx">与</strong> <a class="ae ix" href="https://sh-tsang.medium.com/review-zhu-tmm20-generative-adversarial-network-based-intra-prediction-for-video-coding-c8a217c564ea" rel="noopener"> <strong class="bd kx">朱的比较</strong></a></figcaption></figure><ul class=""><li id="2495" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">虽然所提出的方法的BD-rate降低比<a class="ae ix" href="https://sh-tsang.medium.com/review-zhu-tmm20-generative-adversarial-network-based-intra-prediction-for-video-coding-c8a217c564ea" rel="noopener">朱TMM’20</a>的方法小，但是它获得了<strong class="ja hj">低得多的编码器和解码器复杂度</strong>。</li></ul></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h2 id="25bc" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated">参考</h2><p id="36ee" class="pw-post-body-paragraph iy iz hi ja b jb ln ij jd je lo im jg jh np jj jk jl nq jn jo jp nr jr js jt hb bi translated">【2021 ELECGJ】【钟elec gj’21】<br/><a class="ae ix" href="https://www.mdpi.com/2079-9292/10/2/132" rel="noopener ugc nofollow" target="_blank">一种基于GAN的视频帧内编码</a></p><h1 id="418d" class="kv kw hi bd kx ky ns la lb lc nt le lf io nu ip lh ir nv is lj iu nw iv ll lm bi translated">生成对抗网络</h1><p id="bc96" class="pw-post-body-paragraph iy iz hi ja b jb ln ij jd je lo im jg jh np jj jk jl nq jn jo jp nr jr js jt hb bi translated"><strong class="ja hj">图像合成</strong> [ <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a> ] [ <a class="ae ix" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a> ] [ <a class="ae ix" rel="noopener" href="/@sh.tsang/review-lapgan-laplacian-generative-adversarial-network-gan-e87200bbd827">拉普甘</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/review-dcgan-deep-convolutional-generative-adversarial-network-gan-ec390cded63c">DCGAN</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-pix2pix-image-to-image-translation-with-conditional-adversarial-networks-gan-ac85d8ecead2" rel="noopener">pix 2 pix</a>]<br/><strong class="ja hj">超分辨率</strong>[<a class="ae ix" rel="noopener" href="/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490">SRGAN&amp;SRResNet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-enhancenet-automated-texture-synthesis-super-resolution-8429635aa75e">EnhanceNet</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-esrgan-enhanced-super-resolution-generative-adversarial-networks-super-resolution-e8533ad006b5">ESRGAN</a><br/><strong class="ja hj">模糊</strong></p><h2 id="6fa3" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated">编解码器帧内预测</h2><p id="5cb6" class="pw-post-body-paragraph iy iz hi ja b jb ln ij jd je lo im jg jh np jj jk jl nq jn jo jp nr jr js jt hb bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(情)(情)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(情)(况)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(有)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(好)(的)(情)(情)(意)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(。</p><h2 id="23fa" class="mp kw hi bd kx mq mr ms lb mt mu mv lf jh mw mx lh jl my mz lj jp na nb ll nc bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>