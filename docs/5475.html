<html>
<head>
<title>Apache Spark: Bucketing and Partitioning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark:分桶和分区。</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/apache-spark-bucketing-and-partitioning-8feab85d5136?source=collection_archive---------0-----------------------#2021-10-07">https://medium.com/nerd-for-tech/apache-spark-bucketing-and-partitioning-8feab85d5136?source=collection_archive---------0-----------------------#2021-10-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fc30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">概述分区和分桶策略，以最大限度地提高收益，同时最大限度地减少负面影响。如果您可以减少洗牌的开销、对序列化的需求和网络流量，为什么不呢？最终，性能、更好的集群利用率和成本效益战胜了一切。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/682abe08ed473648926378ea2083f805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQDZ-l0_RwtHJQz0WVkUZg.png"/></div></div></figure></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="8665" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您要在大规模解决方案中充分利用数据集，您需要更好地了解数据集。<br/>通过更好地理解数据集，优化可以产生更好的结果。</p><blockquote class="jw jx jy"><p id="d524" class="if ig jz ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated"><strong class="ih hj">基数</strong>:指一列中包含的<strong class="ih hj">数据</strong>的唯一性。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kd"><img src="../Images/79787ac05046c1a437c8169638a702b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MPSb_JndD8z6CFeMySLVZw.png"/></div></div></figure><p id="046b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决定选择哪种策略完全取决于基数。如果数据集的基数比率是常数或有限的，那么最佳策略是-</p><h1 id="e8af" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">分区:</h1><p id="67f5" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在分布式系统中，分区指的是分成几个部分(仅当数据集被多次重用时有用)<em class="jz"/>。</p><p id="b5ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分区数据很容易查询，因为它们有助于跳过大量数据以更快地获得结果，如果处理不当，可能会导致小文件问题。</p><p id="4fb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在大型数据集中，您通常希望按日期、类型等过滤器进行查询。如果我们按照日期和类型对数据进行分区，我们的查询将跳过不必要的分区，只读取那些保证包含所需字段的部分。<em class="jz">多个</em>分区允许在更多的工作人员之间分配工作，但是<em class="jz">更少的分区</em>允许在更大的块中完成工作(并且通常更快)。</p><blockquote class="jw jx jy"><p id="5354" class="if ig jz ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated">注意:分区通常不应该包含超过128MB，单个混洗块限制是2GB，并且RDD的所有键/值对都支持分区。</p></blockquote><h2 id="1d60" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">我们可以通过两种方式创建具有特定分区的RDDs</h2><ol class=""><li id="a03d" class="lv lw hi ih b ii lc im ld iq lx iu ly iy lz jc ma mb mc md bi translated"><strong class="ih hj"> partitionBy() </strong> -通过提供显式分割器。这种转换允许在RDD上应用自定义分区逻辑。</li></ol><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="be76" class="lh kf hi mf b fi mj mk l ml mm">val users = spark.read.load("/path/to/users")</span><span id="6148" class="lh kf hi mf b fi mn mk l ml mm">users.write<br/>  .partitionBy("favorite_color")</span></pre><p id="3e56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.我们还可以使用特定的划分器应用返回rdd的转换。</p><ul class=""><li id="2b81" class="lv lw hi ih b ii ij im in iq mo iu mp iy mq jc mr mb mc md bi translated"><strong class="ih hj">加入</strong></li></ul><h2 id="8186" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">示例:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="6fc2" class="lh kf hi mf b fi mj mk l ml mm">val users = spark.read.load("/path/to/users").repartition('userId)</span><span id="8623" class="lh kf hi mf b fi mn mk l ml mm">val joined1 = users.join(addresses, "userId")<br/>joined1.show()</span></pre><ul class=""><li id="3de3" class="lv lw hi ih b ii ij im in iq mo iu mp iy mq jc mr mb mc md bi translated"><strong class="ih hj">左外连接</strong></li></ul><h2 id="8a6c" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">示例:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="e737" class="lh kf hi mf b fi mj mk l ml mm">val rdd1 = sc.parallelize(Seq(("m",55),("m",56),("e",57),("e",58),("s",59),("s",54)))<br/>val rdd2 = sc.parallelize(Seq(("m",60),("m",65),("s",61),("s",62),("h",63),("h",64)))<br/>val leftjoinrdd = rdd1.leftOuterJoin(rdd2)<br/>leftjoinrdd.collect</span></pre><h2 id="07ee" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">输出:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="2eb6" class="lh kf hi mf b fi mj mk l ml mm">Array[(String, (Int, Option[Int]))] = Array((s,(59,Some(61))), (s,(59,Some(62))), (s,(54,Some(61))), (s,(54,Some(62))), (e,(57,None)), (e,(58,None)), (m,(55,Some(60))), (m,(55,Some(65))), (m,(56,Some(60))), (m,(56,Some(65))))</span></pre><ul class=""><li id="7d2b" class="lv lw hi ih b ii ij im in iq mo iu mp iy mq jc mr mb mc md bi translated"><strong class="ih hj"> RightOuterJoin </strong></li></ul><h2 id="eb32" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">示例:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="278a" class="lh kf hi mf b fi mj mk l ml mm">val rdd1 = sc.parallelize(Seq(("m",55),("m",56),("e",57),("e",58),("s",59),("s",54)))<br/>val rdd2 = sc.parallelize(Seq(("m",60),("m",65),("s",61),("s",62),("h",63),("h",64)))<br/>val rightjoinrdd = rdd1.rightOuterJoin(rdd2)<br/>rightjoinrdd.collect</span></pre><h2 id="2f06" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">输出:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="5e41" class="lh kf hi mf b fi mj mk l ml mm">Array[(String, (Option[Int], Int))] = Array((s,(Some(59),61)), (s,(Some(59),62)), (s,(Some(54),61)), (s,(Some(54),62)), (h,(None,63)), (h,(None,64)), (m,(Some(55),60)), (m,(Some(55),65)), (m,(Some(56),60)), (m,(Some(56),65)))</span></pre><ul class=""><li id="3c2c" class="lv lw hi ih b ii ij im in iq mo iu mp iy mq jc mr mb mc md bi translated"><strong class="ih hj"> GroupByKey </strong></li></ul><h2 id="7521" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">示例:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="3ef0" class="lh kf hi mf b fi mj mk l ml mm">val rdd1 = sc.parallelize(Seq(5,10),(5,15),(4,8),(4,12),(5,20),(10,50)))<br/>val rdd2 = rdd1.groupByKey()<br/>rdd2.collect()</span></pre><h2 id="65c0" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">输出:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="d820" class="lh kf hi mf b fi mj mk l ml mm">Array[(Int, Iterable[Int])] = Array((4,CompactBuffer(8,12)), (10,CompactBuffer(50)), (5,CompactBuffer(10,15,20)))</span></pre><ul class=""><li id="f5df" class="lv lw hi ih b ii ij im in iq mo iu mp iy mq jc mr mb mc md bi translated"><strong class="ih hj">减速键</strong></li></ul><h2 id="caed" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">示例:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="a33a" class="lh kf hi mf b fi mj mk l ml mm">val rdd1 = sc.parallelize(Seq(5,10),(5,15),(4,8),(4,12),(5,20),(10,50)))<br/>val rdd2 = rdd1.reduceByKey((x,y)=&gt;x+y)</span></pre><h2 id="b5a7" class="lh kf hi bd kg li lj lk kk ll lm ln ko iq lo lp ks iu lq lr kw iy ls lt la lu bi translated">输出:</h2><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="33ce" class="lh kf hi mf b fi mj mk l ml mm">Array[(Int, Int)] = Array((4,20),(10,50),(5,45))</span></pre><ul class=""><li id="14dd" class="lv lw hi ih b ii ij im in iq mo iu mp iy mq jc mr mb mc md bi translated">分类</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ms"><img src="../Images/01a416cbb07c0c0379d260fbc67fab5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3zNCiw-H4zBWhHM_6_fFhg.png"/></div></div></figure><ul class=""><li id="8b30" class="lv lw hi ih b ii ij im in iq mo iu mp iy mq jc mr mb mc md bi translated">FoldByKey</li></ul><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="b3eb" class="lh kf hi mf b fi mj mk l ml mm"><strong class="mf hj">val</strong> <strong class="mf hj">pairRdd</strong> <strong class="mf hj">=</strong> rdd.map { x <strong class="mf hj">=&gt;<br/>var</strong> <strong class="mf hj">str</strong> <strong class="mf hj">=</strong> x.split(",")<br/><strong class="mf hj">new</strong> <strong class="mf hj">Tuple2</strong>(str(0), <strong class="mf hj">new</strong> <strong class="mf hj">Average</strong>(1, str(1).toDouble, str(1).toDouble))<br/>}<strong class="mf hj"><br/>val</strong> <strong class="mf hj">foldRdd=</strong>pairRdd.foldByKey(<strong class="mf hj">new</strong> <strong class="mf hj">Average</strong>(0, 0.0, 0.0))((x, y)</span></pre><p id="a6cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark分区在键/值对的所有rdd上都可用，并使系统根据每个键的功能对元素进行分组。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="fcf8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果基数很高并且分布均匀，那么最佳策略是-</p><h1 id="e59b" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">分桶:</h1><p id="4ef5" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">如果您有一个用例定期加入某些输入/输出<em class="jz"/>，那么使用bucketBy是一个好方法。在这里，我们强制将数据划分到所需数量的存储桶中。</p><p id="ecff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们选择这种策略是因为分布是统一的，基于一个或多个存储桶列的值，数据被分配到预定义数量的存储桶。属性的过滤将会更快，因为我们可以选择正确的存储桶(当正确应用时，存储桶可以通过避免洗牌而导致连接优化)。</p><h1 id="db81" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">按存储桶列出的转换列表:</h1><ul class=""><li id="b476" class="lv lw hi ih b ii lc im ld iq lx iu ly iy lz jc mr mb mc md bi translated">连接</li><li id="979d" class="lv lw hi ih b ii mt im mu iq mv iu mw iy mx jc mr mb mc md bi translated">明显的</li><li id="6c3e" class="lv lw hi ih b ii mt im mu iq mv iu mw iy mx jc mr mb mc md bi translated">分组依据</li><li id="ce4c" class="lv lw hi ih b ii mt im mu iq mv iu mw iy mx jc mr mb mc md bi translated">reduceBy</li></ul><pre class="je jf jg jh fd me mf mg mh aw mi bi"><span id="4186" class="lh kf hi mf b fi mj mk l ml mm">people.write<br/>  .bucketBy(42, "name")<br/>  .sortBy("age")<br/>  .saveAsTable("people_bucketed")</span></pre><p id="0261" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们需要执行多连接和/或涉及数据改组的转换，并且连接和/或转换中的列与桶中的列相同时，分桶会很有用。如果联接/转换中没有相同的列，则不需要分桶。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="2295" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jz">原载于2021年10月7日</em><a class="ae my" href="https://yolkorbit.com/apache-spark-bucketing-and-partitioning/" rel="noopener ugc nofollow" target="_blank"><em class="jz">【https://yolkorbit.com】</em></a><em class="jz">。</em></p></div></div>    
</body>
</html>