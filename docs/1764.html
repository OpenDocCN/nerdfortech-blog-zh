<html>
<head>
<title>Review — RefineDet: Single-Shot Refinement Neural Network for Object Detection (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Review — RefineDet:用于对象检测的单次细化神经网络(对象检测)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562?source=collection_archive---------7-----------------------#2021-04-05">https://medium.com/nerd-for-tech/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562?source=collection_archive---------7-----------------------#2021-04-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="83ce" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">胜过<a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener"> CoupleNet </a>、<a class="ae ix" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a>、<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank"> G-RMI </a>、<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>、<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------"> TDM </a>、<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快 R-CNN </a>、<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快 R-CNN </a>、<a class="ae ix" href="https://sh-tsang.medium.com/review-ohem-training-region-based-object-detectors-with-online-hard-example-mining-object-ad791ad87612" rel="noopener"> OHEM </a>、<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">约洛夫 2 </a>、<a class="ae ix" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">约洛夫 1 </a>、<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"/></h2></div><p id="687d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">在</span>这篇报道中，回顾了由中国科学院、中国科学院大学和 GE 全球研究院合作的(RefineDet)<strong class="ja hj">单次细化神经网络用于目标检测。在本文中:</strong></p><ul class=""><li id="bba3" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">提出了 RefineDet，它由两个相互连接的模块组成，即<strong class="ja hj">锚细化模块(ARM) </strong>和<strong class="ja hj">对象检测模块(ODM) </strong>。</li><li id="6d58" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj"> ARM 过滤掉负面锚</strong>以减少分类器的搜索空间，<strong class="ja hj">粗略调整锚的位置和大小。</strong></li><li id="90bd" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj"> ODM </strong>将提炼后的锚点作为 ARM 的输入，进一步改进回归和<strong class="ja hj">预测多类标签</strong>。</li></ul><p id="493f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这是一篇发表在<strong class="ja hj"> 2018 CVPR </strong>的论文，引用超过<strong class="ja hj"> 670 次</strong>。(<a class="kr ks ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----5fc483449562--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="a56f" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">概述</h1><ol class=""><li id="a6a2" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt lx kj kk kl bi translated">网络架构</li><li id="27e3" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">锚点细化模块(ARM) </strong></li><li id="3ad3" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">物体检测模块(ODM) </strong></li><li id="7ce1" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">传输连接块(TCB) </strong></li><li id="bd5e" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">损失函数&amp;推断</strong></li><li id="6ad1" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">实验结果</strong></li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="2844" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">1.RefineDet:网络架构</h1><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es ly"><img src="../Images/d6d835a07e286f67bfd45563f6845963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kg6SUQeb0jk86CPHZS4HPw.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc"> RefineDet:网络架构</strong></figcaption></figure><ul class=""><li id="a39c" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">与<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>类似，RefineDet <strong class="ja hj">产生固定数量的边界框</strong> <strong class="ja hj">和分数</strong>来指示这些框中不同类别对象的存在，<strong class="ja hj">之后是非最大抑制(NMS) </strong>来产生最终结果。</li><li id="584e" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">RefineDet 由两个相互连接的模块组成，即<strong class="ja hj">锚点细化模块(ARM) </strong>和<strong class="ja hj">对象检测模块(ODM)。</strong></li><li id="761d" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">ILSVRC CLS-LOC 预训练<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------"> VGG </a> -16 和<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> -101 作为主干。</li><li id="d77e" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">(主干部分末尾有一些小的修改/细化，请随意看论文。)</li></ul><h2 id="1286" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">1.1.两步级联回归</h2><ul class=""><li id="650b" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">如前所述，<strong class="ja hj"> ARM 过滤掉负面锚点</strong>以减少分类器的搜索空间，<strong class="ja hj">粗略调整锚点的位置和大小。</strong></li><li id="4b4b" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj"> ODM </strong>将提炼后的锚点作为 ARM 的输入，进一步<strong class="ja hj"> r 改进回归</strong>和<strong class="ja hj">预测多类标签</strong>。</li></ul><h2 id="8fd7" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">1.2.锚的设计和匹配</h2><ul class=""><li id="4b7b" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated"><strong class="ja hj">总步长为 8、16、32 和 64 像素的 4 个特征层</strong>用于处理不同比例的对象。</li><li id="d7a2" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">每个要素层都与一个特定的锚定比例和 3 个纵横比相关联。</li><li id="5f82" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">(本文与<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>高度相关，有兴趣请阅读<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>。)</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="5e2c" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">2.一个<strong class="ak"> nchor 细化模块(</strong> ARM)</h1><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nc"><img src="../Images/a6892a45baf96b01f11c670bc52c25b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*ssXftk4Q_xhKqRQRGmLhAA.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc"/><a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd lc">SSD</strong></a>中预定义的锚框</figcaption></figure><ul class=""><li id="85ce" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>中有预定义的锚盒，具有固定位置、比率和大小。(有兴趣请随意看<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>。)</li></ul><blockquote class="nd ne nf"><p id="6707" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">ARM 的目的是<strong class="ja hj">移除负锚点</strong>以便<strong class="ja hj">减少分类器的搜索空间</strong>并且<strong class="ja hj">粗略调整锚点</strong>的位置和大小以便为后续回归器提供更好的初始化。</p></blockquote><ul class=""><li id="01af" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">具体来说，<strong class="ja hj"> <em class="ng"> n 个</em>锚框</strong>与特征图上每个规则划分的单元相关联。</li><li id="ca1d" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">在每个特征映射单元，<strong class="ja hj">预测细化锚框</strong>的四个偏移。</li><li id="9c8d" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">两个置信度得分</strong>用于指示那些框中前景对象的存在。</li></ul><h2 id="c08d" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">2.1.负锚过滤</h2><ul class=""><li id="f49b" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated"><strong class="ja hj">如果其负置信度大于预设阈值(即，经验上为 0.99)，则在训练 ODM 时丢弃锚盒。</strong>很确定是后台。</li><li id="eab3" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">因此，只有细化的硬负锚定框和细化的正锚定框被传递来训练 ODM。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="8468" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated"><strong class="ak"> 3。对象检测模块(ODM) </strong></h1><ul class=""><li id="9ec8" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">在获得精化锚盒之后，精化锚盒被传递到 ODM 中的相应特征地图。</li></ul><blockquote class="nd ne nf"><p id="14d6" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">ODM 的目标是<strong class="ja hj">回归精确的对象位置</strong>和<strong class="ja hj">基于细化的锚点预测多类标签</strong>。</p></blockquote><ul class=""><li id="1dbb" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">具体来说，计算<strong class="ja hj"> <em class="ng"> c </em>类得分</strong>和<strong class="ja hj"> 4 个物体相对于精化锚盒的精确偏移量</strong>，为每个精化锚盒产生<strong class="ja hj"> <em class="ng"> c </em> + 4 个输出，完成检测任务。</strong></li></ul><h2 id="fadf" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">3.1.硬负开采</h2><ul class=""><li id="7d6a" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">硬负挖掘用于减轻极端的前景-背景类别不平衡。</li><li id="e836" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">选择一些损失值最高的负锚盒，使负锚和正锚的比例低于 3 : 1 </strong>，而不是使用所有的负锚或者在训练中随机选择负锚。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="9b66" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">4.<strong class="ak">传输连接块(TCB) </strong></h1><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nk"><img src="../Images/5f64f01fb2074c0441fba0da069cad62.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*iVl9SLOg6l4VEfvInX2SNw.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">传输连接块(TCB) </strong></figcaption></figure><ul class=""><li id="84a9" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">TCB 将特征从 ARM 转换到 ODM 进行检测。</li><li id="9e49" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">TCBs 是<strong class="ja hj">通过将高层特征</strong>添加到转移特征中来整合大规模上下文，以提高检测精度。</li><li id="d086" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">为了匹配它们之间的维度，<strong class="ja hj">反卷积操作用于放大高级特征图</strong>和<strong class="ja hj">以元素方式对它们求和。</strong></li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="8051" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">5.损失函数和推理</h1><h2 id="3011" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">5.1.损失函数</h2><ul class=""><li id="a691" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">因此，RefineDet 的损失函数由两部分组成，即<strong class="ja hj">臂的损失</strong>和<strong class="ja hj">ODM 的损失</strong>。</li><li id="72a2" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">对于手臂，给每个锚点分配一个二进制类别标签(是否为对象),并同时回归其位置和大小以获得细化的锚点。</li><li id="d127" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">此后，具有小于阈值的负置信度的精炼锚被传递到 ODM，以进一步预测对象类别和准确的对象位置和大小。</li><li id="f8ba" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">损失函数是:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nl"><img src="../Images/c24f37d303fde3e885edf067c2ffd9b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*-lmUgCdNxN0vsCx4mFmI6g.png"/></div></figure><ul class=""><li id="581a" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">其中<strong class="ja hj"> <em class="ng"> Narm </em> </strong>和<strong class="ja hj"> <em class="ng"> Nodm </em> </strong>分别是 arm 和 ODM 中正锚的<strong class="ja hj">号。</strong></li><li id="1fe7" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">二元分类损失<em class="ng"> Lb </em> </strong>是两个类(对象对非对象)的交叉熵/对数损失</li><li id="250e" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">多类分类损失<em class="ng">Lm</em>T47】是多类置信度上的软最大损失。</strong></li><li id="f6b1" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">类似于<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快速 R-CNN </a>，平滑 L1 损耗被用作<strong class="ja hj">回归损耗<em class="ng">Lr</em>T53】。</strong></li><li id="1ccd" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">[ <em class="ng">李</em> * ≥ 1]表示对负锚点忽略回归损失。</li></ul><h2 id="7d2a" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">5.2.推理</h2><ul class=""><li id="13f6" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">ARM 首先过滤掉负置信度得分大于阈值 0.99 的规则平铺锚。</li><li id="6e69" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">ODM 接管这些改进的锚点，并输出每幅图像的前 400 个高置信度检测。</li><li id="c080" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">应用具有每类 0.45 的 jaccard 重叠的 NMS。</li><li id="a33a" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">保留每幅图像的前 200 个高置信度检测，以产生最终的检测结果。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="2991" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">6.实验结果</h1><h2 id="169e" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">6.1.消融研究</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nm"><img src="../Images/9e801ecc5f409b6c19b9c02460745135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*QJDROIEYWsf_jLKFqqLKFg.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">VOC 2007 测试集上各种设计的有效性</strong></figcaption></figure><ul class=""><li id="0570" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">所有模型都在 VOC 2007 和 VOC 2012 trainval 集合上进行训练，并在 VOC 2007 测试集合上进行测试。</li></ul><blockquote class="nd ne nf"><p id="a871" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">在低维输入(即 320×320)的情况下，RefineDet 使用上述所有技术生成了<strong class="ja hj"> 80.0%的地图</strong>，这是<strong class="ja hj">第一个使用如此小的输入图像实现 80%以上地图的方法。</strong></p></blockquote><h2 id="8a15" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">6.2.PASCAL VOC 2007 和 2012</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nn"><img src="../Images/fa83b2a9a32da7861892729a85dbcbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WV2fYDhEeM4WFp0vH51keg.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">PASCAL VOC 2007 年&amp; 2012 年</strong>检测结果</figcaption></figure><h2 id="4057" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">6.2.1.VOC 2007</h2><ul class=""><li id="db4d" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">在 VOC 2007 上，通过使用更大的输入尺寸 512×512，<strong class="ja hj">refined 512</strong>实现了<strong class="ja hj"> 81.8% mAP </strong>，超过了<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>和<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a>。</li><li id="f08f" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">与两阶段方法相比，RefineDet512 的性能优于除<a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener"> CoupleNet </a>之外的大多数方法。</li><li id="9a8a" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">通过多尺度测试策略，RefineDet 实现了 83.1%(refined t320+)和 83.8%(refined t512+)的映射</strong>，比最先进的方法好得多。</li><li id="1612" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">RefineDet 在<strong class="ja hj"> 24.8ms (40.3 FPS) </strong>和<strong class="ja hj"> 41.5ms (24.1 FPS) </strong>中处理一幅图像，输入尺寸分别为<strong class="ja hj"> 320×320 </strong>和<strong class="ja hj"> 512×512 </strong>。</li></ul><blockquote class="nd ne nf"><p id="d655" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">RefineDet 是<strong class="ja hj">第一个在 PASCAL VOC 2007 上实现 80%以上 mAP </strong>检测精度的实时方法。</p></blockquote><ul class=""><li id="1393" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj"> RefineDet 在特征图上关联更少的锚盒</strong>(例如，<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a> 512 中的 24564 个锚盒与 RefineDet512 中的 16320 个锚盒相比)。</li><li id="417f" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">只有<a class="ae ix" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank"> YOLOv1 </a>和<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a> 300 比 RefineDet320 略快，但精度比 RefineDet 差 16.6%和 2.5%。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es no"><img src="../Images/0f397cd102e2732f9da7bbbee534259e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2jzEL0NeR6jTTTDF5H0hw.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">refined 512 在 PASCAL VOC 2007 测试集上的定性结果</strong></figcaption></figure><h2 id="3c6a" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">6.2.2.VOC 2012</h2><ul class=""><li id="0e49" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">对于 VOC 2012，所有方法都在 VOC 2007 和 VOC 2012 trainval 集合加上 VOC 2007 测试集合上训练，并在 VOC 2012 测试集合上测试。</li><li id="4cbc" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">refined t320</strong>获得最高<strong class="ja hj"> 78.1%的 mAP </strong>，这<strong class="ja hj">甚至优于大多数那些使用大约 1000×600 输入大小的两阶段方法</strong>。</li><li id="cd70" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">使用输入尺寸<strong class="ja hj"> 512×512 </strong>，RefineDet 将 mAP 提高到<strong class="ja hj"> 80.1% </strong>，超过所有一阶段方法的<strong class="ja hj">，仅略低于<a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener"> CoupleNet </a>。</strong></li><li id="de3c" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">通过多尺度测试，RefineDet 获得了 82.7%(refined t320+)和 83.5%(refined t512+)的最新地图。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es np"><img src="../Images/64e27a98ca554dcf8ce43fd981d54a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AcYfFQAZFjRxM37jf91kwQ.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">refined 512 在 PASCAL VOC 2012 测试集上的定性结果</strong></figcaption></figure><h2 id="2a18" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">6.3.可可女士</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nq"><img src="../Images/a2af03041ca426c20a0470761c004b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkAWm-Q7xE2YL_4piobTjg.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">在 MS COCO 测试开发装置上的检测结果</strong></figcaption></figure><ul class=""><li id="9809" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">这里也使用了基于 ResNet  -101 的 RefineDet。</li><li id="23a9" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">trainval35k 集用于训练，结果由测试开发评估服务器进行评估。</li><li id="75e1" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">用</strong> <a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------"> <strong class="ja hj"> VGG </strong> </a> <strong class="ja hj"> -16 提炼出的 t320 产生了 29.4%的 AP </strong>，优于所有其他基于<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------"> VGG </a> -16 的方法(例如<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a> 512 和<a class="ae ix" href="https://sh-tsang.medium.com/review-ohem-training-region-based-object-detectors-with-online-hard-example-mining-object-ad791ad87612" rel="noopener"> OHEM </a> ++)。</li><li id="9154" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">refined t320 与</strong><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="ja hj">ResNet</strong></a><strong class="ja hj">-101 达到 32.0% AP，RefineDet512 达到 36.4% AP，超过了除<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------"> TDM </a>、可变形 R-FCN ( <a class="ae ix" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN </a>)、<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a> 800、umd_det、<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank"> G-RMI </a>之外的大多数检测方法</strong>所有这些方法都使用大得多的输入图像进行训练和测试。</li></ul><blockquote class="nd ne nf"><p id="d246" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">通过多尺度测试，RefineDet 的最佳性能为 41.8%，这是最先进的，超过了所有已发表的两阶段和一阶段方法。</p></blockquote><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nr"><img src="../Images/210abed83da44886b0d326ebb147844b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xkq8_01lkcqnWab3FtpkVg.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">MS COCO 测试开发装置上 RefineDet512 的定性结果</strong></figcaption></figure><h2 id="e347" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">6.4.针对 PASCAL VOC 微调 MS COCO 模型</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ns"><img src="../Images/1ce131aed1bc2ccb6ff52ddedfc83822.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*aFSL6CAf9fFAlhzH_eDeQQ.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">PASCAL VOC 数据集上的检测结果</strong></figcaption></figure><ul class=""><li id="6f9e" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">通过微调在 MS COCO 上预训练的检测模型，RefineDet 在<strong class="ja hj"> VOC 2007 测试集</strong>上实现了 84.0% mAP(refined t320)和<strong class="ja hj">85.2% mAP</strong>(<strong class="ja hj">refined t512</strong>)，在<strong class="ja hj"> VOC 2012 测试上实现了 82.7% mAP(refined t320)和<strong class="ja hj">85.0% mAP</strong>(<strong class="ja hj">refined t512</strong>)</strong></li><li id="661e" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">使用<strong class="ja hj">多尺度测试</strong>后，检测准确率分别提升至 85.6%、85.8%、86.0%和<strong class="ja hj"> 86.8% </strong>。</li></ul><blockquote class="nd ne nf"><p id="9fb8" class="iy iz ng ja b jb jc ij jd je jf im jg nh ji jj jk ni jm jn jo nj jq jr js jt hb bi translated">基于<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------"> VGG </a> -16 的单一型号 RefineNet512+目前在 VOC 2012 排行榜上排名前五。</p></blockquote></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="f1a9" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">参考</h2><p id="7a65" class="pw-post-body-paragraph iy iz hi ja b jb ls ij jd je lt im jg jh nt jj jk jl nu jn jo jp nv jr js jt hb bi translated">【2018 CVPR】【RefineDet】<br/><a class="ae ix" href="https://arxiv.org/abs/1711.06897" rel="noopener ugc nofollow" target="_blank">单镜头细化神经网络用于物体检测</a></p><h2 id="03a7" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">目标检测</h2><p id="dec1" class="pw-post-body-paragraph iy iz hi ja b jb ls ij jd je lt im jg jh nt jj jk jl nu jn jo jp nv jr js jt hb bi translated"><strong class="ja hj"> 2014 </strong> : [ <a class="ae ix" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------">过食</a>][<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------">R-CNN</a>]<br/><strong class="ja hj">2015</strong>:[<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快 R-CNN </a> ] [ <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快 R-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a><br/><strong class="ja hj">2016 [<a class="ae ix" href="https://towardsdatascience.com/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac?source=post_page---------------------------" rel="noopener" target="_blank">GBD-网/GBD-v1&amp;GBD-v2</a>][<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae ix" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a><br/><strong class="ja hj">2017</strong>:[<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a?source=post_page---------------------------">NoC</a>][<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------">TDM</a>[<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>[<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 9000] [</a><a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener">couple net</a>]<br/><strong class="ja hj">2018</strong>:[<a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolov 3</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">Cascade R-CNN</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener">refined et</a><br/><strong class="ja hj">2019</strong></strong></p><h2 id="99cb" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>