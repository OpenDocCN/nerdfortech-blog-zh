<html>
<head>
<title>Review — Nasiri VCIP’20: Prediction-Aware Quality Enhancement of VVC Using CNN (VVC Filtering)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾—纳西里·VCIP 20:使用 CNN (VVC 滤波)提高 VVC 的预测感知质量</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-nasiri-vcip20-prediction-aware-quality-enhancement-of-vvc-using-cnn-vvc-filtering-88350fb01e2?source=collection_archive---------11-----------------------#2021-03-21">https://medium.com/nerd-for-tech/review-nasiri-vcip20-prediction-aware-quality-enhancement-of-vvc-using-cnn-vvc-filtering-88350fb01e2?source=collection_archive---------11-----------------------#2021-03-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b696" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">受<a class="ae ix" href="https://sh-tsang.medium.com/review-edsr-mdsr-enhanced-deep-residual-networks-for-single-image-super-resolution-super-4364f3b7f86f" rel="noopener"> EDSR </a>启发的网络，以帧内预测信号为附加输入，<strong class="ak">在 Y、U、V 分量上平均 BD-Rate 增益分别为 6.7%、12.6%、14.5%</strong>。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/20ee74e233aeb9635589d9d092a3d3c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J2xCF1a7LEL1x1-R1UIRmw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">VVC 质量提升(QE)框架</strong></figcaption></figure><p id="afb1" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，由 IRT b &lt; &gt; com、雷恩大学、INSA 雷恩大学和 AVIWEST 共同撰写的<strong class="jr hj">使用 CNN </strong>(纳西里·VCIP 20)对 VVC 的预测感知质量增强进行了回顾。在本文中:</p><ul class=""><li id="ead5" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">卷积神经网络(CNN)至<strong class="jr hj">在解码</strong>后增强 VVC 编码帧的质量，以减少低比特率伪像。</li><li id="e5a4" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">帧内预测信息也用于训练。</strong></li></ul><p id="584a" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这是 2020 年 VCIP 的一篇论文。(<a class="li lj ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----88350fb01e2--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="c5d0" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">概述</h1><ol class=""><li id="b7d1" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk mn la lb lc bi translated"><strong class="jr hj">帧内编码和压缩伪像之间的关系，以及动机</strong></li><li id="6a65" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">提议的网络架构</strong></li><li id="f3ce" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">实验结果</strong></li></ol></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="47e5" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">1.<strong class="ak">帧内编码和压缩伪像之间的关系，以及动机</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mo"><img src="../Images/0814156f7d698eb572596ea678590354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*HspZj2E8JJqxvQ2lAgLOXg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">16×16 块 k 及其两个最佳 IPM(I = 38，50)，具有相似的成本，但不同的率失真权衡导致不同的压缩损失模式</strong></figcaption></figure><p id="a5d4" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">在 VVC 有 67 种帧内预测模式(IPM ),代表 65 种角度 IPM，加上 DC 和平面。</p><blockquote class="mp mq mr"><p id="82c1" class="jp jq ms jr b js jt ij ju jv jw im jx mt jz ka kb mu kd ke kf mv kh ki kj kk hb bi translated">严格的比特率限制可能会导致这样的情况:使块的 R-D 成本最小化的最佳 IPM<strong class="jr hj">不一定是对块纹理建模最准确的 IPM。</strong></p></blockquote><ul class=""><li id="82e3" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">从上面的例子可以看出，尽管它们的研发成本相似，<strong class="jr hj">这两种 IPM 产生非常不同的重建信号，具有不同类型的压缩损失模式。</strong></li><li id="4e88" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">这种行为<strong class="jr hj">是由于所选模式的两种不同研发权衡</strong>。</li></ul><blockquote class="mp mq mr"><p id="f21c" class="jp jq ms jr b js jt ij ju jv jw im jx mt jz ka kb mu kd ke kf mv kh ki kj kk hb bi translated"><strong class="jr hj">针对块</strong>、帧或整个序列<strong class="jr hj">的质量增强(QE)任务可能会受到由编码器确定的编码模式(例如 IPM) </strong>的不同选择的显著影响。</p><p id="1db8" class="jp jq ms jr b js jt ij ju jv jw im jx mt jz ka kb mu kd ke kf mv kh ki kj kk hb bi translated">这种假设是本工作中使用帧内预测信息来训练质量增强网络的主要动机。</p></blockquote></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="5e8d" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated"><strong class="ak"> 2。提议的网络架构</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mw"><img src="../Images/2cbbfc522b7f9fd2d6934ccd3c70a684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7LkaI6WxKsjfphVTcGMAUA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">使用预测和重构信号作为输入的所提出方法的网络架构</strong></figcaption></figure><ul class=""><li id="b0a4" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">这个网络的灵感来自于<a class="ae ix" href="https://sh-tsang.medium.com/review-edsr-mdsr-enhanced-deep-residual-networks-for-single-image-super-resolution-super-4364f3b7f86f" rel="noopener"> EDSR </a>。</li><li id="5335" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">第一卷积层通过级联接收<strong class="jr hj">重构<em class="ms"> C </em>和预测帧<em class="ms"> P </em>作为输入。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mx"><img src="../Images/61c6bbc45de366476561d0f031e15e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*C9Gdml-7Kx9nDbqiAMFVgQ.png"/></div></figure><ul class=""><li id="7a4f" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">在下一步骤中，在一个卷积层之后，使用了<strong class="jr hj"> 32 个相同的残差块(</strong><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jr hj">ResNet</strong></a><strong class="jr hj">)</strong>，每个残差块由两个卷积层组成，并且在它们之间有一个<strong class="jr hj"> ReLU </strong>层。</li><li id="a29a" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> <strong class="jr hj">批量归一化</strong> </a>应用于剩余块之后。</li><li id="4f3c" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">在第一个和最后一个剩余块的输入之间使用一个长跳跃连接</strong>。</li><li id="7d12" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">使用剩余块后的两个以上卷积层</strong>。</li><li id="0ffd" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">最后，最后的卷积层具有一个特征图，该特征图<strong class="jr hj">构造输出帧^ <em class="ms"> O </em>。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es my"><img src="../Images/db4cf498d5dd8504a1e102db6e03e398.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*YrJejpSu4XNzUCO8IpN0KA.png"/></div></figure><ul class=""><li id="880e" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">其中<em class="ms"> F </em> 1()和<em class="ms"> F </em> 2()为 3×3×256 卷积层，分别有和没有 ReLU 激活层。</li><li id="1373" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><em class="ms"> F </em> 3()是一个 3×3×1 的卷积层，ReLU。</li><li id="d765" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">相对于原始帧 O 的 L2 范数被用作训练阶段的成本函数:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mz"><img src="../Images/16c44f54b9763d242ff40ebb8c251586.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*j8SVhMkT8zKM0FhhpAgv5g.png"/></div></figure><ul class=""><li id="dfd1" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">不同 QPs 中每个组件的一个网络用上述网络架构训练</strong>。</li><li id="2d4f" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">使用两个图像数据集<strong class="jr hj"> DIV2K </strong>和<strong class="jr hj"> Flickr2K </strong>进行训练。</li><li id="f602" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">VTM-5.0 与全帧内配置一起使用，使用 6 个 qp，在 22 和 47 之间。</li><li id="8588" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">使用 64×64 贴片，每批 32 片。</li><li id="9347" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">在训练结束时，<strong class="jr hj">针对 6 个 qp 中的<strong class="jr hj"> 3 个组件，共获得 36 个训练模型</strong>。</strong></li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="b004" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated"><strong class="ak"> 3。实验结果</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es na"><img src="../Images/5a432ec29fef4b3c7f31b1934ad3a618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6aHbdedSkkVjFSr7P1F4JA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">BD-率(%) </strong></figcaption></figure><ul class=""><li id="cc71" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">在 CTC QP 范围</strong>，所提出的方法可以在 Y、U 和 V 分量上分别实现 6.7%、12.6%和 14.5%的平均 BD-rate 增益<strong class="jr hj">。</strong></li><li id="fe1c" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">在 Y、U 和 V 分量上，具有预测信号<strong class="jr hj">的所提出的方法分别优于没有预测信号的所提出的方法 0.9%、8.1%和 4.8%。</strong></li><li id="83c6" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">与其他两个 JVET 解决方案相比，所提出的方法显示出显著的增益。</li><li id="7e61" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">在伪影明显更强的高 QP 范围</strong>，所提出的方法可以在 Y、U 和 V 分量上分别实现 8.3%、15.8%和 16.2%的平均 BD-rate 增益。</li><li id="2ba6" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于 U 和 v，使用预测信号实现的 BD-rate 增益相对较高。这是因为在 VVC，有先进的色度编码工具来利用冗余。此类工具的示例包括带色度缩放的亮度映射(LMCS)、联合 Cb-Cr 残差编码(JCCR)、交叉分量线性建模(CCLM)和称为亮度导出模式(DM)的特定色度 IPM。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h2 id="0e98" class="nb ls hi bd jo nc nd ne lw nf ng nh ma jy ni nj mc kc nk nl me kg nm nn mg no bi translated">参考</h2><p id="9bc0" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy np ka kb kc nq ke kf kg nr ki kj kk hb bi translated">【2020 VCIP】【纳西里·VCIP 20】<br/><a class="ae ix" href="https://ieeexplore.ieee.org/document/9301884" rel="noopener ugc nofollow" target="_blank">使用 CNN 的 VVC 预测感知质量增强</a></p><h2 id="8dc9" class="nb ls hi bd jo nc nd ne lw nf ng nh ma jy ni nj mc kc nk nl me kg nm nn mg no bi translated">编解码器过滤</h2><p id="c039" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy np ka kb kc nq ke kf kg nr ki kj kk hb bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(能)(说)(什)(么)(话)(,)(我)(们)(还)(不)(能)(说)(出)(什)(么)(话)(,)(我)(们)(还)(不)(能)(说)(出)(什)(么)(话)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(有)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(情)(况)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(情)(情)(况)(呢)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(好)(的)(,)(我)(们)(都)(是)(很)(好)(的)(。</p><h2 id="c628" class="nb ls hi bd jo nc nd ne lw nf ng nh ma jy ni nj mc kc nk nl me kg nm nn mg no bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>