<html>
<head>
<title>NLP Zero to One: Deep Learning Theory Basics (Part 3/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP零到一:深度学习理论基础(第3/30部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-deep-learning-theory-basics-part-3-30-baa8cbbe271d?source=collection_archive---------12-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-deep-learning-theory-basics-part-3-30-baa8cbbe271d?source=collection_archive---------12-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a486" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">感知器，神经网络，激活函数</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/21462ae06d3aa1813a1897e38d94b1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMfrWwBELdyzgXmS4J1EdA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="1b8f" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="cd6f" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">神经网络的想法从人脑的生物神经元中汲取灵感。神经网络是一个由小计算单元组成的网络，每个计算单元取一个输入值“<strong class="kh hj">X”</strong>的向量，输出单个值“<strong class="kh hj">y”</strong>。神经网络通常被称为深度学习，因为这些网络有许多层小型计算单元。在这个博客中，我们将介绍每个NLP从业者都应该知道的基本神经网络。</p><h1 id="d45e" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">基本计算单元:感知器算法..</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lb"><img src="../Images/d2cf2a430c4188244f6b2a2491ab1a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SHepfvO_F_2gf0Jdds6fZg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图1:单个神经元中的感知器算法(神经网络中的基本计算单元)</figcaption></figure><blockquote class="lc ld le"><p id="9a27" class="kf kg lf kh b ki lg ij kk kl lh im kn li lj kq kr lk ll ku kv lm ln ky kz la hb bi translated">深度学习是结合人工神经元网络，在神经元之间传递信息。这些神经元中的每一个都从其输入向量中学习不同的函数，并输出单个值。</p></blockquote><p id="73c9" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated">在我们讨论深度学习的概念之前，让我们试着了解一下发生在每个神经元(基本计算单元)内部的<strong class="kh hj">感知器算法</strong>。感知器算法的基本形式与逻辑回归的数学相同。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lo"><img src="../Images/7258be1590dd5977b36dc76b81dc7483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*aSPjQRZM2bcA2et9hW_3Bg.png"/></div></figure><p id="9a55" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated">“b”是可学习的偏差术语。我们可以把b看作是w₀.的一个附加重量<br/>单独学习的重量wᵢ乘以x的xᵢ，并传递给函数<strong class="kh hj"> f(。)</strong>得到输出<strong class="kh hj"> y. </strong> f(。)称为激活功能。值得注意的是，偏置项或<strong class="kh hj">w₀也是可学习的参数。该偏差项允许模型在感知器级别将<em class="lf">决策边界从原点</em>移开。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/9fd649316eff5c5a1e636f6ceab5b658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1FoElm6jR9wLOZ1SzF8CA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由谷歌剪贴板生成</figcaption></figure><p id="e455" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated">可以清楚地看到，感知器假设输入变量<strong class="kh hj"> X </strong>和输出变量<strong class="kh hj"> y </strong>之间存在线性关系。在现实世界的问题中，感知器的这些线性假设经常失效。</p><h1 id="3cff" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">前馈全连接神经网络..</h1><ol class=""><li id="1c7a" class="lq lr hi kh b ki kj kl km ko ls ks lt kw lu la lv lw lx ly bi translated">将这些神经元连接成不同层次的网络。</li><li id="eeed" class="lq lr hi kh b ki lz kl ma ko mb ks mc kw md la lv lw lx ly bi translated">在每个中子中使用可微分的非线性激活函数</li></ol><p id="93dc" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated">神经网络由相互连接的神经元组成，数据单向流动，因此被称为前馈神经网络。层被定义为一组神经元。这些层是“完全连接的”，这意味着每层中的每个神经元都将前一层中所有神经元的输出作为输入，并且在两个相邻层的每对神经元之间都有联系。NN必须包含一个输入和输出层以及至少一个隐藏层。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/a7b0559b9154a0fbd65c6f7841e6bacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KdtdpCO0mVC8WZKm4kvS4g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">单层神经网络的插图，由作者生成</figcaption></figure><p id="923a" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated">h1和h2是隐藏层中的节点。观察隐藏层中的每个节点都完全连接到输入x。<strong class="kh hj">非线性激活函数</strong>应用于每个神经元的末端，这允许输出值是其输入的<strong class="kh hj">非线性加权组合，从而创建下一层使用的非线性特征。1.</strong></p><h1 id="b4e1" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">非线性激活函数</h1><p id="2594" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">在节点中，使用神经网络激活函数中的基本计算块来确保输出值是其输入的非线性加权组合。非线性激活函数在提高神经网络的代表性方面起着非常重要的作用。我们将讨论三个流行的非线性函数f(。).</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/458717af133632ebfbb22b42f0ba78ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9vUqL7un_WVXLAkH673hQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源[1]</figcaption></figure><h2 id="79d4" class="mg jo hi bd jp mh mi mj jt mk ml mm jx ko mn mo jz ks mp mq kb kw mr ms kd mt bi translated">乙状结肠的</h2><p id="62a5" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">使用<strong class="kh hj">范围(0，1)，</strong>该功能作为连续挤压功能。它还具有连续导数，非常适合梯度下降法。<br/></p><h2 id="3d76" class="mg jo hi bd jp mh mi mj jt mk ml mm jx ko mn mo jz ks mp mq kb kw mr ms kd mt bi translated">双曲正切</h2><p id="bf4d" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">对于范围(1，1)，这是以零为中心的函数，它解决了sigmoid激活函数的一个问题。<br/> <strong class="kh hj">缺点</strong>:梯度饱和在函数的极端，这将导致梯度非常接近0。</p><h2 id="e585" class="mg jo hi bd jp mh mi mj jt mk ml mm jx ko mn mo jz ks mp mq kb kw mr ms kd mt bi translated">热卢</h2><p id="7df6" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">与range(0,♾)，简单，快速激活功能，通常发现在计算机视觉。如果输入大于0，函数是线性的。<br/> ReLU函数的计算速度非常快，因为sigmoid和tanh函数需要指数运算。由于在一个方向上的不饱和梯度，它保证了更好的收敛性。<br/> <strong class="kh hj">缺点</strong>:如果涉及大梯度，必须非常小心地使用Relu，大梯度更新会阻止神经元再次更新。因此，在处理Relu时，学习评级必须保持较低。</p><h1 id="2812" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">注意:</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/fb5c5a84c0f31b8a1f12d168df15e75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*Fbih2_ghkHKRK_zPN2J68A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">参考文献1中的泄漏Relu函数</figcaption></figure><p id="e0a7" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated"><strong class="kh hj"> Leaky ReLU </strong> : Leaky ReLU引入了一个α参数，允许小梯度反向传播。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mv"><img src="../Images/6aa126fb6de5094b01c5101cdeccf01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbcx-JhLnsHRnoFr_21Z0g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">参考文献1中的Softmax函数</figcaption></figure><p id="1f2c" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated"><strong class="kh hj">soft max:</strong>soft max函数允许我们输出K个类别的分类概率分布。我们可以使用<strong class="kh hj"> softmax根据神经元的输出产生一个概率向量</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="2fe3" class="pw-post-body-paragraph kf kg hi kh b ki lg ij kk kl lh im kn ko lj kq kr ks ll ku kv kw ln ky kz la hb bi translated">上一篇:<a class="ae mx" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-sparse-document-representations-part-2-30-d7ce30b96d63?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP零对一:稀疏文档表示(Part 2/30) </strong> </a> <br/>下一篇:<a class="ae mx" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-deep-learning-training-procedure-part-4-30-c8d1e3ba0db6?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP零对一:深度学习训练程序(Part 4/30) </strong> </a></p></div></div>    
</body>
</html>