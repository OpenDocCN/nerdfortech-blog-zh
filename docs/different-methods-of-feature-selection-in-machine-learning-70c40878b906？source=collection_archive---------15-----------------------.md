# 机器学习中不同的特征选择方法

> 原文：<https://medium.com/nerd-for-tech/different-methods-of-feature-selection-in-machine-learning-70c40878b906?source=collection_archive---------15----------------------->

如何自动化选择特征的过程

![](img/509dbd529113e5e9d08b6710275166a5.png)

[图像来源](https://images.unsplash.com/photo-1526379095098-d400fd0bf935?ixid=MXwxMjA3fDB8MHxzZWFyY2h8MTN8fG1hY2hpbmUlMjBsZWFybmluZ3xlbnwwfHwwfA%3D%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=800&q=60)

在数据科学中，有许多不同的方法来构建要素以模拟复杂的关系-尽管有时这可能会很麻烦。但是，在这篇博文中，你将了解到你可以使用的各种策略，只使用对你的模型最重要的特性！

## 定义特征选择

**特征选择**是一种方法，通过这种方法，您可以选择特定于模型设计的特征子集。这一过程有许多优势，最明显的是机器学习算法的性能增强。

第二个优点包括降低计算复杂度。如果模型中的特征数量减少，计算模型的参数就变得越简单，这也意味着保留模型特征所需的数据量也会减少。

第三，另一个优势包括更好地理解您的结果-您可以在特征选择过程中更多地了解特征如何相互影响。

不幸的是，没有简单明了的方法来选择在模型中包含哪些特性。但是，有许多技术可以用来有效地处理特征。有四种主要的特性选择方法，我将讨论它们各自的优缺点。

## 领域知识

评估重要要素时，了解与数据集相关的特定环境是最关键的因素之一。这叫做**领域知识**。领域知识的一些例子可能包括阅读以前讨论过相关主题的研究论文，或者要求关键利益相关者决定他们认为对目标变量预测最关键的变量是什么。

## 过滤方法

**过滤方法**是在作为预处理阶段运行模型之前进行的特征选择方法。它们通过分析变量之间的相互关系来运作。基于所使用的模型，使用各种指标来决定哪些元素将被省略，哪些元素将被保留。过滤方法通常会返回一个*特性排名*，它会告诉你特性是如何相互比较排列的，并且它们会删除被发现过时的变量。

线性回归中的典型过滤方法是移除彼此高度相关的要素。使用方差阈值是另一种过滤方法。这种方法为特征之间的必要差异设置了一定的阈值，以便将它们合并到模型中。这背后的逻辑是，如果变量没有很高的方差，它们就不会显著调整，因此对因变量没有太大影响。

数据科学家负责确定保留顶级功能的截止点，这通常通过交叉验证来确定。

## 包装方法

使用各种特征组合来训练模型，然后测量效率，**包装器方法**评估最优特征子集。任何子集都用于训练模型，然后在测试集上进行测试。由于包装器方法非常耗时，将它们用于大型特性集变得很困难。正如人们所预料的，包装器方法最终会在计算上非常昂贵，但是在决定最优子集时，它们是非常高效的。

*递归特征消除*，从模型中出现的所有特征开始，逐个消除，是线性回归中包装器方法的一个例子。由于某个特征已从模型中排除，导致模型拟合的统计相关性下降最少的特征子集将意味着哪个缺失特征对预测最没有用。*正向选择*是该过程的逆过程，经历相同的过程，但方向相反——它从单个特征开始，然后一次添加一个特征，以更好地提高模型的效率。

## 嵌入式方法

**嵌入式方法**是在机器学习算法的实际设计中使用的特征选择方法。*正则化*，尤其是*套索正则化*，是最流行的嵌入式方法形式，因为它有可能自动减少特征集合。拉索回归通常被暗指为 *L1 范数正则化*。

我希望这能帮助你更好地理解机器学习中更流行的特征选择方法的优点和缺点。

感谢您的阅读！

[领英](https://www.linkedin.com/in/acusio-bivona-7a315818b/)