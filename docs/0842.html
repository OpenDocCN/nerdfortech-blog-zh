<html>
<head>
<title>How to Make Your Chat Bot Stand Out</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何让你的聊天机器人脱颖而出</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/how-to-make-your-chat-bot-standout-3cf39bfee488?source=collection_archive---------3-----------------------#2021-02-18">https://medium.com/nerd-for-tech/how-to-make-your-chat-bot-standout-3cf39bfee488?source=collection_archive---------3-----------------------#2021-02-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="010e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><em class="ix"> NLP模型、情感分析、问题回答、摘要、知识图表、欺诈检测、个性化&amp;回退</em></h2></div><p id="26f6" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">交谈是人类与生俱来的，就像解渴或充饥一样。但是<em class="ju">即使对于强大的机器来说，同样的事情也是令人惊讶的复杂</em>，这导致了有趣的自然语言处理规则和具有挑战性的聊天机器人用例。无论聊天机器人有多复杂，它们对增强人类能力或产生商业线索都有极大的帮助。让我们深入了解各种NLP技术和有趣的功能，这些技术和功能可以让您的机器人出类拔萃！</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/6681b0d08a10a31e56ef054e0f01567d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMtqYGOR7gnmO0gSZCW2iQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated"><strong class="bd kl">聊天机器人用例概述(图片由作者提供)</strong></figcaption></figure><h1 id="6904" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated"><strong class="ak">为什么</strong>理解<strong class="ak">语言对计算机来说很难？</strong></h1><p id="df46" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated"><em class="ju">为了理解挑战的数量，考虑这些情况:</em></p><p id="46c6" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> A .字不一样，但意思是一样的！</strong></p><blockquote class="li lj lk"><p id="2f5c" class="iy iz ju ja b jb jc ij jd je jf im jg ll ji jj jk lm jm jn jo ln jq jr js jt hb bi translated">-你多大了？</p><p id="4c40" class="iy iz ju ja b jb jc ij jd je jf im jg ll ji jj jk lm jm jn jo ln jq jr js jt hb bi translated">-你多大了？</p></blockquote><p id="d3f7" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> B .词语相同，但意思不同！</strong></p><blockquote class="li lj lk"><p id="d4b3" class="iy iz ju ja b jb jc ij jd je jf im jg ll ji jj jk lm jm jn jo ln jq jr js jt hb bi translated">-你要去哪里？</p><p id="a390" class="iy iz ju ja b jb jc ij jd je jf im jg ll ji jj jk lm jm jn jo ln jq jr js jt hb bi translated">-你从哪里来？</p></blockquote><p id="ca3e" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> C .同一个词在不同的语境下可以有不同的意思！</strong></p><blockquote class="li lj lk"><p id="f4af" class="iy iz ju ja b jb jc ij jd je jf im jg ll ji jj jk lm jm jn jo ln jq jr js jt hb bi translated">-沙发装不进门，因为太窄了。</p><p id="5148" class="iy iz ju ja b jb jc ij jd je jf im jg ll ji jj jk lm jm jn jo ln jq jr js jt hb bi translated">-沙发装不进门，因为“<strong class="ja hj"> it </strong>太宽了。</p></blockquote><p id="8f6b" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">单词“it”在第一句中的意思是“door ”,而在第二句中是“sofa ”! <strong class="ja hj"> <em class="ju">那么，机器是如何理解或者编码一个单词的呢？</em> </strong></p><p id="85c7" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">答案在于NLP的基本原理，概括为:</p><blockquote class="lo"><p id="79b8" class="lp lq hi bd lr ls lt lu lv lw lx jt dx translated">从一个人所交往的公司你就可以知道这个词:弗斯(1957)</p></blockquote><p id="0914" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">即，可以通过分析大型语言语料库中词汇共现的<strong class="ja hj">模式来导出单词的语义表示。</strong></p><p id="eb09" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在NLP中，你将单词表示为向量。因此，嵌入层将是NLP模型中的初始层，用于捕获上下文。在高层次上，<strong class="ja hj"> NLP可以用，</strong>来完成</p><ol class=""><li id="dd3b" class="md me hi ja b jb jc je jf jh mf jl mg jp mh jt mi mj mk ml bi translated"><strong class="ja hj"> <em class="ju">向量空间模型</em> </strong></li><li id="c74d" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj"> <em class="ju">概率模型</em> </strong></li><li id="1d7f" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj"> <em class="ju">序列型号</em> </strong></li><li id="8b24" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj"> <em class="ju">关注车型</em> </strong></li><li id="66af" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj"> <em class="ju">图形型号</em> </strong></li></ol><p id="4066" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">让我们先睹为快，这样你就能从总体上理解NLP，并具体改进聊天机器人。</p><ol class=""><li id="ccb0" class="md me hi ja b jb jc je jf jh mf jl mg jp mh jt mi mj mk ml bi translated"><strong class="ja hj">向量空间模型</strong></li></ol><p id="6ac8" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">单词向量</strong>可以对句子中每个单词周围的上下文进行编码。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es mr"><img src="../Images/53683e3a8ac9d3013f3607fd89e3ed98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qmA2lfH0eQ1eV24A8hj9Q.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">按文档设计:“数据”在“娱乐”文档中出现500次，在“ML”文档中出现9320次</figcaption></figure><p id="9431" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">您可以根据用例使用不同的单词嵌入:</p><ul class=""><li id="ec87" class="md me hi ja b jb jc je jf jh mf jl mg jp mh jt mt mj mk ml bi translated"><strong class="ja hj">手套，Word2Vec </strong>:最流行嵌入<strong class="ja hj">字样</strong></li><li id="4ec4" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj"> PCA，t-SNE </strong>:在<strong class="ja hj">较低维度</strong>创建嵌入</li><li id="53e7" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj">通用语句编码器(使用)</strong>:用于<strong class="ja hj">语句</strong>或段落</li><li id="0e61" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj"> FastText: </strong>比W2V更适合<strong class="ja hj">生僻字</strong>；训练时间长</li><li id="5b3b" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj">庞加莱:</strong>对词语的<strong class="ja hj">层次化</strong>树信息进行编码</li><li id="c4c3" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj">节点2Vec: </strong>用于非线性数据，如<strong class="ja hj">图形</strong></li><li id="6f33" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj">深度学习嵌入</strong>使用BERT、Roberta等。</li></ul><p id="637b" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">嵌入向量</strong>对于做文字运算非常有用，可以找到同义词、聚类、类比、情感等。由于这些<strong class="ja hj"> 2属性:</strong></p><ol class=""><li id="5ac4" class="md me hi ja b jb jc je jf jh mf jl mg jp mh jt mi mj mk ml bi translated">每个单词的嵌入是一个固定大小的向量</li><li id="e152" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated">如果两个单词意思相似，它们的<strong class="ja hj">向量将接近</strong></li></ol><p id="131d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">要计算单词间的相似度</strong>，使用以下任一方法比较向量:</p><p id="b6d1" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju"> a) </em> <strong class="ja hj"> <em class="ju">角度</em> </strong> <em class="ju">:余弦相似度</em></p><p id="f7bb" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju"> b) </em> <strong class="ja hj"> <em class="ju">距离</em> </strong> <em class="ju">:欧氏距离</em></p><p id="2374" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">嵌入向量长度可以基于语料库中的样本数量而变化，但是向量之间的角度保持不变。因此，<strong class="ja hj">余弦相似性是比欧几里德距离更好的度量</strong>来寻找相似性。</p><p id="0a1e" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">其他比较指标包括<em class="ju">字移动器距离(WMD)、平滑逆频率、詹森-香农距离、K均值、变分自动编码器(VAE)、连体曼哈顿LSTM (MaLSTM)等。</em></p><p id="c170" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> 2。概率模型:</strong></p><p id="d176" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">概率NLP模型用于<strong class="ja hj">用例</strong>，如下所示。</p><p id="7427" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">i) <strong class="ja hj">拼写校正:</strong>使用<em class="ju">最小编辑距离</em>算法，通过动态编程找到插入、删除和替换操作的最小成本。如果成本低于阈值，则修改拼写。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es mu"><img src="../Images/a3209ee0acbef347126e24ed216e4f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*Al9fP3UU02PoET_A7Yy6kA.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图片<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">承蒙</a></figcaption></figure><p id="6ca6" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">ii) <strong class="ja hj">自动更正</strong>:即使拼写正确，单词也可能上下文不正确。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es mv"><img src="../Images/77d2e68b8799b7ead735da33206dc196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*DNPGqZXGsBdKDqWS_7XxxA.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">通过查看单词周围的单词，单词“deer”将被更正为“dear”</figcaption></figure><p id="729a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">单词概率</strong>通过语料库中的单词数÷总单词数来计算</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es mw"><img src="../Images/1a4941446d9d8e14195fb6cc6f0cca19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i6ePMM5uOz0VjkG6yRdK3g.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图片<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">礼貌</a></figcaption></figure><p id="2a8f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">要自动更正，找到最接近的单词并替换。</p><p id="56e8" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">iii) <strong class="ja hj">词性标注:马尔可夫链</strong>是一种描述可能事件序列的随机模型。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es mx"><img src="../Images/1c27b25839e3db9db9bdd0c7e5560faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*CX7k1uTUF4mjNWKyQB_WMQ.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">可视化表示在一个句子中从动词-动词和动词-名词转换的概率(<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">礼貌</a>)</figcaption></figure><p id="4f92" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">iv) <strong class="ja hj">自动完成:</strong> n-gram语言模型用于自动完成给定的句子。n元概率是二元和三元概率的扩展。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es my"><img src="../Images/f436277e6a5b2520aabae591de9c7306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*us-QPGF91SgJW3Z3R5xo2w.png"/></div></div></figure><p id="c17f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">为了计算序列概率，<strong class="ja hj">应用条件概率和链式法则。</strong>但是精确的序列可能不会出现在语料库中，因此使用‘朴素的’<strong class="ja hj">马尔可夫假设</strong>当前单词仅依赖于前一个单词。</p><p id="fb5c" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">为了处理语料库中没有出现的n元语法，我们可以使用诸如<em class="ju">拉普拉斯平滑、Katz-Backoff和插值之类的技术。</em></p><p id="c83a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> 3。基于序列的模型:</strong></p><p id="8e49" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">传统模型<strong class="ja hj">需要巨大的语料库和巨大的RAM </strong>来存储每个词组合{w1，w2}的条件概率P (w1，w2)。此外，更大的n元模型需要更多的内存来捕获更长的依赖关系。</p><p id="b11a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">由于这种方法很快变得不切实际，我们不得不使用rnn，因为它们从头到尾都在传播信息。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es my"><img src="../Images/bd7a8ce4f5a5e1ef513b696d00d07b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAs8Gx7xSne3oaSdqIVLbA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">最后一步进行的计算包含了句子中的所有单词(<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">礼貌</a>)</figcaption></figure><p id="b796" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">RNNs的主要优点是它们在序列内传播信息，并且<strong class="ja hj"/><strong class="ja hj">计算共享大部分参数。</strong></p><p id="a48f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">对于长单词序列，即使是普通的rnn也会丢失相关信息。<strong class="ja hj"> GRUs(门控循环单元)</strong>可以解决这个问题，因为它具有控制<strong class="ja hj">从过去忘记多少信息以及从当前输入提取多少信息的参数。</strong></p><p id="3afb" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">连体网络</strong>，由两个相同的神经网络最后合并而成的神经网络，在NLP中有很多应用。他们的目标是<strong class="ja hj">找出是什么让两种输入相似，又是什么让它们不同。</strong> <em class="ju">例如，它们</em> <strong class="ja hj"> <em class="ju"> </em> </strong> <em class="ju">可以被用于</em> <strong class="ja hj"> <em class="ju">来识别问题重复</em> </strong> <em class="ju">，即先前由聊天机器人回答的或者已经在库中的问题。</em></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es mz"><img src="../Images/10b87053dde60ac6c2b7aa866ef5377e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DCjbZyAKRq_T5Vs1"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图片<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">承蒙</a></figcaption></figure><p id="8e9e" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在测试上述模型时，您将执行<strong class="ja hj">一次性学习、</strong>，即找出两个输入问题之间的相似性得分。因此，问题从K-类分类变为<strong class="ja hj">测量两个类之间的相似性。</strong></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es na"><img src="../Images/2bf1274c935e2c0f7476e3e2b1d3a186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RG5zCCb_7EOyxJxf"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">如果要在K类中区分一个样本很少的新签名，就不需要重新训练了！(<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">礼遇</a>)</figcaption></figure><p id="0440" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">一次性学习是聊天机器人背后的<strong class="ja hj">基本原则，因为很难用几个例句对每个“意图”进行“分类”。</strong></p><p id="0e58" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> 4。注意型号:</strong></p><p id="7936" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">传统的Seq2Seq模型，如LSTMs和GRUs，通常用于避免消失梯度。但是，由于seq2seq使用固定长度的内存，较长的序列会出现问题。因此，该模型对于较短的序列表现得非常好，但是对于较长的序列则不然。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es nb"><img src="../Images/2a1960a6379ef0cf71a62bb3b5606a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WmthPN67Lfog9P8O"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">在编码器的最终隐藏状态中，单个输入开始堆积(<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">承蒙</a>)</figcaption></figure><p id="b054" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">一种解决方案是将输入中的每个单词向量给解码器，而不是将其粉碎在一个隐藏状态向量中。但是我们需要<strong class="ja hj">关注每一步最有可能出现的单词</strong>来节省内存。因此，您需要添加一个层来帮助解码器了解哪些输入对每个预测更重要。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es nc"><img src="../Images/afb57a3d51a540615635d7a2fdfa569a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eeISSLBhTaAIJVEP"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">注意力层赋予橙色块更多的权重(<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">礼貌</a>)</figcaption></figure><p id="ea4f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">机器翻译:</strong>注意机制使用输入(编码器隐藏状态)和输出(解码器隐藏状态)的编码表示。当单词组是翻译对时，输入向量和输出向量将是相似的，因此点积将是高的。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es nd"><img src="../Images/f7a2a818640becc22e5cf7bbdcdf1f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*1Q9nSTCqxkdDLAnhmBRbcQ.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated"><strong class="bd kl">注意矩阵:</strong>浅色方块表示路径(<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">礼貌</a>)</figcaption></figure><p id="1d79" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">变压器</strong>是rnn处理序列数据的绝佳替代品。由于他们使用<strong class="ja hj">多头注意力层</strong>而不是RNN的循环层，因此有了并行计算的空间。</p><p id="a7f2" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">最先进的变形金刚:</strong></p><p id="d45a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> a) CBOW </strong>:上下文窗口是固定的。如果你需要更多的上下文呢？</p><p id="de7f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> b) ELMo </strong>:使用左边的RNN&amp;右边(双向)预测中心字</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ne"><img src="../Images/bb70fcbd0065f14243bc9b86f6ba0213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-CEvPj1BqI5ULq7F"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">要预测“正确”这个词，你需要上下文</figcaption></figure><p id="9f3e" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> c) GPT-2: </strong>只有解码器堆栈，并且是单向的</p><p id="398e" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> d)伯特</strong>:只有编码器<strong class="ja hj">堆栈&amp;是一个经过训练的</strong>双向<strong class="ja hj">LSTM</strong></p><p id="7507" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> e) T5: </strong>有<strong class="ja hj">编解码</strong>栈加双向上下文。这款<strong class="ja hj"> </strong>多任务变压器可以做多项任务。例如:回答问题、总结等。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es nf"><img src="../Images/b3988576bcaec11aeb8aef74a8a9aa95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x93jygZrFXLz2yV2k6DpAw.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">模型演变的顺序(<a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">承蒙</a>)</figcaption></figure><p id="a47c" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> 5。基于图形的模型:</strong></p><p id="56b9" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在基于图的模型中，单词成为节点，单词(关系)的共现形成边。一个众所周知的图排名算法是<strong class="ja hj"> Google的PageRank </strong>。另一个<strong class="ja hj"> </strong>众所周知的，<strong class="ja hj"> TextRank </strong>算法可以用于关键词提取和文本摘要。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ng"><img src="../Images/4a5bdd9f3909a6e5baa966a3bcf73778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vF4V3c7TZRC2JLUrl15skg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">段落的文本等级图。<a class="ae ms" rel="noopener" href="/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5">礼遇</a></figcaption></figure><p id="d390" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">NLP库从句子中生成<strong class="ja hj">句法&amp;语义解析图</strong>。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es nh"><img src="../Images/37fd8cb8cbd038e26794160815a1be95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZtihkMUcc17dkjMa.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">由<strong class="bd kl"> Spacy </strong> tagger ( <a class="ae ms" href="https://spacy.io/usage/linguistic-features" rel="noopener ugc nofollow" target="_blank">礼貌</a>)输出的解析器</figcaption></figure><p id="483f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">从文本文档中提取<strong class="ja hj">潜在结构，并将其存储在一个图中，可以实现几种不同的用例。<strong class="ja hj"> <em class="ju"> Eg </em> </strong> <em class="ju">:自然语言搜索，文档相似度</em></strong></p><p id="7aac" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">知识图(KG) </strong>有助于通过相互关联的实体将数据置于上下文中——<strong class="ja hj"/>现实世界的对象、事件或抽象概念，如文档——使您能够搜索“事物”。<strong class="ja hj"> <em class="ju"> Eg </em> </strong> <em class="ju"> : DBPedia、Geonames、Wordnet、FactForge等。</em></p><p id="1c4d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju"> KGs结合了</em>的特性</p><ul class=""><li id="f031" class="md me hi ja b jb jc je jf jh mf jl mg jp mh jt mt mj mk ml bi translated"><strong class="ja hj">数据库</strong>:通过结构化查询浏览</li><li id="2afe" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj">图形</strong>:分析为网络数据结构</li><li id="4731" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated"><strong class="ja hj">知识库</strong>:可用于推断新的事实</li></ul><p id="2944" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">对于像问题回答和信息检索这样的任务，KGs正变得越来越受欢迎。最近，<strong class="ja hj">图形神经网络(GNNs) </strong>已经被用于捕获固有地存储在这些知识图中的结构信息，因为它们通常是不完整的。</p><p id="feb0" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">到目前为止，我们已经看到了可以用于任何NLP任务的各种模型，包括聊天机器人。在实践中，建议使用诸如<strong class="ja hj"> Spacy、ChatterBot、NLTK、TextBlob、PyNLPl等库。</strong>处理自然语言处理功能。现在让我们来看看一些可以使它们变得强大的特定于bot的特性。</p><h1 id="9d2f" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">类固醇上的聊天机器人:突出的功能</h1><h1 id="ed26" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated"><strong class="ak"> 1)感情分析</strong></h1><p id="ea2f" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">在任何业务中，最重要但无形的指标是客户的情绪。我们可以使用聊天机器人<strong class="ja hj">通过机器学习或深度学习技术</strong>捕捉用户情绪，后者更准确。</p><p id="0220" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">情感分析有两种类型:</p><p id="fb72" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">i) <strong class="ja hj">二元情感分类器:</strong>您可以<strong class="ja hj">使用ML技术，如逻辑回归、SVC、DT或深度学习，</strong>给定二元标记数据。另一种方式是使用带有情感管道的拥抱脸变形金刚。</p><p id="f674" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">ii) <strong class="ja hj">多类情感:</strong>理解不同程度的情感更具挑战性，但也更有用。明智的做法是建立一个<strong class="ja hj">定制的LSTM网络</strong>来完成这项任务。经过分析，<strong class="ja hj"> LSTM优于Logistic回归、SVC、DT、GBDT &amp;随机森林。</strong></p><figure class="jw jx jy jz fd ka"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="a8ff" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">定制LSTM建筑:</strong></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es nk"><img src="../Images/6e87a2459c76c37b46bc9fe063d0272b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ii5BniLquJShvzEdthsFFA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">用于情感分析的多LSTM层架构:200 (1) -150 (2) LSTM堆栈</figcaption></figure><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es nl"><img src="../Images/8b9338f123bdbbc42482d2a18c5adf39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*FzdvUzPOmanWT9jprdnB0g.png"/></div></figure><p id="9e55" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">情感机器人在行动:</strong></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es nm"><img src="../Images/55efa445b3eb20cf94b2b5dc0f0de0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/1*Gm24OVfIOisOc4JMd5jXcA.gif"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">5级情感识别</figcaption></figure><p id="b988" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju">你可以在这里看到源代码</em><a class="ae ms" href="https://github.com/AdroitAnandAI/Multi-Class-Sentimental-Analysis-Deep-Learning" rel="noopener ugc nofollow" target="_blank"><em class="ju"/></a></p><p id="2860" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">基于用户情绪，企业可以追求<strong class="ja hj">满意的客户(引导一代)</strong>或者给<strong class="ja hj">不满意的客户提供替代选择(后备)</strong></p><h1 id="b0bb" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">2)好友推荐</h1><p id="f71c" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">一般来说，一旦Bot开始推荐文章、电影、医生、投资或下一个最佳行动，它就会成为一个私人伙伴。为了实现这一点，主要有<strong class="ja hj"> 4种推荐系统算法:</strong></p><ol class=""><li id="8bc0" class="md me hi ja b jb jc je jf jh mf jl mg jp mh jt mi mj mk ml bi translated"><strong class="ja hj">协同过滤:最流行。</strong>它的基本原理是，过去同意的用户将来也会同意。</li><li id="cbac" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj">基于内容的过滤:没有冷启动问题。</strong>根据内容本身的属性查找相似的项目。</li><li id="ac0c" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated">混合系统:现代推荐器结合了协作过滤和基于内容的模型。</li><li id="345a" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj">深度学习方法:</strong>使用协作过滤&amp;基于内容的特征创建嵌入。由于DL系统<strong class="ja hj">是非线性的</strong>，它们会先发制人，避免过度简化，并支持<strong class="ja hj">跨域</strong>数据集。</li></ol><p id="b407" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">为了增加实现的味道，让我们在Matlab中做<strong class="ja hj">协同过滤。</strong></p><figure class="jw jx jy jz fd ka"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="3641" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju">你可以在这里</em> 查看完整源代码 <a class="ae ms" href="https://github.com/AdroitAnandAI/ML-Algorithms-in-MATLAB/tree/master/8.%20Anomaly%20detection%20and%20Recommender%20systems" rel="noopener ugc nofollow" target="_blank"> <em class="ju"/></a></p><h1 id="56d0" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">3)问题回答(CDQA和ODQA)</h1><p id="95e8" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">对人类来说，从文章中理解并找到答案是一项艰巨的任务。如果一个机器人能够做到这一点，那么没有什么比它更好的了！我们可以使用<strong class="ja hj"> MRC(机器阅读理解)模型</strong>来完成同样的任务，并使用<strong class="ja hj"> LDA-NMF </strong>主题建模和语义搜索来扩大规模。</p><p id="e34a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">要回答给定上下文的问题，需要对上下文和查询之间的复杂交互进行建模。典型地，这些机制使用<strong class="ja hj">注意力</strong>来将这样的交互转换成向量。有许多MRC模型，如<em class="ju"> BiDAF、DAANet、RNet、QANet、CogQA、XLNet、ReasoNet等。</em></p><p id="a692" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">双向注意力流(BiDAF)模型</strong>是一个<strong class="ja hj">闭域问答</strong>模型<strong class="ja hj"> </strong> ( <strong class="ja hj"> CDQA </strong>)可以回答仿真问题。BiDAF使用多级分层上下文表示来实现查询感知的上下文表示，而无需早期摘要。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es nn"><img src="../Images/5ef73a024b2b78dfe41686b48d70c8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8JOl0fTo55q1IyOemZGAMA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">BiDAF架构(<a class="ae ms" href="https://github.com/ElizaLo/Question-Answering-based-on-SQuAD" rel="noopener ugc nofollow" target="_blank">礼貌</a>)</figcaption></figure><p id="95e6" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> MRC机器人在行动:</strong></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es no"><img src="../Images/e281597eba63f12c86942ebb324f7348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/1*8nRkyEtSYYsbwEK5LfmIvQ.gif"/></div></figure><p id="b0c2" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju">你可以在这里看到源代码</em><a class="ae ms" href="https://github.com/AdroitAnandAI/Question-Answering-MRC-with-AI" rel="noopener ugc nofollow" target="_blank"><em class="ju"/></a></p><p id="4ef1" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">另一方面，开放领域问题回答(ODQA) </strong>涉及从覆盖广泛主题的广泛文档集合中寻找自然语言查询的答案。虽然性能落后于CDQA，<strong class="ja hj">这个模块增强了机器人的对话体验，当定位在<em class="ju">级联后退机器人架构中的CDQA之后和人类移交</em></strong>之前。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es np"><img src="../Images/34ac610c6655e173e0a44cc4f647ab0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DtYbQ6Bryz2reHosuwwzbw.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">来自庞大知识库的开放领域问题回答(<a class="ae ms" href="http://ai.stanford.edu/blog/answering-complex-questions/" rel="noopener ugc nofollow" target="_blank">礼貌</a>)</figcaption></figure><p id="bfaa" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju">热门车型包括，</em></p><p id="520d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">I)<strong class="ja hj">DeepPavlov的ODQA:</strong>接受一批查询作为输入，并从维基百科文章中返回最佳答案</p><p id="603e" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">ii) <strong class="ja hj">脸书研究公司的DrQA:</strong>针对潜在的非常大的非结构化文档语料库中的“大规模机器阅读”(MRS)任务</p><p id="f6d7" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">iii) <strong class="ja hj">强化的阅读器分级器(R3): </strong> ODQA，具有基础事实分级器组件以及基于强化学习的训练</p><h1 id="8224" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">4)总结</h1><p id="da9e" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated"><strong class="ja hj">想象你给一个机器人发了一篇很长的文章或者一个PDF的书。如果机器人回复它的摘要，会节省多少时间？这正是总结模型所做的。</strong></p><p id="2e3a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这在各种bot用例中都很有用。例如，生成<strong class="ja hj">机器人对话的要点</strong>以供进一步参考，或者<strong class="ja hj">机器人可以用其领域的精选文章</strong>的摘要来问候。</p><p id="335d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">概括地说，自动文本摘要有两种方法:</p><ul class=""><li id="98f4" class="md me hi ja b jb jc je jf jh mf jl mg jp mh jt mt mj mk ml bi translated"><strong class="ja hj">摘录:</strong>从源文本中选择子序列，然后排列它们以形成摘要(类似于荧光笔)</li><li id="a35e" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mt mj mk ml bi translated">抽象:理解意图，用自己的话写总结(类似于钢笔)</li></ul><p id="1c6a" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">传统上，使用<strong class="ja hj"> seq2seq模型进行总结时要注意。然而，在这个模型中，从原文中复制一个单词太难了，导致事实上不准确或重复的摘要。</strong></p><p id="9c42" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">指针生成器网络</strong>通过<strong class="ja hj">允许从源代码复制单词</strong>，同时<strong class="ja hj">保留从固定词汇表生成单词</strong>的能力，从而实现了两全其美。</p><p id="977f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">各种变压器型号，如<em class="ju"> T5、BART、GPT2、Longformer、Pegasus等。</em>用于根据手头的用例生成概要。</p><h1 id="06e9" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">5)欺诈检测</h1><p id="cdbb" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">在金融等敏感领域，异常检测占据了中心位置。部署在此类领域的机器人应该有能力检测它们，无论是假冒用户还是不太可能的交易。</p><p id="719d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">一般来说，<strong class="ja hj">你可以使用高斯模型</strong>通过寻找概率非常低的值来检测异常值。让我们考虑一个2D样本数据集。</p><p id="a57d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">a) <strong class="ja hj">估计均值和方差</strong>以拟合高斯分布。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es nq"><img src="../Images/d997e526392f4670130a0476959ec063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/0*Q1V9e0Bn5346O_8e"/></div></figure><p id="279f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">b)逐步增加ε<strong class="ja hj">阈值</strong>的值。</p><p id="1c20" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">c)对于每个ε，根据精确度和召回率计算F1分数。</p><p id="f210" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">d)找出F1分数最高的那个<strong class="ja hj">最佳阈值。</strong>外界值为分类异常。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es nr"><img src="../Images/57d17faef22fd3e5c8362f2ce6474b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/1*vgPkLABrcIB4Te4To8Ua2w.gif"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated"><a class="ae ms" href="https://www.edx.org/new/course/the-analytics-edge" rel="noopener ugc nofollow" target="_blank">图片由</a>提供</figcaption></figure><figure class="jw jx jy jz fd ka"><div class="bz dy l di"><div class="ni nj l"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">使用高斯消去法查找数据异常的Matlab代码</figcaption></figure><p id="6126" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><em class="ju">你可以在这里</em> 查看源代码 <a class="ae ms" href="https://github.com/AdroitAnandAI/ML-Algorithms-in-MATLAB/tree/master/8.%20Anomaly%20detection%20and%20Recommender%20systems" rel="noopener ugc nofollow" target="_blank"> <em class="ju"/></a></p><p id="d42d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">为了检测欺诈，<strong class="ja hj">朴素贝叶斯分类器</strong>被广泛使用。无监督方法的标准方法包括<strong class="ja hj">localooutlierfactor、一类SVM或IsolationForest。</strong>相反，你可以<strong class="ja hj">使用PCA或t-SNE </strong>降低维度，并尝试重建数据，类似于<strong class="ja hj">自动编码器</strong>。如果重建不完美，那么就存在潜在的欺诈。</p><h1 id="d7db" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">6)知识图表</h1><p id="e42e" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">驱动聊天机器人的知识库可以存储在知识图中，基于这个知识图，机器人可以回答人类语言的询问。各种知识图解决方案包括<em class="ju"> Grakn、TigerGraph、Neo4j、GraphDB、JanusGraph等。</em></p><p id="6315" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">基于用户意图和实体，<strong class="ja hj"> bot后端可以生成一个Grakn查询，并通过图形遍历检索查询到的仿真陈述。通过这种方式，机器人交互变得非常自然和强大。</strong></p><h1 id="1803" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">7)基于图像的交互</h1><p id="d2aa" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">能够使用图像与机器人交互，使数字体验更加丰富。这可以通过两种方式在<strong class="ja hj">中完成:</strong></p><p id="c01e" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">a) <strong class="ja hj">用户向机器人:</strong>用户向机器人发送图像。机器人分析图像，并回复图像中的细节。<strong class="ja hj"> <em class="ju">例如</em> </strong> <em class="ju">:机器人后端使用YOLO来检测图像中的对象，或者使用定制的人工智能模型来诊断X射线图像中的疾病。</em></p><p id="20f6" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">b) <strong class="ja hj"> Bot to User: </strong>基于上下文，Bot可以给出图像的智能选择，让用户选择。<strong class="ja hj"> <em class="ju">例如</em> </strong> <em class="ju">:显示礼物选项的图片，让用户选择图片。</em></p><h1 id="c1e7" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">8)个性化</h1><p id="7194" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">个性化有多个层次。仅仅给机器人取名为“T1”，说“史蒂夫”，或者“T2”称呼用户的名字“T3”就是一种简单的个性化。还可以在机器人对话中注入一些错别字，使聊天机器人不那么像机器人。</p><p id="b458" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">此外，机器人可以存储用户的一些关键事实，并在以后继续对话。<strong class="ja hj">历史记忆</strong>可以给机器人使用者带来神奇的体验。想想Fin-bot建议你通过记录日常开销来减少酒店食物？</p><h1 id="273a" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">9)预测分析</h1><p id="ac62" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">这是人工智能的经典用例，也是个性化的自然延伸。例如，<em class="ju">基于LSTM时间序列分析，Fin-bot可以预测低账户余额，或者eCom-bot可以预见农产品价格的下降。</em></p><h1 id="cd4f" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated"><strong class="ak"> 10)位置特征</strong></h1><p id="92bb" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">能够获取地区新闻或使用地理位置API根据用户的位置建议附近的餐馆或医院增加了机器人的体验。</p><h1 id="fcbc" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated"><strong class="ak"> 11)语音识别</strong></h1><p id="ee16" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">除了图像，支持语音的交互也增加了用户的便利性。更好的是，如果机器人能够使用定制训练的人工智能模型识别地方口音，正如我的博客中所解释的</p><h1 id="23a4" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">12)回退机制</h1><p id="0b15" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">为了最大化查询覆盖率，建议采用<em class="ju">多级级联回退</em>架构，如<em class="ju">常见问题解答-CDQA-ODQA-人工切换</em>。最后但同样重要的是，机器人可以根据用户的选择或情感切换到真人。</p><h1 id="63f9" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">结论</h1><p id="1c24" class="pw-post-body-paragraph iy iz hi ja b jb ld ij jd je le im jg jh lf jj jk jl lg jn jo jp lh jr js jt hb bi translated">各种NLP技术和库可以为机器人添加有用的功能，如自动拼写纠正或使用n-gram模型完成单词。<em class="ju">我们已经建立了一个端到端的渠道，为上面讨论的每个突出特征整合了更好的模型。想象一个机器人，它可以从来自多个领域的文章、研究论文或书籍中总结或回答查询。<strong class="ja hj">如果上面的阅读激起了你的好奇心，或者你可能想要一个机器人来改善你的业务，那么你可以联系我</strong> <a class="ae ms" href="https://www.linkedin.com/in/ananduthaman/" rel="noopener ugc nofollow" target="_blank"> <strong class="ja hj">这里</strong> </a></em></p><h1 id="3266" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">源代码仓库:</h1><ol class=""><li id="a5fb" class="md me hi ja b jb ld je le jh ns jl nt jp nu jt mi mj mk ml bi translated"><strong class="ja hj">感伤分析:</strong> IPython代码<a class="ae ms" href="https://github.com/AdroitAnandAI/Multi-Class-Sentimental-Analysis-Deep-Learning" rel="noopener ugc nofollow" target="_blank"> <em class="ju">此处</em> </a></li><li id="56e1" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj">问答:</strong> Python代码<a class="ae ms" href="https://github.com/AdroitAnandAI/Question-Answering-MRC-with-AI" rel="noopener ugc nofollow" target="_blank"> <em class="ju">此处</em> </a></li><li id="7bb8" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj">欺诈检测:</strong> MATLAB代码<a class="ae ms" href="https://github.com/AdroitAnandAI/ML-Algorithms-in-MATLAB/tree/master/8.%20Anomaly%20detection%20and%20Recommender%20systems" rel="noopener ugc nofollow" target="_blank">T3】此处T5】</a></li><li id="7caa" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj">推荐系统:</strong> MATLAB代码<a class="ae ms" href="https://github.com/AdroitAnandAI/ML-Algorithms-in-MATLAB/tree/master/8.%20Anomaly%20detection%20and%20Recommender%20systems" rel="noopener ugc nofollow" target="_blank"> <em class="ju">此处</em> </a></li><li id="d4ae" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><strong class="ja hj">语音识别:</strong>自定义模式<a class="ae ms" href="https://github.com/AdroitAnandAI/Indian-Accent-Speech-Recognition" rel="noopener ugc nofollow" target="_blank"> <em class="ju">此处</em> </a></li></ol><h1 id="c595" class="km kn hi bd kl ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">参考</h1><ol class=""><li id="7ded" class="md me hi ja b jb ld je le jh ns jl nt jp nu jt mi mj mk ml bi translated"><a class="ae ms" href="https://www.deeplearning.ai/program/natural-language-processing-specialization/" rel="noopener ugc nofollow" target="_blank">deep learning . ai NLP Specialization</a>作者<em class="ju"> Younes Bensouda Mourri、ukasz Kaiser、Eddy Shyu </em></li><li id="e8c5" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><a class="ae ms" href="https://web.eecs.umich.edu/~mihalcea/papers/nastase.jnle15.pdf" rel="noopener ugc nofollow" target="_blank">自然语言处理中的图形综述</a>V . nasta se，R . Mihalcea，Radev博士，自然语言工程，2015年</li><li id="39cb" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><a class="ae ms" href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" rel="noopener ugc nofollow" target="_blank">由Udacity公司生产的NLP纳米学位(nd 892)</a></li><li id="8d85" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><a class="ae ms" href="https://www.edx.org/new/course/the-analytics-edge" rel="noopener ugc nofollow" target="_blank">Edx(麻省理工学院)的分析边缘</a></li><li id="564b" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><a class="ae ms" rel="noopener" href="/neuralspace/graphs-neural-networks-in-nlp-dc475eb089de">https://medium . com/neural space/graphs-neural-networks-in-NLP-DC 475 EB 089 de</a></li><li id="1ac3" class="md me hi ja b jb mm je mn jh mo jl mp jp mq jt mi mj mk ml bi translated"><a class="ae ms" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" rel="noopener ugc nofollow" target="_blank">http://www . abigailsee . com/2017/04/16/taming-rnns-for-better-summarying . html</a></li></ol></div></div>    
</body>
</html>