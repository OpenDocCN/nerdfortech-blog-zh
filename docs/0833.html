<html>
<head>
<title>A Beginner’s Guide to Your First Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的第一次分类的初学者指南</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/a-beginners-guide-to-your-first-classification-f8547d679411?source=collection_archive---------8-----------------------#2021-02-17">https://medium.com/nerd-for-tech/a-beginners-guide-to-your-first-classification-f8547d679411?source=collection_archive---------8-----------------------#2021-02-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9a25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我说<em class="jd">初学者</em>时，我不仅仅是指<em class="jd">你</em>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/6a53a514ce3f6cd6f87a6e06f4468cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DWEsEsG5OQOcDWJxiaGmsQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">所有照片由Pixabay.com提供</figcaption></figure><p id="991f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上次，以防你错过，我们一头扎进了一个简单的线性回归<a class="ae ju" href="https://sinnott-cj.medium.com/a-beginners-guide-to-your-first-linear-regression-bdc4c79bce24" rel="noopener"/>。在我们重新分析之前，让我们先熟悉一下二进制分类。</p><p id="4303" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是二元分类？我们将训练我们的机器来判断一个东西是1还是0。很简单，对吧？准备好在你的笔记本上写代码(或者复制粘贴——我不做评价)。</p><p id="41a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从导入我们将需要的库开始:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="cd5f" class="ka kb hi jw b fi kc kd l ke kf">import numpy as np<br/>import pandas as pd<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.linear_model import LogisticRegression</span></pre><p id="2364" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，让我们载入一个你以前可能见过的数据集——心脏病UCI，可以在<a class="ae ju" href="https://www.kaggle.com/ronitf/heart-disease-uci?select=heart.csv" rel="noopener ugc nofollow" target="_blank">这里</a>找到。快速提示:如果您正在与“文件不存在”错误作斗争，请使用“pwd”在新的单元格中打印您的工作目录，并使用“cd”命令移动到正确的文件目录，这也可以在笔记本的任何单元格中运行。</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="bec9" class="ka kb hi jw b fi kc kd l ke kf">df = pd.read_csv('./data/heart.csv')</span></pre><p id="9059" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经读入了数据集，让我们看看我们正在处理什么。</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="ecc8" class="ka kb hi jw b fi kc kd l ke kf">df.head(5)</span><span id="8c6b" class="ka kb hi jw b fi kg kd l ke kf">df.info() #in a new cell</span><span id="623c" class="ka kb hi jw b fi kg kd l ke kf">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 303 entries, 0 to 302<br/>Data columns (total 14 columns):<br/> #   Column    Non-Null Count  Dtype  <br/>---  ------    --------------  -----  <br/> 0   age       303 non-null    int64  <br/> 1   sex       303 non-null    int64  <br/> 2   cp        303 non-null    int64  <br/> 3   trestbps  303 non-null    int64  <br/> 4   chol      303 non-null    int64  <br/> 5   fbs       303 non-null    int64  <br/> 6   restecg   303 non-null    int64  <br/> 7   thalach   303 non-null    int64  <br/> 8   exang     303 non-null    int64  <br/> 9   oldpeak   303 non-null    float64<br/> 10  slope     303 non-null    int64  <br/> 11  ca        303 non-null    int64  <br/> 12  thal      303 non-null    int64  <br/> 13  target    303 non-null    int64  <br/>dtypes: float64(1), int64(13)<br/>memory usage: 33.3 KB</span></pre><p id="998a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你会注意到的第一件事是，东西看起来很干净。没有空值，没有伪装成对象的整数，也没有要重命名的列——除了一些探索之外，我们没有什么可做的。</p><p id="7bb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们看看我们的目标，方便地命名为“目标”</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="6119" class="ka kb hi jw b fi kc kd l ke kf">df.target.value_counts()</span><span id="15b3" class="ka kb hi jw b fi kg kd l ke kf">1    165<br/>0    138<br/>Name: target, dtype: int64</span></pre><p id="d6f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果这真的是你的第一次分类，那么你可能会对这个结果有点困惑。我们不是给目标标上“有心脏病”或“没有心脏病”，而是简单地标上1或0。事情并不总是这么简单，但是如果你的目标是二进制分类，最终你需要你的目标是二进制的。出于我们的目的，1将是阳性结果，0将是阴性结果，因为患者没有心脏病。</p><p id="f43b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一件需要注意的事情是我们职业的平衡。</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="ed0b" class="ka kb hi jw b fi kc kd l ke kf">df.target.value_counts(normalize = True)</span><span id="4d59" class="ka kb hi jw b fi kg kd l ke kf">1    0.544554<br/>0    0.455446<br/>Name: target, dtype: float64</span></pre><p id="979f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有相当平衡的类，这是理想的，尤其是数据行如此之少。我推荐用熊猫的。describe()函数来自己探索这些数据。</p><p id="f895" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，让我们生成几个简单的图来更好地理解我们的分析。</p><p id="bd62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，相关矩阵:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="178d" class="ka kb hi jw b fi kc kd l ke kf">import seaborn as sns <br/>import matplotlib.pyplot as plt</span><span id="f0b1" class="ka kb hi jw b fi kg kd l ke kf">sns.heatmap(df.corr()[['target']], <br/>            annot = True,<br/>            cmap='coolwarm',<br/>            vmin=-1.,<br/>            vmax=1.,<br/>            linewidths=.01<br/>).set_title('Correlation matrix');</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kh"><img src="../Images/8b8c0751a7eae0e8137a050a86ff3da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*m9UZdBF4UhIphy1uJxHgCw.png"/></div></figure><p id="f1a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">没什么太意外的！许多强相关和负相关的实验结果可能看起来有点奇怪(我们是数据科学家，不是医学博士科学家)，但对成功的分类是有用的。</p><p id="8f96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我建议制作一个pairplot，我会将代码粘贴到下面，您可以自己探索:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="d9e0" class="ka kb hi jw b fi kc kd l ke kf">sns.pairplot(df)</span></pre><p id="389c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们需要将数据分成变量，然后分成训练集和测试集。这可能是你第一次将变量分成训练集和测试集，但是不要被吓倒。这使得跟踪我们正在做的事情有点棘手，但是当我们最后评估我们的工作时，这一切都是有意义的。</p><p id="833d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">定义我们的X和y变量如下所示:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="966c" class="ka kb hi jw b fi kc kd l ke kf">X = df.drop(columns = 'target')<br/>y = df['target']</span></pre><p id="8859" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单，和上周的线性回归一样。接下来，一点新的东西:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="0b64" class="ka kb hi jw b fi kc kd l ke kf">from sklearn.model_selection import train_test_split</span><span id="a860" class="ka kb hi jw b fi kg kd l ke kf">X_train, X_test, y_train, y_test = train_test_split(X, y)</span></pre><p id="9cd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">默认情况下，这将我们的数据分成80/20。我建议在括号内使用shift + tab来探索文档字符串和可控参数，我建议我们做任何事情都要这样做。</p><p id="7bfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们需要使用另一个Sklearn工具来标准化我们的数据。这是一个必要的步骤，因为我们所有的数据都在不同的尺度上。如果我们无法衡量(新注册的韵文)，我们就会过分重视特定的变量。</p><p id="0615" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这也可能是你第一次试衣改造；请记住，我们的“训练”变量是我们用来教导我们的机器的，我们的测试集是我们如何评估我们所做的，因此我们经常会“适应和转换”我们的训练变量，但只是“转换”我们的测试。</p><p id="e09e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有东西看起来都是这样的:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="d11a" class="ka kb hi jw b fi kc kd l ke kf">from sklearn.preprocessing import StandardScaler</span><span id="e46d" class="ka kb hi jw b fi kg kd l ke kf">ss = StandardScaler() <br/># instantiating<br/> <br/>X_train_scaled = ss.fit_transform(X_train) <br/># fitting and transforming in one step</span><span id="aece" class="ka kb hi jw b fi kg kd l ke kf">X_test_scaled  = ss.transform(X_test) <br/># only transforming our test set</span></pre><p id="d144" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们通过查看每列的第一个值来看看它是如何工作的:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="9311" class="ka kb hi jw b fi kc kd l ke kf">X_train_scaled[0]</span><span id="d598" class="ka kb hi jw b fi kg kd l ke kf">array([-0.26638864, 0.6815542 , 0.03773129, -0.66672428, 1.55906522, -0.382707, 0.88990834, 1.00301838, -0.70944433, -0.74607132, 1.03344495, -0.74360567, -0.48812338])</span></pre><p id="71b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看起来很棒——现在开始有趣的事情吧！我们来分类。</p><p id="ebd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们需要一个空模型将如何执行的概念:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="6f34" class="ka kb hi jw b fi kc kd l ke kf">print(f'Null model accuracy: {round(max(y_test.mean(), 1 - y_test.mean()), 2)}')</span><span id="251b" class="ka kb hi jw b fi kg kd l ke kf">Null model accuracy: 0.61</span></pre><p id="5d55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">61%的准确率是要打破的数字。</p><p id="135c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，这应该是一个初学者的教训，它是，但我想探索一个新的图书馆，我要带你和我一起去。如果没有冒险精神，数据科学家什么都不是。</p><p id="da2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最初，我们打算运行Sklearn的逻辑回归。相反，我们将使用<a class="ae ju" href="https://lazypredict.readthedocs.io" rel="noopener ugc nofollow" target="_blank">Lazy Predict</a>——一种工具，它将允许我们同时运行大量算法，并为我们提供每个算法的准确性度量。这不一定构成您分析的全部，相反，它是一个工具，我们将使用它来选择一个特定的模型，然后进行调整和完善。</p><p id="6363" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们开始之前，您很可能需要安装这个库；之后，导入分类。</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="3e43" class="ka kb hi jw b fi kc kd l ke kf">!pip install lazypredict</span><span id="5b91" class="ka kb hi jw b fi kg kd l ke kf">from lazypredict.Supervised import Classification</span></pre><p id="5db8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，让我们设置我们的模型，或者在这种情况下，模型搜索:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="5090" class="ka kb hi jw b fi kc kd l ke kf">clf = Classification()<br/># instantiating</span><span id="c895" class="ka kb hi jw b fi kg kd l ke kf">models, predictions = clf.fit(X_train_scaled, X_test_scaled, y_train, y_test)<br/># fitting our model and predicting in one step</span><span id="c833" class="ka kb hi jw b fi kg kd l ke kf">print(models)<br/># printing the results</span></pre><p id="aa42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的结果应该是这样的:</p><pre class="jf jg jh ji fd jv jw jx jy aw jz bi"><span id="6bcd" class="ka kb hi jw b fi kc kd l ke kf"><strong class="jw hj">                            Accuracy</strong> <strong class="jw hj">Bal.Accuracy  ROC AUC  F1 Score</strong>    <br/><strong class="jw hj">Model</strong>                                                                           <br/>ExtraTreesClassifier             0.88     0.87     0.87      0.88   <br/>XGBClassifier                    0.87     0.86     0.86      0.87   <br/>Perceptron                       0.86     0.86     0.86      0.86   <br/>LGBMClassifier                   0.86     0.85     0.85      0.86   <br/>BernoulliNB                      0.86     0.85     0.85      0.86   <br/>RandomForestClassifier           0.86     0.85     0.85      0.86   <br/>LinearDiscriminantAnalysis       0.86     0.84     0.84      0.85   <br/>RidgeClassifierCV                0.86     0.84     0.84      0.85   <br/>RidgeClassifier                  0.86     0.84     0.84      0.85   <br/>LinearSVC                        0.86     0.84     0.84      0.85   <br/>GaussianNB                       0.84     0.83     0.83      0.84   <br/>LogisticRegression               0.84     0.83     0.83      0.84   <br/>BaggingClassifier                0.83     0.82     0.82      0.83   <br/>CalibratedClassifierCV           0.84     0.82     0.82      0.84   <br/>SVC                              0.83     0.82     0.82      0.83   <br/>NearestCentroid                  0.83     0.82     0.82      0.83   <br/>NuSVC                            0.83     0.82     0.82      0.83   <br/>AdaBoostClassifier               0.84     0.82     0.82      0.84   <br/>KNeighborsClassifier             0.83     0.81     0.81      0.83   <br/>QuadraticDiscriminantAnalysis    0.82     0.81     0.81      0.82   <br/>SGDClassifier                    0.82     0.80     0.80      0.81   <br/>LabelSpreading                   0.78     0.77     0.77      0.78   <br/>LabelPropagation                 0.78     0.77     0.77      0.78   <br/>ExtraTreeClassifier              0.78     0.76     0.76      0.78   <br/>PassiveAggressiveClassifier      0.75     0.74     0.74      0.75   <br/>DecisionTreeClassifier           0.72     0.72     0.72      0.73   <br/>DummyClassifier                  0.63     0.63     0.63      0.63   <br/><br/>                               <strong class="jw hj">Time Taken</strong>  <br/><strong class="jw hj">Model</strong>                                      <br/>ExtraTreesClassifier                 0.09  <br/>XGBClassifier                        0.03  <br/>Perceptron                           0.01  <br/>LGBMClassifier                       0.02  <br/>BernoulliNB                          0.01  <br/>RandomForestClassifier               0.11  <br/>LinearDiscriminantAnalysis           0.01  <br/>RidgeClassifierCV                    0.01  <br/>RidgeClassifier                      0.01  <br/>LinearSVC                            0.01  <br/>GaussianNB                           0.01  <br/>LogisticRegression                   0.01  <br/>BaggingClassifier                    0.02  <br/>CalibratedClassifierCV               0.04  <br/>SVC                                  0.01  <br/>NearestCentroid                      0.01  <br/>NuSVC                                0.01  <br/>AdaBoostClassifier                   0.09  <br/>KNeighborsClassifier                 0.01  <br/>QuadraticDiscriminantAnalysis        0.01  <br/>SGDClassifier                        0.01  <br/>LabelSpreading                       0.01  <br/>LabelPropagation                     0.01  <br/>ExtraTreeClassifier                  0.01  <br/>PassiveAggressiveClassifier          0.01  <br/>DecisionTreeClassifier               0.01  <br/>DummyClassifier                      0.01</span></pre><p id="8829" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">酷豆！我们有每个模型的基线精度，以及运行它所花费的时间，当我们有更大的数据集时，这将是有用的。另一个伟大的特点是包括平衡的准确性，ROC AUC和f1评分。虽然您可能还不熟悉这些指标，但在选择模型时考虑它们是很重要的——事实上，我们使用的心脏病数据集是一个完美的例子，说明了为什么这些指标是无价的。</p><p id="1417" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在处理患者健康结果时，我们是愿意将健康的患者诊断为患有心脏病，还是告诉患病的患者他们很健康，并拒绝他们的治疗？这是一个数据科学的道德困境，我们可以使用所有的评估指标来解决，我们可以调整我们的模型，以支持一个或另一个方向(在这种情况下，最大限度地减少假阴性)。在以后的博客文章中，我们将进一步讨论评估。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ki"><img src="../Images/145b6d4e8643064957baceacb999902c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5qVaN6B5JeDVrGsh7dWYbQ.jpeg"/></div></figure><p id="eb95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，恭喜你！你已经解决了第一个分类问题。虽然仅给出模型性能图表似乎有点模糊，但请放心，基础工作已经做好，在接下来的几周内，我们将利用上面所做的工作，进一步研究特性化、模型选择、超参数调整和评估。</p><p id="3326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">干得好！希望您能在下一期文章中找到回来的路。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ki"><img src="../Images/5177a8387d03e508e16e282bd3389c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZqmtUzDlgrKuzN5egbq3gw.jpeg"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">所有照片由Pixabay.com提供</figcaption></figure></div></div>    
</body>
</html>