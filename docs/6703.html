<html>
<head>
<title>Overview of Normalization Techniques in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的规范化技术综述</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf?source=collection_archive---------3-----------------------#2022-04-23">https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf?source=collection_archive---------3-----------------------#2022-04-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7828" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">理解深度学习中不同规范化方法的简单指南。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/7ec98a4aca0c5c05bdb01be109ec9c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l8iElQ-XKE5qhs-4"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">安妮·尼加德在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="bf42" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">训练深度神经网络是一项具有挑战性的任务。多年来，研究人员想出了不同的方法来用T4加速学习过程，用T6稳定学习过程。标准化是一种被证明在这方面非常有效的技术。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kk"><img src="../Images/0cb7c7998d93b75c5f85d332921d188d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pYpjsQPaL8R7mVWXyzE4Xg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">不同类型的标准化</figcaption></figure><p id="4288" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这篇博客中，我将使用类比和形象化的方法来回顾每一种方法，这将帮助你理解它们背后的动机和思维过程。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="bf78" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">为什么要正常化？</h1><p id="e4a0" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">假设我们有两个特征和一个简单的神经网络。一个是范围在0到65的<strong class="jq hj">年龄</strong>，另一个是范围在0到10 000的<strong class="jq hj">工资</strong>。我们将这些特征输入模型并计算梯度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/30743dbbf46ecc4092fcb151cfe8ca46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDPk_MZkbGMtaz1cZstaZw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">n —示例</figcaption></figure><p id="1621" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">不同规模的输入导致不同的权重更新和优化步骤的最小值。这也使得损失函数的形状不成比例。在这种情况下，我们需要使用<strong class="jq hj">较低的学习速率</strong>以避免超调，这意味着学习过程较慢。</p><p id="c565" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">解决方案是输入标准化。它通过<strong class="jq hj">减去<strong class="jq hj">平均值</strong>(居中)并除以<strong class="jq hj">标准偏差</strong>来缩小特征。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lq"><img src="../Images/83ebe1293a875252bd585a65bc4378d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_VpXVM7jhkD6SJEcZA-hg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">n —示例</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/3b7c52050c2a4b14587e9b5037455b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efpPxl15HumHHn9vGG91vw.png"/></div></div></figure><p id="843d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个过程也被称为'<em class="ls">白化</em>，其中值具有0均值和<em class="ls">单位</em>方差。它提供<strong class="jq hj">更快的收敛</strong>和更<strong class="jq hj">稳定的训练</strong>。</p><p id="2fd1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是一个如此简洁的解决方案，那么我们为什么不将网络中每一层的激活规范化呢？</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="b1cc" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">激活</h1><h2 id="997c" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">批量标准化</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/db2940f07eb8d8b129e89d2369942762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kO_DbzV50-ZhL1YM_IYCg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">N —批次，C —通道，H，W—空间宽度和高度。<a class="ae jn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c784" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">2015年，Sergey Ioffe和Christian Szegedy[3]提出了解决内部协变量转换问题的想法。用简单的英语来说，这意味着输入层分布<strong class="jq hj">由于权重更新而不断变化</strong>。在这种情况下，下一层总是需要适应新的分布。导致收敛较慢，训练不稳定。</p><p id="8bc4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">批量标准化提供了一种方法，即<strong class="jq hj">控制</strong>和<strong class="jq hj">优化</strong>每一层之后的分布。该过程与输入归一化相同，但是我们添加了两个<strong class="jq hj">可学习的</strong>参数，<strong class="jq hj"> γ </strong>和<strong class="jq hj"> β </strong>。</p><p id="5a21" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我没有把所有的数学方程式放进去，而是创建了一些代码片段，我觉得这些代码片段可读性更强，也更直观。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="45ed" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这两个参数是使用反向传播沿着网络学习的。它们通过<strong class="jq hj">缩放</strong> (γ)和<strong class="jq hj">移动</strong> (β)激活来优化分布。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/03516e142cb0e52a598f5d7c13c5e590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toQtoV-69Z3QCJZG3D9dIg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">伽玛和贝塔效应</figcaption></figure><p id="aa91" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">由于我们有固定的分布，我们可以增加学习率和加快收敛。除了计算上的提升，BN还作为一种<strong class="jq hj">正则化</strong>技术。数据集统计数据近似值产生的噪声消除了漏失需求。</p><p id="1a9d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但这是一把双刃剑。这种估计只适用于较大的批量。当示例数量较少时，性能会急剧下降。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/7fd275045223ec5b444928dd05731b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0zG0CkOWalXEIjFnuSIXGQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">ResNet50验证的错误。<a class="ae jn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="7ba2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">BN的另一个缺点是<strong class="jq hj">测试时间</strong>。我们以自动驾驶汽车为例。你传递的是行驶过程中摄像机记录的单帧，而不是一批图像。在这种情况下，网络必须使用预先计算的训练均值和方差，这可能会导致<strong class="jq hj">不同的结果</strong>。</p><p id="f21c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个问题的重要性促使社区创建替代方法来避免对批处理的依赖。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h2 id="f389" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">图层规范化</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mm"><img src="../Images/4773410035d83e0f11abe684ee46e6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgYUivzxq5nNGAHwquCepg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="e316" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是Geoffrey E. Hinton等人在2016年[4]首次尝试减少批量约束。主要是因为递归神经网络，不清楚如何将BN应用于它们。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/322e2fde0de68b5803dab8a131929ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBnaj35rTfXr2X4fh6BVhA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">RNN建筑</figcaption></figure><p id="7f61" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在深度神经网络中，很容易存储每个BN层的统计数据，因为层数是固定的。然而，在RNNs中，输入和输出形状的长度不同。因此，在这种情况下，最好使用单个时间步长(示例)的统计数据进行归一化，而不是整个批次。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="768b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在此方法中，batch(N)中的每个示例都在[C，H，W]维度上进行归一化。像BN一样，它可以加速和稳定训练，但是没有批量的限制。此外，该方法可用于批次等于1的<strong class="jq hj">在线学习</strong>任务。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h2 id="e905" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">实例规范化</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mo"><img src="../Images/81df852bc9d6cdf62c18441689af3763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SDV1VMxneJulS5ZjR1HBYA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="cbb8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Dmitry Ulyanov等人在2016年的论文[5]中介绍了实例标准化。这是减少对批处理的依赖以改善<strong class="jq hj">样式传输</strong>网络的结果的另一种尝试。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="aedb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">跨批次和通道的标准化允许从图像中移除特定的对比度信息，这有助于<strong class="jq hj">一般化</strong>。</p><p id="5247" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这种方法在像Pix2Pix或CycleGAN这样的<strong class="jq hj">生成</strong>模型中流行起来，并成为著名的StyleGAN2中使用的自适应实例规范化的先驱。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h2 id="3f41" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">群体规范化</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/c5f31eff7e41b9739297c2387f80eb49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WIQi_j3vvBTgF6XZotqUEw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="bd2b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">2018年[1]的论文中介绍了组标准化，它直接解决了CNN的BN限制。主要指责是<strong class="jq hj">分布式学习</strong>，批量拆分成很多机器。这些都是在少数例子上训练出来的，比如6-8，在某些情况下，甚至是1-2。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/5b9df64c61ee1f29851bee6e747331aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idLN7EqHyoV9jqXnAC4Y6w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">分布学习</figcaption></figure><p id="003a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了解决这个问题，他们引入了层和实例规范化的混合。GN将通道分成<strong class="jq hj">组</strong>并在它们之间进行标准化。这种方案使得计算与批量大小无关。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="0560" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">GN优于在小批量上训练的BN，但是不能胜过大批量的结果。然而，这是一个很好的起点，导致了另一种方法，它与GN相结合，超过了BN的结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/deb69dec0a5b87f39c8dbed452aef6dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MVxVaR5S12n0tZP0Q-VPEg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">ResNet-50在ImageNet上的验证错误。<a class="ae jn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="d190" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">砝码</h1><h2 id="cb6e" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">重量标准化</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/3e50eeebd7f1b8a4e0b29b0e3bc3c6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSv5FwOLr14M7Xp0mxQmdg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://arxiv.org/pdf/1903.10520.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c88c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们已经标准化了输入和层输出。唯一剩下的就是重量。它们可以不受任何控制地<strong class="jq hj">变</strong> <strong class="jq hj">大</strong>，尤其是当我们无论如何都要标准化输出的时候。通过标准化权重，我们实现了更平滑的损失景观和更稳定的训练。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="8fcc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如我之前提到的，体重标准化是群体标准化的绝佳伴奏。将这些方法结合起来比每台机器仅使用一个样品的BN(大批量)产生更好的结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/c5398ffffd7f430f55caf24223dbb709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qBX-wmcrGYnQL8WR41SA9w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">红色代表ImageNet，蓝色代表Coco数据集。<a class="ae jn" href="https://arxiv.org/pdf/1903.10520.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="bb9b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">他们还提出了被称为批量通道标准化的BCN方法。简而言之，每一层同时使用BN和GN。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="5af6" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated"><strong class="ak">结论</strong></h1><p id="831f" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">规范化是深度学习中的一个基本概念。它加快了计算速度并稳定了训练。这些年来发展了许多不同的技术。希望您已经理解了它们背后的潜在思想，并且现在已经确定地知道了为什么以及在您的项目中何处使用它们。</p><p id="7c34" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">查看我的<a class="ae jn" href="https://maciejbalawejder.medium.com/" rel="noopener"> <strong class="jq hj">中型</strong> </a>和<a class="ae jn" href="https://github.com/maciejbalawejder" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> Github </strong> </a> <strong class="jq hj"> </strong>简介如果你想看我的其他项目。</p><h1 id="3962" class="ks kt hi bd ku kv mu kx ky kz mv lb lc io mw ip le ir mx is lg iu my iv li lj bi translated">参考</h1><p id="e031" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>分组归一化</p><p id="33b0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://arxiv.org/abs/1903.10520" rel="noopener ugc nofollow" target="_blank">【2】</a>批次通道归一化和权重标准化的微批次训练</p><p id="5d21" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">【3】</a>批量规格化:通过减少内部协变量转移加速深度网络训练</p><p id="2e5f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1607.06450.pdf" rel="noopener ugc nofollow" target="_blank">【4】</a>图层归一化</p><p id="da0c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1607.08022.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a>实例规范化:快速风格化缺少的要素</p><p id="f222" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">【6】</a>用于图像识别的深度残差学习</p></div></div>    
</body>
</html>