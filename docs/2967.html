<html>
<head>
<title>Linear Regression From Scratch PT3: Stochastic Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始线性回归 PT3:随机梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/linear-regression-from-scratch-pt3-stochastic-gradient-descent-7def4378f729?source=collection_archive---------11-----------------------#2021-05-25">https://medium.com/nerd-for-tech/linear-regression-from-scratch-pt3-stochastic-gradient-descent-7def4378f729?source=collection_archive---------11-----------------------#2021-05-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="4f28" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">欢迎回来！这是线性回归模型的第 3 部分。在本文中，我们将探索著名的“随机梯度下降(SGD)”算法。我将尝试解释这种非常流行的算法如何工作的直觉，以及它与上一系列文章中探讨的简单批量梯度下降算法的不同之处。如果你没有看过之前关于这个主题的文章，可以在这里找到嵌入的链接:<a class="ae jh" href="https://mardiyyah.medium.com/linear-regression-from-scratch-pt1-484ff41e5a3" rel="noopener"> <strong class="il hj"> Part1 </strong> </a>和<a class="ae jh" rel="noopener" href="/nerd-for-tech/linear-regression-from-scratch-pt2-the-gradient-descent-algorithm-f30d42fea40c"> <strong class="il hj"> Part 2 </strong> </a>。请务必阅读它们，因为本文是建立在前面讨论的基础上的。</p></blockquote><p id="0c04" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">在上一篇文章中，我们讨论了在解决线性回归问题时，使用数值方法而不是分析方法(封闭形式的解决方案)。我们特别探索了一类流行的数值方法；“梯度下降”。正如所解释的，它是机器学习(包括像深度学习这样的子领域)中使用的最流行的优化技术，它有三种变体；</p><ul class=""><li id="f38e" class="jl jm hi il b im in iq ir ji jn jj jo jk jp jg jq jr js jt bi translated">批量梯度下降(BGD)</li><li id="44ee" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">随机梯度下降(SGD)</li><li id="95c5" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">小批量梯度下降，也称为小批量随机梯度下降。(百万新元)</li></ul><p id="3112" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">本文的重点将是“随机梯度下降(SGD)”。</p><blockquote class="if ig ih"><p id="7e11" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><strong class="il hj">T11】</strong></p></blockquote><h1 id="a6e3" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">随机梯度下降:直觉</h1><p id="3db5" class="pw-post-body-paragraph ii ij hi il b im kx io ip iq ky is it ji kz iw ix jj la ja jb jk lb je jf jg hb bi translated">回想一下，在批量梯度下降(BGD)中，所有训练样本(X)都用于计算梯度，并在迭代的单个步骤中更新权重。</p><p id="9009" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">对于随机梯度下降(SGD)，区别在于梯度计算是如何进行的。SGD 不是同时使用所有的训练样本，而是随机选择一个训练样本进行梯度计算。我什么意思？</p><p id="cd8e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">还记得梯度下降的伪代码吗？</p><ul class=""><li id="d8a0" class="jl jm hi il b im in iq ir ji jn jj jo jk jp jg jq jr js jt bi translated">初始化权重的随机猜测(从θ的随机猜测开始)</li><li id="5aba" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">使用“<strong class="il hj">所有样本(x)”</strong>计算梯度</li><li id="1c1a" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">使用更新规则更新权重</li><li id="8011" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">重复直到收敛</li></ul><p id="a518" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">对于 SGD，这变成了；</p><ol class=""><li id="e746" class="jl jm hi il b im in iq ir ji jn jj jo jk jp jg lc jr js jt bi translated">初始化权重的随机猜测(从θ的随机猜测开始)。</li><li id="249d" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg lc jr js jt bi translated">从训练数据集中随机选择一个样本(x(i))。</li><li id="d561" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg lc jr js jt bi translated">计算其梯度(即仅 x(i)的梯度)。</li><li id="574e" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg lc jr js jt bi translated">使用更新规则更新权重。</li><li id="9ac6" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg lc jr js jt bi translated">对所有训练样本(从 x(i)到 x(n))重复步骤(2–4)，直到收敛。</li></ol><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/8a2a4d5c56f733f9f4722165d979aaf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYOcAhGlPIvwZHdKijMCHg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">更新 SGD 的规则</figcaption></figure><p id="a65e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">其中:x(i)这里是单个训练观察/样本，y(i)是映射到 x(i)的标签/目标。</p><blockquote class="if ig ih"><p id="2a9b" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">有趣的事实:随机==随机。训练样本是随机选择的。</p></blockquote><p id="95c1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">SGD 的优点是速度快，需要的内存少。然而，它从来没有像我们期望的那样真正收敛，而是达到了最优解的一个近似值。你会注意到它有一个更嘈杂的下坡运动，并在全球最小值附近振荡(但从未真正击中要害)。其计算复杂度为常数时间 O(1)。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es lt"><img src="../Images/adce74ebf740d127a11e29cb9b987496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*SzSx1YbyMc6EAc85.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">来源:<a class="ae jh" href="https://www.researchgate.net/figure/Stochastic-gradient-descent-compared-with-gradient-descent_fig3_328106221" rel="noopener ugc nofollow" target="_blank">研究门</a></figcaption></figure><blockquote class="if ig ih"><p id="58bc" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">“SGD 的随机性质意味着一些噪声将被引入到权重设置中，但该技术总体上仍然非常有效，并被广泛使用。”— <strong class="il hj">研究之门(链接到下面的出版物)</strong></p></blockquote><p id="d869" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">SGD 也被称为“无偏估计量”，因为梯度的期望与 BGD 相同，但方差较小。</p><blockquote class="lu"><p id="a5fc" class="lv lw hi bd lx ly lz ma mb mc md jg dx translated">“估计量是一种函数，它试图在给定样本数据的情况下估计总体数据的一些有用质量。偏差是机器学习中估计器的一个非常重要的性质。SGD 是无偏的，因为对梯度的期望与对 BGD 的期望相同。</p><p id="457b" class="lv lw hi bd lx ly lz ma mb mc md jg dx translated">我知道这些术语可能听起来有点复杂，我建议你参考下面的链接，我已经将链接附在了解释这些概念的博客帖子上。"</p></blockquote><p id="ee38" class="pw-post-body-paragraph ii ij hi il b im me io ip iq mf is it ji mg iw ix jj mh ja jb jk mi je jf jg hb bi translated"><strong class="il hj">随机梯度下降优于批量梯度下降</strong></p><ul class=""><li id="cb7d" class="jl jm hi il b im in iq ir ji jn jj jo jk jp jg jq jr js jt bi translated">批次梯度下降缓慢。它需要在每次迭代中计算整个训练样本的梯度。另一方面，随机梯度下降更快，因为它一次只使用一个样本。</li><li id="6f9e" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">随机梯度下降是一种无偏估计量。</li><li id="fde7" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">它的复杂度是常数时间 O(1)，BDG 的复杂度是 O(N)。</li></ul></div><div class="ab cl mj mk gp ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hb hc hd he hf"><h1 id="697d" class="jz ka hi bd kb kc mq ke kf kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw bi translated">随机梯度下降的代码实现</h1><p id="054e" class="pw-post-body-paragraph ii ij hi il b im kx io ip iq ky is it ji kz iw ix jj la ja jb jk lb je jf jg hb bi translated">如前所述，对于上一篇文章中用于批量梯度下降的代码，唯一需要做的细微区别是迭代要素(X)，一次获取一个观测值(行)(更像水平分割数据)，并将其用于梯度计算，而不是整个数据集。</p><p id="4507" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">让我们看看它是如何工作的。</p><p id="5157" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">我们将跳过数据预处理步骤，直接进入模型类。</p><p id="9cc0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">在此之前，让我们快速了解一下关于 SGD 的两个重要术语:</p><p id="512f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">这些是；</p><ul class=""><li id="9e76" class="jl jm hi il b im in iq ir ji jn jj jo jk jp jg jq jr js jt bi translated">世</li><li id="02d5" class="jl jm hi il b im ju iq jv ji jw jj jx jk jy jg jq jr js jt bi translated">迭代次数(n_iters)</li></ul><p id="ab5f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">因此<strong class="il hj">历元</strong>被定义为对整个数据集(所有训练数据)的一次遍历。而迭代是对一个训练数据的传递。对于批量梯度下降，这两个术语有时可以互换使用，因为所有训练数据都在一个步骤中考虑。然而，对于 SGD，这些术语是不同的(正如您在许多深度学习应用程序中也会看到的)，Epoch 覆盖整个数据集，而 iterations 考虑我们在水平分割数据时采取的步骤(一次进行一次观察)。</p><p id="0d2a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">你会对代码有更好的直觉。</p><p id="6877" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">反映该变化的唯一功能是体能/训练功能。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mv"><img src="../Images/cff4a865cbdaa80a752b2eb6c9da9fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n-jAh0XAk8MiUzFdQyGJaA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">训练功能</figcaption></figure><p id="351c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">如上面的代码片段所示，启动了第二个循环，指示数据的水平切片(X _ train . shape[0]= = row/observation)。注意到这个切片被改变形状的那一行了吗？<em class="ik">你能猜到为什么吗？</em></p><p id="cfd7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">没错。现在，我们有了一个向量，“行向量”，而不是一个矩阵。没有整形，代码返回一个错误(<em class="ik">记住矩阵乘法的规则</em>)。修改后的“X”仅替换训练功能中的首字母 X。</p><p id="625e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">还有，“<em class="ik"> X_train.shape[0] </em>的值就是这里的迭代次数(n_iters)。所以你也可以将<em class="ik"> n_iters </em>定义为一个变量，见下图。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mw"><img src="../Images/12ef4a0237beae86d086c7f03ba507b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8AUs32aVmWu5RLhZUuGWHw.png"/></div></div></figure><p id="2b26" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">注意到切片也是为“Y”实现的(如公式中所示)。</p><p id="dd8f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">其他所有功能保持不变；"预测和均方误差函数."</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mx"><img src="../Images/61fb3276369a09c76d6529840a94409a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jo2JvKqd3ootuMFINAH1xA.png"/></div></div></figure><p id="0d5b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">通过初始化模型来运行代码，看看损失图与 BGD 有什么不同。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es my"><img src="../Images/29d57da0e72a60a2f4212522d6333177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0t4jpqAv15zpaJkh0Tna5Q.png"/></div></div></figure><blockquote class="if ig ih"><p id="2ac3" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">你有没有注意到和 BGD 相比使用了更小的学习率？你能看到振荡和嘈杂的运动吗？</p></blockquote><p id="23df" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">一般来说，建议对 SGD 使用较小的学习速率，因为它每次都依赖于单个数据进行计算。它非常嘈杂(如上面的损耗图所示，并且更倾向于过冲，因为它在全局最小值附近振荡。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mz"><img src="../Images/47d9e709cf28affc6a1c19ec4bf9d60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*OX7D7t1KGoigNBg7U1WyAA.png"/></div></figure><h1 id="b3c9" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><em class="na">干得好！</em></h1><h1 id="1094" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="3337" class="pw-post-body-paragraph ii ij hi il b im kx io ip iq ky is it ji kz iw ix jj la ja jb jk lb je jf jg hb bi translated">我希望你喜欢阅读这篇文章，并对 SGD 的工作原理有所了解。SGD 在机器学习的子领域——“深度学习”中的应用更为普及。它是最常用的优化器。</p><p id="32bc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">在下一个系列中，我们将讨论小批量随机梯度体面(<em class="ik">最酷的一批)😄</em>)。</p><blockquote class="lu"><p id="299e" class="lv lw hi bd lx ly lz ma mb mc md jg dx translated">“只要我们努力，我们就会随着成长而不断进步。随着我们的前进，我们不断取得进步。继续前进！!"</p></blockquote></div><div class="ab cl mj mk gp ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hb hc hd he hf"><h1 id="e49f" class="jz ka hi bd kb kc mq ke kf kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw bi translated">社交媒体连接:</h1><p id="2c9e" class="pw-post-body-paragraph ii ij hi il b im kx io ip iq ky is it ji kz iw ix jj la ja jb jk lb je jf jg hb bi translated">领英:<a class="ae jh" href="https://www.linkedin.com/in/aminah-mardiyyah-rufa-i/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/aminah-mardiyyah-rufa-i/</a></p><p id="4548" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">推特:@diyyah92</p></div><div class="ab cl mj mk gp ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hb hc hd he hf"><h1 id="d68b" class="jz ka hi bd kb kc mq ke kf kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw bi translated">代码库</h1><div class="nb nc ez fb nd ne"><a href="https://github.com/Aminah92/Foundations-of-Machine-Learning/blob/main/Linear-Models/LINEAR%20REGRESSION%20FROM%20SCRATCH%20PT%203-%20SGD.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">amina h92/机器学习基础</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">在 GitHub 上创建一个帐户，为 amina h92/机器学习基础开发做贡献。</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">github.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ln ne"/></div></div></a></div></div><div class="ab cl mj mk gp ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hb hc hd he hf"><h1 id="2145" class="jz ka hi bd kb kc mq ke kf kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw bi translated">链接到以前关于这个主题的文章</h1><p id="b9b1" class="pw-post-body-paragraph ii ij hi il b im kx io ip iq ky is it ji kz iw ix jj la ja jb jk lb je jf jg hb bi translated">第一部分:<a class="ae jh" href="https://mardiyyah.medium.com/linear-regression-from-scratch-pt1-484ff41e5a3" rel="noopener">https://mardiyyah . medium . com/linear-regression-from-scratch-pt1-484 ff 41e 5a 3</a></p><p id="7484" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">第二部分:<a class="ae jh" rel="noopener" href="/nerd-for-tech/linear-regression-from-scratch-pt2-the-gradient-descent-algorithm-f30d42fea40c">https://medium . com/nerd-for-tech/linear-regression-from scratch-pt2-the-gradient-descent-algorithm-f 30d 42 FEA 40 c</a></p></div><div class="ab cl mj mk gp ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hb hc hd he hf"><h1 id="c669" class="jz ka hi bd kb kc mq ke kf kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw bi translated">参考资料和资源</h1><p id="c07f" class="pw-post-body-paragraph ii ij hi il b im kx io ip iq ky is it ji kz iw ix jj la ja jb jk lb je jf jg hb bi translated"><a class="ae jh" href="https://www.researchgate.net/publication/328106221_Deep_learning_and_virtual_drug_screening#pf6" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/328106221 _ Deep _ learning _ and _ virtual _ drug _ screening # pf6</a></p><p id="07e2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><a class="ae jh" href="https://florian.github.io/estimators/" rel="noopener ugc nofollow" target="_blank">https://florian.github.io/estimators/</a></p><p id="21e4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><a class="ae jh" href="https://www.quora.com/Whats-the-difference-between-epoch-and-iteration-when-training-a-Perceptron" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/what-what-the-difference-when-training-a-Perceptron</a></p></div></div>    
</body>
</html>