<html>
<head>
<title>Linear regression for 5-year-olds</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5岁儿童的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/linear-regression-for-5-year-olds-1bdc5d0badc4?source=collection_archive---------14-----------------------#2021-07-02">https://medium.com/nerd-for-tech/linear-regression-for-5-year-olds-1bdc5d0badc4?source=collection_archive---------14-----------------------#2021-07-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fcca" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第3/3部分:从头开始用python实现</h2></div><blockquote class="ix iy iz"><p id="a607" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">1/3部分链接:<a class="ae jx" rel="noopener" href="/@mohitpatil246/linear-regression-for-5-year-olds-2e8d83e9a680">直觉</a><br/>2/3部分链接:<a class="ae jx" href="https://mohitpatil246.medium.com/linear-regression-for-5-year-olds-1a854c88bbb5" rel="noopener">算法背后的数学</a></p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es jy"><img src="../Images/e53d7b0a8c3e9c9efc8fd0077bb9d884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8V3BhaLgQYh8-OkYYuMc0Q.jpeg"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx translated">照片由<a class="ae jx" href="https://www.pexels.com/@negativespace?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">负空间</a>从<a class="ae jx" href="https://www.pexels.com/photo/computer-keyboard-34153/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄</figcaption></figure><p id="791e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi kr translated">大家好！这是《5岁儿童线性回归》的第三部分，从头开始讲述python中算法的实现。到目前为止，我们已经了解了什么是线性回归，它是如何工作的，以及如何计算系数来找到最佳拟合线。</p><p id="5a8e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated"><em class="jc">现在理论够多了，让我们开始编码吧！</em></p><p id="c7df" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">我们将要处理的数据集是- <strong class="jd hj">联合循环发电厂数据集。</strong></p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es la"><img src="../Images/e642b183f5309511f0e6e9eb42bff245.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*S2u5jx1JxgjE5WPx51MRtw.png"/></div></figure><p id="a298" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated"><em class="jc">其中，<br/></em><strong class="jd hj"><em class="jc"/></strong><em class="jc">→平均温度<br/></em><strong class="jd hj"><em class="jc">E</em></strong><em class="jc">→排气真空<br/></em><strong class="jd hj"><em class="jc">AP</em></strong><em class="jc">→环境压力<br/></em><strong class="jd hj"><em class="jc">RH</em></strong><em class="jc">→相对湿度<br/> </em> <strong class="jd hj"> <em class="jc"> PE</em></strong></p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><p id="47b8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">这里，我们有兴趣预测的因变量/值是<strong class="jd hj"> PE </strong> ( <em class="jc">电厂每小时的电能输出</em>)。我们试图建立的模型是只需要一个独立变量的线性回归。<br/>从下图可以看出，唯一与我们的因变量有线性关系的预测量/自变量是<strong class="jd hj">在<em class="jc"> ( </em> </strong> <em class="jc">平均温度)</em>。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es li"><img src="../Images/7b770e3d7f5b1c75dd2491561fd2695d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4CmAES9jvxpFrF0VVn2jw.png"/></div></div></figure><p id="60cd" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">因此，我们将尝试找出<strong class="jd hj">平均温度</strong>和<strong class="jd hj">电能输出之间的关系。为了更好地可视化，我们将只从总数据集中提取3000个值来构建我们的模型。</strong></p><blockquote class="ix iy iz"><p id="2370" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">概述- <br/> 1。<a class="ae jx" href="https://medium.com/p/1bdc5d0badc4/8e7f" rel="noopener">基本预处理</a> <br/> 2。分割数据<br/> 3。计算参数(或训练我们的模型)<br/> 4。做一些预测。可视化结果<br/> 6。评估我们的模型</p></blockquote></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="8e7f" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">预处理-</h1><p id="dc49" class="pw-post-body-paragraph ja jb hi jd b je mb ij jg jh mc im jj ko md jm jn kp me jq jr kq mf ju jv jw hb bi translated">在训练我们的数据之前，我们应该总是预处理我们的数据，这通常包括处理缺失值、去除异常值等。</p><ul class=""><li id="3997" class="mg mh hi jd b je jf jh ji ko mi kp mj kq mk jw ml mm mn mo bi translated"><strong class="jd hj">缺失值- </strong></li></ul><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="cd22" class="mu lk hi mq b fi mv mw l mx my">df1.isnull().sum()</span></pre><blockquote class="mz"><p id="af51" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated">AT- 0 <br/> PE- 0</p></blockquote><p id="28bb" class="pw-post-body-paragraph ja jb hi jd b je nj ij jg jh nk im jj ko nl jm jn kp nm jq jr kq nn ju jv jw hb bi translated">我们的数据集中没有缺失值。</p><ul class=""><li id="a0ae" class="mg mh hi jd b je jf jh ji ko mi kp mj kq mk jw ml mm mn mo bi translated"><strong class="jd hj">离群值- </strong> <br/>这里，z-score用于查找我们数据集中的任何离群值-</li></ul><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="127f" class="mu lk hi mq b fi mv mw l mx my">z_scores = stats.zscore(df1)</span><span id="b9c8" class="mu lk hi mq b fi no mw l mx my">abs_z_scores = np.abs(z_scores)<br/>filtered_entries = (abs_z_scores &lt; 3).all(axis=1)<br/>new_df = df1[filtered_entries]</span><span id="043f" class="mu lk hi mq b fi no mw l mx my">print('Length of dataset before removing outliers: %.0f' % (len(df1)))<br/>print('Length of dataset after removing outliers: %.0f' % (len(new_df)))</span></pre><blockquote class="mz"><p id="f19c" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated">剔除异常值前的数据集长度:3000 <br/>剔除异常值后的数据集长度:3000</p></blockquote><p id="0a2c" class="pw-post-body-paragraph ja jb hi jd b je nj ij jg jh nk im jj ko nl jm jn kp nm jq jr kq nn ju jv jw hb bi translated">未检测到异常值。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="cdf4" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">分割数据-</h1><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="fa78" class="mu lk hi mq b fi mv mw l mx my"># Shuffle the dataset <br/>shuffle_df = new_df.sample(frac=1)</span><span id="17c9" class="mu lk hi mq b fi no mw l mx my"># Define a size for the train set <br/>train_size = int(0.7 * len(new_df))</span><span id="f36b" class="mu lk hi mq b fi no mw l mx my"># Split the dataset <br/>x_train = list((shuffle_df[:train_size]).iloc[:,0])<br/>y_train = list((shuffle_df[:train_size]).iloc[:,1])<br/>x_test = list((shuffle_df[train_size:]).iloc[:,0])<br/>y_test = list((shuffle_df[train_size:]).iloc[:,1])</span></pre><ul class=""><li id="8a90" class="mg mh hi jd b je jf jh ji ko mi kp mj kq mk jw ml mm mn mo bi translated">首先，我们使我们的数据随机，所以没有连续的值被选择。参数“frac”指定在随机样本中返回的行的分数，因此“frac=1”意味着返回所有行(按随机顺序)。</li><li id="ad24" class="mg mh hi jd b je np jh nq ko nr kp ns kq nt jw ml mm mn mo bi translated">此外，我们将70%的数据用于训练，其余的(30%)用于测试。</li><li id="1f1d" class="mg mh hi jd b je np jh nq ko nr kp ns kq nt jw ml mm mn mo bi translated">然后，我们将数据分成训练集和测试集。</li></ul></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="9406" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">计算参数-</h1><h2 id="9e7c" class="mu lk hi bd ll nu nv nw lp nx ny nz lt ko oa ob lv kp oc od lx kq oe of lz og bi translated">1.计算B(系数/斜率)</h2><blockquote class="ix iy iz"><p id="184c" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">为了计算方差和协方差，我们需要均值-</p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es oh"><img src="../Images/712086a491a1fd4a1bde91184b68fb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*eFw58eoR4v7Qx_TT8GH_0g.png"/></div></figure><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="62ca" class="mu lk hi mq b fi mv mw l mx my">def mean(values):<br/>    return sum(values) / float(len(values))</span><span id="db4d" class="mu lk hi mq b fi no mw l mx my">mean_x = mean(x_train)<br/>print('Mean of x: %.3f' % (mean_x))</span><span id="8866" class="mu lk hi mq b fi no mw l mx my">mean_y = mean(y_train)<br/>print('Mean of y: %.3f' % (mean_y))</span></pre><blockquote class="mz"><p id="8a02" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated"><strong class="ak">x的平均值:19.659<br/>y的平均值:454.441 </strong></p></blockquote><blockquote class="ix iy iz"><p id="242d" class="ja jb jc jd b je nj ij jg jh nk im jj jk nl jm jn jo nm jq jr js nn ju jv jw hb bi translated">然后计算方差</p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es oi"><img src="../Images/57ba5a346078641d6b0379b1f78e4197.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*258mCoc5JISIyCufIum6Ow.png"/></div></figure><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="6804" class="mu lk hi mq b fi mv mw l mx my">def variance(values, mean):<br/>    return sum([(x-mean)**2 for x in values])/(len(values)-1)</span><span id="fbf2" class="mu lk hi mq b fi no mw l mx my">var_x = variance(x_train, mean_x)<br/>print('Variance of x: %.3f' % (var_x))</span><span id="a9bc" class="mu lk hi mq b fi no mw l mx my">var_y = variance(y_train, mean_y)<br/>print('Variance of y: %.3f' % (var_y))</span></pre><blockquote class="mz"><p id="bb6a" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated">x的方差:54.993<br/>y的方差:291.298</p></blockquote><blockquote class="ix iy iz"><p id="9d64" class="ja jb jc jd b je nj ij jg jh nk im jj jk nl jm jn jo nm jq jr js nn ju jv jw hb bi translated">求x和y的协方差</p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es oj"><img src="../Images/278601975a75211eef75f6dc7bbb0abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*m_LEOJ9nQFQirmZl_sv9sw.png"/></div></figure><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="0d04" class="mu lk hi mq b fi mv mw l mx my">def covariance(x_train, mean_x, y_train, mean_y):<br/>    covar = 0.0<br/>    for i in range(len(x_train)):<br/>        covar += ((x_train[i] - mean_x) * (y_train[i] - mean_y))/(len(x_train)-1)<br/>    return covar</span><span id="2159" class="mu lk hi mq b fi no mw l mx my">covar = covariance(x_train, mean_x, y_train, mean_y)<br/>print('Covariance of x and y: %.3f' % (covar))</span></pre><blockquote class="mz"><p id="5a6d" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated">x和y的协方差:-120.357</p></blockquote><blockquote class="ix iy iz"><p id="1dcf" class="ja jb jc jd b je nj ij jg jh nk im jj jk nl jm jn jo nm jq jr js nn ju jv jw hb bi translated">寻找参数-</p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ok"><img src="../Images/05da4ef283e7e42224a4540f029edc23.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*Wbyx0U9G0Fm-gG-xhdItCg.png"/></div></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ol"><img src="../Images/2750eb7b12a3d161ae168c8f014141fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*iUhgQtPipVm4QdNLsehMJg.png"/></div></figure><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="eaa3" class="mu lk hi mq b fi mv mw l mx my">B = covar/var_x<br/>print('Value of coefficient (B): %.3f' % (B))</span><span id="bdef" class="mu lk hi mq b fi no mw l mx my">a = mean_y - B*mean_x<br/>print('Value of intercept (a) : %.3f' % (a))</span></pre><blockquote class="mz"><p id="0115" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated">系数(B)值:-2.189 <br/>截距(a)值:497.465</p></blockquote></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="80c4" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">现在，让我们做一些预测-</h1><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="bb49" class="mu lk hi mq b fi mv mw l mx my">y_pred=[]<br/>for i in range(len(x_test)):<br/>    temp = a + B*x_test[i]<br/>    y_pred.append(temp)</span><span id="c0b2" class="mu lk hi mq b fi no mw l mx my">pred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})  <br/>pred_df</span></pre><p id="1d84" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">比较实际值和预测值</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es om"><img src="../Images/0b8469a23c2a6406d3c03f6a9f8c8303.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*Lx74rqSx-wiBd6mwQ_E71Q.png"/></div></figure></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="a7a3" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">可视化训练集结果-</h1><p id="6490" class="pw-post-body-paragraph ja jb hi jd b je mb ij jg jh mc im jj ko md jm jn kp me jq jr kq mf ju jv jw hb bi translated">(要在我们的训练数据集上绘制回归线，我们还必须预测训练数据的y值)</p><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="723b" class="mu lk hi mq b fi mv mw l mx my">train=[]<br/>for i in range(len(x_train)):<br/>    temp = a + B*x_train[i]<br/>    train.append(temp)</span><span id="6ed5" class="mu lk hi mq b fi no mw l mx my">plt.scatter(x_train,y_train, color = "red", edgecolors = "white", linewidths = 0.4, alpha = 0.7)<br/>plt.plot(x_train, train, color='blue')<br/>plt.title('Average Temperature Vs. Electrical Energy Output (Training set)')<br/>plt.xlabel('Average Temperature')<br/>plt.ylabel('Electrical Energy Output')<br/>plt.show()</span></pre><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es on"><img src="../Images/9f75ae2a7e6b24d40d2309c9ca8f56cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*zg040RNUbV-xPJeIOWZ6Pw.png"/></div></figure><h1 id="6a4f" class="lj lk hi bd ll lm oo lo lp lq op ls lt io oq ip lv ir or is lx iu os iv lz ma bi translated">可视化测试集结果-</h1><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="c5df" class="mu lk hi mq b fi mv mw l mx my">plt.scatter(x_test,y_test, color = "red", edgecolors = "white", linewidths = 0.4, alpha = 0.7)<br/>plt.plot(x_test, y_pred, color='blue')<br/>plt.title('Average Temperature Vs. Electrical Energy Output (Test set)')<br/>plt.xlabel('Average Temperature')<br/>plt.ylabel('Electrical Energy Output')<br/>plt.show()</span></pre><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ot"><img src="../Images/d805426cc188d2a983c8467427546a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*4iMMZVC0BiL6ptOFr7SUkQ.png"/></div></figure><p id="2089" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">从上面的图中，我们可以看到预测值非常接近回归线，这意味着我们的预测值是好的。</p><blockquote class="ix iy iz"><p id="0896" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">在回归中，有一个假设，即残差应该是同方差的，也就是说，给定因变量的自变量的方差应该是恒定的，或者我们可以说，随着预测值的增加，预测值的方差应该保持恒定。</p><p id="961a" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">在绘制残差图之后，如果残差随着预测值的增加而呈扇形散开，那么我们就有了所谓的异方差，这是不希望的。</p></blockquote><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="7d99" class="mu lk hi mq b fi mv mw l mx my">a = sns.residplot(x=x_test, y=y_test, lowess=True).set_title("Residual plot (Predictor vs. Residuals)")<br/>plt.xlabel('Average Temperature')<br/>plt.ylabel('Residuals')<br/>plt.show()</span></pre><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ou"><img src="../Images/ecbfa67b3a67c8f77e14fb2b5f063f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*rQ-L_X9wmBHgGDH4idiZTA.png"/></div></figure><p id="8967" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">从上图可以看出，残差近似呈正态分布。</p><blockquote class="ix iy iz"><p id="96c2" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">同样，在线性回归中，误差应该是不相关的</p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ov"><img src="../Images/3501b9bd16701b47fc28f3d8fab9c33e.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*rZP9BNAjCeDl4sicOaC78g.png"/></div></figure><p id="82ea" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">可以观察到残差之间不存在相关性，或者我们可以说数据中不存在自相关。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="1749" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">评估-</h1><p id="6636" class="pw-post-body-paragraph ja jb hi jd b je mb ij jg jh mc im jj ko md jm jn kp me jq jr kq mf ju jv jw hb bi translated">现在，让我们评估我们的模型。RMSE是完成这项任务的好办法。RMSE是残差的标准差，也就是说，它说明了残差在最佳拟合线周围的分布情况。值越接近零，我们的模型越好。</p><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="254d" class="mu lk hi mq b fi mv mw l mx my">rmse = 0<br/>for i in range(len(y_test)):<br/>    rmse += (y_test[i] - y_pred[i]) ** 2</span><span id="f082" class="mu lk hi mq b fi no mw l mx my">rmse = np.sqrt(rmse/len(y_test))</span><span id="fbfc" class="mu lk hi mq b fi no mw l mx my">print('Root Mean Squared Error: %.3f' % (rmse))</span></pre><blockquote class="mz"><p id="6e0c" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated">均方根误差:5.501</p></blockquote><p id="bf86" class="pw-post-body-paragraph ja jb hi jd b je nj ij jg jh nk im jj ko nl jm jn kp nm jq jr kq nn ju jv jw hb bi translated">RMSE的值非常接近零，这意味着预测是好的。</p><h2 id="83f0" class="mu lk hi bd ll nu nv nw lp nx ny nz lt ko oa ob lv kp oc od lx kq oe of lz og bi translated">同样，让我们看看R的平方值</h2><pre class="jz ka kb kc fd mp mq mr ms aw mt bi"><span id="e8f6" class="mu lk hi mq b fi mv mw l mx my">SSt = 0<br/>SSr = 0</span><span id="a35f" class="mu lk hi mq b fi no mw l mx my">for i in range(len(y_test)):<br/>    SSt += (y_test[i] - mean_y) ** 2<br/>    SSr += (y_test[i] - y_pred[i]) ** 2</span><span id="1dfb" class="mu lk hi mq b fi no mw l mx my">R2 = 1 - (SSr/SSt)<br/>print('R-square: %.3f'%R2)</span></pre><blockquote class="mz"><p id="80cc" class="na nb hi bd nc nd ne nf ng nh ni jw dx translated">r平方:0.905</p></blockquote><p id="1b06" class="pw-post-body-paragraph ja jb hi jd b je nj ij jg jh nk im jj ko nl jm jn kp nm jq jr kq nn ju jv jw hb bi translated">我们有一个很好的R平方值，也称为决定系数。它告诉我们自变量解释了因变量的多少变化。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="bbcd" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">摘要</h1><p id="90c1" class="pw-post-body-paragraph ja jb hi jd b je mb ij jg jh mc im jj ko md jm jn kp me jq jr kq mf ju jv jw hb bi translated">这是线性回归系列的最后一部分。我希望这篇文章对你有用！<br/>代码链接-<a class="ae jx" href="https://github.com/Mohit246/Medium-code/tree/main/Linear%20regression" rel="noopener ugc nofollow" target="_blank">5岁儿童线性回归</a></p></div></div>    
</body>
</html>