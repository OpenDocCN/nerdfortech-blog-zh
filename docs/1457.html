<html>
<head>
<title>Review — GRF-DSOD &amp; GRF-SSD: Improving Object Detection from Scratch via Gated Feature Reuse (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述——GRF-DSOD和GRF-SSD:通过门控特征重用(物体检测)从零开始改进物体检测</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-grf-dsod-grf-ssd-improving-object-detection-from-scratch-via-gated-feature-reuse-495c11b627d3?source=collection_archive---------7-----------------------#2021-03-20">https://medium.com/nerd-for-tech/review-grf-dsod-grf-ssd-improving-object-detection-from-scratch-via-gated-feature-reuse-495c11b627d3?source=collection_archive---------7-----------------------#2021-03-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f992" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">胜过<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>、<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a>、<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">T5】SSD</a><strong class="ak">、</strong> <a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"> R-FCN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">速度更快R-CNN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN / DCNv1 </a>、<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>参数更少</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/df2f6101a1a58836a87b9a777b939b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fm5si1kd2Zdl_D1M3CXzhw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">提议的GFR-DSOD概述</strong></figcaption></figure><p id="c153" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这篇论文中，对卡耐基梅隆大学、伊利诺伊大学厄巴纳-香槟分校、IBM Research AI、麻省理工学院-IBM Watson AI实验室、谷歌AI &amp; UMass Amherst和史蒂文斯理工学院的<strong class="jr hj">通过门控特征重用</strong>从零开始改进物体检测(GRF-DSOD &amp; GRF-SSD)进行了综述。在本文中:</p><ul class=""><li id="19d3" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">提出了<strong class="jr hj">门控特征重用(GFR)模块</strong>，以使挤压和激励能够<strong class="jr hj">自适应地增强或减弱监督</strong>。</li><li id="cfda" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">一个<strong class="jr hj">特征金字塔结构</strong>将丰富的空间和语义特征压缩到一个预测层中，其中<strong class="jr hj">加强了特征表示</strong>并且<strong class="jr hj">减少了要学习的参数数量</strong>。</li><li id="1ba4" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">值得注意的是，该网络可以从刮擦开始训练，不需要ImageNet预训练。</li></ul><p id="c3f2" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这是<strong class="jr hj"> 2019 BMVC </strong>的一篇论文。(<a class="li lj ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----495c11b627d3--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="1815" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">概述</h1><ol class=""><li id="eeff" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk mn la lb lc bi translated"><strong class="jr hj">迭代特征再利用</strong></li><li id="44df" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">门控自适应重新校准</strong></li><li id="75fe" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">功能重用为</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"><strong class="jr hj"/></a><strong class="jr hj">和</strong> <a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="jr hj"> SSD </strong> </a></li><li id="de1d" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">实验结果</strong></li></ol></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="983e" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">1.迭代特征再利用</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mo"><img src="../Images/b99f04a77250d1d8cb334953ad344393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*6o4lC_JxiuK4NoMtrQH40A.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">说明迭代特征金字塔的构建模块</strong></figcaption></figure><ul class=""><li id="1556" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">如<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>(第一图)所示，生成不同比例尺的特征图<strong class="jr hj">大比例尺特征图被下采样并与当前特征图连接。</strong></li><li id="79c4" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">这里，除了下采样的特征图之外，<strong class="jr hj">小尺度特征图也被上采样并与当前特征图连接。</strong></li><li id="ba2b" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">下采样路径主要由一个最大池层(内核大小=2×2，步距=2)和一个conv层(内核大小=1×1，步距=1)组成，以减少通道尺寸。</li><li id="aa27" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">上采样路径通过双线性上采样以及随后的conv层(内核大小= 1×1，步长= 1)来执行去卷积操作。</li><li id="c128" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">利用粗分辨率和细分辨率特征，<strong class="jr hj">引入具有1×1 conv层加上3×3 conv层的瓶颈块来学习新特征。</strong></li><li id="ffc5" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">参数数量是</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"><strong class="jr hj">DSOD</strong></a><strong class="jr hj">的三分之一。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mp"><img src="../Images/eb94e83712cc9603107a38e7ed1b571f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m2EKZC-f5y2ICh5U.jpg"/></div></div></figure><blockquote class="mq mr ms"><p id="4afd" class="jp jq mt jr b js jt ij ju jv jw im jx mu jz ka kb mv kd ke kf mw kh ki kj kk hb bi translated"><strong class="jr hj">通过上采样和下采样，可以将不同比例的特征图连接在一起，以检测不同大小的对象，如上所示。</strong></p></blockquote></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="3b78" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">2.门控自适应重新校准</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mx"><img src="../Images/731515c7a490ad298215877e9138e4dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xnXIfXYFTjB5XStZL32E6w.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">说明一个门的结构，包括:(一)通道级关注；㈡全球一级的关注；以及(iii)身份映射。</strong></figcaption></figure><h2 id="d3f8" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">2.1.通道级注意力</h2><ul class=""><li id="0fe1" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">使用SENet中的挤压和激励模块。</li><li id="9424" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">挤压</strong>阶段可以表述为<em class="mt"> U </em>上的<strong class="jr hj">全局汇集操作</strong>:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nm"><img src="../Images/c38461f93dd7ec3879d320eee971ad47.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*32sMFXrMoNfhJCUfgfVpXw.png"/></div></figure><ul class=""><li id="bdf7" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">激发</strong>阶段是<strong class="jr hj">两个全连接层</strong>加上一个<strong class="jr hj">s形</strong>激活；</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nn"><img src="../Images/7c0e8ea11bee8b1b098fb64bb9904e76.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*NI17ij4HCEk9_YrHK-sv3w.png"/></div></figure><ul class=""><li id="4c1c" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">其中<em class="mt"> σ </em>是sigmoid函数。</li><li id="270a" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">那么，~ <em class="mt"> U </em>的计算公式为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es no"><img src="../Images/e0a64f84627db553830b31787ae941b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*EtR3qL8g2jSQMIIFKHxq3Q.png"/></div></figure><p id="40f2" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">其中⨂表示逐信道乘法。</p><h2 id="ce88" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">2.2.全球层面的关注</h2><ul class=""><li id="9c5d" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">全局注意力以<em class="mt"> s </em>(挤压阶段的输出)为输入，只生成一个元素。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es np"><img src="../Images/b9f724f8845cd0656fe6c3a0b4d9bfe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*jd0qlnYYqHmS_F8z9lRsNQ.png"/></div></figure><ul class=""><li id="5969" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">其中<em class="mt"> e </em>为全球关注度。</li><li id="1436" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">最后，~ <em class="mt"> V </em>的计算公式为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nq"><img src="../Images/5bf5549d2ba90a273607ecec7f593aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*yMdIUW1LFkFOqkW65yjWCA.png"/></div></figure><h2 id="a1aa" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">2.3.身份映射</h2><ul class=""><li id="27bd" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">执行逐元素加法运算以获得最终输出:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nr"><img src="../Images/8fb435852c2f7e9dc02118421009486a.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*Onh1Qc0ogNJfztRY7q3nEQ.png"/></div></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="c13c" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">3.<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>和<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">固态硬盘</a>的特性复用</h1><ul class=""><li id="4363" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">所提出的方法是一种<strong class="jr hj">通用解决方案</strong>，用于在基于深度卷积神经网络的检测器内构建迭代特征金字塔和门，因此非常容易应用于现有框架。</li></ul><h2 id="9974" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">3.1.GRF-DSOD</h2><ul class=""><li id="61b2" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">为<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>调整门控特征重用有两个步骤。</li><li id="4626" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">首先，迭代特征重用是为了替换<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>预测层中的密集连接。</li><li id="24f5" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">接下来，在每个预测层中添加门。</li></ul><h2 id="e3b3" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">3.2.GRF固态硬盘</h2><ul class=""><li id="e4bf" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">对于<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>，进行类似操作得到GFR-SSD。具体来说，<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>中的多余层被替换为预测层中的GFR结构和级联门。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="0de8" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">4.实验结果</h1><ul class=""><li id="c069" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">值得注意的是，该网络可以从刮擦开始训练，不需要ImageNet预训练。</li><li id="c0c1" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于VOC 2007，使用VOC 2007 trainval和VOC 2012 trainval(“07+12”)的联合来训练网络，并在VOC 2007测试集上进行测试。</li><li id="7d4f" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于VOC 2012，使用VOC 2012 trainval和VOC 2007 trainval + test对网络进行训练，并在VOC 2012测试集上进行测试。</li><li id="f1d6" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于COCO，训练集中有80k个图像，验证集中有40k个图像，测试集中有20k个图像(测试开发)。</li></ul><h2 id="b2bf" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">4.1.PASCAL VOC 2007上的烧蚀实验</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ns"><img src="../Images/47d1eb630d97b45185c2080c483cb731.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*u1au6kxMCFw8Vrs7k4JHJA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">PASCAL VOC 2007上的闸门结构设计烧蚀实验</strong></figcaption></figure><ul class=""><li id="83bd" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">在采用渠道注意、全局注意和身份映射后，我们分别获得了0.4%、0.2%和0.2%的增益。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nt"><img src="../Images/676a9550b8a32ebd1e39343d4a13d590.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*Ukjz2pzMZ3QmM2fzhYW_rw.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">PASCAL VOC 2007上的烧蚀实验</strong></figcaption></figure><ul class=""><li id="097d" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">没有门的<strong class="jr hj">特征金字塔的结果(78.6%)与GFR-DSOD320(第6行)相当，与基线(77.8%)相比提高了0.8%。</strong></li><li id="4e39" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">这表明我们的特征重用结构对提高最终检测性能有很大贡献。</li><li id="1794" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">添加没有迭代特征金字塔的门的<strong class="jr hj">的结果(78.6%)也优于基线结果0.8% mAP。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nu"><img src="../Images/2d43a79ee25526e1a214c1b047562d9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*uIvZDbybKFjf15qwHBzXRA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo"/><a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd jo">SSD</strong></a><strong class="bd jo">300的烧蚀实验从零开始</strong></figcaption></figure><ul class=""><li id="5395" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">还有，<strong class="jr hj"> GFR结构帮助原有的</strong><a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jr hj">SSD</strong></a><strong class="jr hj">大幅度提升性能。</strong></li></ul><h2 id="953c" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">4.2.PASCAL VOC 2007年和2012年的结果</h2><ul class=""><li id="91a1" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">在第二个表中，GFR-DSOD达到79.2%，比基线方法<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a> (77.8%)要好。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nv"><img src="../Images/01c282fff093c2914ed0558a09c212a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qrD1HK99N-20d-eP5GZrQw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">VOC 2007测试集上的检测实例有</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"><strong class="bd jo">DSOD</strong></a><strong class="bd jo">/GFR-DSOD型号</strong></figcaption></figure><ul class=""><li id="cdcb" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">所提出的方法在小物体和密集场景上都取得了更好的结果。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nw"><img src="../Images/4737117e48a8c06d5fcd6063be62c390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BXPWqcZhh1QqEPGBn3QQEg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">收敛速度比较</strong></figcaption></figure><ul class=""><li id="aee7" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">因此，GFR-DSOD比DSOD的收敛速度相对快38%。</li><li id="94d3" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于推理时间，使用<strong class="jr hj"> 300×300 </strong>输入，完整的<strong class="jr hj"> GFR-DSOD </strong>可以在批量为1的单个Titan X GPU上以<strong class="jr hj"> 17.5 fps </strong>运行图像。速度类似于<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a> 300，预测结构密集。</li><li id="ed98" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">当输入尺寸放大到<strong class="jr hj"> 320×320 </strong>时，速度下降到<strong class="jr hj"> 16.7 fps </strong>和<strong class="jr hj"> 16.3 fps </strong>(默认框更多)。</li><li id="e6d0" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">作为对比，<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a> 321运行速度为11.2 fps，<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a> 321运行速度为9.5 fps，采用<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> -101骨干网。方法比这两个竞争对手快多了。</li><li id="0ebc" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">在PASCAL VOC 2012 Comp3挑战赛上，GFR-DSOD的结果(72.5%)比之前最先进的<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a> (70.8%)高出1.7% mAP。</li><li id="e475" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">加入VOC 2007作为训练数据后，得到了77.5%的mAP。</li></ul><h2 id="9be8" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">4.3.可可小姐的结果</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nx"><img src="../Images/70dce73d58244c7c75d3b9a53e61faf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1KIDUJJWHnt8DVgFh15Pg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">在MS COCO 2015测试开发装置上比较两级检测器。</strong></figcaption></figure><ul class=""><li id="7ddc" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">GFR-DSOD可以用更少的参数(21.2米对21.9米)实现比基准方法<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"><strong class="jr hj">DSOD</strong></a><strong class="jr hj">【30.0%对29.4%)更高的<strong class="jr hj">性能。</strong></strong></li><li id="52af" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">结果与<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank">FPN</a>320/540【22】(30.0%对29.7%)不相上下，但<strong class="jr hj">模型的参数只有</strong><a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jr hj">【FPN</strong></a><strong class="jr hj">的1/6。</strong></li><li id="a399" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">所以，最终，GFR-DSOD在{0.5:0.95} mAP上胜过<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>、<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a>、<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>、<a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"> R-FCN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快R-CNN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------" rel="noopener" target="_blank"> DCN / DCNv1 </a>和<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h2 id="88be" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">参考</h2><p id="33a8" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy ny ka kb kc nz ke kf kg oa ki kj kk hb bi translated">【2019 BMVC】【GRF-DSOD &amp; GRF-SSD】<br/><a class="ae ix" href="https://arxiv.org/abs/1712.00886" rel="noopener ugc nofollow" target="_blank">通过门控特征重用从零开始改进对象检测</a></p><h2 id="61fa" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated">目标检测</h2><p id="3d38" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy ny ka kb kc nz ke kf kg oa ki kj kk hb bi translated"><strong class="jr hj"> 2014 </strong> : [ <a class="ae ix" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------">过食</a>][<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------">R-CNN</a>]<br/><strong class="jr hj">2015</strong>:[<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------">快R-CNN </a> ] [ <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快R-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a><br/><strong class="jr hj">2016<strong class="jr hj"> GBD-v1&amp;GBD-v2[<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae ix" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a><br/><strong class="jr hj">2017</strong>:[<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a?source=post_page---------------------------">NoC</a>][<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------">TDM</a>][<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>[<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 9000</a>] <br/><strong class="jr hj">2018</strong>:[<a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolov 3</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">Cascade R-CNN</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>]<br/><strong class="jr hj">2019</strong>:[<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5">DCN v2</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-rethinking-imagenet-pre-training-image-classification-object-detection-semantic-683f6575a2be" rel="noopener">反思ImageNet前期培训【T7</a></strong></strong></p><h2 id="7081" class="my ls hi bd jo mz na nb lw nc nd ne ma jy nf ng mc kc nh ni me kg nj nk mg nl bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>