<html>
<head>
<title>First 5 things to start learning PyTorch Tensors in Sagemaker Notebooks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Sagemaker 笔记本上开始学习 PyTorch 张量的前 5 件事</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/first-5-things-to-start-learning-pytorch-tensors-in-sagemaker-notebooks-67151576f6e3?source=collection_archive---------1-----------------------#2020-12-12">https://medium.com/nerd-for-tech/first-5-things-to-start-learning-pytorch-tensors-in-sagemaker-notebooks-67151576f6e3?source=collection_archive---------1-----------------------#2020-12-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5a5a8ce0a677f2d2f078eee1c3ed02ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSOKd_7RwGwoDFQP5zj10g.png"/></div></div></figure><p id="94cc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们开始 AWS 的火炬之旅吧！这 11 个基本功能分布在以下部分:</p><ul class=""><li id="3b89" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><strong class="is hj">创建张量:</strong>空()和零()</li><li id="ae2b" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">测量张量:</strong>大小()和形状()，总和()和维度</li><li id="1cf1" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">改变和复制张量:【Reshape()、view()和 randn()</strong></li><li id="c54e" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">修改张量</strong> s: Unsqueeze()</li><li id="1606" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">比较张量</strong>:元素式等式:Eq</li></ul><p id="8a09" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先让我们启动一个新的 sagemaker 实例:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kc"><img src="../Images/4c612520a2f996637ff461b4a0eff887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFvak7RG6NyCaA2_wJ1sqw.png"/></div></div></figure><p id="58e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在转到 Jupyter 实验室，使用 Pytorch 的内核导入或创建一个新的笔记本:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/c7673f0d7e116961d01eea11217b1f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybXO_1mMCQMhuNpK7CW8Fw.png"/></div></div></figure><p id="fbee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">选择合适的内核，在这种情况下可能是:conda_pytorch_p36</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/fdbe430ec314218dc0eaf8807a3b8730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8Zw3vFUG0J67c63MmMODA.png"/></div></div></figure><p id="b6ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里是我将从零开始的地方。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/27b0637bcb3668b45b009b17c5c81305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IH2x2hpcR51A4z_xwrKz2w.png"/></div></div></figure><p id="eebb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在开始之前，让我们安装并导入 PyTorch</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="1a57" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Uncomment and run the appropriate command for your operating system, if required</em></span><span id="f8ed" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv"># Linux / Binder</em><br/><em class="kv"># !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f </em><a class="ae kx" href="https://download.pytorch.org/whl/torch_stable.html" rel="noopener ugc nofollow" target="_blank"><em class="kv">https://download.pytorch.org/whl/torch_stable.html</em></a></span><span id="fe2a" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv"># Windows</em><br/><em class="kv"># !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f </em><a class="ae kx" href="https://download.pytorch.org/whl/torch_stable.html" rel="noopener ugc nofollow" target="_blank"><em class="kv">https://download.pytorch.org/whl/torch_stable.html</em></a></span><span id="d713" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv"># MacOS</em><br/><em class="kv"># !pip install numpy torch torchvision torchaudio</em></span></pre><p id="77be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从笔记本实例导入 Pytorch</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="002e" class="kp kq hi kl b fi kr ks l kt ku"><strong class="kl hj">import</strong> <strong class="kl hj">torch</strong></span></pre><h1 id="78e2" class="ky kq hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">函数 1 —空和零—如何初始化张量</h1><p id="702c" class="pw-post-body-paragraph iq ir hi is b it lv iv iw ix lw iz ja jb lx jd je jf ly jh ji jj lz jl jm jn hb bi translated">我们需要开始使用基本的 pytorch 函数，第一件事是创建我们的矩阵</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="9576" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Creates a 3 x 2 matrix which is empty</em><br/>a = torch.empty(3, 2)<br/>print(a)</span><span id="4538" class="kp kq hi kl b fi kw ks l kt ku">tensor([[1.5842e-35, 0.0000e+00],<br/>        [4.4842e-44, 0.0000e+00],<br/>        [       nan, 0.0000e+00]])</span></pre><p id="3951" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PyTorch 是如何为这个张量分配内存的。不管怎样，它不会抹去记忆中任何以前的内容。</p><p id="5b1a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">默认情况下，初始化张量时使用的是 float32 dtype。可以在这里回顾:<a class="ae kx" href="https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type" rel="noopener ugc nofollow" target="_blank">https://py torch . org/docs/stable/generated/torch . set _ default _ tensor _ type . html # torch . set _ default _ tensor _ type</a></p><p id="1597" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是你也可以开始使用 torch.zeros</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="315a" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Applying the zeros function and </em><br/><em class="kv"># storing the resulting tensor</em><br/> <br/>a = torch.zeros([3, 5]) <br/>print("a = ", a) <br/>  <br/>b = torch.zeros([2, 4]) <br/>print("b = ", b) <br/>  <br/>c = torch.zeros([4, 1]) <br/>print("c = ", c) <br/>  <br/>d = torch.zeros([4, 4, 2]) <br/>print("d = ", d)</span></pre><p id="0fd6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结果将是:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="8a68" class="kp kq hi kl b fi kr ks l kt ku">a =  tensor([[0., 0., 0., 0., 0.],<br/>        [0., 0., 0., 0., 0.],<br/>        [0., 0., 0., 0., 0.]])<br/>b =  tensor([[0., 0., 0., 0.],<br/>        [0., 0., 0., 0.]])<br/>c =  tensor([[0.],<br/>        [0.],<br/>        [0.],<br/>        [0.]])<br/>d =  tensor([[[0., 0.],<br/>         [0., 0.],<br/>         [0., 0.],<br/>         [0., 0.]],</span><span id="0d25" class="kp kq hi kl b fi kw ks l kt ku">[[0., 0.],<br/>         [0., 0.],<br/>         [0., 0.],<br/>         [0., 0.]],</span><span id="24a2" class="kp kq hi kl b fi kw ks l kt ku">[[0., 0.],<br/>         [0., 0.],<br/>         [0., 0.],<br/>         [0., 0.]],</span><span id="0f60" class="kp kq hi kl b fi kw ks l kt ku">[[0., 0.],<br/>         [0., 0.],<br/>         [0., 0.],<br/>         [0., 0.]]])</span></pre><p id="eba2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个张量用零填充，因此 PyTorch 分配内存并对其中的张量元素进行零初始化</p><p id="753a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您不能更改创建张量的方式，如果您创建了零张量，请确保没有引用任何其他张量。</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="94a6" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># correctly initialized</em><br/>a = torch.empty(3,3)<br/>print(a)</span><span id="f7b7" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#also correct</em><br/>b = torch.empty(1,2,3)</span><span id="bc8f" class="kp kq hi kl b fi kw ks l kt ku">print(b)</span></pre><p id="c5b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结果将是:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="1764" class="kp kq hi kl b fi kr ks l kt ku">tensor([[2.4258e-35, 0.0000e+00, 1.5975e-43],<br/>        [1.3873e-43, 1.4574e-43, 6.4460e-44],<br/>        [1.4153e-43, 1.5274e-43, 1.5695e-43]])<br/>tensor([[[2.3564e-35, 0.0000e+00, 1.4013e-45],<br/>         [1.4574e-43, 6.4460e-44, 1.4153e-43]]])</span></pre><p id="2559" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们看看，为什么不能使用 zeros 函数:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="3ef4" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># incorrect reference, you must create a new one</em><br/>c = torch.zeros(b,1) <br/>print("c = ", c)</span></pre><p id="19f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="0299" class="kp kq hi kl b fi kr ks l kt ku">---------------------------------------------------------------------------<br/>TypeError                                 Traceback (most recent call last)<br/>&lt;ipython-input-35-c044e2995879&gt; in &lt;module&gt;()<br/><strong class="kl hj">      1</strong> # incorrect reference, you must create a new one<br/>----&gt; 2 c = torch.zeros(b,1)<br/><strong class="kl hj">      3</strong> print("c = ", c)</span><span id="4edb" class="kp kq hi kl b fi kw ks l kt ku">TypeError: zeros() received an invalid combination of arguments - got (Tensor, int), but expected one of:<br/> * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)<br/> * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)</span></pre><p id="8cf2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们仔细回顾一下零点方法:</p><p id="b47e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的文件说:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="ec72" class="kp kq hi kl b fi kr ks l kt ku">Syntax: torch.zeros(size, out=None)</span><span id="9370" class="kp kq hi kl b fi kw ks l kt ku">Parameters:<br/>size: a sequence of integers defining the shape of the output tensor<br/>out (Tensor, optional): the output tensor</span><span id="f27c" class="kp kq hi kl b fi kw ks l kt ku">Return type: A tensor filled with scalar value 0, of same shape as size.</span></pre><p id="d14a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">torch.zeros 和 torch.empty 是开始使用 pytorch 张量并学习一点矩阵和向量的第一批函数</p><h1 id="e377" class="ky kq hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">函数 2——张量大小、形状和维度运算</h1><p id="1d43" class="pw-post-body-paragraph iq ir hi is b it lv iv iw ix lw iz ja jb lx jd je jf ly jh ji jj lz jl jm jn hb bi translated">让我们理解 Pytorch 中的维度。</p><p id="45b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们创建一些张量，并确定每个张量的大小</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="63ce" class="kp kq hi kl b fi kr ks l kt ku"><strong class="kl hj">import</strong> <strong class="kl hj">torch</strong> <br/><em class="kv"># Create a tensor from data</em><br/>c = torch.tensor([[3.2 , 1.6, 2], [1.3, 2.5 , 6.9]])<br/>print(c)</span><span id="6188" class="kp kq hi kl b fi kw ks l kt ku">print(c.size())</span></pre><p id="0dea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="3fee" class="kp kq hi kl b fi kr ks l kt ku">tensor([[3.2000, 1.6000, 2.0000],<br/>        [1.3000, 2.5000, 6.9000]])<br/>torch.Size([2, 3])</span></pre><p id="aa92" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看 torch.shape，并仔细看看这里是如何给出大小的</p><p id="90a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在在下一个例子中，让我们用形状函数来得到张量的大小</p><p id="1d55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="03f9" class="kp kq hi kl b fi kr ks l kt ku">x = torch.tensor([<br/>     [1, 2, 3],<br/>     [4, 5, 6]<br/>   ])<br/>x.shape<br/>torch.Size([2, 3])</span></pre><p id="fdd7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="5821" class="kp kq hi kl b fi kr ks l kt ku">torch.Size([2, 3])</span></pre><p id="9169" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">形状和大小给了我们张量相同的正确维数。</p><p id="6d88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，我们有一个三维张量(3 维)</p><p id="713f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">维度 0 维度 1 和维度 2</p><p id="038b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们创建一个新的张量:</p><p id="b92f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="be0e" class="kp kq hi kl b fi kr ks l kt ku">y = torch.tensor([<br/>     [<br/>       [1, 2, 3],<br/>       [4, 5, 6]<br/>     ],<br/>     [<br/>       [1, 2, 3],<br/>       [4, 5, 6]<br/>     ],<br/>     [<br/>       [1, 2, 3],<br/>       [4, 5, 6]<br/>     ]<br/>   ])</span><span id="f7ee" class="kp kq hi kl b fi kw ks l kt ku">y.shape</span></pre><p id="6701" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="90a8" class="kp kq hi kl b fi kr ks l kt ku">torch.Size([3, 2, 3])</span></pre><p id="c758" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看如何使用 3d 张量对每个维度层进行操作，看看它的表现如何</p><p id="4639" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="8bac" class="kp kq hi kl b fi kr ks l kt ku">sum1 = torch.sum(y, dim=0)<br/>print(sum1)</span><span id="4f7f" class="kp kq hi kl b fi kw ks l kt ku">sum2 = torch.sum(y, dim=1)<br/>print(sum2)</span><span id="c848" class="kp kq hi kl b fi kw ks l kt ku">sum3 = torch.sum(y, dim=2)<br/>print(sum3)</span><span id="d060" class="kp kq hi kl b fi kw ks l kt ku">tensor([[ 3,  6,  9],<br/>        [12, 15, 18]])<br/>tensor([[5, 7, 9],<br/>        [5, 7, 9],<br/>        [5, 7, 9]])<br/>tensor([[ 6, 15],<br/>        [ 6, 15],<br/>        [ 6, 15]])</span></pre><p id="8584" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们现在可以看到，随着我们的发展，3d 张量变得更加复杂，我们可以在每个维度内执行自定义操作</p><p id="9247" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目前限制为 3 个 dims，因此我们无法执行此操作:</p><p id="c09b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[]</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="5309" class="kp kq hi kl b fi kr ks l kt ku">sum2 = torch.sum(y, dim=3)<br/>print(sum1)</span></pre><p id="73ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out [ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="74ad" class="kp kq hi kl b fi kr ks l kt ku">---------------------------------------------------------------------------<br/>IndexError                                Traceback (most recent call last)<br/>&lt;ipython-input-56-7caf723c6102&gt; in &lt;module&gt;()<br/>----&gt; 1 sum2 = torch.sum(y, dim=3)<br/><strong class="kl hj">      2</strong> print(sum1)</span><span id="45aa" class="kp kq hi kl b fi kw ks l kt ku"><strong class="kl hj">IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)</strong></span></pre><p id="0a3d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了增加更多的维度层，我们可以回顾 unsqueeze 函数→。unsqueeze()但是现在让我们来看看下一个基本函数</p><h1 id="adff" class="ky kq hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">功能 3 —整形、查看和随机</h1><p id="7227" class="pw-post-body-paragraph iq ir hi is b it lv iv iw ix lw iz ja jb lx jd je jf ly jh ji jj lz jl jm jn hb bi translated">NunPy 中有一个名为 ndarray.reshape()的函数用于整形数组。</p><p id="e2b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在在 pytorch 中，有 torch.view(张量)也是为了同样的目的，但同时，也有 torch.reshape(张量)。</p><p id="64d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们弄清楚它们之间的区别，以及何时应该使用它们中的任何一个。</p><p id="d3b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们将使用一个新的函数来随机化我们的张量。</p><p id="460f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="b7d1" class="kp kq hi kl b fi kr ks l kt ku"><strong class="kl hj">import</strong> <strong class="kl hj">torch</strong> <br/>x = torch.randn(5, 3)</span><span id="0e8f" class="kp kq hi kl b fi kw ks l kt ku">print(x)</span></pre><p id="2122" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out []</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="21bd" class="kp kq hi kl b fi kr ks l kt ku">tensor([[-0.5793,  0.6999,  1.7417],<br/>        [-0.9810,  0.0626,  0.4100],<br/>        [-0.6519, -0.0595, -1.2156],<br/>        [-0.3973, -0.3103,  1.6253],<br/>        [ 0.2775, -0.0045, -0.2985]])</span></pre><p id="5dcf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是 torch.randm 的基本用法，所以现在让我们使用另一个变量“y”的视图并描述所有元素</p><p id="ad46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="a1d0" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Return a view of the x, but only having </em><br/><em class="kv"># one dimension and max number of elements</em><br/>y = x.view(5 * 3)<br/> <br/><em class="kv">#lets see the size of every tensor</em><br/>print("lets see the size of every tensor")<br/>print('Size of x:', x.size())<br/>print('Size of y:', y.size())<br/> <br/><em class="kv">#and the elements of very tensor to compare</em><br/>print("and the elements of very tensor to compare")<br/>print("X:", x)<br/>print("Y:", y)</span></pre><p id="452d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out []</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="923f" class="kp kq hi kl b fi kr ks l kt ku">lets see the size of every tensor<br/>Size of x: torch.Size([5, 3])<br/>Size of y: torch.Size([15])<br/>and the elements of very tensor to compare<br/>X: tensor([[-0.5793,  0.6999,  1.7417],<br/>        [-0.9810,  0.0626,  0.4100],<br/>        [-0.6519, -0.0595, -1.2156],<br/>        [-0.3973, -0.3103,  1.6253],<br/>        [ 0.2775, -0.0045, -0.2985]])<br/>Y: tensor([-0.5793,  0.6999,  1.7417, -0.9810,  0.0626,  0.4100, -0.6519, -0.0595,<br/>        -1.2156, -0.3973, -0.3103,  1.6253,  0.2775, -0.0045, -0.2985])</span></pre><p id="2c88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">看看 Y 张量，它只有一维。因此，对于某些操作来说，查看另一个张量可能很困难</p><p id="4011" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们使用整形来复制 X 的精确尺寸</p><p id="ba5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="8540" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Get back the original tensor with reshape()</em><br/>z = y.reshape(5, 3)<br/>print(z)</span><span id="4e67" class="kp kq hi kl b fi kw ks l kt ku">tensor([[-0.2927,  0.0329,  0.8485],<br/>        [ 1.9581,  0.8313, -0.1529],<br/>        [-0.2330, -0.1887,  1.8206],<br/>        [ 1.5252,  1.0909,  0.0547],<br/>        [-0.1231, -0.4238, -0.6724]])</span></pre><p id="ae01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们不仅可以重塑原始形状，还可以通过一些与最大元素相关的有限操作来改变尺寸:</p><p id="7f57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，让我们重塑为 1 维以上</p><p id="9ef6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="8e29" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Get back the original tensor with reshape()</em><br/>z = y.reshape(15)<br/>print(z)</span><span id="6aaa" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#reshaping 15 elements, 1 dim</em><br/>z = y.reshape(3*5)<br/>print(z)</span><span id="fa5d" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#reshaping in different order, 3 dimensions</em><br/>z = y.reshape(3,5)<br/>print(z)</span><span id="2468" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#Reshaping with more dimensions but its 15 elements always</em><br/>z = y.reshape(3,5,1)<br/>print(z)</span><span id="e77d" class="kp kq hi kl b fi kw ks l kt ku">tensor([-0.5793,  0.6999,  1.7417, -0.9810,  0.0626,  0.4100, -0.6519, -0.0595,<br/>        -1.2156, -0.3973, -0.3103,  1.6253,  0.2775, -0.0045, -0.2985])<br/>tensor([-0.5793,  0.6999,  1.7417, -0.9810,  0.0626,  0.4100, -0.6519, -0.0595,<br/>        -1.2156, -0.3973, -0.3103,  1.6253,  0.2775, -0.0045, -0.2985])<br/>tensor([[-0.5793,  0.6999,  1.7417, -0.9810,  0.0626],<br/>        [ 0.4100, -0.6519, -0.0595, -1.2156, -0.3973],<br/>        [-0.3103,  1.6253,  0.2775, -0.0045, -0.2985]])<br/>tensor([[[-0.5793],<br/>         [ 0.6999],<br/>         [ 1.7417],<br/>         [-0.9810],<br/>         [ 0.0626]],</span><span id="7db4" class="kp kq hi kl b fi kw ks l kt ku">        [[ 0.4100],<br/>         [-0.6519],<br/>         [-0.0595],<br/>         [-1.2156],<br/>         [-0.3973]],</span><span id="93be" class="kp kq hi kl b fi kw ks l kt ku">        [[-0.3103],<br/>         [ 1.6253],<br/>         [ 0.2775],<br/>         [-0.0045],<br/>         [-0.2985]]])</span></pre><p id="ee05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们整形超过张量中元素的数量:</p><p id="c50f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="b0e6" class="kp kq hi kl b fi kr ks l kt ku">z = y.reshape(16)<br/>print(z)</span></pre><p id="b9e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out []</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="c697" class="kp kq hi kl b fi kr ks l kt ku">---------------------------------------------------------------------------<br/>RuntimeError Traceback (most recent call last)<br/>&lt;ipython-input-78-c7ae174fce73&gt; in &lt;module&gt;()<br/>----&gt; 1 z = y.reshape(16)<br/><strong class="kl hj">      2</strong> print(z)</span><span id="30e4" class="kp kq hi kl b fi kw ks l kt ku">RuntimeError: shape '[16]' is invalid for input of size 15</span></pre><p id="3cff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于 z = y . shape(3 * 6)或放置更多张量中不存在的元素，它也会失败。</p><p id="b66e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们继续下一节。</p><h1 id="5f26" class="ky kq hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">功能 4 —取消队列()</h1><p id="c94a" class="pw-post-body-paragraph iq ir hi is b it lv iv iw ix lw iz ja jb lx jd je jf ly jh ji jj lz jl jm jn hb bi translated">主要是，它允许我们在您定义的特定索引上添加更多的维度。</p><p id="69f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们来看看:</p><p id="ccf2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="25e1" class="kp kq hi kl b fi kr ks l kt ku"><strong class="kl hj">import</strong> <strong class="kl hj">torch</strong></span><span id="5d98" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#dim=1, that is (3)</em><br/>x = torch.tensor([1, 2, 3])<br/>print('x: ', x)<br/>print('x.size: ', x.size())</span><span id="9e07" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#x1 becomes a matrix of (3,1)</em><br/>x1 = torch.unsqueeze(x, 1)<br/>print('x1: ', x1)<br/>print('x1.size: ', x1.size())</span></pre><p id="984e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out []</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="f4c2" class="kp kq hi kl b fi kr ks l kt ku">x:  tensor([1, 2, 3])<br/>x.size:  torch.Size([3])<br/>x1:  tensor([[1],<br/>        [2],<br/>        [3]])<br/>x1.size:  torch.Size([3, 1])<br/>x2:  tensor([[1, 2, 3]])<br/>x2.size:  torch.Size([1, 3])</span></pre><p id="5fc2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的初始张量是张量([1，2，3])，输出大小是[3]。</p><p id="7f0c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后我们继续用 unsqueeze 运算增加 1 维，即 torch.unsqueeze(x，1)，x1 的大小是[3，1]。</p><p id="5386" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="047e" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv">#x2 becomes a matrix of (1,3)</em><br/>x2 = torch.unsqueeze(x, 0)<br/>print('x2: ', x2)<br/>print('x2.size: ', x2.size())</span></pre><p id="f7a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out []</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="efb4" class="kp kq hi kl b fi kr ks l kt ku">x2:  tensor([[1, 2, 3]])<br/>x2.size:  torch.Size([1, 3])</span></pre><p id="4de3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们执行 torch.unsqueeze(x，0)时，x2 的大小是[1，3]。</p><p id="eb6d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="b89e" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Example 3 - breaking (to illustrate when it breaks)</em><br/></span><span id="a96c" class="kp kq hi kl b fi kw ks l kt ku">print(x.unsqueeze())</span></pre><p id="8806" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out []</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="21da" class="kp kq hi kl b fi kr ks l kt ku">---------------------------------------------------------------------------<br/>TypeError  Traceback (most recent call last)<br/>&lt;ipython-input-89-5a320a828907&gt; in &lt;module&gt;()<br/><strong class="kl hj">      2</strong> <br/><strong class="kl hj">      3</strong> <br/>----&gt; 4 print(x.unsqueeze())</span><span id="712e" class="kp kq hi kl b fi kw ks l kt ku">TypeError: unsqueeze() missing 1 required positional arguments: "dim"</span></pre><p id="b4b6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们必须正确地指定维度，虽然我们只是添加 1 个维度，但有必要这样放置:x . un queze(0)</p><h1 id="df0d" class="ky kq hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">功能 5 —火炬 Eq(元素间相等)</h1><p id="6c86" class="pw-post-body-paragraph iq ir hi is b it lv iv iw ix lw iz ja jb lx jd je jf ly jh ji jj lz jl jm jn hb bi translated">这个函数属于比较类别，它计算元素的相等性，并返回一个布尔张量。如果相等，则为真，否则为假。</p><p id="3d25" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们回顾一下如何处理不同大小的张量:</p><p id="8042" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="6de7" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Example 1 - working </em><br/>x1 = torch.tensor([[1, 2], [3, 4.]])<br/>x2 = torch.tensor([[2, 2], [2, 5]])<br/>x3 = torch.randn(3,5)</span><span id="b65e" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#size x1 and z2</em><br/>print(x1.size())<br/>print(x2.size())</span><span id="102e" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv"># tensors 1 and 2</em><br/>print(x1)<br/>print(x2)<br/></span><span id="de8b" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#size x3</em><br/>print(x3.size())</span><span id="d375" class="kp kq hi kl b fi kw ks l kt ku"><em class="kv">#tensors 3</em><br/>print(x3)</span><span id="2512" class="kp kq hi kl b fi kw ks l kt ku">torch.eq(x1,x2)</span><span id="f963" class="kp kq hi kl b fi kw ks l kt ku">torch.Size([2, 2])<br/>torch.Size([2, 2])<br/>tensor([[1., 2.],<br/>        [3., 4.]])<br/>tensor([[2, 2],<br/>        [2, 5]])<br/>torch.Size([3, 5])<br/>tensor([[-1.3040, -0.4658, -0.5269,  0.7409,  0.9135],<br/>        [ 1.0780,  2.0584, -0.9629, -1.1412, -0.3105],<br/>        [ 0.3613, -1.4196,  2.1145,  0.3649,  0.2037]])</span></pre><p id="4ce8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="433b" class="kp kq hi kl b fi kr ks l kt ku">tensor([[False,  True],<br/>        [False, False]])</span></pre><p id="559b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">x1 和 x3 具有相同的大小，但是 x3 是[3，5]，具有更大的大小。</p><p id="26a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">比较 x1 和 x2 就可以了。</p><p id="abd4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="9485" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Example 2 - working (with broadcasting)</em><br/>x4 = torch.tensor([[1, 2], [3, 4]])<br/>print(x4.size())<br/>x5 = torch.tensor([2, 5])<br/>print(x5.size())<br/>torch.eq(x4, x5)</span><span id="9a44" class="kp kq hi kl b fi kw ks l kt ku">torch.Size([2, 2])<br/>torch.Size([2])</span></pre><p id="a4fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Out[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="942a" class="kp kq hi kl b fi kr ks l kt ku">tensor([[False, False],<br/>        [False, False]])</span></pre><p id="56b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们也可以比较不同的大小，只有当第二个大小为[2]的 conf 值(在本例中为 x5)与第一个大小为[2，2]的 seconf 值是可扩展的</p><p id="250f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[ ]:</p><pre class="kd ke kf kg fd kk kl km kn aw ko bi"><span id="6b86" class="kp kq hi kl b fi kr ks l kt ku"><em class="kv"># Example 3 - breaking (to illustrate when it breaks)</em><br/>x6 = torch.tensor([[0, 2, 4], [3, 4, 5]])<br/>print(x6.size())<br/>x7 = torch.tensor([[2, 3], [2, 4]])<br/>print(x7.size())</span><span id="70b0" class="kp kq hi kl b fi kw ks l kt ku">torch.eq(x6, x3)</span><span id="dd1b" class="kp kq hi kl b fi kw ks l kt ku">torch.Size([2, 3])<br/>torch.Size([2, 2])</span><span id="8019" class="kp kq hi kl b fi kw ks l kt ku">---------------------------------------------------------------------------<br/>RuntimeError                              Traceback (most recent call last)<br/>&lt;ipython-input-98-ac2ad1ecd5b0&gt; in &lt;module&gt;()<br/><strong class="kl hj">      5</strong> print(x7.size())<br/><strong class="kl hj">      6</strong> <br/>----&gt; 7 torch.eq(x6, x3)</span><span id="5a26" class="kp kq hi kl b fi kw ks l kt ku">RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1</span></pre><p id="dd81" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，如果第二个参数形状不能与第一个参数一起转换，我们就不能比较不同的大小。</p><h1 id="4204" class="ky kq hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h1><p id="7e1f" class="pw-post-body-paragraph iq ir hi is b it lv iv iw ix lw iz ja jb lx jd je jf ly jh ji jj lz jl jm jn hb bi translated">我们回顾了涵盖 10 多个 PyTorch 函数的 5 个基本主题。</p><p id="fa50" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下一篇文章中，我将讨论线性回归。</p><h1 id="3a38" class="ky kq hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">参考链接</h1><p id="3dd6" class="pw-post-body-paragraph iq ir hi is b it lv iv iw ix lw iz ja jb lx jd je jf ly jh ji jj lz jl jm jn hb bi translated">提供你的参考资料和其他关于张量的有趣文章的链接</p><ul class=""><li id="a0c1" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">张量运算的官方文档:<a class="ae kx" href="https://pytorch.org/docs/stable/torch.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/torch.html</a></li><li id="b1bb" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">这里的挤压和不挤压功能:<a class="ae kx" href="https://www.programmersought.com/article/65705814717/" rel="noopener ugc nofollow" target="_blank">https://www.programmersought.com/article/65705814717/</a></li><li id="702a" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">了解 Pytorch 中的维度:<a class="ae kx" href="https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be" rel="noopener" target="_blank">https://towards data science . com/understanding-dimensions-in-py torch-6edf 9972d 3 be</a></li></ul><p id="2e15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"># AWS # reinvention 2020 # AWS Peru # awsug Peru # AWS cloud</p><p id="cba3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Carlos Cortez—AWS UG perúLeader/AWS ML 社区构建者<br/>ccortez @ AWS . PE<br/><a class="ae kx" href="https://dev.to/ccortezb" rel="noopener ugc nofollow" target="_blank">@ ccortezb</a><br/>播客:imperiocloud.com @ imperiocloud<br/>twitch.tv/awsugperu<br/>cennticloud.thinkific.com</p><div class="ma mb ez fb mc md"><a href="https://www.linkedin.com/in/carloscortezcloud/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hj fi z dy mi ea eb mj ed ef hh bi translated">Carlos Cortez ML 工程师—高级云架构师—专业服务集群— BGH 技术合作伙伴|…</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">大家好，我是 AWS 用户组秘鲁🇵🇪 (2014)的创始人，也是 AWS 解决方案架构师认证…</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">www.linkedin.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr io md"/></div></div></a></div></div></div>    
</body>
</html>