<html>
<head>
<title>Design Streaming Data pipeline using Kafka</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Kafka设计流数据管道</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/design-data-pipeline-and-streaming-using-kafka-5c6fb1fdc122?source=collection_archive---------3-----------------------#2021-04-25">https://medium.com/nerd-for-tech/design-data-pipeline-and-streaming-using-kafka-5c6fb1fdc122?source=collection_archive---------3-----------------------#2021-04-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/670f37a43b7ef65083b6d5c2d4e0b0ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJtol1j1RVArfh1jgnxUWA.png"/></div></div></figure><blockquote class="iq ir is"><p id="6ca0" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这篇文章中，我将解释一些数据管道用例，以及Kafka如何在concpt教授如何建立数据流管道中发挥作用。</p></blockquote><p id="b396" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">在许多大中型项目中，您需要使用某种在不同情况下有效的数据系统，例如，您需要SQLDB来处理事务并保证软件中的数据一致性，您需要不同的模型来报告、用户事件和CDC(更改数据捕获)等，您可以通过许多不同的方式来完成这项工作，例如，有些SQLDB系统支持实时同步(CDC ),如Postgres等，但在本文中，我将重点讨论Kafka、Kafka streams和Kafka connect在这种情况下的有用程度。假设我们有一个软件，如产品评级系统，我们有SQLDB，它将客户评级存储在SQL结构中，另一方面，我们需要将这些数据发送到我们的报告数据系统，将用户事件发送到Casandra DB，并记录到弹性搜索或时间序列，如Influx DB等，我们有不同的系统，例如通知系统，它可以监听低于3的用户评级，以联系他们，以及许多不同的用例。</p><p id="4b9a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">解决这个问题的传统思路是编写某种从SQL到elastic API的数据传输应用程序，不管有没有Kafka等等。</p><p id="2548" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">那么什么是卡夫卡？</strong></p><p id="5eb8" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">Kafka本身不是本文的重点，但简而言之，Apache Kafka是一个开源的分布式事件流平台，被数千家公司用于高性能数据管道、流分析、数据集成和任务关键型应用程序。</p><p id="1d63" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">为什么是卡夫卡？</strong></p><p id="fe1b" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">可扩展、耐用和容错的Kafka可以与Spark Streaming、Storm、HBase、Flink和Spark配合使用，以实时摄取、分析和处理流数据。Kafka是一种数据流，用于供给Hadoop BigData湖。Kafka经纪人支持海量信息流，以便在Hadoop或Spark中进行低延迟的后续分析。</p><p id="382b" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我推荐阅读我的文章来理解卡夫卡的关键概念</p><div class="jv jw ez fb jx jy"><a href="https://regoo707.medium.com/understanding-kafka-key-concepts-7ba021a3c08c" rel="noopener follow" target="_blank"><div class="jz ab dw"><div class="ka ab kb cl cj kc"><h2 class="bd hj fi z dy kd ea eb ke ed ef hh bi translated">理解卡夫卡的关键概念</h2><div class="kf l"><h3 class="bd b fi z dy kd ea eb ke ed ef dx translated">首先，这篇文章是写给现在使用Kafka或者可能会在未来的项目中使用Kafka的人的…</h3></div><div class="kg l"><p class="bd b fp z dy kd ea eb ke ed ef dx translated">regoo707.medium.com</p></div></div></div></a></div><p id="1da8" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">现在，我们如何减少构建在数据系统之间传输数据的服务的工作量，正如你在顶部的设计中看到的，我使用了Kafka connect。</p><p id="4aec" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">什么是卡夫卡连接？</strong></p><p id="32e4" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">Kafka Connect是一个工具，用于在Apache Kafka和其他数据系统之间可扩展和可靠地传输数据。它使得快速定义将大型数据集移入和移出Kafka的连接器变得简单。Kafka Connect可以接收整个数据库或从所有应用服务器收集指标到Kafka主题中，使数据可用于低延迟的流处理。导出连接器可以将Kafka主题中的数据传递到二级索引(如Elasticsearch)或批处理系统(如Hadoop)中，以便进行离线分析。</p><p id="0c6c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">【Kafka connect如何工作？</p><p id="44d2" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">您可以将Kafka Connect部署为在单台机器上运行作业的独立进程(例如，日志收集)，或者部署为支持整个组织的分布式、可伸缩、容错服务。Kafka Connect提供了较低的准入门槛和较低的运营开销。您可以从用于开发和测试的独立环境开始，然后扩展到完整的生产环境，以支持大型组织的数据管道。</p><p id="0b74" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">Kafka Connect包括两种类型的连接器</p><ul class=""><li id="2c90" class="kh ki hi iw b ix iy jb jc js kj jt kk ju kl jr km kn ko kp bi translated"><strong class="iw hj">源连接器</strong>接收整个数据库，并将表更新传输到Kafka主题。source connector还可以从所有应用程序服务器收集指标，并将它们存储在Kafka主题中，从而使数据可用于低延迟的流处理。</li><li id="4b6e" class="kh ki hi iw b ix kq jb kr js ks jt kt ju ku jr km kn ko kp bi translated"><strong class="iw hj"> Sink connector </strong>将Kafka主题的数据传递到二级索引(如Elasticsearch)或批处理系统(如Hadoop)以进行离线分析。</li></ul><p id="bc37" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">正如你在上面的架构中看到的，我们使用了SQLDB source connector，可能是Postgres、MSSQL、MySQL或其他，我们使用了另一个sink connect将数据从Kafka推送到Elastic search，等等。</p><p id="795a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们需要什么？</p><p id="6a89" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">1-带有zookeeper的Kafka集群您可以使用Docker进行POC</p><p id="f38b" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">2- debezium Kafka连接器在本文中，我以Postgres为例，所以我使用debezium Kafka连接器来捕获从Postgres到Kafka的数据更改(插入/更新/删除),以便稍后使用elastic sink connect将其发送到elastic</p><p id="9c42" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">3-模式注册汇合模式注册为您的元数据提供了一个服务层。它为存储和检索Avro、JSON模式和其他模式提供了一个RESTful接口。它基于指定的主题名称策略存储所有模式的版本化历史，提供多种兼容性设置，并允许根据配置的兼容性设置和对这些模式类型的扩展支持来发展模式。它提供了插入Apache Kafka客户端的序列化程序，这些客户端处理以任何支持的格式发送的Kafka消息的模式存储和检索。在本文中，我们将使用Avro。</p><p id="be5a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">4- JAVA(可选)，因为我们讨论的是仅在java Kafka客户端上支持的流应用Kafka-streams API，也许您会发现其他编程语言上的某种客户端支持流API，但我不认为它像JAVA客户端一样强大或完整，如果我们将编写像通知系统一样的流API，我们只使用JAVA her。</p><p id="5fde" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">5- KAFKA-SQL(可选)Confluent KSQL是针对Apache Kafka实现实时数据处理的流式SQL引擎，KSQL是可伸缩的、弹性的、容错的，并且它支持广泛的流式操作，包括数据过滤、转换、聚合、连接、窗口等等。</p><p id="96cc" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">卡夫卡流是什么？</strong></p><p id="2140" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">Kafka Streams是一个用于构建流应用程序的库，特别是将输入Kafka主题转换为输出Kafka主题的应用程序(或者调用外部服务，或者更新数据库，等等)。它允许您以分布式和容错的方式用简洁的代码来实现这一点。流处理是一种计算机编程范式，相当于数据流编程、事件流处理和反应式编程，它允许一些应用程序更容易地利用有限形式的并行处理。Kafka流支持某种无状态API，如过滤器、映射、平面映射..另一方面支持有状态API，如计数、窗口、分组等</p><p id="e060" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">因此，在运行所有组件后，我们需要使用docker或其他方式，我们需要做以下工作</p><p id="2f9b" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">1-在SQL source connect上创建作业，该作业捕获对Kafka的表单表的更改，为此我们将调用connect API</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="017b" class="le lf hi la b fi lg lh l li lj"><a class="ae lk" href="http://localhost:8083/connectors" rel="noopener ugc nofollow" target="_blank">http://localhost:8083/connectors</a> POST<br/>{</span><span id="2fae" class="le lf hi la b fi ll lh l li lj">"name":"rating-system-connector",</span><span id="d9e0" class="le lf hi la b fi ll lh l li lj">"config":{</span><span id="98d6" class="le lf hi la b fi ll lh l li lj">"connector.class":"io.debezium.connector.postgresql.PostgresConnector",</span><span id="4150" class="le lf hi la b fi ll lh l li lj">"database.hostname":"localhost",</span><span id="220c" class="le lf hi la b fi ll lh l li lj">"database.port":"5433",</span><span id="4094" class="le lf hi la b fi ll lh l li lj">"database.user":"postgres",</span><span id="d689" class="le lf hi la b fi ll lh l li lj">"database.password":"P@ssw0rd",</span><span id="e7f2" class="le lf hi la b fi ll lh l li lj">"database.dbname":"rating_system_demo",</span><span id="2b02" class="le lf hi la b fi ll lh l li lj">"database.server.name":"ratingserver",</span><span id="8916" class="le lf hi la b fi ll lh l li lj">"table.whitelist":"rating.rates",</span><span id="e663" class="le lf hi la b fi ll lh l li lj">"key.converter.schemas.enable":"true",</span><span id="5597" class="le lf hi la b fi ll lh l li lj">"key.converter": "org.apache.kafka.connect.storage.StringConverter",</span><span id="49e5" class="le lf hi la b fi ll lh l li lj">"value.converter": "io.confluent.connect.avro.AvroConverter",</span><span id="6f85" class="le lf hi la b fi ll lh l li lj">"value.converter.schema.registry.url": "http://schema-registry:8081",</span><span id="60e9" class="le lf hi la b fi ll lh l li lj">"value.converter.schemas.enable": "true"</span><span id="0849" class="le lf hi la b fi ll lh l li lj">}</span><span id="e2de" class="le lf hi la b fi ll lh l li lj">}</span></pre><p id="3cee" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们调用connect API来创建一个后台作业，并告知connect有关数据库连接的信息，如服务器、用户名、密码、数据库名称、表等，以及模式注册URL和转换器类型AVRO，如我们之前在模式注册中所解释的。(<strong class="iw hj"> Avro </strong>是一个开源的数据序列化系统，有助于系统、编程语言和处理框架之间的数据交换。<strong class="iw hj"> Avro </strong>帮助定义数据的二进制格式，并将其映射到您选择的编程语言)</p><p id="c798" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">通过创建一个作业，表中所做的每一个更改都将通过after/before模式推送到Kafka，例如，如果您插入一个新记录，则在对象为null之后和之前将包含作为JSON对象结构的插入记录数据，在更新之前和之后以及删除之前等等。此外，如果您只是不时地对表单数据进行快照，例如，每隔7天，您可以使用合流JDBSOURCE连接器，在这种情况下，您也可以调用另一个连接器来创建作业</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="4bf2" class="le lf hi la b fi lg lh l li lj">{</span><span id="e6b0" class="le lf hi la b fi ll lh l li lj">"name": "jdbc_source_mysql_01",</span><span id="8397" class="le lf hi la b fi ll lh l li lj">"config": {</span><span id="c64b" class="le lf hi la b fi ll lh l li lj">"connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",</span><span id="741d" class="le lf hi la b fi ll lh l li lj">"connection.url": "jdbc:postgres://localhost:5432/customers",</span><span id="67c1" class="le lf hi la b fi ll lh l li lj">"value.converter.schema.registry.url": "http://schema-registry:8081",</span><span id="0474" class="le lf hi la b fi ll lh l li lj">"connection.user": "root",</span><span id="9cfc" class="le lf hi la b fi ll lh l li lj">"connection.password": "P@ssw0rd",</span><span id="628b" class="le lf hi la b fi ll lh l li lj">"topic.prefix": "customer-events-",</span><span id="d9be" class="le lf hi la b fi ll lh l li lj">"table.whitelist" : "customer_events",</span><span id="4c5c" class="le lf hi la b fi ll lh l li lj">"timestamp.column.name": "created_on",</span><span id="68c6" class="le lf hi la b fi ll lh l li lj">"transforms":"createKey,extractInt",</span><span id="d953" class="le lf hi la b fi ll lh l li lj">"transforms.createKey.type":"org.apache.kafka.connect.transforms.ValueToKey",</span><span id="d1fd" class="le lf hi la b fi ll lh l li lj">"transforms.createKey.fields":"id",</span><span id="f589" class="le lf hi la b fi ll lh l li lj">"transforms.extractInt.type":"org.apache.kafka.connect.transforms.ExtractField$Key",</span><span id="1376" class="le lf hi la b fi ll lh l li lj">"transforms.extractInt.field":"id",</span><span id="6653" class="le lf hi la b fi ll lh l li lj">"mode":"bulk"</span><span id="7a03" class="le lf hi la b fi ll lh l li lj">}</span><span id="9450" class="le lf hi la b fi ll lh l li lj">}</span></pre><p id="3a2f" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">2-我们需要sink来连接消费来自Kafka的数据，并通过弹性搜索API或报告数据库等推送数据。调用elastic-sink-connect端点创建作业</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="88c4" class="le lf hi la b fi lg lh l li lj">{<br/> "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",<br/> "tasks.max": "1",<br/> "topics": "example.elasticsearch.data",<br/> "name": "example-elasticsearch-connector",<br/> "connection.url": "http://elasticsearch:9200",<br/> "type.name": "_doc"<br/>}</span></pre><p id="3122" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们刚刚告诉connect我们的弹性API URL和index _doc类型，或者可能是log。</p><p id="7313" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">现在我们制作了一个数据管道系统，我们可以使用KSQL对流进行一些连续的查询，或者构建一个java流应用程序。例如，如果我们需要获得基于地区或国家或时间基础的比率计数，我们需要在Kafka主题之上创建一个流</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="b222" class="le lf hi la b fi lg lh l li lj">CREATE STREAM RATES_STREAM<br/>  (id BIGINT,<br/>   customer_name VARCHAR,<br/>   Country_name VARCHAR<br/>   rate INT)<br/>  WITH (KAFKA_TOPIC='RATES',<br/>        VALUE_FORMAT='AVRO')<br/>  EMIT CHANGES;</span></pre><p id="7da9" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">首先，我们从我们的主题创建了一个流，然后我们可以创建另一个流</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="bd75" class="le lf hi la b fi lg lh l li lj">CREATE STREAM RATES_COUNT AS<br/>      SELECT COUNT(*), Country_name FROM RATES_STREAM<br/>      GROUP BY Country_name <br/>      EMIT CHANGES;</span></pre><p id="0a0a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">然后你可以很容易地从流中选择</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="ec1d" class="le lf hi la b fi lg lh l li lj">SELECT * FROM RATES_COUNT EMIT CHANGES;</span></pre><p id="56d3" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">或者您可以创建您的流应用程序，例如，如果我们需要使用JAVA中的Kafka stream API来捕获在我们的产品上添加低费率的客户</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="149c" class="le lf hi la b fi lg lh l li lj">Properties config = <em class="iv">getConfig</em>();<br/>StreamsBuilder streamsBuilder = new StreamsBuilder();<br/>KStream&lt;String, Rate&gt; stream = streamsBuilder.stream("rates");<br/>stream.peek(OrderServiceApplication::<em class="iv">PrintAll</em>)<br/>      .filter((k,v)-&gt; v.rate &lt; 3)<br/>      .peek(OrderServiceApplication::SendNotification);<br/><br/><br/>Topology topology = streamsBuilder.build();<br/>KafkaStreams streams = new KafkaStreams(topology, config);<br/>streams.start();</span></pre><p id="9a26" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">更多..，所以我将在另一篇文章中深入解释streams和KSQL技巧</p><p id="3a72" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">最后，我希望本文能帮助您设计流式数据管道系统。</p></div></div>    
</body>
</html>