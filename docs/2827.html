<html>
<head>
<title>Deep Learning Activation Functions &amp; their mathematical implementation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习激活函数及其数学实现。</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/deep-learning-activation-functions-their-mathematical-implementation-b620d536d39b?source=collection_archive---------9-----------------------#2021-05-21">https://medium.com/nerd-for-tech/deep-learning-activation-functions-their-mathematical-implementation-b620d536d39b?source=collection_archive---------9-----------------------#2021-05-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1729984a16ec9ca6936cd807af6e9e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGFvJgMe21_5fCzUUyLwrw.png"/></div></div></figure><p id="4fb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">激活函数，也称为<strong class="is hj">传递函数</strong>在设计神经网络中至关重要。激活函数在某种意义上很重要，因为它用于确定<strong class="is hj">神经网络</strong>的输出。它将结果值映射到0到1或-1到1之间，等等。(取决于功能)。激活功能还有另一个名字叫做<strong class="is hj">挤压功能，</strong>当激活功能的范围受限时使用这个名字。激活函数应用于神经网络的每个节点，并确定神经元是否应该被“激发”/“激活”。</p><blockquote class="jo jp jq"><p id="e25c" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">为什么在选择激活功能时仔细选择很重要。</strong></p></blockquote><p id="2496" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当在隐藏和输出层中实现时，激活函数的选择非常关键。模型的准确性和损失非常依赖于激活函数。此外，必须根据您对模型性能的预期来选择它们。例如，在二元分类问题中，sigmoid函数是最佳选择。</p><blockquote class="jo jp jq"><p id="5870" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">激活功能的类型。</strong></p></blockquote><p id="9adc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">激活功能可以大致分为两类:</p><ol class=""><li id="f31b" class="jv jw hi is b it iu ix iy jb jx jf jy jj jz jn ka kb kc kd bi translated">线性激活函数。</li><li id="222f" class="jv jw hi is b it ke ix kf jb kg jf kh jj ki jn ka kb kc kd bi translated">非线性激活函数。</li></ol></div><div class="ab cl kj kk gp kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hb hc hd he hf"><p id="4473" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">要导入的库</strong></p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="731c" class="kz la hi kv b fi lb lc l ld le">import math as m<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import tensorflow as tf<br/>from tensorflow import keras<br/>from tensorflow.keras import layers</span></pre><blockquote class="jo jp jq"><p id="be55" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">乙状结肠功能</strong></p></blockquote><p id="ba5e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jr">乙状结肠激活功能也称为逻辑功能。sigmoid函数在回归分类问题中非常流行。sigmoid函数给出0到1范围内的值。</em></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/99f5ff938ef725579d4625fdf3a640a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*l29vq8YuFX6HFuJKbMBNyA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">sigmoid函数</figcaption></figure><p id="1e7d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">要实现的代码:</strong></p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="3f9a" class="kz la hi kv b fi lb lc l ld le">def sigmoid(x):<br/>    return 1 / (1 + m.exp(-x))</span><span id="6f15" class="kz la hi kv b fi lk lc l ld le">values_of_sigmoid = []<br/>values_of_x = []<br/>for i in range(-500,500,1):<br/>    i = i*0.01<br/>    values_of_x.append(i)<br/>    values_of_sigmoid.append(sigmoid(i))</span><span id="430f" class="kz la hi kv b fi lk lc l ld le">plt.plot( values_of_x ,values_of_sigmoid)<br/>plt.xlabel("values of x")<br/>plt.ylabel("value of sigmoid")</span></pre><blockquote class="jo jp jq"><p id="9f7e" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">2.<strong class="is hj">双曲正切函数</strong></p></blockquote><p id="b163" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jr">该功能与乙状结肠激活功能非常相似。该函数将任何实数值作为输入，并输出-1到1范围内的值。输入越大(越正)，输出值越接近1.0，而输入越小(越负)，输出越接近-1.0。双曲正切激活函数计算如下。</em></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/c86d5a92904c678b928c7631a84d8686.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*HukJUCeWeRPn3Azzs1mH1A.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">TanH函数</figcaption></figure><p id="5cac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">代码实现:</strong></p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="61c9" class="kz la hi kv b fi lb lc l ld le">def tanh(x):<br/>    return (m.exp(x) - m.exp(-x)) / (m.exp(x) + m.exp(-x))</span><span id="dd16" class="kz la hi kv b fi lk lc l ld le">values_of_tanh = []<br/>values_of_x = []<br/>for i in range(-500,500,1):<br/>    i = i*0.001<br/>    values_of_x.append(i)<br/>    values_of_tanh.append(tanh(i))</span><span id="c00d" class="kz la hi kv b fi lk lc l ld le">plt.plot( values_of_x ,values_of_tanh)<br/>plt.xlabel("values of x")<br/>plt.ylabel("value of tanh")</span></pre><blockquote class="jo jp jq"><p id="ed24" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> 3。Softmax功能</strong></p></blockquote><p id="7595" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Softmax激活函数输出总和为1.0的值的向量，该向量可以被解释为类成员的概率。Softmax是argmax函数的一个“<em class="jr">更软的</em>”版本，它允许一个赢家通吃函数的类似概率的输出。</p><div class="kq kr ks kt fd ab cb"><figure class="lm ij ln lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/170af83adde21ae47782f673cb8309e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*yN-XKUF8CtohUAehQQNF_A.png"/></div></figure><figure class="lm ij ls lo lp lq lr paragraph-image"><img src="../Images/a564e077ef317064780f1f7f7792ac60.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*mwjUY6EDIhVrnbsZcPoScg.png"/><figcaption class="lg lh et er es li lj bd b be z dx lt di lu lv translated">softmax函数</figcaption></figure></div><p id="0744" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">代码实现:</strong></p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="053c" class="kz la hi kv b fi lb lc l ld le">def softmax(x):<br/>    e_x = np.exp(x - np.max(x))<br/>    return e_x / e_x.sum()</span><span id="3ad5" class="kz la hi kv b fi lk lc l ld le">values_of_x = [i*0.01 for i in range(-500,500)]<br/>plt.plot(scores ,softmax(values_of_x))<br/>plt.xlabel("values of x")<br/>plt.ylabel("value of softmax")</span></pre><blockquote class="jo jp jq"><p id="4069" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> 4。整流线性单元功能</strong></p></blockquote><p id="c6df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ReLU或整流线性激活函数可能是隐藏层最常用的函数。它还可以有效地克服其他以前流行的激活功能的限制，如Sigmoid和Tanh。具体来说，它不太容易受到阻止深度模型被训练的<strong class="is hj">消失梯度下降问题</strong>的影响，尽管它可能遭受其他问题，如<strong class="is hj">饱和</strong>单元。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">ReLU函数</figcaption></figure><p id="f456" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要实现的代码:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="3be5" class="kz la hi kv b fi lb lc l ld le">def ReLU(x):<br/>    return max(0,x)</span><span id="49d4" class="kz la hi kv b fi lk lc l ld le">values_of_relu = []<br/>values_of_x = []<br/>for i in range(-500,500,1):<br/>    i = i*0.01<br/>    values_of_x.append(i)<br/>    values_of_relu.append(ReLU(i))</span><span id="8cfb" class="kz la hi kv b fi lk lc l ld le">plt.plot(values_of_x,values_of_relu)</span></pre><blockquote class="jo jp jq"><p id="ec1a" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> 5。泄漏的ReLU </strong></p></blockquote><p id="e8ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">校正线性单元的问题:当给ReLU一个负值时，它会立即变为零，这降低了模型拟合或根据数据正确训练的能力。这意味着给予ReLU激活函数的任何负输入都会在图形中立即将值变成零，这反过来会通过不适当地映射负值来影响结果图形。</p><p id="9b86" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了克服这个问题，引入了泄漏ReLU。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/784e1555e49bebf5bd273f92451ee76b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGazDurOnSW8sBsSNVEVHA.jpeg"/></div></figure><p id="4956" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要实现的代码:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="7cd5" class="kz la hi kv b fi lb lc l ld le">def leaky_ReLU(x):<br/>    return max(0.1*x,x)</span><span id="da70" class="kz la hi kv b fi lk lc l ld le">values_of_L_relu = []<br/>values_of_x = []<br/>for i in range(-500,500,1):<br/>    i = i*0.01<br/>    values_of_x.append(i)<br/>    values_of_L_relu.append(leaky_ReLU(i))</span><span id="7276" class="kz la hi kv b fi lk lc l ld le">plt.plot(values_of_x,values_of_L_relu)</span></pre><blockquote class="jo jp jq"><p id="a2ae" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> 6。其他一些激活功能和实现:</strong></p></blockquote><p id="75aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 6.1指数线性单位:</strong></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/5a860c4a3b7e075f856215d26ee853ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*9SZYTKYf1D8w8-Jz0-V-rQ.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">指数线性单位</figcaption></figure><p id="0df8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要实现的代码:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="3b20" class="kz la hi kv b fi lb lc l ld le">activation_elu = layers.Activation(‘elu’)</span><span id="2638" class="kz la hi kv b fi lk lc l ld le">x = tf.linspace(-3.0, 3.0, 100)<br/>y = activation_elu(x) # once created, a layer is callable just like a function</span><span id="e9fe" class="kz la hi kv b fi lk lc l ld le">plt.figure(dpi=100)<br/>plt.plot(x, y)<br/>plt.xlim(-3, 3)<br/>plt.xlabel(“Input”)<br/>plt.ylabel(“Output”)<br/>plt.show()</span></pre><p id="7489" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 6.2比例指数线性单位:</strong></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/93bd520ce05d2936d64b2a96ab3ff09e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*4oMRiUCCtowFyzFWUauGSQ.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">标度指数线性单位</figcaption></figure><p id="8f63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要实现的代码:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="0cb8" class="kz la hi kv b fi lb lc l ld le">activation_selu = layers.Activation('selu')</span><span id="8eb4" class="kz la hi kv b fi lk lc l ld le">x = tf.linspace(-3.0, 3.0, 100)<br/>y = activation_selu(x) # once created, a layer is callable just like a function</span><span id="8261" class="kz la hi kv b fi lk lc l ld le">plt.figure(dpi=100)<br/>plt.plot(x, y)<br/>plt.xlim(-3, 3)<br/>plt.xlabel("Input")<br/>plt.ylabel("Output")<br/>plt.show()</span></pre><p id="93f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 6.3嗖嗖声:</strong></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/0a308e1c3e084c33e96408c787774982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*ItZBsx9hRH8X0epFPsel-w.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">嗖嗖</figcaption></figure><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="f90e" class="kz la hi kv b fi lb lc l ld le">activation_swish = layers.Activation(‘swish’)</span><span id="3399" class="kz la hi kv b fi lk lc l ld le">x = tf.linspace(-3.0, 3.0, 100)<br/>y = activation_swish(x) # once created, a layer is callable just like a function</span><span id="c842" class="kz la hi kv b fi lk lc l ld le">plt.figure(dpi=100)<br/>plt.plot(x, y)<br/>plt.xlim(-3, 3)<br/>plt.xlabel(“Input”)<br/>plt.ylabel(“Output”)<br/>plt.show()</span></pre></div><div class="ab cl kj kk gp kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hb hc hd he hf"><blockquote class="jo jp jq"><p id="2cd0" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">隐藏层激活功能:</strong></p></blockquote><p id="881c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通常支持递归神经网络使用双曲正切函数或sigmoid激活函数，或者两者都使用。例如，LSTM通常将Sigmoid激活用于循环连接，将Tanh激活用于输出。</p><p id="1926" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.多层感知器(MLP): ReLU激活函数。</p><p id="9fcd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.卷积神经网络(CNN): ReLU激活函数。</p><p id="addc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.递归神经网络:Tanh和/或Sigmoid激活函数。</p><p id="bb6a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">好吧，如果你不确定使用哪种激活功能，你当然可以尝试不同的组合，寻找最合适的。</p><blockquote class="jo jp jq"><p id="127f" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">输出层激活功能:</strong></p></blockquote><p id="7655" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">必须根据您正在解决的问题类型来选择输出层激活函数。例如，如果你有一个线性回归问题，那么一个线性激活函数将是有帮助的。以下是您可能会遇到的一些常见问题和使用的激活功能。</p><ul class=""><li id="0fbd" class="jv jw hi is b it iu ix iy jb jx jf jy jj jz jn mb kb kc kd bi translated"><strong class="is hj">二元分类</strong>:一个节点，乙状结肠激活。</li><li id="b4e5" class="jv jw hi is b it ke ix kf jb kg jf kh jj ki jn mb kb kc kd bi translated"><strong class="is hj">多类分类</strong>:每类一个节点，softmax激活。</li><li id="6071" class="jv jw hi is b it ke ix kf jb kg jf kh jj ki jn mb kb kc kd bi translated"><strong class="is hj">多标记分类</strong>:每类一个节点，sigmoid激活。</li></ul><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/2b59b24c536bb5761bab4f3d0b5a50bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmFlJgoW0TQ8yjNzu7yWdA.png"/></div></div></figure></div></div>    
</body>
</html>