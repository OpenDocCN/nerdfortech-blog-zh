<html>
<head>
<title>NLP Zero to One: Dense Representations, Word2Vec (Part 5/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP零到一:密集表示，Word2Vec(第5/30部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-dense-representations-word2vec-part-5-30-9b38c5ccfbfc?source=collection_archive---------6-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-dense-representations-word2vec-part-5-30-9b38c5ccfbfc?source=collection_archive---------6-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="9e75" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">词语嵌入和语义表征</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/0abe6f9559b0255a14a8ed3e4549cac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhxQ6-i78vRmOC5OYgQw6A.png"/></div></div></figure><h1 id="1117" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">介绍..</h1><p id="195e" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们已经了解了如何为文档创建稀疏向量，如“TF-IDF”和“BoW ”,其维度对应于语料库的<strong class="kd hj">词汇表中的单词(这个维度将是巨大的)。请浏览我在这个系列中的早期博客:<a class="ae kx" href="https://medium.com/p/9b96854d412a/edit" rel="noopener"> NLP零到一:稀疏文档表示(第2/30部分)</a>了解稀疏表示。(*稀疏意味着向量表示中有太多的零)<br/>正如在之前的博客中所讨论的，这些长的稀疏向量表示有一些严重的缺点，如“TF-IDF”和“BoW”。<br/> 1。大内存和昂贵的计算，因为向量很长<br/> 2。因为文档中单词的顺序是不相关的。<br/> 3。难以建模，因为要训练的模型参数的数量将在输入向量长度的范围内，这是巨大的。</strong></p><p id="7e12" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">在这篇博客中，我们将看到如何使用降维技术和<strong class="kd hj">重要的深度学习</strong>来解决这些问题。使用不同的技术，我们将提取强大的单词表示，称为<strong class="kd hj">嵌入</strong>(密集、短向量)。与TFIDF或BoW不同，这些向量的长度范围是50–300。这些向量在每个NLP问题中比稀疏向量工作得更好，因为单词的顺序/结构起着主要作用。所以意思相近的词有相似的表示。<br/>例如:“boat”和“ship”在稀疏向量表示中表示两个不同的事物，但是嵌入成功地捕捉到了这些词之间的相似性。有两个最流行和开源的嵌入模型Word2Vec和GLoVe。word2vec方法速度快，训练效率高，并且容易在线获得静态代码和预训练的嵌入。</p><h1 id="19b8" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">Word2Vec..</h1><p id="62fb" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">在本节中，我们将了解如何使用深度学习来创建单词嵌入。这些嵌入是如此强大，以至于皇后的向量表示非常类似于v(国王)v(男人)+ v(女人)的向量表示。这些表示在捕捉句法关系方面是强大的。</p><p id="c655" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">但我们即将了解如何使用深度学习来创建这种嵌入，如果我们从神经架构和该架构背后的直觉开始，那将是最好的。提出了两种流行的不同架构，每个NLP从业者必须熟悉这两种架构。提议的架构由<strong class="kd hj">连续词袋(CBOW)模型</strong>和<strong class="kd hj">跳格模型组成。</strong></p><h1 id="8080" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">连续词袋..</h1><h2 id="8bfb" class="ld jk hi bd jl le lf lg jp lh li lj jt kk lk ll jv ko lm ln jx ks lo lp jz lq bi translated">直觉..</h2><p id="dca6" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">以相似方式/上下文使用的单词导致相似的表示。例如，类似sad和unhappy这样的同义词也用在类似的上下文中。但是我们如何定义语境呢？</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/ded53cd8390e146284a328680f61d4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVfDaSqasN4yJNQ5OuBORw.png"/></div></div></figure><p id="0c1f" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">相邻单词将使用目标单词的上下文(在上面的例子中是“sad”)。所以这里的上下文只是目标左右两边的c <em class="ls">单词的窗口。</em></p><h2 id="5108" class="ld jk hi bd jl le lf lg jp lh li lj jt kk lk ll jv ko lm ln jx ks lo lp jz lq bi translated">分类问题设置..</h2><p id="7cb8" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">本着所描述的直觉，我们将尝试基于上下文单词(周围的单词)来预测当前的目标单词(“sad”)。考虑用于预测的周围单词的数量被称为<strong class="kd hj">上下文窗口</strong>。对于上面的例子，如果上下文窗口等于2，那么训练数据将是(["将"、"是"、"存储器"、"到"]、["sad"])。因此，如果你仔细观察，这个神经架构是无人监管的，我们需要给出的只是巨大的语料库(所有文档的集合)，仅此而已。它可以以滚动方式创建<strong class="kd hj"> X(输入为周围变量)、y(目标词)</strong><strong class="kd hj"/>如下图<strong class="kd hj"> </strong>和<strong class="kd hj"> </strong>所示，它可以从语料库中构造密集的词嵌入。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/c2f366dd021d76ad0ea2c4bab4153dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJpZlkWXY5maFDu0vrWYqg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">描述以滚动方式收集训练数据的图片</figcaption></figure><p id="ebd1" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">一旦如上所述从语料库中创建了<strong class="kd hj"> X: </strong>输入/上下文单词和<strong class="kd hj"> y: </strong>输出/目标单词，接下来的任务是设计一个为我们进行分类的模型，其中我们尝试从上下文单词中预测目标单词。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/7a093f83008e004895bda6d3a142d20a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1096mj2B78w7_V1purA_Dw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">分类设置说明</figcaption></figure><h2 id="868f" class="ld jk hi bd jl le lf lg jp lh li lj jt kk lk ll jv ko lm ln jx ks lo lp jz lq bi translated">神经架构..</h2><p id="b085" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">这种神经架构有3个重要方面:输入层、λ层/平均层和密集softmax。最重要也是最容易混淆的组件是<strong class="kd hj">输入层。</strong>输入层通常称为嵌入层。<strong class="kd hj"> <br/> </strong>假设我们有一个N个单词的词汇表，我们计划得到一个K大小的密集向量。输入层通过嵌入矩阵N将每个上下文单词映射到K维的密集向量表示，因此它是一个<strong class="kd hj"> NxK矩阵，其中每个单词都有一个K大小的向量。</strong></p><p id="3ea1" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated"><strong class="kd hj">输入图层描述:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/d4dec6ecb9f4c98d3f318ebd9e7e892c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPGW2BjgMJkOBowok8GVZQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">图片描绘输入以后，对于一个单词Wi，有相应的K长度嵌入向量</figcaption></figure><p id="349c" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">一旦我们理解了输入层的工作原理，架构的其余部分就很容易理解了。所有的上下文单词被馈送到嵌入层/输入层，并且对应于上下文单词的所有向量被平均。这方面由λ层或平均层处理。一旦平均向量被馈送到softmax层，该层从语料库的整个词汇中预测目标单词。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/4c38651c2aa02e3693e9ea886db0999c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DrKsQmzUShmBdkZAVPCyXw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">整个CBOW神经架构的描述</figcaption></figure><h2 id="2cdd" class="ld jk hi bd jl le lf lg jp lh li lj jt kk lk ll jv ko lm ln jx ks lo lp jz lq bi translated">CBOW培训..</h2><p id="28db" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">现在我们理解了直觉和随后的神经结构。现在，我们将尝试深入了解训练机制以及权重是如何更新的。我敦促读者浏览我早期的博客(<a class="ae kx" href="https://kowshikchilamkurthy.medium.com/nlp-theory-and-code-deep-learning-training-procedure-part-4-40-f05866105717?source=your_stories_page-------------------------------------" rel="noopener"> NLP理论和代码:深度学习训练程序(第4/40部分)</a>)以了解训练如何在神经网络中进行。</p><p id="b34c" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">训练过程可以通过回答两个问题来描述:<br/> 1 .什么是可训练/可学习的参数？<br/> 2。什么是损失函数？</p><p id="af98" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated"><strong class="kd hj">什么是可训练/可学习的参数？<br/> </strong>嵌入层被随机初始化，嵌入层中的所有数字都是可训练参数。因此，随着更多的数据输入到模型中，嵌入层变得越来越好。<br/> <strong class="kd hj">什么是损失函数？</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/2476efb1dedf6a2c0cc1bd20ee713170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*uFJRNC-eAIF3pFY-bAvYSQ.png"/></div></figure><p id="5fbe" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">给定上下文词的目标词条件概率的对数。我们将预测的单词与实际的目标单词进行匹配，通过利用<strong class="kd hj">类别交叉熵损失</strong>来计算损失，并在每个时期执行反向传播，以在该过程中更新嵌入层。</p><h1 id="baed" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">注意..</h1><p id="e6bd" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们将在本系列的下一篇博客中讨论Word2Vec中的下一个流行架构“Skip-gram”。我们还将讨论手套，另一个流行的预训练嵌入。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">由作者生成</figcaption></figure><p id="fa2e" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">上一篇:<a class="ae kx" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-deep-learning-training-procedure-part-4-30-c8d1e3ba0db6?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kd hj"> NLP零对一:深度学习训练过程(第4/30部分)</strong> </a> <br/>下一篇:<a class="ae kx" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-count-based-embeddings-glove-part-6-40-c5bb3ebfd081?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kd hj"> NLP零对一:基于计数的嵌入，手套(第6/40部分)</strong> </a></p></div></div>    
</body>
</html>