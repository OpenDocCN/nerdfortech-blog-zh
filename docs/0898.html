<html>
<head>
<title>Review — Rethinking ImageNet Pre-training (Object Detection, Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾—重新思考ImageNet预培训(对象检测、语义分割)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-rethinking-imagenet-pre-training-image-classification-object-detection-semantic-683f6575a2be?source=collection_archive---------2-----------------------#2021-02-21">https://medium.com/nerd-for-tech/review-rethinking-imagenet-pre-training-image-classification-object-detection-semantic-683f6575a2be?source=collection_archive---------2-----------------------#2021-02-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a84e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从头开始培训不比ImageNet预培训差</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/1b6dac17bba8c5c1ab485c7a5ba1918c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*f-XJNQo4VTSK3s_NGdaVQg.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">模型，</strong><a class="ae jk" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd jj">ResNet</strong></a><strong class="bd jj">50-</strong><a class="ae jk" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd jj">FPN</strong></a><strong class="bd jj">使用</strong> <a class="ae jk" href="https://sh-tsang.medium.com/review-group-norm-gn-group-normalization-image-classification-5f7fe0f58eb6" rel="noopener"> <strong class="bd jj"> GN </strong> </a> <strong class="bd jj">，从随机初始化训练出来的需要更多的迭代才能收敛，但收敛到的解并不比微调对应的差。</strong></figcaption></figure><p id="a93f" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">在</span>这个故事里，简单回顾了脸书AI Research (FAIR)的<strong class="jn hj">反思ImageNet前期培训</strong>。</p><blockquote class="kq kr ks"><p id="0f22" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated">许多论文都是从零开始进行预培训，而不是培训。然而，预先训练好的知识转移到其他计算机视觉任务中真的有用吗？</p></blockquote><p id="19c8" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">在这个故事中，发现了一些事实:</p><ul class=""><li id="953c" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated"><strong class="jn hj">来自随机初始化的训练惊人地健壮</strong>，即使在以下情况下，结果仍然成立:(I)仅使用10%的训练数据，(ii)用于更深更广的模型，以及(iii)用于多个任务和指标。</li><li id="f3b1" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated"><strong class="jn hj"> ImageNet预训练</strong>在训练初期加速收敛，但<strong class="jn hj">不一定提供正则化或提高最终目标任务精度</strong>。</li></ul><p id="852f" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">这是一篇发表在<strong class="jn hj"> 2019 ICCV </strong>的论文，被引用超过<strong class="jn hj"> 350次</strong>。(<a class="ll lm ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----683f6575a2be--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="c8ba" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">(实验设置上有很多细节，让实验公平。为了使故事简短，我会跳过一些细节和结果。如果有兴趣，请随时访问该文件。)</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="0a00" class="lu lv hi bd jj lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">概述</h1><ol class=""><li id="5898" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg mq ld le lf bi translated"><strong class="jn hj">训练图像数量&amp;设置</strong></li><li id="55b7" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj">从零开始训练，以匹配准确度</strong></li><li id="9056" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj">用较少的数据从零开始训练</strong></li><li id="a10c" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj">讨论</strong></li></ol></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="6d3c" class="lu lv hi bd jj lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">1.训练图像和设置的数量</h1><h2 id="e600" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">1.1.涉及的训练图像数量</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/8eb43297dced86727aa5e3c5310ec738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*2MMsCdM1yiKLqnzzRoEj2Q.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">在所有训练迭代期间看到的图像、实例和像素的总数，用于预训练+微调(绿色条)与随机初始化(紫色条)。</strong></figcaption></figure><ul class=""><li id="3bd5" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated"><strong class="jn hj">典型的ImageNet预训练</strong> <strong class="jn hj">涉及超过一百万幅图像</strong>迭代一百个时期。除了从这种大规模数据中学习任何语义信息之外，<strong class="jn hj">预训练模型还学习了低级特征。</strong></li><li id="1189" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">另一方面，<strong class="jn hj">当从头开始训练</strong>时，模型必须学习低级和高级语义，因此<strong class="jn hj">可能需要更多迭代</strong>才能很好地收敛。</li><li id="a559" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">如上所示，<strong class="jn hj">如果计算图像级样本，从头开始的情况比其微调对应的情况看到的样本少得多。</strong></li><li id="43b4" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">实际上，如果我们计算像素级样本，样本数只会更接近。</li></ul><h2 id="de6e" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">1.2.设置</h2><ul class=""><li id="1367" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated"><a class="ae jk" rel="noopener" href="/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4">用<a class="ae jk" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>屏蔽R-CNN </a>，用<a class="ae jk" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------" rel="noopener" target="_blank"> ResNeXt </a>加特征金字塔网络(<a class="ae jk" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>)主干。</li><li id="5ad1" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">GN  /SyncBN用于替换所有‘冻结BN’。SyncBN就是在多个GPU下使用BN。</li><li id="90b5" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">用<strong class="jn hj"> 90k迭代</strong>(即，<strong class="jn hj"> 1×调度</strong>’)或<strong class="jn hj"> 180k迭代</strong>(‘<strong class="jn hj">2×调度</strong>’)将模型微调为所谓的“<strong class="jn hj"> 6× </strong> <strong class="jn hj">调度</strong>”，其中有<strong class="jn hj"> 540k迭代</strong>。</li></ul></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="b9f7" class="lu lv hi bd jj lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">2.从头开始训练以匹配准确度</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/8bc5cad4516e723326ba180d054db947.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*qEzICKk_WOQT56vt8GUH9w.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">在COCO val2017上学习APbbox的曲线使用</strong> <a class="ae jk" rel="noopener" href="/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4"> <strong class="bd jj">遮罩R-CNN </strong> </a> <strong class="bd jj">用R101-</strong><a class="ae jk" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd jj">FPN</strong></a><strong class="bd jj">和</strong> <a class="ae jk" href="https://sh-tsang.medium.com/review-group-norm-gn-group-normalization-image-classification-5f7fe0f58eb6" rel="noopener"> <strong class="bd jj"> GN </strong> </a></figcaption></figure><ul class=""><li id="3f09" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated"><strong class="jn hj">典型的微调时间表(2×)适用于预训练模型</strong>以接近最优。<strong class="jn hj">但是这些时间表对于从零开始训练的模特</strong>来说是不够的，如果只是短期训练就显得逊色了。</li></ul><blockquote class="kq kr ks"><p id="09c2" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated">如果使用5倍或6倍的时间表，从零开始训练的模型可以赶上它们的微调对应物。当它们收敛到一个最优值时，它们的检测AP不会比它们的微调对应物差。</p></blockquote></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="9dc6" class="lu lv hi bd jj lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">3.用更少的数据从头开始训练</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/d23005dcdf98e0655046436d019b7fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*e2whkXtwPCqLJTiGIX1ZCQ.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">10k COCO图像训练</strong></figcaption></figure><ul class=""><li id="e334" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">使用10k COCO图像的较小训练集(即，小于完整COCO集的1/10)。</li><li id="7be7" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">有预训练的模型，60k迭代达到26.0 AP，但训练多了有轻微退化。</li></ul><blockquote class="kq kr ks"><p id="6f89" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated">从零开始训练的对应模型在220k迭代时具有25.9 AP，这是相对准确的。</p></blockquote></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="629d" class="lu lv hi bd jj lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated"><strong class="ak"> 4。讨论</strong></h1><ul class=""><li id="2cfb" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated">以上实验也引发了作者的以下讨论。</li></ul><h2 id="8598" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">4.1.ImageNet前期培训有必要吗？</h2><ul class=""><li id="86bc" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated"><strong class="jn hj">不</strong>，如果我们有足够的目标数据。</li><li id="2898" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">这表明<strong class="jn hj">收集目标数据的注释(而不是预训练数据)对于提高目标任务性能更有用</strong>。</li></ul><h2 id="b229" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">4.2.ImageNet有用吗？</h2><ul class=""><li id="899f" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated"><strong class="jn hj">是</strong>。</li><li id="0798" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">ImageNet预训练缩短了研究周期，<strong class="jn hj">更容易获得令人鼓舞的结果，</strong>和<strong class="jn hj">从预训练权重进行微调比从头开始收敛更快。</strong></li></ul><h2 id="a752" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">4.3.大数据有帮助吗？</h2><ul class=""><li id="d043" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated"><strong class="jn hj">是</strong>。</li><li id="f7d6" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">但是，如果我们考虑到收集和清理数据的额外工作，一个通用的大规模、分类级别的预训练集<strong class="jn hj">并不理想。</strong></li><li id="782d" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">如果大规模分类级预训练的增益成指数递减，<strong class="jn hj">在目标域中收集数据将更有效。</strong></li></ul><h2 id="592f" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">4.4.我们应该追求普遍代表性吗？</h2><ul class=""><li id="362a" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated"><strong class="jn hj">是的。</strong></li><li id="1b60" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">作者认为<strong class="jn hj">学习普遍表象是一个值得称赞的目标。</strong></li><li id="3d86" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">这项研究表明，社区在评估预先训练的特征时应该更加小心。</li></ul></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h2 id="d7c6" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">参考</h2><p id="30b4" class="pw-post-body-paragraph jl jm hi jn b jo ml ij jq jr mm im jt ju ni jw jx jy nj ka kb kc nk ke kf kg hb bi translated">【2019 ICCV】【反思ImageNet预训】<br/> <a class="ae jk" href="https://arxiv.org/abs/1811.08883" rel="noopener ugc nofollow" target="_blank">反思ImageNet预训</a></p><h2 id="1da8" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">目标检测</h2><p id="aa54" class="pw-post-body-paragraph jl jm hi jn b jo ml ij jq jr mm im jt ju ni jw jx jy nj ka kb kc nk ke kf kg hb bi translated"><strong class="jn hj"> 2014 </strong> : [ <a class="ae jk" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------">过食</a>][<a class="ae jk" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------">R-CNN</a>]<br/><strong class="jn hj">2015</strong>:[<a class="ae jk" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------">快R-CNN </a> ] [ <a class="ae jk" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快R-CNN</a>][<a class="ae jk" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae jk" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a><br/><strong class="jn hj">2016<strong class="jn hj"> GBD-v1&amp;GBD-v2[<a class="ae jk" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae jk" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a><br/><strong class="jn hj">2017</strong>:[<a class="ae jk" rel="noopener" href="/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a?source=post_page---------------------------">NoC</a>][<a class="ae jk" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae jk" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------">TDM</a>][<a class="ae jk" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>[<a class="ae jk" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 9000</a>] [<a class="ae jk" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">Cascade R-CNN</a>][<a class="ae jk" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae jk" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>]<br/><strong class="jn hj">2019</strong>:[<a class="ae jk" rel="noopener" href="/towards-artificial-intelligence/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5">DCN v2</a>][<a class="ae jk" href="https://sh-tsang.medium.com/review-rethinking-imagenet-pre-training-image-classification-object-detection-semantic-683f6575a2be" rel="noopener">反思ImageNet前期培训</a> ]</strong></strong></p><h2 id="0044" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated">实例分割</h2><p id="9fbf" class="pw-post-body-paragraph jl jm hi jn b jo ml ij jq jr mm im jt ju ni jw jx jy nj ka kb kc nk ke kf kg hb bi translated"><strong class="jn hj">2014–2015</strong>:[<a class="ae jk" rel="noopener" href="/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------">SDS</a>][<a class="ae jk" href="https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------" rel="noopener" target="_blank">Hypercolumn</a>][<a class="ae jk" href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------" rel="noopener" target="_blank">deep Mask</a><br/><strong class="jn hj">2016</strong>:[<a class="ae jk" href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------" rel="noopener" target="_blank">sharp Mask</a>][<a class="ae jk" href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------" rel="noopener" target="_blank">multipath net</a>][<a class="ae jk" href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------" rel="noopener" target="_blank">MNC</a>][<a class="ae jk" href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------" rel="noopener" target="_blank">instance fcn</a><br/><strong class="jn hj">2017【T21</strong></p><h2 id="42d6" class="mr lv hi bd jj ms mt mu lz mv mw mx md ju my mz mf jy na nb mh kc nc nd mj ne bi translated"><a class="ae jk" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>