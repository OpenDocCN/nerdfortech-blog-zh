<html>
<head>
<title>Multiple Linear Regression and Gradient Descent using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 进行多元线性回归和梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/multiple-linear-regression-and-gradient-descent-using-python-b931a2d8fb24?source=collection_archive---------2-----------------------#2022-02-26">https://medium.com/nerd-for-tech/multiple-linear-regression-and-gradient-descent-using-python-b931a2d8fb24?source=collection_archive---------2-----------------------#2022-02-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="84d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章将解释多元线性回归及其在 Python 中的实现。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/4a2d1d5cfb60c00fda3b8bb8a7798fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JC9-PH8Ma3W7h_g-ful2fQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">安妮·斯普拉特在<a class="ae jt" href="https://unsplash.com/s/photos/linear-regression?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="27f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们深入多元线性回归之前，在这篇<a class="ae jt" rel="noopener" href="/@gilsatpray/gradient-descent-in-machine-learning-linear-regression-e94570f505e6">的文章</a>中，我们先来看看简单的线性回归。</p><div class="ju jv ez fb jw jx"><a rel="noopener follow" target="_blank" href="/@gilsatpray/gradient-descent-in-machine-learning-linear-regression-e94570f505e6"><div class="jy ab dw"><div class="jz ab ka cl cj kb"><h2 class="bd hj fi z dy kc ea eb kd ed ef hh bi translated">机器学习简介—简单线性回归和梯度下降</h2><div class="ke l"><h3 class="bd b fi z dy kc ea eb kd ed ef dx translated">线性回归是每个想学习机器学习的人的第一步。或者任何想成为…</h3></div><div class="kf l"><p class="bd b fp z dy kc ea eb kd ed ef dx translated">medium.com</p></div></div><div class="kg l"><div class="kh l ki kj kk kg kl jn jx"/></div></div></a></div><p id="094e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用那篇文章中的一些公式和代码。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h2 id="55fc" class="kt ku hi bd kv kw kx ky kz la lb lc ld iq le lf lg iu lh li lj iy lk ll lm ln bi translated">多元线性回归</h2><p id="9fa1" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">回想一下简单线性回归的方程式</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/3441af8a811b75ca4343d560f311bae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*vrV-Z_zf5DUOmpdPCc3mZg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图一。简单线性回归</figcaption></figure><p id="936a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预测的标注(y hat)仅取决于单个要素 x。如果我们有一个包含多个独立要素的数据集呢？如果是这种情况，那么我们需要多元线性回归。等式是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/a61fa905617ea73d28dc35fba6c804e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*vmi4RHGYXswB1b4K1ZrSAw.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/cc153a326b60800d8f76f8f719c45ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*rdb32K2rynTeC0tagbps_A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图二。多元线性回归</figcaption></figure><p id="47aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中:</p><p id="b273" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Yi =第 I 个样本的预测标签</p><p id="3fe6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Xij =第 I 个标签的第 j 个特征</p><p id="43df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">W0 =回归截距或权重</p><p id="e233" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Wj =第 j 个特征回归权重</p><p id="4ab6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，当标签 y 仅依赖于一个变量 x 时，方程变成简单的线性方程 y=w1x + w0。</p><p id="f78d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用矩阵符号来表示多元线性回归</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/704a88928131798b5669b24a842242b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:154/format:webp/1*SFnbCEs008GxLxRmayKvhA.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/a370f9eec7635632247780f002af4760.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*-acaqWx1WR-i7VjMuRhetw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图 3。多元线性回归的矩阵表示法</figcaption></figure><p id="4078" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度下降</strong></p><p id="d97f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多元线性回归中要最小化的成本函数是均方误差:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/e0439b287ecae566d1fc7c9c5cf84cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*xdvoLybRkQy7FBVXxCdWdg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图 4 .成本函数及其偏导数</figcaption></figure><p id="650e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在矩阵形式中，成本函数的偏导数可以写成</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/3d22fb8ebb7028630ad1c554ceff1e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*yHLbFCYp4p572mGkdqBiuA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图。6 成本函数导数的矩阵符号</figcaption></figure><p id="d90a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在 k+1 次迭代中更新的权重变成</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/89b4eec019161b5b91aef3b8ee9d6565.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*pdTMhwL9vYqFrubYV7mCfg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图 7。权重学习规则</figcaption></figure><p id="2421" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> alpha </strong>是学习率。</p><h2 id="e1d4" class="kt ku hi bd kv kw kx ky kz la lb lc ld iq le lf lg iu lh li lj iy lk ll lm ln bi translated">履行</h2><p id="7c7c" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">回想一下我们为简单线性回归编写的模型</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="b74e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将更新代码，因此它也适用于多元线性回归情况。</p><p id="a3ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更新构造函数</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="8722" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在构造函数中，我们只是添加了初始化权重矩阵作为模型属性。</p><p id="6267" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">按照以下步骤更新拟合方法:</p><ol class=""><li id="f335" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated">通过添加一个值等于 1 的列作为截距(w0)来修改特征矩阵</li><li id="3685" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">将权重矩阵的值初始化为零。</li><li id="cec1" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">对于迭代 1 直到 n(类<em class="mr"> iters </em>属性)</li><li id="3bd0" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">使用图 3 计算预测标签矩阵。</li><li id="1e78" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">计算步骤 3 中已知标签矩阵和预测标签之间的误差。</li><li id="787c" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">用图形计算成本函数的偏导数。6</li></ol><p id="af7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.使用图 7 中的公式更新权重矩阵。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="fb33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后对于预测方法:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="d271" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们回顾一下我们的更新模型:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="3f2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在多个变量数据集上测试模型之前，让我们测试简单线性回归案例的模型，我们将重用来自<a class="ae jt" rel="noopener" href="/@gilsatpray/gradient-descent-in-machine-learning-linear-regression-e94570f505e6">先前帖子</a>的测试分数数据集</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="0fd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MSE 的输出在 3.30 左右，与之前的模型相同。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><p id="17a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们测试这个多变量回归模型。我们将使用来自<a class="ae jt" href="https://www.kaggle.com/dansbecker/underfitting-and-overfitting/data?select=train.csv" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的房价数据集。下载并保存到数据集文件夹。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="f11b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将预测<code class="du ms mt mu mv b">SalePrice</code>，让我们稍微探索一下数据集</p><pre class="je jf jg jh fd mw mv mx my aw mz bi"><span id="cde2" class="kt ku hi mv b fi na nb l nc nd">df["SalePrice"].describe()</span><span id="1355" class="kt ku hi mv b fi ne nb l nc nd">count      1460.000000<br/>mean     180921.195890<br/>std       79442.502883<br/>min       34900.000000<br/>25%      129975.000000<br/>50%      163000.000000<br/>75%      214000.000000<br/>max      755000.000000<br/>Name: SalePrice, dtype: float64</span></pre><p id="7668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看<code class="du ms mt mu mv b">SalePrice</code>的分布图</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nf"><img src="../Images/fe435033d1322517fa682a62e5cf475f.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*DTeKCBnmjLtsPFyuuApkPg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图 8。销售价格直方图</figcaption></figure><p id="680f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数价格在 10 万到 20 万之间，但在更贵的方面似乎有很多异常值。</p><p id="c983" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就目前而言，我们不想在我们的数据集中得到任何缺失的值。因此，让我们检查哪些列包含丢失的值</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><pre class="je jf jg jh fd mw mv mx my aw mz bi"><span id="fc25" class="kt ku hi mv b fi na nb l nc nd">PoolQC          1453<br/>MiscFeature     1406<br/>Alley           1369<br/>Fence           1179<br/>FireplaceQu      690<br/>LotFrontage      259<br/>GarageYrBlt       81<br/>GarageType        81<br/>GarageFinish      81<br/>GarageQual        81<br/>GarageCond        81<br/>BsmtFinType2      38<br/>BsmtExposure      38<br/>BsmtFinType1      37<br/>BsmtCond          37<br/>BsmtQual          37<br/>MasVnrArea         8<br/>MasVnrType         8<br/>Electrical         1</span><span id="2153" class="kt ku hi mv b fi ne nb l nc nd">Columns with missing values: 19</span></pre><p id="91be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此有 19 列缺少值，我们将跳过它们并使用另一列进行回归。我们将使用相关性相对较高的列，这是其中的一些:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><pre class="je jf jg jh fd mw mv mx my aw mz bi"><span id="68db" class="kt ku hi mv b fi na nb l nc nd">               OverallQual  GrLivArea  GarageArea  SalePrice<br/>OverallQual     1.000000   0.593007    0.562022   0.790982<br/>GrLivArea       0.593007   1.000000    0.468997   0.708624<br/>GarageArea      0.562022   0.468997    1.000000   0.623431<br/>SalePrice       0.790982   0.708624    0.623431   1.000000</span></pre><p id="1675" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">缩放数据，使其快速收敛</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ng"><img src="../Images/569bb6c2f1b091acd78757cbbc3700cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*lEUj8P4TEiL9V7ykL5Ommg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图 9。标量方程</figcaption></figure><p id="60eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中是特征的平均值，σ是标准偏差</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="08f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以拟合数据集</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mb mc l"/></div></figure><h2 id="0c5f" class="kt ku hi bd kv kw kx ky kz la lb lc ld iq le lf lg iu lh li lj iy lk ll lm ln bi translated">结论</h2><p id="1c2e" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">在本文中，我们了解到:</p><ol class=""><li id="0f45" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated">一元线性方程向多元线性方程的推广</li><li id="0d6b" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">从代数和矩阵符号上推导多元线性回归方程。</li><li id="31a9" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">在多元线性回归中实现梯度下降</li></ol><p id="b1f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请分享这个帖子，如果你喜欢就给它一个掌声。</p></div></div>    
</body>
</html>