<html>
<head>
<title>Q Learning — From the basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">q 学习——从基础开始</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/q-learning-from-the-basics-b68e74f97254?source=collection_archive---------3-----------------------#2021-07-13">https://medium.com/nerd-for-tech/q-learning-from-the-basics-b68e74f97254?source=collection_archive---------3-----------------------#2021-07-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ccc3887d3108403cbc5399c4757ee9b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BnQ4Pp-yYHmW7nO-0tGuUg.png"/></div></div></figure><p id="de3f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强化学习(RL)是人工智能研究的一个子集，它处理训练代理以最大化奖励功能。RL 系统通常用于难以判断单个动作但易于对整体表现评分的任务。这些系统有两个核心组件:<strong class="is hj">代理</strong>和<strong class="is hj">环境</strong>。</p><h2 id="0ccc" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">代理人</h2><p id="2dbd" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">代理是接受给定状态并返回动作的函数。这些系统可以建立在任何东西上——神经网络和查找表就是两个常见的例子。神经网络通常用于复杂环境的应用中，而改进的查找表(Q 表)用于较简单的环境中。</p><h2 id="46bd" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">环境</h2><p id="50fe" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">环境是代理学习导航的硬编码系统。现在，我们可以把环境看作一个黑匣子，它接受一个动作，并返回一个奖励和下一个状态。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/b6b9ba988f875e1f4c1d7401616c1b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wbWLBPkeN8hXcbFCbnN25w.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">代理人在一个环境中行动，然后返回一个状态和奖励</figcaption></figure><p id="129a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">作为一个例子，想象使用一个 pong 游戏作为环境。代理采取一个动作，比如向下移动桨，这个动作由环境处理。然后，环境返回下一个状态，比如球拍和球的位置，加上一个衡量代理人表现如何的奖励。</p></div><div class="ab cl kx ky gp kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hb hc hd he hf"><h1 id="3a16" class="le jp hi bd jq lf lg lh ju li lj lk jy ll lm ln kb lo lp lq ke lr ls lt kh lu bi translated">构建强化学习模型</h1><p id="c744" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">本文中的所有代码都将使用 python 编写。OpenAI 的<a class="ae lv" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">体育馆</a>库，可以通过 pip 或 anaconda 安装。另一个选择是使用 Google Colab——但是，您将无法观察代理在环境中的操作。</p><h2 id="aeb3" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">创建随机爬山者</h2><p id="b4cd" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">让我们从构建最简单的代理开始——随机漫步机。随机漫步机是一种不管状态如何都采取随机行动的代理。这意味着它们非常简单，但也永远不会改进，使它们成为无效的 RL 模型。</p><p id="5926" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要创建这个随机漫步机，我们需要做两件事:构建一个环境和一个代理。本文中的所有环境都将使用<a class="ae lv" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> Gym </a>库来构建。下载完库<strong class="is hj">健身房。create() </strong>可以用来初始化环境。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="ca25" class="jo jp hi lx b fi mb mc l md me">import gym</span><span id="6f5e" class="jo jp hi lx b fi mf mc l md me">env = gym.create('MountainCar-v0') # create the hill climbing environment<br/>env.reset() # remember to reset the environment before taking any actions</span></pre><p id="7717" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建代理甚至更容易——因为它只是一个随机漫步机，我们可以使用 gym 的<strong class="is hj">env . action _ space . sample()</strong>函数来生成一个有效的随机动作。每个动作都是一个整数，其中 0 表示向左移动，2 表示向右移动，1 表示什么都不做。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="817d" class="jo jp hi lx b fi mb mc l md me">action = env.action_space.sample()<br/>env.step(action)</span></pre><p id="73da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们可以将代理的动作代码放在 for 循环中，这样它就可以执行多个动作，这样就可以了:在一个新创建的环境中随意走动。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="60f7" class="jo jp hi lx b fi mb mc l md me">import gym</span><span id="a076" class="jo jp hi lx b fi mf mc l md me">env = gym.make('MountainCar-v0')<br/>env.reset()</span><span id="00b4" class="jo jp hi lx b fi mf mc l md me">for i in range(1000):<br/>    env.render()<br/>    <br/>    action = env.action_space.sample()<br/>    env.step(action)</span><span id="820c" class="jo jp hi lx b fi mf mc l md me">env.close()</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/c4ef8328cb8dcceb140eaaebc4f7f9d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/1*YZXifBMSlHtzrlp40NQ0cQ.gif"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">样本随机漫步—脚本可在此处<a class="ae lv" href="https://nbviewer.jupyter.org/github/Robert-MacWha/Reinforcement_Learning-From_The_Basics/blob/main/1_random_walker.ipynb" rel="noopener ugc nofollow" target="_blank">获得</a></figcaption></figure><h1 id="5712" class="le jp hi bd jq lf mh lh ju li mi lk jy ll mj ln kb lo mk lq ke lr ml lt kh lu bi translated">构建智能模型</h1><p id="4818" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">如果你只是想看到一个代理与环境交互，那么拥有一个随机漫步机是很好的，但是如果我们想让代理学习，就需要一些更复杂的东西。最简单的方法是做一个 Q 表。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/4d51e7c5d1017c32b8e8a6d47b8a3fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*-lIwcEPoyIuz5Tba5S0SOA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">乒乓球游戏的查找表</figcaption></figure><p id="0280" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Q 表可以被认为是一个查找表。在一列中，您有代理可能遇到的每个状态。在另一列中，您有在所述状态下应该采取的动作。</p><p id="489d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了创建这个 Q 表，我们需要知道两件事。代理可以采取多少个动作，可以处于多少种状态？</p><p id="1f9f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">获取动作的数量相对容易——只需从<strong class="is hj"> env.action_space.n </strong>变量中读取即可。状态空间也可以通过随机观察的长度很容易地找到。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="3efa" class="jo jp hi lx b fi mb mc l md me">ACTION_SPACE = env.action_space.n # 3</span><span id="6dc8" class="jo jp hi lx b fi mf mc l md me">OBSERVATION_SPACE = len(env.observation_space.sample()) # continuous(2)</span></pre></div><div class="ab cl kx ky gp kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hb hc hd he hf"><h2 id="d979" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">创建 Q 表</h2><p id="1ef6" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">一旦确定了动作和状态空间，我们就可以开始构建 Q 表了。首先要做的是把环境返回的连续观测空间，剁成一组离散的单元。这样做是因为在连续观察空间中有无限多个离散状态。</p><div class="kp kq kr ks fd ab cb"><figure class="mn ij mo mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/d65974d631a793d163b9dc4cfaf60b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*q2tbR_DxvZosIxWunTdwww.png"/></div></figure><figure class="mn ij mo mp mq mr ms paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/888a9e6f52fbbea27d53ba1ae233295c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lN5fxO637ljtUhFqdZVfaA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx mt di mu mv translated">这两种状态在技术上是不同的，但是它们是如此的相似，以至于把它们如此看待是没有意义的。</figcaption></figure></div><p id="740b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为什么我们关心是否有无限多的离散状态？如果不这样做，我们就永远无法建立 Q 表。想象一下，编写一本说明书，其中需要为日期、时间、天气和用户的中间名的每一种可能的配置单独写一页。这是不可能的。因此，我们将把相似的状态组合成离散的块。</p><p id="846a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，让我们创建 Q 表。对于这种环境，它的形状为[20，20，3]——前两个维度对应于观察空间中每个参数的 20 个离散单元，最后一个维度对应于三个动作。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="5fcc" class="jo jp hi lx b fi mb mc l md me">Q_INCREMENTS = 20 # the number of discrete cells<br/>DISCRETE_OS_SIZE = [Q_INCREMENTS] * OBSERVATION_SPACE # an array of shape [20, 20]</span><span id="12f1" class="jo jp hi lx b fi mf mc l md me">q_table = np.random.uniform(<br/>    low=-1, # low value is minimum reward<br/>    high=0, # high value is maximum reward<br/>    size=(DISCRETE_OS_SIZE + [ACTION_SPACE])) # an array of shape [20, 20, 3]</span></pre><p id="0e46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有了 Q 表，我们还需要构建一个函数，将连续状态转换为离散单元，这样我们就可以索引 Q 表。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="5eb9" class="jo jp hi lx b fi mb mc l md me">def continuous_Observation_To_Index(env, observation, increments):<br/>    <br/>    # get the range of the observation_space so it can be normalized<br/>    obs_min = env.observation_space.low<br/>    obs_max = env.observation_space.high</span><span id="4940" class="jo jp hi lx b fi mf mc l md me">    # normalize the observation so it goes from 0-1<br/>    obs = (obs - obs_min) / (obs_max - obs_min)</span><span id="7750" class="jo jp hi lx b fi mf mc l md me">    # convert the normalized observation into an integer index<br/>    indice = tuple(np.round(obs * increments).astype(int))</span><span id="87e2" class="jo jp hi lx b fi mf mc l md me">    return indice</span></pre><p id="1bd9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们现在可以修改 random walker，使其基于带有这两个代码块的预先生成的 Q 表采取行动。在任何给定的状态下，步行者将采取 Q 表中对应值最高的动作。这样做是因为稍后当我们开始训练代理人时，最高价值将与产生最高未来回报的行动相对应。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="3b59" class="jo jp hi lx b fi mb mc l md me"># store the initial state of the environment<br/>done = False<br/>observation = env.reset()</span><span id="a679" class="jo jp hi lx b fi mf mc l md me">while not done:<br/>    env.render()</span><span id="2d48" class="jo jp hi lx b fi mf mc l md me">    # get the action coresponding to the current observation<br/>    indice = obs_To_Index(env, observation, Q_INCREMENTS)<br/>    action = q_table[indice].argmax()</span><span id="fa81" class="jo jp hi lx b fi mf mc l md me">    # take the action<br/>    new_observation, reward, done, info = env.step(action)<br/>    observation = new_observation</span><span id="bac8" class="jo jp hi lx b fi mf mc l md me">env.close()</span></pre></div><div class="ab cl kx ky gp kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hb hc hd he hf"><h2 id="a135" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">培养</h2><p id="382d" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">现在我们已经有了一个 Q 表，一切都可以正常工作了，对吗？不完全是。我们有说明书，但目前所有的页面都充满了胡言乱语。下一步是更新 Q 表的值，以更好地反映代理在给定状态下应该采取什么行动。</p><p id="9bba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了更新 Q 表，我们可以使用贝尔曼方程，这是一个从当前状态和奖励预测未来奖励的函数。这个等式通过缓慢更新 Q 表中的值来匹配<strong class="is hj">预测的未来回报</strong>、<strong class="is hj">、</strong>一个状态有多“好”的数字表示。</p><p id="c97c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们如何找到这个预测的未来回报？这就是我们存储在 Q 表中的值。因此，为了计算 Q 表的新值，我们可以使用以下等式:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/3051059b7f88db76fea5974dacdbaba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_QQU-FO5mardAhqS97qxqg.png"/></div></div></figure><p id="1ee8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个等式可以被翻译成代码并在我们的算法中实现，这样代理就可以开始学习了。首先，需要定义学习率(lr)和折扣(Y)超参数。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="cf03" class="jo jp hi lx b fi mb mc l md me">LEARNING_RATE = 0.1 # lr - how quickly values in the q table change<br/>DISCOUNT      = 0.95 # Y - how much the agent cares about future rewards</span></pre><p id="1074" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我们可以指定训练模型的时期数。把一个时期想象成一个训练周期。代理不会在一堂课中神奇地学会所有东西，所以我们需要通过多次训练来巩固课程。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="2d50" class="jo jp hi lx b fi mb mc l md me">EPOCHS        = 5000</span></pre><p id="372c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，我们可以将上面的贝尔曼方程转换成 python 代码，这样它就可以集成到我们的脚本中。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="1d74" class="jo jp hi lx b fi mb mc l md me"># calculate the predicted future reward<br/>new_indice = obs_To_Index(env, new_observation, Q_INCREMENTS)<br/>future_reward = reward + DISCOUNT * q_table[new_indice].max()</span><span id="0934" class="jo jp hi lx b fi mf mc l md me"># update the value in the q table<br/>current_q = q_table[indice + (action,)]<br/>new_q = (1 - LEARNING_RATE) * <br/>         current_q + LEARNING_RATE * future_reward</span></pre><p id="070e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦完成，我们将修改先前脚本的培训部分，以集成培训代码。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="1ee7" class="jo jp hi lx b fi mb mc l md me">for e in range(EPOCHS):</span><span id="6e74" class="jo jp hi lx b fi mf mc l md me">    # store the initial state of the environment<br/>    done = False<br/>    observation = env.reset()</span><span id="86ad" class="jo jp hi lx b fi mf mc l md me">    while not done:<br/>        env.render()</span><span id="48d3" class="jo jp hi lx b fi mf mc l md me">        # get the action coresponding to the current observation<br/>        indice = obs_To_Index(env, observation, Q_INCREMENTS)<br/>        action = q_table[indice].argmax()</span><span id="0965" class="jo jp hi lx b fi mf mc l md me">        # take the action<br/>        new_observation, reward, done, info = env.step(action)</span><span id="43d6" class="jo jp hi lx b fi mf mc l md me">        # train the Q-Table</span><span id="1f59" class="jo jp hi lx b fi mf mc l md me">        # calculate the predicted future reward<br/>        new_indice = obs_To_Index(env, new_observation, Q_INCREMENTS)<br/>        future_reward = reward + DISCOUNT * q_table[new_indice].max()</span><span id="ccf0" class="jo jp hi lx b fi mf mc l md me">        # update the value in the q table<br/>        current_q = q_table[indice + (action,)]<br/>        new_q = (1 - LEARNING_RATE) * <br/>                current_q + LEARNING_RATE * future_reward</span><span id="e453" class="jo jp hi lx b fi mf mc l md me">        # update the observation<br/>        observation = new_observation</span><span id="622d" class="jo jp hi lx b fi mf mc l md me">env.close()</span></pre><p id="6f7d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在您已经完成了脚本的编写，试着运行一下，在代理训练的时候等待几百个纪元。这可能需要一段时间，取决于你的硬件，但一旦完成，你会留下一个非常有能力的车。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/7e98c3c9e90ca9c75ca0853bd2ae8c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/1*naNyJc8yVgDl5G5centZgA.gif"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">经过全面培训的 Q 表示例—脚本可用<a class="ae lv" href="https://nbviewer.jupyter.org/github/Robert-MacWha/Reinforcement_Learning-From_The_Basics/blob/main/3_trainable_q_table.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure></div><div class="ab cl kx ky gp kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hb hc hd he hf"><h2 id="bc67" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">埃普西隆</h2><p id="e676" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">还有最后一个应该对代理进行的调整，以便它可以学习导航更复杂的环境。该代理目前可以导航简单的环境，但是，一旦它找到解决方案，它将停止改进，即使该解决方案是次优的。这是因为改善它的表现将需要执行不会导致最高预测回报的行动，这是代理目前不能做的。</p><p id="ccb6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在简单的环境中，这不会是一个问题，但是在更复杂的模拟中，有不同的获得奖励的方法，这可能对代理的成功有害。</p><p id="774d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，我们如何阻止代理一次又一次地采取相同的动作呢？为什么，强行让它在一定比例的时间里采取随机行动。这就是 epsilon 所做的，它让代理探索环境并寻找更好的解决方案。</p><p id="3faf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Epsilon 的实现非常简单。你只需要两个新的超参数——ε和ε衰变。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="44f4" class="jo jp hi lx b fi mb mc l md me">EPSILON = 0.5          # how often the agent takes random actions<br/>EPSILON_DECAY = 0.9998 # rate at which epsilon gets reduced</span></pre><p id="2744" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">艾司隆控制代理采取随机行动的速度。在培训过程开始时，这个数字会很高，但随着代理的改进，我们会降低这个数字。这个还原过程由ε衰变变量处理。</p><p id="c86f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">实现这个系统需要在选择动作的地方改变代码。模型现在有时会采取随机的探索性行动，而不是总是采取 Q 表指定的行动。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="d866" class="jo jp hi lx b fi mb mc l md me"># select the action to take<br/>if random.uniform(0, 1) &lt; EPSILON:<br/>    action = env.action_space.sample() # random action (exploration)<br/>else:<br/>    action = q_table[indice].argmax()  # action from the q table</span></pre><p id="6694" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，我们需要在每个时期结束时减少ε。</p><pre class="kp kq kr ks fd lw lx ly lz aw ma bi"><span id="5497" class="jo jp hi lx b fi mb mc l md me">EPSILON *= EPSILON_DECAY</span></pre><p id="3332" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">就是这样！对于像代理这样的简单环境，使用 epsilon 进行训练可能需要稍长的时间，但这是为生成最佳解决方案而付出的合理代价。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/5b1570d46e4ee5b88168a38790b511b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/1*aVHwirRknzurV4rO49Mi_w.gif"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">带 epsilon 的完整训练 Q 表示例—脚本可用<a class="ae lv" href="https://nbviewer.jupyter.org/github/Robert-MacWha/Reinforcement_Learning-From_The_Basics/blob/main/4_trainable_q_table_epsilon.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure></div><div class="ab cl kx ky gp kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hb hc hd he hf"><blockquote class="my mz na"><p id="d3b7" class="iq ir nb is b it iu iv iw ix iy iz ja nc jc jd je nd jg jh ji ne jk jl jm jn hb bi translated">感谢阅读我的文章！请随意查看我的<a class="ae lv" href="https://tks.life/profile/robert.macwha#portfolio" rel="noopener ugc nofollow" target="_blank">作品集</a>，如果你有什么要说的，请在<a class="ae lv" href="https://www.linkedin.com/in/robert-macwha-0555141b6/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我发消息，或者在 Medium 上关注我，以便在我发布另一篇文章时得到通知！</p></blockquote></div></div>    
</body>
</html>