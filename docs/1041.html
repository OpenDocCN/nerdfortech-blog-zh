<html>
<head>
<title>NLP Zero to One: Count based Embeddings, GloVe (Part 6/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP零到一:基于计数的嵌入，手套(第6/30部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-count-based-embeddings-glove-part-6-40-c5bb3ebfd081?source=collection_archive---------14-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-count-based-embeddings-glove-part-6-40-c5bb3ebfd081?source=collection_archive---------14-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="68d6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">基于共现的模型和动态逻辑回归。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/250434336dcbba53c5c73a9e1929a69c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKBmy7IWYH9xvuKNDhObzw.png"/></div></div></figure><h1 id="d03a" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">介绍..</h1><p id="3c1c" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">在之前的博客中，我们定义了嵌入，并讨论了Word2Vec中流行的神经架构之一。在这篇博客中，我们将简要讨论另一个著名的神经架构，叫做Skip-gram。我们将花费大量时间了解其他可用的嵌入，如GloVe。</p><h1 id="a432" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">跳跃图..</h1><p id="bb9a" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">CBOW模型被训练来基于附近的上下文单词预测目标单词，skip-gram模型被训练来基于目标单词预测附近的上下文单词。与CBOW模型正好相反。对于上下文窗口c，跳过语法模型被训练来预测目标单词周围的c个单词。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kx"><img src="../Images/56001b5af9232b3c5595fa90c7776694.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tTVa_9Cl5XLP6fBVVmVDTQ.png"/></div></div></figure><p id="ca39" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">skip-gram的神经结构和训练与CBOW非常相似。因此，我们将把我们的讨论限制在跳跃图的神经结构上。跳跃图模型的目标是最大化平均对数概率:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/7bc5a2b9ebbc46bf6b3b9b67d4f57d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*o8hZf1CM_zHq5fl3yQonjA.png"/></div></figure><h1 id="6c4b" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">Word2Vec的缺点..</h1><p id="ae36" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">尽管word2vec给自然语言处理带来了巨大的改进和提高，但它们也不是没有缺点。</p><ol class=""><li id="53c2" class="le lf hi kd b ke ky kh kz kk lg ko lh ks li kw lj lk ll lm bi translated">Word2vec是基于本地上下文的，通常在捕获语料库的统计数据方面表现不佳。</li><li id="b1df" class="le lf hi kd b ke ln kh lo kk lp ko lq ks lr kw lj lk ll lm bi translated">无法处理未知的或不在词汇表中的单词:如果你的模型以前没有遇到过一个单词，它将不知道如何解释它或如何为它建立一个向量</li><li id="e9fd" class="le lf hi kd b ke ln kh lo kk lp ko lq ks lr kw lj lk ll lm bi translated">Word2Vec是基于本地上下文的，通常在捕获语料库的统计数据方面表现不佳。</li></ol><h1 id="e115" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">基于共现的模型..</h1><p id="dea3" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">众所周知，像Word2Vec这样的基于局部上下文的方法不能捕获语料库的全局统计/结构。除了本地化方法之外，还有另一个学派建议对文本或语料库进行全球理解。这个学派提出，通过分析单词在可用语料库中所有文档中的出现，可以理解单词之间的强关联。这种方法被称为共现模型，因为词的共现可以揭示它们的语义接近度和意义。<br/>我们需要一种方法来量化两个词“W1”、“W2”的共现。逐点互信息(PMI)是一种非常流行的同现测度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/4e79b00f79cb7e8163a852002f22ef38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*mWaGYpj1x9RMP9719sSrcQ.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">采购经理指数（Purchasing Managers' Index的缩写）</figcaption></figure><p id="8741" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">p(w)是单词出现的概率，p(w1，w2)是联合概率。高的<strong class="kd hj"> PMI表明单词之间有很强的关联性。</strong></p><p id="bb0a" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">同现方法通常是非常高维的，并且需要很多存储空间。NLP工程师通常利用降维技术来处理高维数据。尽管由于巨大的存储需求，基于全局共现的模型成功地捕获了全局统计，但是这些模型不能代替静态的Word2Vec嵌入。</p><h1 id="887f" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">手套(全局向量)..</h1><p id="bc54" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">除了word2vec之外，使用最广泛的静态嵌入模型是Global Vectors的GloVe简称。该模型基于捕获全局语料库统计数据。该方法结合了共现方法和浅窗口方法。让我们简单了解一下手套向量是如何创建的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lw"><img src="../Images/d13d70d07b0ed8b0a8827f0cbfb29290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0jSrYEp7A_kwzsw80_e2YQ.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">作者生成的图像</figcaption></figure><p id="127a" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated"><strong class="kd hj">单词-单词共现矩阵:</strong> A矩阵X，其中单元格Xij为A表示Wi在语料库中的Wj的上下文中出现的频率或者Wi和Wj在语料库中共现的次数。<br/> <strong class="kd hj">概率比</strong> : GloVe是基于词-词共现矩阵的概率比，这是起点。让我们看一个例子来理解这个概念(概率比)背后的直觉。<br/>设<strong class="kd hj"> P(k|w) </strong>为单词k出现在单词w的上下文中的概率，单词{“水”、“冰”}一起出现所以<strong class="kd hj"> P </strong>(“冰”/“水”)会高。还有字{“水”、“汽”}一起出现，所以<strong class="kd hj"> P </strong>(“汽”/“水”)也会高。<br/> <strong class="kd hj">比例:P </strong>(“冰”/“水”)⊙<strong class="kd hj">P</strong>(“蒸汽”/“水”)；因为分子和分母都很高，所以比率将接近1。这个比率= 1所解释的是，“水”(也称为<em class="lx">探测词</em>)与单词“冰”和“蒸汽”非常接近，因为它们一起出现。这个比率给了我们三个不同单词之间关系的提示。我们将利用这个想法来构建向量。</p><h1 id="9faf" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">手套训练..</h1><p id="11c7" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们将需要建立单词向量来显示每一对单词<em class="lx"> i </em>和<em class="lx"> j </em>如果同时出现的话是如何同时出现的。GloVe预测周围单词的方式是通过执行<strong class="kd hj">动态逻辑回归，在给定中心单词的情况下，最大化上下文单词出现的概率。</strong></p><p id="1b42" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">对于每个“U”和“V”，我们将使用软约束创建向量。我们将通过最小化目标函数<em class="lx"> J </em>来找到向量，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/546fbc3d3095779ae9f731b06c9aa4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*7k8Q0tcR_K9TdEwiQ8wcOw.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">目标函数</figcaption></figure><p id="bd63" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">其中V是词汇量的大小，X是<strong class="kd hj">单词-单词共现矩阵。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/44a5ba7cb295dedee0589fd827103649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*Raq07IoO-hVVoSUcgfru4A.png"/></div></figure><p id="ace0" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">f(。)是一个加权函数，并且具有截断的幂律形式，以处理低共现单词，这些单词比频繁出现的一次携带更少的信息。所以我们在目标函数中给对应于这些低共现词的损失较小的权重。</p><p id="846f" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">手套嵌入可以通过向量加法和减法表达语义和句法关系。在许多NLP任务中，GloVe甚至比Word2Vec执行得更好，因为GloVe还捕获了全局上下文依赖。</p><h1 id="537d" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">注意..</h1><p id="7878" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated"><strong class="kd hj">分级Softmax: </strong>普通<strong class="kd hj"/>CBOW和skip-gram中的Softmax层是输出层中的用户，用于预测单词和计算损失，使用softmax预测单词可能是非常计算昂贵的步骤，因为词汇量很大。分层softmax的计算效率更高，它使用输出层的二叉树表示。<br/> <strong class="kd hj">负采样:</strong>替代基于噪声对比估计的分层softmax)。基本思想是，好的模型应该能够通过逻辑回归将数据与噪声区分开来。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div></figure><p id="c245" class="pw-post-body-paragraph kb kc hi kd b ke ky ij kg kh kz im kj kk la km kn ko lb kq kr ks lc ku kv kw hb bi translated">接下来:<a class="ae lz" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-training-embeddings-using-gensim-and-visualisation-part-7-30-f0540e976568?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kd hj"> NLP零对一:使用Gensim和可视化训练嵌入(第7/30部分)</strong> </a> <br/>上一篇:<a class="ae lz" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-dense-representations-word2vec-part-5-30-9b38c5ccfbfc?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kd hj"> NLP零对一:密集表示，Word2Vec(第5/30部分)</strong> </a></p></div></div>    
</body>
</html>