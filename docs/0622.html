<html>
<head>
<title>Spark Read from &amp; Write to HBase table using DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用数据帧对HBase表进行Spark读写</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/spark-read-from-write-to-hbase-table-using-dataframes-5c3b585c161?source=collection_archive---------0-----------------------#2021-01-29">https://medium.com/nerd-for-tech/spark-read-from-write-to-hbase-table-using-dataframes-5c3b585c161?source=collection_archive---------0-----------------------#2021-01-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fbe0" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">有可能使用结构化的API来操作非结构化的数据吗？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/4d2c753b0ff16dec1ddee4cc77c5af1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6sJPZd49DMQ72YWYKLxfA.png"/></div></div></figure><p id="660a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi kf translated">如今，对NoSQL数据库的需求变得非常迫切，我们用不同的形式和结构操作数据，HBase (Hadoop Base)就是为了满足这种需求而创建的，它建立在HDFS (Hadoop分布式文件系统)之上，提供了跨多个节点的基于对象的数据的分布式存储。</p><p id="2dac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">与任何其他NoSQL数据库类似，HBase将数据表示为由字段(列)组成的对象，每个对象(相当于关系数据库中的一个数据行)都有一个唯一的键(比其邻居多或少的列)。</p><p id="7dec" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">此外，Hbase引入了列族的概念，将相关数据分组在一起，以实现更优化的读写操作。下图为我们提供了有关此事的更多细节:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ko"><img src="../Images/8fdf5184b25a3fc52955e620f8bca64e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWRpdW8U4uSaQs71G1jQlQ.jpeg"/></div></figure><p id="2c3b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">每个行键都被编入索引，以执行快速数据检索和更新操作。</p><p id="79ec" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我希望你喜欢这个简短的介绍，很明显，HBase有很多有趣的功能，从数据分布，NoSQL数据表示，到对更新操作的支持，它真的很酷，对吗？尽管我们仍然没有提到非常重要的一点，特别是在生产环境中，如何将其与现有的大数据环境集成？</p><p id="2140" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了缩小答案的范围，我想在本文中重点讨论Spark-Hbase集成，Spark几乎存在于任何集群部署中，它被认为是最受欢迎的大数据工具之一，因此，很明显它们最终会相互交流。</p><p id="dc7e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">HBase有一个用于数据操作的默认API，提供的主要方法如下:</p><ul class=""><li id="b7e8" class="kp kq hi jl b jm jn jp jq js kr jw ks ka kt ke ku kv kw kx bi translated"><strong class="jl hj">扫描</strong>:扫描一定范围的行</li><li id="3faf" class="kp kq hi jl b jm ky jp kz js la jw lb ka lc ke ku kv kw kx bi translated"><strong class="jl hj">获取</strong>:使用一个键获取特定的行</li><li id="f09b" class="kp kq hi jl b jm ky jp kz js la jw lb ka lc ke ku kv kw kx bi translated"><strong class="jl hj"> Put </strong>:在表格中插入一行</li></ul><p id="7c30" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Hbase-Spark社区支持这种低级交流:</p><div class="ld le ez fb lf lg"><a href="https://hbase.apache.org/book.html#_basic_spark" rel="noopener  ugc nofollow" target="_blank"><div class="lh ab dw"><div class="li ab lj cl cj lk"><h2 class="bd hj fi z dy ll ea eb lm ed ef hh bi translated">Apache HBase参考指南</h2><div class="ln l"><h3 class="bd b fi z dy ll ea eb lm ed ef dx translated">13.4.1.注意变化！首先，我们将介绍升级到HBase时您可能会遇到的部署/运营变化…</h3></div><div class="lo l"><p class="bd b fp z dy ll ea eb lm ed ef dx translated">hbase.apache.org</p></div></div></div></a></div><p id="c853" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">它可以很好地处理小规模的数据，但是，对于大数据集，性能问题会一个接一个地出现，相信我，当我第一次开始使用HBase时，我也经历过类似的情况。</p><p id="e2b9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们已经走到了路的尽头，有什么解决办法？，如何既保留NoSQL的好处，又不以牺牲性能为代价？</p><p id="0b56" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Spark不是无缘无故地如此受欢迎，它有一个丰富的社区，当我第一次在工作中面临这一困难时，它让我的日子变得很糟糕，但最终，我找到了解决所有问题的方法，一种以优化的方式读写HBase的方法，这是在Spark DataFrames的帮助下。</p><p id="7a45" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我希望现在我已经引起了你的注意，你很想知道解决方案是什么？让我们一起来发现吧。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><h1 id="49fd" class="lw lx hi bd ly lz ma mb mc md me mf mg io mh ip mi ir mj is mk iu ml iv mm mn bi translated">从HBase读取数据</h1><p id="bf43" class="pw-post-body-paragraph jj jk hi jl b jm mo ij jo jp mp im jr js mq ju jv jw mr jy jz ka ms kc kd ke hb bi kf translated">公园数据帧是数据的结构化表示，支持类似SQL的操作，以同样方式与HBase交互的关键是在对象字段和我们的数据帧列之间创建映射</p><p id="480e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了详细说明我们的用例，我们首先使用HBase Shell创建一个测试表，然后用一些数据填充它:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/406755dfb6309b4837cf9474109bcad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7LmpTPQdhyUVQWFUIhY8tQ.png"/></div></div></figure><p id="0ca0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我创建了两个纵队家族<strong class="jl hj">骑兵</strong>和<strong class="jl hj">步兵</strong>(我是古代战争的忠实粉丝)，表格的全扫描显示了可用的数据:</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="f138" class="mz lx hi mv b fi na nb l nc nd">hbase(main):154:0&gt; scan 'default:WAR_PLAN'</span><span id="af2f" class="mz lx hi mv b fi ne nb l nc nd">ROW                                    COLUMN+CELL</span><span id="1be9" class="mz lx hi mv b fi ne nb l nc nd">1        column=cavalry:number, timestamp=1609327934978, value=65</span><span id="a24d" class="mz lx hi mv b fi ne nb l nc nd">2        column=infantry:number, timestamp=1609327954343, value=35</span><span id="075b" class="mz lx hi mv b fi ne nb l nc nd">3        column=cavalry:number, timestamp=1609327983183, value=50</span><span id="cbe6" class="mz lx hi mv b fi ne nb l nc nd">3        column=infantry:number, timestamp=1609327965760, value=50</span></pre><p id="315d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，让我们编写一些Spark代码来检索这些数据，在下面的例子中，我将使用Scala编程语言，我为Python爱好者道歉，尽管使用Pyspark也可以做到这一点:</p><p id="54c5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们首先启动Hbase和Spark之间的连接，HBase完全依赖于<a class="ae nf" href="https://zookeeper.apache.org/" rel="noopener ugc nofollow" target="_blank"> Zookeeper </a>进行节点间的状态通信，因此，首先，我们提供其相关的仲裁(运行Zookeeper服务器的主机列表)和客户端端口。</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="957d" class="mz lx hi mv b fi na nb l nc nd">import org.apache.hadoop.hbase.spark.HBaseContext<br/>import org.apache.hadoop.hbase.HBaseConfiguration</span><span id="c81d" class="mz lx hi mv b fi ne nb l nc nd">val conf = new HBaseConfiguration()    conf.set("hbase.zookeeper.quorum", "hostname1,hostname2...")  conf.set("hbase.zookeeper.property.clientPort", "2181") <br/>new HBaseContext(spark.sparkContext, conf)</span><span id="c7a1" class="mz lx hi mv b fi ne nb l nc nd">val hbaseTable = "default:WAR_PLAN"</span><span id="7fd5" class="mz lx hi mv b fi ne nb l nc nd">val columnMapping = """id string :key,        <br/>                      |infantryNumber string infantry:number,        <br/>                      |cavalryNumber string cavalry:number"""<br/>                      .stripMargin    </span><span id="4445" class="mz lx hi mv b fi ne nb l nc nd">val hbaseSource = "org.apache.hadoop.hbase.spark" </span></pre><p id="38fe" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">HBase表是指我们正在尝试加载的表，其格式遵循以下语法:</p><p id="6fcf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="ng">命名空间:表名</em> </strong></p><p id="3aa5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">映射是一个逗号分隔的列表，每个元素格式遵循以下规则:</p><p id="29e3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="ng"> DataframeColumnName类型column family:HbaseColumnName</em></strong></p><p id="5f8c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后，hbaseSource对应于负责确保Spark和HBase之间通信的库，为了使用它，应该将下面一行添加到您的<strong class="jl hj"> sbt </strong>依赖项中(我使用的是<a class="ae nf" href="https://www.cloudera.com/products/open-source/apache-hadoop/key-cdh-components.html" rel="noopener ugc nofollow" target="_blank"> Cloudera CDH </a>提供的库):</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="ebd6" class="mz lx hi mv b fi na nb l nc nd">libraryDependencies += "org.apache.hbase" % "hbase-spark" % "2.1.0-cdh6.3.4"</span></pre><p id="97a2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下面是相应的Maven库，如果你使用的是包管理器而不是<strong class="jl hj"> sbt : </strong></p><div class="ld le ez fb lf lg"><a href="https://mvnrepository.com/artifact/org.apache.hbase/hbase-spark" rel="noopener  ugc nofollow" target="_blank"><div class="lh ab dw"><div class="li ab lj cl cj lk"><h2 class="bd hj fi z dy ll ea eb lm ed ef hh bi translated">maven Repository:org . Apache . h base " h base-spark</h2><div class="ln l"><h3 class="bd b fi z dy ll ea eb lm ed ef dx translated">编辑描述</h3></div><div class="lo l"><p class="bd b fp z dy ll ea eb lm ed ef dx translated">mvnrepository.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm jh lg"/></div></div></a></div><p id="44f5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，让我们将已经定义的参数合并到一行代码中，并将我们的数据加载到DataFrame中:</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="74ac" class="mz lx hi mv b fi na nb l nc nd">val hbaseData = sql.read.format(hbaseSource).option("hbase.columns.mapping", columnMapping).option("hbase.table", hbaseTable)</span><span id="803a" class="mz lx hi mv b fi ne nb l nc nd">val hbaseDf= hbaseData.load() </span></pre><p id="4314" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这项工作几乎完成了，我们的配方变了一些魔术，这是我们的数据看起来像这样:</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="a0e3" class="mz lx hi mv b fi na nb l nc nd">+-------------+--------------+--------+</span><span id="6d07" class="mz lx hi mv b fi ne nb l nc nd">|cavalryNumber|infantryNumber|  id    |</span><span id="114c" class="mz lx hi mv b fi ne nb l nc nd">+-------------+--------------+--------+</span><span id="a8d0" class="mz lx hi mv b fi ne nb l nc nd">|           65|          null|   1    |</span><span id="2893" class="mz lx hi mv b fi ne nb l nc nd">|         null|            35|   2    |</span><span id="381a" class="mz lx hi mv b fi ne nb l nc nd">|           50|            50|   3    |</span><span id="8585" class="mz lx hi mv b fi ne nb l nc nd">+-------------+--------------+--------+</span></pre><p id="17ca" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这是一个spark结构化数据帧，表示实际驻留在Hbase中的数据，这真的很酷，Spark数据帧API对我们来说已经完全可用(Hbase数据帧上的一些操作将被排除，如joins…，进一步的解释将在单独的文章中提供)。</p><h1 id="b227" class="lw lx hi bd ly lz nn mb mc md no mf mg io np ip mi ir nq is mk iu nr iv mm mn bi translated">将数据写入HBase</h1><p id="1ccc" class="pw-post-body-paragraph jj jk hi jl b jm mo ij jo jp mp im jr js mq ju jv jw mr jy jz ka ms kc kd ke hb bi kf translated">一旦创建了相应的spark数据帧，写入Hbase表将不再困难(这正是我们在上一步中所做的)。</p><p id="b480" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将使用与之前相同的Spark-Hbase API，它不仅有助于读取，而且可以将使用Hive sql查询构建的结构化数据帧写入具有非结构化模式的Hbase表中。</p><p id="edf3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我已经准备了一个有两列和几行的Hive表，下面是表的内容:</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="8333" class="mz lx hi mv b fi na nb l nc nd">hive&gt; select * from work.war_plan</span><span id="f158" class="mz lx hi mv b fi ne nb l nc nd">id      infantrynumber   cavalrynumber</span><span id="199a" class="mz lx hi mv b fi ne nb l nc nd">4             56              7</span><span id="d719" class="mz lx hi mv b fi ne nb l nc nd">5             6               99</span><span id="3831" class="mz lx hi mv b fi ne nb l nc nd">6             50              27</span></pre><p id="5453" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">回到编码的时候了，我们首先在我们的Hive表的顶部构建一个spark数据帧:</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="b6df" class="mz lx hi mv b fi na nb l nc nd">val hiveTmp = spark.sql("select * from default.war_plan") <br/>val columns: Array[String]= hbaseDf.columns<br/>val hiveDf = hiveTmp.select(columns.head, columns.tail: _*) </span><span id="0e4f" class="mz lx hi mv b fi ne nb l nc nd">hiveDf.createOrReplaceTempView("hiveDataframe")</span></pre><p id="b133" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然后，我们使用Spark-SQL insert语句将数据从Hive数据仓库移动到Hbase存储中:</p><pre class="iy iz ja jb fd mu mv mw mx aw my bi"><span id="6504" class="mz lx hi mv b fi na nb l nc nd">val insertStatement = "insert into hbaseDataframe select * from hiveDataframe"</span><span id="eab3" class="mz lx hi mv b fi ne nb l nc nd">spark.sql(insertStatement)</span></pre><p id="5e87" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里所有的魔法都发生在引擎盖中，Spark首先将insert语句翻译成它的对等Hbase put方法，然后，使用Hbase Spark原生API运行生成的指令，<a class="ae nf" href="https://hbase.apache.org/book.html#_basic_spark" rel="noopener ugc nofollow" target="_blank">此链接提供了一个示例代码</a>。</p><p id="6554" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Spark-Hbase Dataframe API不仅易于使用，而且还极大地提高了读写性能。事实上，在连接建立阶段，每个Spark执行器直接与同一本地节点上的Hbase区域通信，因此读写操作以分布式模式执行。</p><h1 id="c55d" class="lw lx hi bd ly lz nn mb mc md no mf mg io np ip mi ir nq is mk iu nr iv mm mn bi translated">结论</h1><p id="641d" class="pw-post-body-paragraph jj jk hi jl b jm mo ij jo jp mp im jr js mq ju jv jw mr jy jz ka ms kc kd ke hb bi translated">我们通过本教程设法与NoSQL数据库交互:Hbase通过Spark Dataframe API，完整代码在我的<a class="ae nf" href="https://github.com/bechirnahali/hbase-spark-integration" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中提供。</p><p id="102b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我希望你觉得这个帖子有用，内容丰富，不要忘记在评论区讨论它，欢迎所有问题，谢谢你的阅读。</p></div></div>    
</body>
</html>