<html>
<head>
<title>Natural Language Processing | Feature Extraction Techniques.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理|特征提取技术。</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/natural-language-processing-feature-extraction-techniques-745f690041e6?source=collection_archive---------2-----------------------#2021-08-13">https://medium.com/nerd-for-tech/natural-language-processing-feature-extraction-techniques-745f690041e6?source=collection_archive---------2-----------------------#2021-08-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="55db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数经典的机器学习和深度学习算法都不能接受原始文本。相反，我们需要从原始文本中执行特征提取，以便将数字特征传递给机器学习算法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/b4e4c68820e56fe9dfdfa9ce91a51318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSiFvw-DLQmNgu6F8Q1Iuw.png"/></div></div></figure><h2 id="e3ad" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">词汇袋模型— TF</h2><p id="7448" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">对于非结构化文本，这可能是最简单的向量空间表示模型。向量空间模型是一个简单的数学模型，将非结构化文本(或任何其他数据)表示为数字向量，这样向量的每个维度都是一个特定的特征属性。单词袋模型将每个文本文档表示为数字向量，其中每个维度是来自语料库的特定单词，并且该值可以是它在文档中的频率、出现次数(用1或0表示)或者甚至是加权值。这个模型之所以叫这个名字，是因为每个文档都被字面上表示为它自己的单词的“包”,而不考虑单词顺序、序列和语法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kp"><img src="../Images/0b515059b2f9ed5e68e94a7cc2e7a034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlrONHzBucQxddzmElhVJg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">计数矢量器</figcaption></figure><p id="b565" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这应该会让事情更清楚吧！您可以清楚地看到，特征向量中的每一列或每一个维度代表语料库中的一个单词，每一行代表我们的一个文档。任何单元格中的值表示该单词(由列表示)在特定文档(由行表示)中出现的次数。因此，如果一个文档集由跨越所有文档的N个唯一的单词组成，那么对于每个文档，我们将有一个N维向量。</p><h2 id="8baf" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">N克模型袋</h2><p id="5538" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">一个单词只是一个标记，通常被称为一元语法或一元语法。我们已经知道单词袋模型不考虑单词的顺序。但是如果我们也想考虑出现在序列中的短语或单词集合呢？N-grams帮助我们实现了这一点。N元语法基本上是来自文本文档的单词标记的集合，使得这些标记是连续的并且按顺序出现。二元语法表示2阶的n元语法(两个单词)，三元语法表示3阶的n元语法(三个单词)，依此类推。因此，N-gram模型只是单词模型的扩展，所以我们也可以利用基于N-gram的特征。下面的例子描述了每个文档特征向量中基于二元模型的特征。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ku"><img src="../Images/6d766559badee9ef838ac45763caac83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uTLp7VnCSV-jdRmjtM2FA.png"/></div></div></figure><p id="4b24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这为我们的文档提供了特征向量，其中每个特征由表示两个单词序列的二元语法组成，值表示该二元语法在我们的文档中出现的次数。</p><h2 id="f126" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">使用弓形模型的缺点:</h2><ul class=""><li id="7184" class="kv kw hi ih b ii kk im kl iq kx iu ky iy kz jc la lb lc ld bi translated">如果新句子包含新单词，那么我们的词汇量将会增加，因此向量的长度也会增加。</li><li id="86e8" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">此外，向量也将包含许多0，从而导致稀疏矩阵(这是我们想要避免的)。</li><li id="1663" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">我们不保留句子的语法信息，也不保留课文中单词的顺序信息。</li></ul><h2 id="8235" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">TF-IDF</h2><p id="93ff" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">TF-IDF模型试图通过在其计算中使用一个缩放或归一化因子来解决这个问题。TF-IDF代表术语频率-逆文档频率，它在计算中使用两个度量的组合，即:术语频率(TF)和逆文档频率(IDF)。</p><div class="je jf jg jh fd ab cb"><figure class="lj ji lk ll lm ln lo paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/2dbc4275afad0727532e7276782df004.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*9O4UdAVr6d7E8GYTm0V8lg.png"/></div></figure><figure class="lj ji lp ll lm ln lo paragraph-image"><img src="../Images/803de51090505845f6799b65cbab3797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*YzMDMQUwpJYsGW94Ab3RJA.png"/></figure></div><p id="1758" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数学上，我们可以将TF-IDF定义为tfidf = tf x idf。</p><p id="3484" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，tfidf (w，D)是文档D中单词w的TF-IDF得分<br/> →术语tf (w，D)代表单词w在文档D中的术语频率，可以从单词袋模型中得到。<br/> →术语idf (w，D)是术语w的逆文档频率，可以计算为语料库C中文档总数除以单词w的文档频率的对数变换，基本上就是单词w在语料库中出现的文档频率。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lq"><img src="../Images/9895a13736cc5da66444a7855414e4b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0AuGojFhG2cx6HfbskTY2Q.png"/></div></div></figure><p id="8b51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的每个文本文档的基于TF-IDF的特征向量显示了与原始单词包模型值相比的缩放和归一化值。</p><p id="4860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→单词包只是创建一组向量，包含文档(评论)中的单词出现次数，而TF-IDF模型包含关于最重要单词和不太重要单词的信息。</p><p id="33de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→单词向量袋易于理解。然而，TF-IDF通常在机器学习模型中表现更好。</p><p id="1302" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→尽管单词袋和TF-IDF在各自方面都很受欢迎，但在理解单词的上下文方面仍然存在空白。检测单词“幽灵”和“恐怖”之间的相似性，或者将我们给定的文档翻译成另一种语言，需要关于文档的更多信息。</p><p id="22d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→这就是Word2Vec、连续单词包(CBOW)、Skipgram等单词嵌入技术的地方。进来吧。</p><h2 id="092c" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">Word2Vec模型</h2><p id="9889" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">该模型由谷歌在2013年创建，是一种基于预测性深度学习的模型，用于计算和生成高质量、分布式和连续的单词密集矢量表示，这些表示捕捉上下文和语义的相似性。本质上，这些是无监督的模型，可以接受大量文本语料库，创建可能单词的词汇表，并为表示该词汇表的向量空间中的每个单词生成密集单词嵌入。</p><p id="b944" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常你可以指定单词嵌入向量的大小，向量的总数实质上就是词汇量的大小。这使得这个密集向量空间的维数比使用传统单词袋模型构建的高维稀疏向量空间低得多。</p><p id="2af4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2Vec可以利用两种不同的模型架构来创建这些单词嵌入表示。这些包括:</p><ul class=""><li id="ab19" class="kv kw hi ih b ii ij im in iq lr iu ls iy lt jc la lb lc ld bi translated">连续词汇袋模式<br/>——跳格模式</li></ul><h2 id="371b" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">连续单词袋(CBOW)模型:</h2><p id="87c2" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">CBOW模型架构试图基于源上下文单词(周围单词)来预测当前目标单词(中心单词)。</p><p id="903d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一个简单的句子，“快速的棕色狐狸跳过懒惰的狗”，这可以是(context_window，target_word)对，其中如果我们考虑大小为2的上下文窗口，我们有像([quick，fox]，brown)，([the，brown]，quick)，([the，dog)，lazy)等例子。</p><p id="36fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，该模型试图基于“上下文窗口”单词来预测目标单词。</p><h2 id="84fc" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">跳格模型</h2><p id="ea1e" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">跳格模型体系结构通常试图实现CBOW模型的反向操作。它试图在给定目标单词(中心单词)的情况下预测源上下文单词(周围的单词)。</p><p id="33cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到我们之前的简单句子，“敏捷的棕色狐狸跳过懒惰的狗”。如果我们使用CBOW模型，我们得到(context_window，target_word)对，其中如果我们考虑大小为2的上下文窗口，我们有这样的例子([quick，fox]，brown)，([the，brown]，quick)，([the，dog]，lazy)等等。</p><p id="db91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在考虑到skip-gram模型的目的是从目标单词预测上下文，该模型通常反转上下文和目标，并试图从其目标单词预测每个上下文单词。因此，任务变成了预测给定目标词“brown”或给定目标词“quick”的上下文。</p><p id="8470" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，该模型试图基于目标单词来预测上下文窗口单词。</p><p id="d5d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">采用Gensim的鲁棒Word2Vec模型</strong></p><p id="b479" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由雷迪姆·řehůřek创建的<em class="lu"> gensim </em>框架由Word2Vec模型的健壮、高效和可扩展的实现组成。我们将在我们的样本玩具语料库中利用同样的方法。在我们的工作流程中，我们将标记化我们的规范化语料库，然后关注Word2Vec模型中的以下四个参数来构建它。</p><p id="d411" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ size:单词嵌入维度<br/> → window:上下文窗口大小<br/> → min_count:最小字数<br/> → sample:常用单词的下采样设置<br/> →sg:训练模型，skip-gram为1，否则为CBOW</p><p id="d0a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在语料库上构建一个简单的Word2Vec模型，并可视化嵌入。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/11ead6457653e15a70cfd45c925e0aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1lYMRFVzBNVgTmTMpUlEWw.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/2cd0c4160fcf71816ef9837e04f239bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NGiANELrWjXCAm2HZZmZQ.png"/></div></div></figure><h2 id="190f" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">余弦相似度:</h2><p id="5d73" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">余弦相似度用于衡量单词向量之间的相似程度。余弦相似性本质上是检查两个向量之间的距离。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/620c1822d279033908685ee94a9094f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*YgQYp8-wjbaysnFKATzVTg.png"/></div></figure><p id="4463" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们也可以用单词vectors执行向量运算。</p><p id="2842" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→<strong class="ih hj">new _ vector = king—man+woman</strong></p><p id="4d5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就产生了一个新的向量，然后我们可以尝试找到最相似的向量。</p><p id="21bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→最接近女王矢量的新矢量</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ly"><img src="../Images/27b2b8e1c57337b2aa144c9e461d174f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kXkOVt_G12KBvU2mmr5Zsw.png"/></div></div></figure><p id="0e16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">余弦相似度是两个向量之间角度的余弦值。余弦距离可以通过1-余弦相似度来找到。两个向量之间的角度越高，余弦相似性越低，这给出了高余弦距离值，而两个向量之间的角度越低，余弦相似性越高，这给出了低余弦距离值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lz"><img src="../Images/94215ef10b465440971cafde5ce3fe44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5QlyJcXvDPgeiBGh5bOiig.png"/></div></div></figure><p id="97fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图给出了每个词的前3个相似词。</p><h2 id="8bb3" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">手套型号:</h2><p id="194d" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">GloVe模型代表全局向量，这是一种无监督的学习模型，可用于获得类似于Word2Vec的密集单词向量。然而，该技术是不同的，并且在聚集的全局单词-单词共现矩阵上执行训练，给我们一个具有有意义的子结构的向量空间。这种方法是由Pennington等人在斯坦福发明的，我推荐你阅读关于GloVe的原始论文，[' GloVe:Global Vectors for Word Representation]作者Pennington等人的论文<a class="ae ma" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，这是一篇很好的阅读材料，可以让你对这个模型如何工作有所了解。</p><p id="d494" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">手套模型的基本方法是首先创建由(单词，上下文)对组成的巨大的单词-上下文共现矩阵，使得该矩阵中的每个元素表示单词在上下文(可以是单词序列)中出现的频率。这个想法是应用矩阵分解。</p><p id="398f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到单词-上下文(WC)矩阵、单词-特征(WF)矩阵和特征-上下文(FC)矩阵，我们尝试分解<strong class="ih hj"> WC = WF x FC </strong></p><p id="95e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样我们的目标是通过将<strong class="ih hj"> WF </strong>和<strong class="ih hj"> FC </strong>相乘来重构<strong class="ih hj"> WC </strong>。为此，我们通常用一些随机权重初始化<strong class="ih hj"> WF </strong>和<strong class="ih hj"> FC </strong>，并尝试将它们相乘以获得<strong class="ih hj">WC</strong>(WC的近似值)并测量它与<strong class="ih hj"> WC </strong>的接近程度。我们多次使用随机梯度下降(SGD)来最小化误差。最后，单词特征矩阵(<strong class="ih hj"> WF </strong>)给出了每个单词的单词嵌入，其中F可以预设为特定的维数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mb"><img src="../Images/43199a37a641715a42a26a57f2c3217e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zkmQGmwhM97cYHGShryrpA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">手套模型的实现。</figcaption></figure><h2 id="bcb8" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">快速文本模型:</h2><p id="e978" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">FastText模型由脸书在2016年首次推出，作为普通Word2Vec模型的扩展和改进。基于Mikolov等人的题为['用子词信息丰富词向量']<a class="ae ma" href="https://arxiv.org/pdf/1607.04606.pdf)_" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1607.04606.pdf</a>的原始论文，这是一篇深入了解该模型如何工作的优秀读物。总的来说，FastText是一个用于学习单词表示以及执行健壮、快速和准确的文本分类的框架。该框架是由https://github.com/facebookresearch/fastText的脸书开源的，并声称有如下内容。</p><p id="d52e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→最近最先进的英语单词矢量。<br/> →在维基百科和Crawl上训练的157种语言的词向量。<br/> →语言识别和各种监督任务的模型。</p><p id="c857" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2Vec模型通常忽略每个单词的形态结构，并将单词视为单个实体。FastText模型将每个单词视为一包字符n元语法。这在本文中也被称为子字模型。</p><p id="e6c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在单词的开头和结尾添加特殊的边界符号&lt; and &gt;。这使我们能够将前缀和后缀与其他字符序列区分开来。我们还将单词w本身包含在它的n-grams集合中，以学习每个单词的表示(除了它的字符n-grams)。</p><p id="c5ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以单词where and n=3 (tri-grams)为例，它将由字符n-grams: <wh whe="" her="" ere="" re="">和代表整个单词的特殊序列&lt; where &gt;来表示。注意，对应于单词&lt; her &gt;的序列不同于单词where中的三元组her。</wh></p><p id="acb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，本文推荐在n ≥ 3和n ≤ 6时提取所有n元文法。这是一种非常简单的方法，可以考虑不同组的n元语法，例如取所有的前缀和后缀。我们通常将一个单词的向量表示(嵌入)与每个n元语法相关联。</p><p id="b765" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以用一个单词的n元文法的向量表示的和或者这些n元文法的嵌入的平均值来表示这个单词。因此，由于这种基于单词的字符利用单个单词的n元语法的效果，稀有单词有更高的机会获得良好的表示，因为它们的基于字符的n元语法应该出现在语料库的其他单词中。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mc"><img src="../Images/cabd51db6593ac705dac76067d8bd92c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nbGN6tXCi7wqLPGtBP7PHA.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/d06e64bdfe729e25675f58bb68d57de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ouxAClrApKpjRsvzmLkMg.png"/></div></div></figure><p id="4570" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些是NLP中用于特征提取的嵌入技术。</p><p id="e4a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢你一直读到最后。有关实际实施，请参考本笔记本。<a class="ae ma" href="https://github.com/Rishikumar04/Natural-Language-Processing/blob/main/04_Feature%20Extraction%20Techniques.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a></p></div></div>    
</body>
</html>