<html>
<head>
<title>Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策图表</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/decision-tree-f290b9ac1b20?source=collection_archive---------28-----------------------#2021-05-31">https://medium.com/nerd-for-tech/decision-tree-f290b9ac1b20?source=collection_archive---------28-----------------------#2021-05-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="91da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是一种监督学习算法(具有预定义的目标变量)，主要用于分类问题。它也可以用于回归。</p><p id="7bfc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它适用于<strong class="ih hj">分类和连续</strong>输入和输出变量</p><p id="c79d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种方法中，我们根据输入变量中最重要的分割器/区分器，将总体或样本分成两个或多个同质组(或子总体)。决策树在所有可用变量上拆分节点，然后选择导致最同质子节点的拆分。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c6edcdb70922358d7a01a1ed17e72cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*u3pbp1If9xA40tD0klAXSA.png"/></div></figure><p id="498f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练模型通过学习具有树表示的决策规则来学习预测目标变量的值。树由具有相应属性的节点组成。在每一个节点，我们根据可用的特性询问一个关于数据的问题。左右分支代表可能的答案。对应于预测值的最终节点左节点。每个特性的重要性是通过自顶向下的方法确定的。节点越高；其属性越重要。</p><p id="29fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树使用多种算法来决定将一个节点分成两个或多个子节点—基尼指数、卡方、信息增益、方差。分割决策树的默认方法是<strong class="ih hj">基尼指数</strong>，这是一个特定模型杂质的度量。本质上，它计算某个特定要素被错误分类的概率。当元素被一个类链接时，我们称之为纯元素。我们可以使用随机森林，但基尼指数是首选，因为它不是计算密集型和不涉及对数函数。基尼指数的一个替代方法是熵。在实际情况下，基尼系数和熵在数值上非常相似。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jl"><img src="../Images/d96df0fc249c51280017e44ceaf4be85.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*W1c6Kjdd3p6qJfna527Qew.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">p 和 q 是成功和失败的问题</figcaption></figure><p id="d4bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">信息增益</strong>:需要较少信息的节点称为纯节点，需要较多信息进行拆分或理解/分类的节点称为不纯节点。信息论是定义系统中这种无序程度的一种方法，被称为<strong class="ih hj">熵。</strong>如果样本是完全同质的，则熵为零，并且样本被等分，其熵为 1。熵也称为<strong class="ih hj">分类目标变量</strong>。它选择与父节点和其他分裂相比具有最低熵的分裂，熵越小越好。</p><p id="9622" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关于决策树的一些事实！！</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jq"><img src="../Images/45cba99567354e62c8423711f52cb9b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aFWFcXMEJS3UvUnMF64fPg.png"/></div></div></figure><p id="8709" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树需要很少的数据准备。</p><p id="7035" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从不需要特征缩放或居中。</p><p id="d8a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是随机森林的基本组成部分，可以说是最强大的 ML 算法之一。</p><p id="0ecd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是白盒模型，与随机森林和神经网络不同，这意味着这些模型的内部工作原理非常清楚。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jv"><img src="../Images/2966a9d9d737615b7e3e49093d797170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zN2e7V7_0JOJEgz7xNSb_g.png"/></div></div></figure><p id="42e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从树根开始，使用决策算法分割特征上的数据，从而获得最大的信息增益。</p><p id="7dd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后在每个子节点上重复分裂过程，直到叶子是纯的。</p><p id="0aae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着每个节点上的样本属于同一个类。</p><p id="3a56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，我们可以对树的深度设置一个限制，以防止过度拟合。</p><p id="1dd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">纯度在这里受到损害，因为最终的叶子可能仍有一些纯度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jw"><img src="../Images/eefe2a197d8011b9969c013de139c4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fl2nBFy2qDh5w5Sm9bDMpg.png"/></div></div></figure><p id="dc8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们更详细地看看分类和回归决策树。</p><p id="b8b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分类树与回归树非常相似，只是它用于预测定性响应，而不是定量响应。</p><p id="0d41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在回归中，预测值由训练数据集的平均响应给出。其中在分类中，它由训练集中最常出现的区域类别导出。</p><p id="c801" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在分类问题以及类别预测中，我们还对训练观察的类别比例感兴趣。</p><p id="a6d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，在这两种情况下，我们使用递归二进制分裂来增长树。</p><p id="403e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然 RSS 适合回归，但分类错误率将是分类决策树的选择。</p><p id="83c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们试着概述一个简单决策树的算法</p><ul class=""><li id="7901" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">将所有训练示例放在根位置</li><li id="6292" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">将属性分类</li><li id="8818" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">基于所选属性递归划分示例</li><li id="f778" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">在统计测量的基础上选择测试属性。</li></ul><p id="aca3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">停止分区的条件:</p><p id="a379" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于一个节点，所有样本属于同一个类别。</p><p id="49c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">没有属性可供进一步分割</p><p id="9bb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">没有留下样本进行分类。</p><p id="ea3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，如果我们要实现一个决策树(我在这里选择了 R)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kl"><img src="../Images/6c9ce0cda57dc325ea2e8676fd9b4f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*Y_YiYXzbl0ovohxFjWUXOg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">一个简单的 R 代码</figcaption></figure><ol class=""><li id="293f" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc km kd ke kf bi translated">将数据分为训练和测试。</li><li id="251b" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc km kd ke kf bi translated">列车模型使用<strong class="ih hj"> rpart </strong>功能</li><li id="3549" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc km kd ke kf bi translated">预测测试数据。</li><li id="2fb0" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc km kd ke kf bi translated">评估模型准确性。</li></ol><p id="4453" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树</strong>的优势</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kn"><img src="../Images/e6bdeaf4a11c774074edd5d18546ac5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*bH5BnBN8puZ932rbklY5Pg.png"/></div></figure><ul class=""><li id="dd94" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">树更容易解释</li><li id="615f" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">决策树被认为比其他分类和回归模型更能反映人类的决策。</li><li id="0b59" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">树的视觉表现使解释变得容易。</li><li id="9830" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">树可以处理定性预测，而不需要虚拟变量。</li><li id="a4fb" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">需要更少的数据清理</li><li id="ee2d" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">不受异常值和缺失值的影响。</li><li id="a057" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">这是识别最重要的变量和它们之间关系的最快方法。</li><li id="4b5f" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">它是非参数的，意味着决策树对空间分布和分类器结构没有任何假设。</li></ul><p id="a914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树的缺点</strong></p><ul class=""><li id="10d9" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">另一方面，树的预测精度不如其他回归和分类模型高。</li><li id="e15e" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">树木并不健壮。</li><li id="7d71" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">过度拟合是一个问题，尽管可以使用参数约束和修剪来消除。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ko"><img src="../Images/9361f9454e1ff15ae4ef76040a34029d.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*_0H2Dv1fFUPJJYo_wTWvzQ.png"/></div></figure><p id="bab3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们都知道线性模型，现在我们有了决策树模型。一个显而易见的问题可能会出现——选择哪一个，显而易见的答案是“视情况而定！！!"。是的，没错——这取决于场景；我们正在努力解决的问题。如果特征和响应之间的关系可以用线性关系来近似，那么应该选择线性模型。然而，如果结果是复杂的和非线性的，我们可以尝试决策树方法。</p><p id="66e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回到我们之前离开的地方。<strong class="ih hj">过拟合</strong>。有时，一棵树可能会过度适应数据集，这可能会导致分支过多和准确性降低。显而易见的问题是<strong class="ih hj">如何避免过度拟合？</strong></p><p id="8230" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有两种方法</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kp"><img src="../Images/79467df06e6a2b43d877c1abe220ae8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*xR_Id6ML5acOq3Ji7zSBfg.png"/></div></figure><ul class=""><li id="7f67" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">提前修剪:提前停止树的建造。因此，如果模型的良好性正在恶化(低于阈值)，就不要再分割节点了。要使用的一些参数是-&gt; <em class="kq"> maxdepth </em>:一棵树从根节点到叶节点可以增长的最大值。<em class="kq"> minsplit </em>:发生拆分时必须在一个节点中的记录数。<em class="kq"> minbucket </em>:终止节点中可以出现的最小记录数。</li><li id="3879" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">后期修剪:我们让树充分生长，观察 cp 值。然后我们用最佳 cp 值修剪树。<em class="kq">复杂度参数</em> (cp)用于控制决策树的大小并选择最佳大小。</li></ul><p id="65cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — @ — -</p></div></div>    
</body>
</html>