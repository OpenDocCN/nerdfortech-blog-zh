<html>
<head>
<title>Layers in Neural network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的层</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/layers-in-neural-network-90d48a5a42fb?source=collection_archive---------7-----------------------#2021-02-17">https://medium.com/nerd-for-tech/layers-in-neural-network-90d48a5a42fb?source=collection_archive---------7-----------------------#2021-02-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/57b5e5ff6083c31f5e8642bc40a60d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEY0Z_fWmBhNdxXodGYUew.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">罗布·谢汉在<a class="ae hv" href="https://unsplash.com/s/photos/stacks?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><p id="dfa9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">层是节点/神经元的逻辑集合。在最高级别，每个人工神经网络中有三种类型的层:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jt"><img src="../Images/4a9f49db715672093d1c7285f83582ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uUqqPqHVvtF9X4ldcisRRw.png"/></div></div></figure><p id="529f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不同的层对它们的输入执行不同的转换，有些层比其他层更适合某些任务。</p><blockquote class="jy jz ka"><p id="02c6" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">CNN —图片<br/> RNN —时间序列<br/>密集—多分类<br/>线性—回归</p></blockquote><p id="2c53" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在隐藏段中，每一层学习数据的不同方面，同时最小化成本函数。在这个博客中，我们将讨论一些常见的层类型及其用法。</p><h2 id="c28d" class="kf kg hy bd kh ki kj kk kl km kn ko kp jg kq kr ks jk kt ku kv jo kw kx ky kz bi translated">致密层或完全连接层</h2><blockquote class="la"><p id="456e" class="lb lc hy bd ld le lf lg lh li lj js dx translated"><strong class="ak">机器学习模型的蛮力层</strong></p></blockquote><p id="efd4" class="pw-post-body-paragraph iv iw hy ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">神经网络中的全连接层是来自一层的所有输入都连接到下一层的每个激活单元的那些层。这一层用于将数据放在不同的维度中。在大多数流行的机器学习模型中，最后几层是完全连接的层(密集的),它编译由前面的层提取的数据以形成最终的输出。此外，它还用于多类分类问题。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lp"><img src="../Images/85327462d3614ebfb2d0043bdb00417a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JTnp09yGDBeykj5Cbu0K6Q.png"/></div></div></figure><h2 id="dc02" class="kf kg hy bd kh ki kj kk kl km kn ko kp jg kq kr ks jk kt ku kv jo kw kx ky kz bi translated">批量标准化层</h2><p id="758c" class="pw-post-body-paragraph iv iw hy ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">批次标准化通过减少每个批次内的内部协变量变化来加速收敛。批次范数层对传入的激活进行归一化，并输出新的批次，其中平均值等于 0，标准偏差等于 1。它减去平均值，然后除以批次的标准偏差。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lv"><img src="../Images/2607a324216ef6d8d80a1fbdccc7bd8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ECCFVO3I02JI-IMfThaxw.png"/></div></div></figure><h2 id="ef77" class="kf kg hy bd kh ki kj kk kl km kn ko kp jg kq kr ks jk kt ku kv jo kw kx ky kz bi translated">联营</h2><p id="41e1" class="pw-post-body-paragraph iv iw hy ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">池层是降低高维度的方法。池图层通过汇总要素地图的图片中要素的存在情况，提供了一种对要素地图进行缩减采样的方法。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/2d7dc99d10a284e119ed1fcd4e9aace9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6n9R4zUlhjmeGys3OyPxw.png"/></div></div></figure><blockquote class="jy jz ka"><p id="f3d1" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated"><strong class="ix hz">最大池<br/>平均池</strong></p></blockquote><h2 id="36ff" class="kf kg hy bd kh ki kj kk kl km kn ko kp jg kq kr ks jk kt ku kv jo kw kx ky kz bi translated">退出</h2><p id="3437" class="pw-post-body-paragraph iv iw hy ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">丢弃层获取前一层激活的输出，并随机将激活的某一部分(丢弃率)设置为 0，从而取消或“丢弃”它们。这是一种常用的正则化技术，用于防止神经网络中的过拟合。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/9a335f6da05b7c4297d63b631c84ed93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0x0pForiIhcrQFPGusCQzw.png"/></div></div></figure><h2 id="bdc2" class="kf kg hy bd kh ki kj kk kl km kn ko kp jg kq kr ks jk kt ku kv jo kw kx ky kz bi translated">美国有线新闻网；卷积神经网络</h2><p id="4794" class="pw-post-body-paragraph iv iw hy ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">卷积是将过滤器简单应用于输入，从而导致激活。对输入重复应用相同的滤波器会产生称为特征图的激活图，指示输入(如图像)中检测到的特征的位置和强度。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/c4b327ec738dde455b2ea513692ab6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*ceAXcKj_OEuo1Mf-KW4Tgw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><a class="ae hv" href="https://slugnet.jarrodkahn.com/layers.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure></div></div>    
</body>
</html>