<html>
<head>
<title>Basic Image Classifier Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基本图像分类器项目</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/basic-image-classifier-project-9f3b2c0b7798?source=collection_archive---------7-----------------------#2021-02-19">https://medium.com/nerd-for-tech/basic-image-classifier-project-9f3b2c0b7798?source=collection_archive---------7-----------------------#2021-02-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="6a00" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该项目的目的是:</p><ul class=""><li id="e7fb" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">创建一个复仇者联盟图像的标签数据集-美国队长，钢铁侠，黑寡妇，绿巨人，雷神。</li><li id="1a37" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">训练一个 CNN 能够以合理的准确度对一个看不见的图像进行分类。</li></ul><p id="f965" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">基本上，在图像分类器中有 4 个主要的数据操作步骤。这些是:</p><ol class=""><li id="796d" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jy jq jr js bi translated">数据收集</li><li id="5a34" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">数据预处理</li><li id="385f" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">特征抽出</li><li id="7bee" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">模特培训</li></ol><p id="618d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们一步一步地实现和各种可用的方法。将采用最合适的方法。</p><p id="ee1b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">注:</strong> <em class="jz">我们正在使用</em> <strong class="io hj"> <em class="jz">谷歌 Colab </em> </strong> <em class="jz">在这个项目中，但人们可以使用任何可用的软件。</em></p><h1 id="26e2" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">步骤 1 —数据收集</h1><p id="09b0" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">在这个项目中，我们需要收集图像形式的数据。可以通过手动废弃单个(静态和/或动态)网站来获得图像。但是由于需要大量的图片，所以，许多网站将不得不被废弃。你可以删除谷歌图片或必应图片，而不是手动登录不同的网站。</p><p id="10ef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">可以有许多方法来废弃图像。常用的三种方法如下:</p><ol class=""><li id="c88e" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jy jq jr js bi translated">使用 python 和 web scrapping 工具直接从 google 自动下载某种类型的图像。这种方法是相当非法的，所以，网站和谷歌本身确保这些网络爬虫不工作。这就是代码需要定期更新的原因。</li><li id="35b7" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">通过使用像 fatkun 这样的 Chrome 浏览器扩展。这些扩展比以前的方法使用起来要稳定得多。但是根据这个项目的要求，图片应该从网上删除。</li><li id="338e" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">使用 Python 工具(如 Bing Image Downloader)将所需图像直接导出到目录中。</li></ol><p id="a176" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jz">本项目中，为了方便起见，使用了 Bing 图片下载器。</em></p><p id="d8ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先，我们将安装下载器并通过以下方式导入所需的库:-</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="5bc9" class="lm kb hi li b fi ln lo l lp lq">!pip install bing-image-downloader <br/><strong class="li hj">from</strong> <strong class="li hj">bing_image_downloader</strong> <strong class="li hj">import</strong> downloader</span></pre><p id="8d2e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，它通过使用以下代码行来废弃图像:-</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="07f2" class="lm kb hi li b fi ln lo l lp lq">downloader.download('Captain America Chris Evans', output_dir= './drive/MyDrive/datasets/collection', limit = 400, adult_filter_off = <strong class="li hj">False</strong>, force_replace = <strong class="li hj">False</strong>, timeout = 6000)</span></pre><p id="db3c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">同样，钢铁侠、雷神、绿巨人和黑寡妇的形象也被废弃了。</p><h1 id="cb4c" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">步骤 2 —数据预处理</h1><p id="1e0b" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">这个步骤对于执行图像分类非常重要。它提高了算法的整体效率。</p><p id="768a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这一步在这个项目中是必需的，因为在数据收集之后，观察到收集了许多漫画书中不重要的图像。这些类型的图像将导致消耗更多的系统资源。此外，分类器可能会出错。所以，这些图像不应该直接使用。</p><p id="9f26" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这个项目中，OpenCV 和一种叫做<strong class="io hj"> haar 的技术级联</strong>，用于数据清理目的。他们会检测一张脸和两只眼睛是否清晰可见。如果它们是可见的，则图像被保留，否则图像被丢弃。大多数数据清理工作将使用 python 代码完成，但也有一些清理工作必须手动完成。需要手动检查图像来移除不想要的面部。例如，在钢铁侠的文件夹中，其他角色的脸可能会出现，这降低了模型的效率。</p><p id="bdfd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据清理的步骤:</p><ol class=""><li id="c43f" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jy jq jr js bi translated">使用哈尔级联从原始图像中提取具有两只眼睛的人脸</li><li id="8cfc" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">手动丢弃有两张或多张面孔的照片。也有模糊的照片和其他照片</li></ol><h2 id="d439" class="lm kb hi bd kc lr ls lt kg lu lv lw kk ix lx ly ko jb lz ma ks jf mb mc kw md bi translated">哈尔梯级功能和使用简介:-</h2><p id="d7c4" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">每个图像都有线条和边缘特征。Haar Cascade 使用这种边缘特征的移动窗口来检测眼睛和整个面部。</p><p id="04a8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如，为了检测眼睛，眼睛的区域往往比下面的区域更暗。哈尔喀斯喀特用这个面具来探测这些区域。</p><p id="afa8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">OpenCV 有现成的 API 来检测人脸、眼睛等。手动上传用于运行 API 的 17 个不同的 xml 文件，以便使用 haar 级联函数来检测各种特征。</p><p id="7849" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们导入所需的库，并创建使用面部层叠和眼睛层叠功能的函数:-</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="1fb1" class="lm kb hi li b fi ln lo l lp lq"><strong class="li hj">import</strong> <strong class="li hj">numpy</strong> <strong class="li hj">as</strong> <strong class="li hj">np</strong><br/><strong class="li hj">import</strong> <strong class="li hj">cv2</strong><br/><strong class="li hj">import</strong> <strong class="li hj">matplotlib</strong><br/><strong class="li hj">from</strong> <strong class="li hj">matplotlib</strong> <strong class="li hj">import</strong> pyplot <strong class="li hj">as</strong> plt<br/>%matplotlib inline<br/><br/>face_cascade = cv2.CascadeClassifier("./drive/MyDrive/Colab Notebooks/opencv/haarcascades/haarcascade_frontalface_default.xml")<br/>eye_cascade = cv2.CascadeClassifier("./drive/MyDrive/Colab Notebooks/opencv/haarcascades/haarcascade_eye.xml")</span></pre><p id="9bc7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们试运行一下，看看这些功能是否工作正常:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="dd80" class="lm kb hi li b fi ln lo l lp lq">img = cv2.imread('/content/drive/My Drive/datasets/collection/Thor Chris Hemsworth/Image_8.jpg')<br/><em class="jz">#img.shape</em><br/>plt.imshow(img)<br/>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>gray.shape<br/>plt.imshow(gray, cmap='gray')<br/><br/>face_cascade = cv2.CascadeClassifier('./drive/MyDrive/Colab Notebooks/opencv/haarcascades/haarcascade_frontalface_default.xml')<br/>eye_cascade = cv2.CascadeClassifier('./drive/MyDrive/Colab Notebooks/opencv/haarcascades/haarcascade_eye.xml')<br/>faces = face_cascade.detectMultiScale(gray)<br/>faces</span></pre><p id="dbda" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这几行代码检查图像是否有一张脸和两只眼睛。如果它没有任何面孔或眼睛，那么它将返回一个错误。如果图像有，那么它将返回一个类似如下所示的输出</p><figure class="ld le lf lg fd mf er es paragraph-image"><div class="er es me"><img src="../Images/e6fb63f845168f2aee892adcf20c110d.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*wiYyTRDYFPf652NSz4P6kw.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">有脸和两只眼睛的图像</figcaption></figure><p id="6368" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，由于函数工作正常，让我们编写代码来检查数据集中的所有图像。符合要求的图像将被转换为灰色，然后进行裁剪。这些图像保存在单独的文件夹中以备将来使用。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="1eb8" class="lm kb hi li b fi ln lo l lp lq"><strong class="li hj">def</strong> get_cropped_image_if_2_eyes(image_path):<br/>    img = cv2.imread(image_path)<br/>    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>    faces = face_cascade.detectMultiScale(gray, 1.3, 5)<br/>    <strong class="li hj">for</strong> (x,y,w,h) <strong class="li hj">in</strong> faces:<br/>        roi_gray = gray[y:y+h, x:x+w]<br/>        roi_color = img[y:y+h, x:x+w]<br/>        eyes = eye_cascade.detectMultiScale(roi_gray)<br/>        <strong class="li hj">if</strong> len(eyes) &gt;= 2:<br/>            <strong class="li hj">return</strong> roi_color<br/><br/>path_to_data = "./drive/My Drive/datasets/collection/"<br/>path_to_cr_data = "./drive/My Drive/datasets/cropped/"<br/><br/><strong class="li hj">import</strong> <strong class="li hj">os</strong><br/>img_dirs = []<br/><strong class="li hj">for</strong> entry <strong class="li hj">in</strong> os.scandir(path_to_data):<br/>    <strong class="li hj">if</strong> entry.is_dir():<br/>        img_dirs.append(entry.path)<br/><br/><strong class="li hj">import</strong> <strong class="li hj">shutil</strong><br/><strong class="li hj">if</strong> os.path.exists(path_to_cr_data):<br/>     shutil.rmtree(path_to_cr_data)<br/>os.mkdir(path_to_cr_data)<br/><br/>cropped_image_dirs = []<br/>celebrity_file_names_dict = {}<br/><br/><strong class="li hj">for</strong> img_dir <strong class="li hj">in</strong> img_dirs:<br/>    count = 1<br/>    celebrity_name = img_dir.split('/')[-1]<br/>    print(celebrity_name)<br/>    <br/>    celebrity_file_names_dict[celebrity_name] = []<br/>    <br/>    <strong class="li hj">for</strong> entry <strong class="li hj">in</strong> os.scandir(img_dir):<br/>        roi_color = get_cropped_image_if_2_eyes(entry.path)<br/>        <strong class="li hj">if</strong> roi_color <strong class="li hj">is</strong> <strong class="li hj">not</strong> <strong class="li hj">None</strong>:<br/>            cropped_folder = path_to_cr_data + celebrity_name<br/>            <strong class="li hj">if</strong> <strong class="li hj">not</strong> os.path.exists(cropped_folder):<br/>                os.makedirs(cropped_folder)<br/>                cropped_image_dirs.append(cropped_folder)<br/>                print("Generating cropped images in folder: ",cropped_folder) <em class="jz">#Checking whether the code is running successfully or not</em><br/>                <br/>            cropped_file_name = celebrity_name + str(count) + ".png" <em class="jz">#changing file type of every image to png</em><br/>            cropped_file_path = cropped_folder + "/" + cropped_file_name <br/>            <br/>            cv2.imwrite(cropped_file_path, roi_color)<br/>            celebrity_file_names_dict[celebrity_name].append(cropped_file_path)<br/>            count += 1</span></pre><h1 id="3de9" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">步骤 3——使用小波变换进行特征提取</h1><p id="1157" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">这个步骤的重要性在于，在特征提取中，彩色图像会导致许多错误。彩色图像可以具有各种阴影和各种颜色，这使得分类器很难识别这样的图像。</p><p id="b00d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了避免这些错误，图像被转换成黑白颜色，不同的区域有不同的对比度。小波变换允许从图像中提取重要特征。通常，在小波变换的图像中，眼睛的区域将与前额的区域区分开来，鼻子也将是明显的等等。</p><p id="6e29" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在浏览图像处理文献时，发现小波变换通常是最有效的提取方法。因此，小波变换被用于这个项目。</p><p id="1aef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输入图像后，它将使用 PYWT (pi 小波变换库)在图像上执行小波变换，并返回新的小波变换图像。信号处理、频域、时域、傅立叶变换的概念已被用于在主要代码中应用小波变换。<strong class="io hj">下面简要解释其中一些概念:</strong></p><blockquote class="mm mn mo"><p id="4144" class="im in jz io b ip iq ir is it iu iv iw mp iy iz ja mq jc jd je mr jg jh ji jj hb bi translated">任何信号，像音频信号、图像也可以被认为是信号。它可以呈现在两种类型的域中。因此，图像可以在像空间(x 和 y)这样的空间域中呈现，或者可以表示为频域。音频信号可以在时域或频域中表示。</p><p id="0096" class="im in jz io b ip iq ir is it iu iv iw mp iy iz ja mq jc jd je mr jg jh ji jj hb bi translated">傅立叶变换将获取一个复信号，并返回构成该复信号的基本信号。例如，让我们考虑一些菜，比如 Dosa。如果在 Dosa 上进行逆向工程，得到的基本成分是水、米粉、乌拉尔豆，可能更多。</p><p id="d1bd" class="im in jz io b ip iq ir is it iu iv iw mp iy iz ja mq jc jd je mr jg jh ji jj hb bi translated">类似的情况还有一个复杂的信号，其中有不同的乐器演奏，也有噪声。有许多噪声消除器件，那么它们实际上是如何消除噪声的呢？这是使用傅立叶变换完成的，因为它可以分离出声带的声音和噪音。它可以将所有这些信号分离成不同的频率，使用频率滤波器可以抑制一些频率，或者可以进行内部放大。某些音频设备中的某些频率，高音或低音可以增加。因为傅立叶变换，所有这些都是可能的。</p><p id="91db" class="im in jz io b ip iq ir is it iu iv iw mp iy iz ja mq jc jd je mr jg jh ji jj hb bi translated">小波变换有点类似于傅立叶变换，它放大图像的某些特征。</p></blockquote><p id="6457" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于进一步的步骤，输入将是垂直堆叠的彩色图像及其小波变换的图像。其代码如下:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="41d7" class="lm kb hi li b fi ln lo l lp lq"><strong class="li hj">import</strong> <strong class="li hj">numpy</strong> <strong class="li hj">as</strong> <strong class="li hj">np</strong> <br/><strong class="li hj">import</strong> <strong class="li hj">pywt</strong> <br/><strong class="li hj">import</strong> <strong class="li hj">cv2</strong>      <br/><strong class="li hj">def</strong> w2d(img, mode='haar', level=1):     <br/> imArray = img     <br/> <em class="jz">#Datatype conversions</em>     <br/> <em class="jz">#convert to grayscale</em>     <br/> imArray = cv2.cvtColor( imArray,cv2.COLOR_RGB2GRAY )     <br/> <em class="jz">#convert to float</em>     <br/> imArray =  np.float32(imArray)        <br/> imArray /= 255;     <br/> <em class="jz"># compute coefficients </em>     <br/> coeffs=pywt.wavedec2(imArray, mode, level=level)       <br/> <em class="jz">#Process Coefficients</em>     <br/> coeffs_H=list(coeffs)       <br/> coeffs_H[0] *= 0;        <br/> <em class="jz"># reconstruction</em>     <br/> imArray_H=pywt.waverec2(coeffs_H, mode);     <br/> imArray_H *= 255;     <br/> imArray_H =  np.uint8(imArray_H)      <br/> <strong class="li hj">return</strong> imArray_H</span></pre><p id="a2a2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们为这 5 个字符中的每一个分配一个数字(或键)。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="8e9b" class="lm kb hi li b fi ln lo l lp lq">class_dict = {}<br/>count = 0<br/><strong class="li hj">for</strong> celebrity_name <strong class="li hj">in</strong> celebrity_file_names_dict.keys():<br/>    class_dict[celebrity_name] = count<br/>    count = count + 1<br/>class_dict</span></pre><blockquote class="mm mn mo"><p id="fe30" class="im in jz io b ip iq ir is it iu iv iw mp iy iz ja mq jc jd je mr jg jh ji jj hb bi translated">{ '黑寡妇·斯嘉丽·约翰逊':2，<br/>《美国队长克里斯·埃文斯》:0，<br/>《绿巨人马克·鲁弗洛》:3，<br/>《钢铁侠托尼·斯塔克》:1，<br/>《雷神克里斯·海姆斯沃斯》:4 }</p></blockquote><p id="9d2a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">创建字典以引用各个字符的所有剪切图像的路径；</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="e58f" class="lm kb hi li b fi ln lo l lp lq">celebrity_file_names_dict = {}<br/><strong class="li hj">for</strong> img_dir <strong class="li hj">in</strong> cropped_image_dirs:<br/>    celebrity_name = img_dir.split('/')[-1]<br/>    file_list = []<br/>    <strong class="li hj">for</strong> entry <strong class="li hj">in</strong> os.scandir(img_dir):<br/>        file_list.append(entry.path)<br/>    celebrity_file_names_dict[celebrity_name] = file_list</span></pre><p id="843c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们创建一个字典，其中彩色图像与小波变换后的图像垂直堆叠，以备将来使用。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="0d49" class="lm kb hi li b fi ln lo l lp lq">X, y = [], []<br/><strong class="li hj">for</strong> celebrity_name, training_files <strong class="li hj">in</strong> celebrity_file_names_dict.items():<br/>    <strong class="li hj">for</strong> training_image <strong class="li hj">in</strong> training_files:<br/>        img = cv2.imread(training_image)<br/>        <strong class="li hj">if</strong> img <strong class="li hj">is</strong> <strong class="li hj">None</strong>:<br/>          <strong class="li hj">continue</strong><br/>        scalled_raw_img = cv2.resize(img, (32, 32)) <em class="jz">#resizing using openCV as images maybe of different sizes</em><br/>        img_har = w2d(img,'db1',5) <em class="jz">#getting the wavelet transformed image</em><br/>        scalled_img_har = cv2.resize(img_har, (32, 32)) <em class="jz">#resizing wavelet transformed image</em><br/>        combined_img = np.vstack((scalled_raw_img.reshape(32*32*3,1),scalled_img_har.reshape(32*32,1))) <em class="jz">#vertically stacking both the images</em><br/>        X.append(combined_img)<br/>        y.append(class_dict[celebrity_name])<br/>X = np.array(X).reshape(len(X),4096).astype(float)</span></pre><h1 id="730d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">步骤 4 —模型训练:使用 SVM 和启发式微调</h1><p id="0dab" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">在这个项目中，首先 SVM 被用来训练主模型。</p><p id="62a7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然后使用<strong class="io hj"> GridSearch </strong>测试其他模型，以决定哪个模型最适合这个项目。</p><p id="ed00" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">GridSearch CV 用于超调参数。它有助于决定哪种模型表现最好。</p><p id="1c9f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在我们的项目中，我们将候选模型定义如下，以供比较:</p><ol class=""><li id="c6fd" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jy jq jr js bi translated">参数为的 SVM-C 值为 1，10，100，1000，核值为 rbf 和线性。</li><li id="ad1f" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">参数为的随机森林—估计量(或决策树)的数量为 1，5，10。</li><li id="6073" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jy jq jr js bi translated">参数为的逻辑回归——C 值为 1，5，10。</li></ol><p id="e9d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jz">最后，最佳模型存储在“已训练模型. pkl”中，类别字典也被保存。</em></p><p id="034a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">SVM 培训代码:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="46a4" class="lm kb hi li b fi ln lo l lp lq"><strong class="li hj">from</strong> <strong class="li hj">sklearn.svm</strong> <strong class="li hj">import</strong> SVC<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.preprocessing</strong> <strong class="li hj">import</strong> StandardScaler<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.model_selection</strong> <strong class="li hj">import</strong> train_test_split<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.pipeline</strong> <strong class="li hj">import</strong> Pipeline<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.metrics</strong> <strong class="li hj">import</strong> classification_report</span><span id="aacb" class="lm kb hi li b fi ms lo l lp lq">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)<br/><br/><em class="jz">#Pipeline is created to scale the Data.  </em><br/>pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel = 'rbf', C = 10))])<br/>pipe.fit(X_train,y_train)<br/>pipe.score(X_test,y_test)</span></pre><p id="184d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这给出了一个 0.9 分。18966.888688898617</p><p id="c1cb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们得到一份完整的分类报告。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="d899" class="lm kb hi li b fi ln lo l lp lq">print(classification_report(y_test, pipe.predict(X_test)))</span></pre><p id="d607" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这产生了以下输出:</p><figure class="ld le lf lg fd mf er es paragraph-image"><div class="er es mt"><img src="../Images/dde3c59645e5b6c5d0f077a35dcc46a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*iRE-hIydEfzIsfUbPQya_w.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">使用 SVM 的分类报告</figcaption></figure><p id="15d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">培训和测试前面提到的其他模型:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="8bb8" class="lm kb hi li b fi ln lo l lp lq"><strong class="li hj">from</strong> <strong class="li hj">sklearn</strong> <strong class="li hj">import</strong> svm<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.ensemble</strong> <strong class="li hj">import</strong> RandomForestClassifier<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.linear_model</strong> <strong class="li hj">import</strong> LogisticRegression<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.pipeline</strong> <strong class="li hj">import</strong> make_pipeline<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.model_selection</strong> <strong class="li hj">import</strong> GridSearchCV<br/>model_params = {<br/>    'svm': {<br/>        'model': svm.SVC(gamma='auto',probability=<strong class="li hj">True</strong>),<br/>        'params' : {<br/>            'svc__C': [1,10,100,1000],<br/>            'svc__kernel': ['rbf','linear']<br/>        }  <br/>    },<br/>    'random_forest': {<br/>        'model': RandomForestClassifier(),<br/>        'params' : {<br/>            'randomforestclassifier__n_estimators': [1,5,10]<br/>        }<br/>    },<br/>    'logistic_regression' : {<br/>        'model': LogisticRegression(solver='liblinear',multi_class='auto'),<br/>        'params': {<br/>            'logisticregression__C': [1,5,10]<br/>        }<br/>    }<br/>}<br/>scores = []<br/>best_estimators = {}<br/><strong class="li hj">import</strong> <strong class="li hj">pandas</strong> <strong class="li hj">as</strong> <strong class="li hj">pd</strong><br/><strong class="li hj">for</strong> algo, mp <strong class="li hj">in</strong> model_params.items():<br/>    pipe = make_pipeline(StandardScaler(), mp['model'])<br/>    clf =  GridSearchCV(pipe, mp['params'], cv=5, return_train_score=<strong class="li hj">False</strong>) <br/>    <em class="jz"># cv=5 =&gt; There will be 5 folds of testing the model and </em><br/>    <em class="jz"># then will avereage out the scores </em><br/>    clf.fit(X_train, y_train)<br/>    <em class="jz"># Scores are appended and a data frame is created from it</em><br/>    scores.append({<br/>        'model': algo,<br/>        'best_score': clf.best_score_,<br/>        'best_params': clf.best_params_<br/>    })<br/>    best_estimators[algo] = clf.best_estimator_<br/>    <br/>df = pd.DataFrame(scores,columns=['model','best_score','best_params'])<br/>df</span></pre><p id="631c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所得分数的最终报告如下:</p><figure class="ld le lf lg fd mf er es paragraph-image"><div class="er es mu"><img src="../Images/700361744dd1fabbe58ae67211ec5467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*yEQTtpE4Ta8Fpn0rquNjxQ.png"/></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">三个测试模型的分数</figcaption></figure><p id="fe42" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这些分数在训练数据上。现在，让我们获得模型在测试数据上的得分:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="f83c" class="lm kb hi li b fi ln lo l lp lq">best_estimators['svm'].score(X_test,y_test)<br/>best_estimators['random_forest'].score(X_test,y_test)<br/>best_estimators['logistic_regression'].score(X_test,y_test)</span></pre><p id="0689" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">获得的分数如下:</p><p id="8912" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">SVM: 0.9770992366412213 <br/>随机森林:0.933331297709 9237<br/>逻辑回归:0。59666.76666666667</p><p id="d83e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如我们所注意到的，SVM 在训练数据和测试数据上都表现良好，因此，支持向量机将用于这个项目。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="f3d9" class="lm kb hi li b fi ln lo l lp lq">best_clf = best_estimators['svm']</span></pre><p id="f17e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在绘制混淆矩阵:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="3344" class="lm kb hi li b fi ln lo l lp lq"><strong class="li hj">from</strong> <strong class="li hj">sklearn.metrics</strong> <strong class="li hj">import</strong> confusion_matrix<br/>cm = confusion_matrix(y_test, best_clf.predict(X_test))<br/><strong class="li hj">import</strong> <strong class="li hj">seaborn</strong> <strong class="li hj">as</strong> <strong class="li hj">sn</strong><br/>plt.figure(figsize = (10,7))<br/>sn.heatmap(cm, annot=<strong class="li hj">True</strong>)<br/>plt.xlabel('Predicted')<br/>plt.ylabel('Truth')</span></pre><figure class="ld le lf lg fd mf er es paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="er es mv"><img src="../Images/705a2ff8f1d49ff0f48411fecca7dca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4dhSk0Slc9KYwHKUAwq2hA.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">混淆矩阵</figcaption></figure><p id="aebd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将模型保存在各自的 pkl 文件中，以便将来在制作 Web 应用程序等时使用。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="6165" class="lm kb hi li b fi ln lo l lp lq">!pip install joblib<br/><strong class="li hj">import</strong> <strong class="li hj">joblib</strong> <br/><em class="jz"># Save the model as a pickle in a file </em><br/>joblib.dump(best_clf, 'saved_model.pkl')</span></pre><p id="d8e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将字典保存为 Json 文件以备将来使用:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="b053" class="lm kb hi li b fi ln lo l lp lq"><strong class="li hj">import</strong> <strong class="li hj">json</strong><br/><strong class="li hj">with</strong> open("class_dictionary.json","w") <strong class="li hj">as</strong> f:<br/>    f.write(json.dumps(class_dict))</span></pre></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="fc1c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">一个模型已经制作成功，现在可以进一步用于制作网站。</strong></p><p id="6a69" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">完整代码的链接是:<a class="ae na" href="https://github.com/Sampurn-Anand/Databyte_inductions/blob/main/Task%202/Inductions_Task2.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a></p><p id="fb31" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">感谢您花费宝贵的时间阅读本文。请在评论中告诉我你的观点和建议。T11】</p></div></div>    
</body>
</html>