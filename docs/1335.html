<html>
<head>
<title>Mathematics behind Gradient Descent..Simply Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降背后的数学..简单解释</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/mathematics-behind-gradient-descent-simply-explained-c9a17698fd6?source=collection_archive---------10-----------------------#2021-03-14">https://medium.com/nerd-for-tech/mathematics-behind-gradient-descent-simply-explained-c9a17698fd6?source=collection_archive---------10-----------------------#2021-03-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/38d848fb2d76d6819d8e4def654d2187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ETRz8rxJwOE6IsVV"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">亚当·本特利在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="5e7f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，我们已经在之前的文章中讨论了<a class="ae iu" href="https://bassemessam-10257.medium.com/linear-regression-9c91172239b1" rel="noopener">线性回归</a>和<a class="ae iu" href="https://bassemessam-10257.medium.com/gradient-descent-simply-explained-with-a-tutorial-e515b0d101e9" rel="noopener">梯度下降</a>。我们得到了一个简单的概念概述和一个实用教程来理解它们是如何工作的。在本文中，我们将看到梯度下降背后的数学原理，以及“优化器”如何获得全局极小点。如果术语“优化器”对您来说是新的，它只是用来确定全局极小点的函数，它指的是线性回归算法中最佳拟合线的系数。顺便说一下，深度学习算法中也使用了类似的概念。让我们来看看这些事情是如何进行的。</p><p id="1553" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当使用“均方误差”来确定最佳拟合线的系数时，我们的主要任务是找到MSE最小的点。换句话说，全局极小点是曲线斜率等于零的点。如果我们能得到MSE(成本函数)对最佳拟合线系数(斜率和截距)的导数，我们就成功了。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/05d427b17d81920ae79e4ceba3ee51fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*7EhYl31q3HKwWcxTrHqFIA.png"/></div></figure><p id="dbe5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">拟合线的方程式是y=mx+b(其中m是直线的斜率，b是y轴的截距。回到导数规则，我们可以将它们应用到我们的MSE方程中。首先，让我们将成本函数J(m)中的(y^-y)项表示为误差，以简化方程。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/cd7c1c9f56f155223ca5ebf224d0821b.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*X5gV9rTzw7nUNwE6l6ttvw.png"/></div></figure><p id="6929" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">利用链式法则，我们可以求出J(m)的导数如下。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/ee21f0491db2390659082fa43f29fef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*meI5wwJ1WoAbX2jqHa-dKA.png"/></div></figure><p id="9910" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">等式的两个分量可以通过以下等式计算。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/2012f73bb59e771b547dc01db89abd8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*Y5J9uNPDNWqTEF7mHrWtcQ.png"/></div></figure><p id="d6cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回到成本函数对斜率(m)的求导，我们会发现它等于某些常数乘以误差(预测y值与实际数据点的差值)。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/3e7785ecba575a1ce751c13bceb7b56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*xDHvOBTMEFC8w8_VeSMROQ.png"/></div></figure><p id="f637" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个等式可以解释为成本函数相对于斜率的变化量，因此，如果我们逐渐改变斜率，直到误差最小，我们就到达了全局最小值点。</p><p id="15ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">m = m+∏m</p><p id="684e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">b = b+⇼b</p><p id="2f7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类似地，截距的值可以通过计算成本函数对截距(b)的导数来找到。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/5b38aebd7dd1302823452ef5cc106bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*pkmXeIlu9IwESVGE-5TOLg.png"/></div></figure><p id="9683" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">变量(m和b)移动的值被称为“学习率”,它由一个小值定义，该小值可以在拟合过程中以这样一种方式确定，即它不会太小而使收敛到全局最小值的时间很快，也不会太大而导致永远达不到全局最小值点。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/53685abc784d5a0ed16585037443a190.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*QbRBb67ycMpPxaaep5BdYg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">具有小学习率值的梯度下降。</figcaption></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/7bf4dac8a5499652fe7decae35e14d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*WIpAf0QwihZ7EeUs3b0jXA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">具有高学习率值的梯度下降</figcaption></figure><p id="f469" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">确定最佳拟合线系数的最终方程如下。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kd"><img src="../Images/df5aacd86e402977a3b7927f3b5920aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*dR0mNiycIjP07kuHGqzGlA.png"/></div></figure><p id="3c9c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们可以定义一个学习率(λ),并慢慢向最小误差点移动，得到我们的最佳拟合线。</p><p id="cf3e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里必须提到的最后一点是，斜率的值可以从理论上由下式确定。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ke"><img src="../Images/73bdff02895d7a2f7b790c27599d7256.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*MSBdmAO-348KrEA4obp-ng.png"/></div></figure><p id="abaf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，截距可以如下找到。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kf"><img src="../Images/0b0a4faecdb1fc3c61f0d8f410c2e4c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/1*WtOaNUUV8BM1RzEKa1Tqhw.png"/></div></figure><p id="2c21" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是在DL中构建线性回归算法或任何优化器的代码中，使用了一个学习速率，并且一直移动，直到它到达全局极小点。</p><h1 id="2277" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">参考文献:</strong></h1><p id="2214" class="pw-post-body-paragraph iv iw hi ix b iy le ja jb jc lf je jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated"><a class="ae iu" href="https://www.youtube.com/watch?v=jc2IthslyzM" rel="noopener ugc nofollow" target="_blank"> 3.5:梯度下降数学——智力与学习</a></p><p id="720e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.youtube.com/watch?v=PaFPbb66DxQ" rel="noopener ugc nofollow" target="_blank"> StatQuest:对数据拟合直线，又名最小二乘法，又名线性回归</a></p></div></div>    
</body>
</html>