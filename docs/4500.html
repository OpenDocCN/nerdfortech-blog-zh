<html>
<head>
<title>What I wish I fully understood before starting Batch Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在开始批量梯度下降之前，我希望我完全理解</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/let-us-write-mini-batch-gradient-descent-using-numpy-51d67793f16f?source=collection_archive---------17-----------------------#2021-07-23">https://medium.com/nerd-for-tech/let-us-write-mini-batch-gradient-descent-using-numpy-51d67793f16f?source=collection_archive---------17-----------------------#2021-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1535" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">利用爱因斯坦求和技术将整批梯度下降转化为小批梯度下降的方法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/d3bb6b1dd41f6b80e22e13df838fcb7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0asTHp5WvqwUF9l6"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">凯利·西克玛在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="77b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">许多关于机器学习的博客解释了整批、小批和随机梯度下降方法的区别。但是它们要么不显示代码，要么为每种方法编写的代码差别很大。今天，我将向您展示如何轻松地将整批梯度下降转换为小批和随机梯度下降版本。随机梯度下降是批量等于 1 的小批量的特例。</p><p id="e102" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从批量梯度下降多类分类器神经网络开始。它将有一个具有泄漏 relu 激活的密集层，随后是另一个具有 softmax 激活功能的密集层，因为我们将为多类分类做它。如果你想从多类切换到二进制，那么在最后一层使用 sigmoid 激活函数。</p><p id="b029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">整批梯度下降</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="dcf3" class="jz ka hi jv b fi kb kc l kd ke">class GradientDescent:<br/>    def __init__(self, x:np.ndarray, y:np.ndarray, units_count:List, lr:float, epochs:int, bias:bool, seed:int) -&gt; None:<br/>        self.x = x<br/>        self.y = y<br/>        self.inp_units = self.x.shape[-1]<br/>        self.batch_size = self.x.shape[-2]<br/>        self.units_count = units_count<br/>        self.lr = lr<br/>        self.epochs = epochs<br/>        self.bias = bias<br/>        self.seed = seed<br/>        self.rng =  np.random.default_rng(seed=seed)<br/>        self.costs = np.zeros(epochs)<br/>        # Hypothesis for each layer<br/>        self.h0 = None<br/>        self.h0a = None<br/>        self.h1 = None<br/>        self.h1a = None<br/>        # Weight changes delta<br/>        self.dJ_dW1 = None<br/>        self.dJ_dW0 = None<br/>        self.dJ_dB1 = None<br/>        self.dJ_dB0 = None<br/>        <br/>    def init_weights(self):<br/>        units_count_arr = np.array([self.inp_units] + self.units_count)<br/>        L, = units_count_arr.shape<br/>        S = units_count_arr.itemsize<br/>        arr_view = as_strided(units_count_arr, shape=(L-1,2), strides=(S,S)).copy()<br/>        self.wghts = [None]*(L-1)<br/>        self.dWs = [None]*(L-1)<br/>        self.biases = [None]*(L-1)<br/>        self.dBs = [None]*(L-1)</span><span id="2a4d" class="jz ka hi jv b fi kf kc l kd ke">        for i in range(L-1):<br/>            w_cols, w_rows = arr_view[i,:]<br/>            self.wghts[i] = self.rng.random((w_rows, w_cols))<br/>            if self.bias:<br/>                self.biases[i] = self.rng.random((w_rows,))<br/>    <br/>    def forward_prop(self, features):<br/>        # Layer 1<br/>        self.h0 = np.einsum('hi,Bi -&gt; Bh', self.wghts[0], features)<br/>        if self.bias:<br/>            self.h0 = self.h0 + self.biases[0]<br/>        self.h0a = lrelu(self.h0)<br/>        <br/>        # Layer 2<br/>        self.h1 = np.einsum('ho,Bo -&gt; Bh', self.wghts[1], self.h0a)<br/>        if self.bias:<br/>            self.h1 = self.h1 + self.biases[1]<br/>        self.h1a = softmax(self.h1, _axis=1)<br/>        return self.h1a, self.h1<br/>    <br/>    def cce_cost(self, hyp, target, batch_size):<br/>        return -np.einsum('Bi,Bi', target, np.log(hyp))/batch_size<br/>    <br/>    def back_prop(self, features, hyp, target, batch_size):<br/>        # Layer 2<br/>        dJ_dH1a = hyp - target <br/>        self.dWs[1] = np.einsum('Bi,Bj -&gt; ij',dJ_dH1a, self.h0a)/batch_size <br/>        dJ_dH1 = np.einsum('Bi,ij -&gt; Bj', dJ_dH1a, self.wghts[1])<br/>        <br/>        # Layer 1<br/>        dJ_dH0a = dJ_dH1*lrelu_prime(self.h0)<br/>        self.dWs[0] = np.einsum('Bi,Bj -&gt; ij',dJ_dH0a, features)/batch_size<br/>        <br/>        if self.bias:<br/>            self.dBs[1] = np.einsum('Bi -&gt; i', dJ_dH1a)/batch_size<br/>            self.dBs[0] = np.einsum('Bi -&gt; i', dJ_dH0a)/batch_size<br/>            <br/>    def update_weights(self):<br/>        for i in range(len(self.wghts)):</span><span id="bd4e" class="jz ka hi jv b fi kf kc l kd ke">self.wghts[i] = self.wghts[i] - self.lr*self.dWs[i]<br/>            if self.bias:<br/>                self.biases[i] = self.biases[i] - self.lr*self.dBs[i]<br/>            <br/>    def optimize(self, features, target, batch_size):<br/>        for epoch in range(self.epochs):<br/>            hyp, logits = self.forward_prop(features)<br/>            self.costs[epoch] = self.cce_cost(hyp, target, batch_size)<br/>            self.back_prop(features, hyp, target, batch_size)<br/>            self.update_weights()</span><span id="8255" class="jz ka hi jv b fi kf kc l kd ke"># MAIN BLOCK<br/>iris = load_iris()<br/>x = iris.data<br/>y = iris.target<br/>x_norm = normalize(x)<br/>x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.33, shuffle=True, random_state=42)<br/>y_train = binarize(y_train)</span><span id="8149" class="jz ka hi jv b fi kf kc l kd ke">_, out_units = y_train.shape<br/>unit_per_layer_counts = [10, out_units]<br/>gd = GradientDescent(x_train, y_train, unit_per_layer_counts, lr=0.01, epochs=100, bias=True, seed= 42)<br/>gd.init_weights()<br/>gd.optimize(gd.x, gd.y, gd.batch_size)<br/>plt.plot(gd.costs)</span></pre><p id="db9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们应该对上面的代码做什么修改才能使它成为 mini batch？我们将需要一种方法来获得我们的现有数据在一个批量大小。让我们先做那件事。在下面的方法中，您可以给出一个批量大小，数据集被分成多个批次，每个批次包含多个样本。如果分割不完美，则丢弃剩余的样本。可以通过用整个数据集中的随机样本填充最后一批中剩余的空白空间来进一步改进。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="1410" class="jz ka hi jv b fi kb kc l kd ke">def batch_data(test, batch_size):<br/>    m,n = test.shape<br/>    S = test.itemsize<br/>    if not batch_size:<br/>        batch_size = m<br/>    count_batches = m//batch_size<br/>    test_batches = as_strided(test, shape=(count_batches, batch_size, n), strides=(batch_size*n*S,n*S,S)).copy()<br/>    return test_batches</span></pre><blockquote class="kg"><p id="04e3" class="kh ki hi bd kj kk kl km kn ko kp jc dx translated">现在让我们以这样一种方式修改主算法，即在每个时期整个数据仅被覆盖一次，并且来自每个批处理的所有误差仅被累积一次<a class="ae jt" href="https://stats.stackexchange.com/a/189251/183128" rel="noopener ugc nofollow" target="_blank"> </a>。！！</p></blockquote><p id="3d4f" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">这似乎比上一个任务难了一点不是吗。这很容易做到。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="3e2a" class="jz ka hi jv b fi kb kc l kd ke">class BatchGradientDescent:<br/>    def __init__(self, x:np.ndarray, y:np.ndarray, units_count:List, batch_size:int, lr:float, epochs:int, bias:bool, seed:int) -&gt; None:<br/>        self.x = x<br/>        self.y = y<br/>        self.inp_units = self.x.shape[-1]<br/>        self.batch_size = batch_size<br/>        self.units_count = units_count<br/>        self.lr = lr<br/>        self.epochs = epochs<br/>        self.bias = bias<br/>        self.seed = seed<br/>        self.rng =  np.random.default_rng(seed=seed)<br/>        self.costs = np.zeros(epochs)<br/>        # Hypothesis for each layer<br/>        self.h0 = None<br/>        self.h0a = None<br/>        self.h1 = None<br/>        self.h1a = None<br/>        # Weight changes delta<br/>        self.dJ_dW1 = None<br/>        self.dJ_dW0 = None<br/>        self.dJ_dB1 = None<br/>        self.dJ_dB0 = None<br/>        <br/>    def init_weights(self):<br/>        units_count_arr = np.array([self.inp_units] + self.units_count)<br/>        L, = units_count_arr.shape<br/>        S = units_count_arr.itemsize<br/>        arr_view = as_strided(units_count_arr, shape=(L-1,2), strides=(S,S)).copy()<br/>        self.wghts = [None]*(L-1)<br/>        self.dWs = [None]*(L-1)<br/>        self.biases = [None]*(L-1)<br/>        self.dBs = [None]*(L-1)</span><span id="cb90" class="jz ka hi jv b fi kf kc l kd ke">        for i in range(L-1):<br/>            w_cols, w_rows = arr_view[i,:]<br/>            self.wghts[i] = self.rng.random((w_rows, w_cols))<br/>            if self.bias:<br/>                self.biases[i] = self.rng.random((w_rows,))<br/>                <br/>    def batch_data(self, data, batch_size):<br/>        m,n = data.shape<br/>        S = data.itemsize<br/>        if not batch_size:<br/>            batch_size = m<br/>        count_batches = m//batch_size<br/>        data_batches = as_strided(data, shape=(count_batches, batch_size, n), strides=(batch_size*n*S,n*S,S)).copy()<br/>        return data_batches, count_batches<br/>    <br/>    def forward_prop(self, features):<br/>        # Layer 1<br/>        self.h0 = np.einsum('hi, Bi -&gt; Bh', self.wghts[0], features)<br/>        if self.bias:<br/>            self.h0 = self.h0 + self.biases[0]<br/>        self.h0a = lrelu(self.h0)<br/>        <br/>        # Layer 2<br/>        self.h1 = np.einsum('ho,Bo -&gt; Bh', self.wghts[1], self.h0a)<br/>        if self.bias:<br/>            self.h1 = self.h1 + self.biases[1]<br/>        self.h1a = softmax(self.h1, _axis=1)<br/>        return self.h1a, self.h1<br/>    <br/>    def cce_cost(self, hyp, target, batch_size):<br/>        return -np.einsum('Bi,Bi', target, np.log(hyp))/batch_size<br/>    <br/>    def back_prop(self, features, hyp, target, batch_size):<br/>        # Layer 2<br/>        dJ_dH1a = hyp - target <br/>        self.dWs[1] = np.einsum('Bi,Bj -&gt; ij',dJ_dH1a, self.h0a)/batch_size <br/>        dJ_dH1 = np.einsum('Bi,ij -&gt; Bj', dJ_dH1a, self.wghts[1])<br/>        <br/>        # Layer 1<br/>        dJ_dH0a = dJ_dH1*lrelu_prime(self.h0)<br/>        self.dWs[0] = np.einsum('Bi,Bj -&gt; ij',dJ_dH0a, features)/batch_size<br/>        <br/>        if self.bias:<br/>            self.dBs[1] = np.einsum('Bi -&gt; i', dJ_dH1a)/batch_size<br/>            self.dBs[0] = np.einsum('Bi -&gt; i', dJ_dH0a)/batch_size<br/>            <br/>    def update_weights(self):<br/>        for i in range(len(self.wghts)):<br/>            self.wghts[i] = self.wghts[i] - self.lr*self.dWs[i]<br/>            if self.bias:<br/>                self.biases[i] = self.biases[i] - self.lr*self.dBs[i]<br/>            <br/>    def optimize(self, features, target, batch_size):<br/>        batched_features, no_of_batches = self.batch_data(features, batch_size)<br/>        batched_target,_ = self.batch_data(target, batch_size)<br/>        for epoch in range(self.epochs):<br/>            batch_costs = np.zeros(no_of_batches)<br/>            for index in np.ndindex(batched_features.shape[:1]):<br/>                hyp, logits = self.forward_prop(batched_features[index,:].squeeze(axis=0))<br/>                _cost = self.cce_cost(hyp, batched_target[index,:].squeeze(axis=0), batch_size)<br/>                batch_costs[index] = _cost<br/>                self.back_prop(batched_features[index,:].squeeze(axis=0), hyp, batched_target[index,:].squeeze(axis=0), batch_size)<br/>                self.update_weights()<br/>            self.costs[epoch] = batch_costs.sum()</span><span id="9bdc" class="jz ka hi jv b fi kf kc l kd ke"># MAIN block<br/>_, out_units = y_train.shape<br/>unit_per_layer_counts = [10, out_units]<br/>bgd = BatchGradientDescent(x_train, y_train, unit_per_layer_counts, batch_size=32, lr=0.01, epochs=1500, bias=True, seed= 42)<br/>bgd.init_weights()<br/>bgd.optimize(bgd.x, bgd.y, bgd.batch_size)<br/>plt.plot(bgd.costs)<br/>plt.xlabel('Epochs')<br/>plt.ylabel('CategoricalCrossEntropyLoss')<br/>plt.title('Change in Categorical CrossEntropy Loss with Epochs')<br/>plt.savefig('CategoricalCrossEntropyLoss.png')</span></pre><p id="0574" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不用担心任何转置或任何一对多矩阵乘法，我们出来干净。这就是爱因斯坦求和技术的力量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/c4238ec5203727d481eea2a0bfd1d52e.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*vVHkAGGUqxAKhd3CDqZUUg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">使用小批量梯度下降的成本与时期的收敛性。-图片归作者所有</figcaption></figure><p id="310d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你不明白为什么 softmax 的反向传播是以我所做的方式实现的，请点击这个<a class="ae jt" href="https://stats.stackexchange.com/a/235541/183128" rel="noopener ugc nofollow" target="_blank">链接</a>获得一个严格的数学解释。对于玩具示例的解释，请遵循此<a class="ae jt" href="https://stats.stackexchange.com/a/306710/183128" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="61d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还想了解更多？查看博客末尾的参考资料。</p></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><p id="6e61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我是 TCS 的机器学习工程师，我的(数字软件和解决方案)团队正在开发令人惊叹的产品。点击下面的链接，了解更多关于我们产品的信息:</p><div class="ld le ez fb lf lg"><a href="https://www.tcs.com/dss" rel="noopener  ugc nofollow" target="_blank"><div class="lh ab dw"><div class="li ab lj cl cj lk"><h2 class="bd hj fi z dy ll ea eb lm ed ef hh bi translated">数字软件和解决方案:提供高度个性化的体验</h2><div class="ln l"><h3 class="bd b fi z dy ll ea eb lm ed ef dx translated">Digital Software &amp; Solutions 的互联智能解决方案将帮助您转变产品和服务…</h3></div><div class="lo l"><p class="bd b fp z dy ll ea eb lm ed ef dx translated">www.tcs.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu jn lg"/></div></div></a></div></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><p id="ca7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="5d5a" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc ma mb mc md bi translated"><a class="ae jt" href="https://stats.stackexchange.com/a/189251/183128" rel="noopener ugc nofollow" target="_blank">https://stats.stackexchange.com/a/189251/183128</a></li><li id="f581" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">https://ajcr.net/Basic-guide-to-einsum/<a class="ae jt" href="https://ajcr.net/Basic-guide-to-einsum/" rel="noopener ugc nofollow" target="_blank"/></li><li id="0750" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">【https://rockt.github.io/2018/04/30/einsum T4】</li><li id="2657" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jt" href="https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/" rel="noopener ugc nofollow" target="_blank">https://obi laniu 6266 h16 . WordPress . com/2016/02/04/Einstein-summation-in-numpy/</a></li><li id="f36e" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jt" href="https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/235528/back propagation-with-soft max-cross-entropy</a></li><li id="74af" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jt" href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ics.uci.edu/~pjsadows/notes.pdf</a></li></ol></div></div>    
</body>
</html>