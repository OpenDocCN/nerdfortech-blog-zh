<html>
<head>
<title>Review — DevNet: Deep Event Network for Multimedia Event Detection and Evidence Recounting (Video Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾— DevNet:用于多媒体事件检测和证据叙述的深度事件网络(视频分类)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-devnet-deep-event-network-for-multimedia-event-detection-and-evidence-recounting-video-140ded103d01?source=collection_archive---------11-----------------------#2021-06-13">https://medium.com/nerd-for-tech/review-devnet-deep-event-network-for-multimedia-event-detection-and-evidence-recounting-video-140ded103d01?source=collection_archive---------11-----------------------#2021-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0ac8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"> AlexNet </a> -Like 网络与 <a class="ae ix" rel="noopener" href="/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679?source=post_page---------------------------"> SPPNet </a>中的<strong class="ak">空间金字塔池层进行视频分类</strong></h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es iy"><img src="../Images/2e20e793d03b2e3a6e9c1b8d9d508bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*0TC4JNzBfmc5cgEnj_zR-w.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated">给定一个测试视频，DevNet 不仅提供事件标签，还提供时空关键证据。</figcaption></figure><p id="74da" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">在</span>这篇报道中，回顾了由清华大学、香港科技大学、悉尼科技大学和卡内基梅隆大学共同开发的<strong class="jn hj">DevNet:Deep Event Network for Multimedia Event Detection and Evidence returning</strong>(DevNet)。</p><blockquote class="kq kr ks"><p id="e83e" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated"><strong class="jn hj">事件</strong>是比概念更高级别的视频序列的语义抽象，并且通常<strong class="jn hj">由多个概念组成。</strong></p><p id="26e1" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated">一个长的无约束视频<strong class="jn hj">可能包含很多不相关的信息</strong>甚至同一个事件标签可能包含<strong class="jn hj">大的类内变化</strong>。</p></blockquote><p id="3340" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">在本文中:</p><ul class=""><li id="9362" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated"><strong class="jn hj">深度事件网络(DevNet) </strong>，旨在<strong class="jn hj">同时检测预定义事件并提供关键的时空证据。</strong></li><li id="77c7" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated"><strong class="jn hj">可以生成时空显著图</strong>来定位关键证据。</li><li id="061b" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">这是第一篇使用 CNN 完成上述任务的论文。</li></ul><p id="770f" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">这是一篇发表在<strong class="jn hj"> 2015 CVPR </strong>的论文，被引用超过<strong class="jn hj"> 280 次</strong>。(<a class="ll lm ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----140ded103d01--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="3cfc" class="lu lv hi bd jk lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">概述</h1><ol class=""><li id="1845" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg mq ld le lf bi translated"><strong class="jn hj"> DevNet:网络架构</strong></li><li id="8995" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj">基于梯度的时空显著图</strong></li><li id="254e" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj">实验结果</strong></li></ol></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="4425" class="lu lv hi bd jk lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated"><strong class="ak"> 1。DevNet:网络架构</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es mr"><img src="../Images/fdf56e07b66e846f8dd5263f2c08ec58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFSfCDJtam6dmVCuUbL_qQ.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk"> DevNet:网络架构</strong></figcaption></figure><h2 id="6c9a" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated">1.1.预训练</h2><ul class=""><li id="7408" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated">CNN 类似于<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"> AlexNet </a>，包含<strong class="jn hj"> 9 个卷积层</strong>和<strong class="jn hj"> 3 个全连接层。</strong></li><li id="dd9f" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">在这两部分之间，采用了一个起源于<a class="ae ix" rel="noopener" href="/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679?source=post_page---------------------------"> SPPNet </a>的<strong class="jn hj">空间金字塔池层</strong>。</li><li id="32a0" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">权重层配置为:conv 64-conv 192-conv 384-conv 384-conv 384-conv 384-conv 384-conv 384-conv 384-full 4096-full 4096-full 1000。</li><li id="da71" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">前两个全连接层之后是一个<a class="ae ix" href="https://sh-tsang.medium.com/paper-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-image-classification-a74b369b4b8e" rel="noopener">压差</a>层，其<a class="ae ix" href="https://sh-tsang.medium.com/paper-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-image-classification-a74b369b4b8e" rel="noopener">压差</a>为 0.5。</li><li id="c88d" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">在 ImageNet 预训练后，在 ILSVRC2014 验证集上，网络达到了 29.7%/10.5%的前 1/前 5 分类误差。</li></ul><h2 id="b1e0" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated"><strong class="ak"> 1.2。DevNet:微调</strong></h2><ul class=""><li id="d7f3" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated">然后，<strong class="jn hj">soft max 分类器和预训练网络的最后全连接层</strong>被<strong class="jn hj">移除</strong>。</li><li id="16b6" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">为了将图像级特征聚集到视频级表示中，<strong class="jn hj">交叉图像最大池</strong>被应用于<strong class="jn hj">融合来自同一视频内所有关键帧</strong>的第二全连接层的输出:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nk"><img src="../Images/d221f6438055c04a442755e0136b0da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*tX8yf0OqsY2bi4vUb7XDDQ.png"/></div></figure><ul class=""><li id="037d" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">其中<em class="kt"> sit </em>是关键帧<em class="kt"> t </em>特征向量的第<em class="kt"> i </em>维，<em class="kt"> fi </em>是视频级特征向量<em class="kt"> f </em>的第<em class="kt"> i </em>维。</li><li id="f9cf" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">softmax loss 被更合适的<strong class="jn hj"> <em class="kt"> c </em> -way 独立逻辑回归</strong>代替，其为每个事件类别产生<strong class="jn hj">检测分数。</strong></li><li id="9db3" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">这种跨图像最大池化后的视频级表示作为事件检测任务的特征。</li><li id="59bc" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">简而言之，<strong class="jn hj">支持向量机</strong>和具有 chi^2 核的核岭回归(KR)被用作<strong class="jn hj">事件分类器。</strong></li></ul></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="c78d" class="lu lv hi bd jk lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">2.基于梯度的时空显著图</h1><ul class=""><li id="6376" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated">我们的事件叙述方法的主要思想是，给定一个已学习的检测 DevNet 和一个感兴趣的类，原始输入图像通过<strong class="jn hj">反向传递</strong>来跟踪，通过它我们可以找到<strong class="jn hj">每个像素如何影响指定事件类</strong>的最终检测分数。</li><li id="2732" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">然而，类得分<em class="kt"> Sc </em> ( <em class="kt"> V </em>)是<em class="kt"> V </em>的高度非线性函数，因此<em class="kt"> Sc </em> ( <em class="kt"> V </em>)是通过在<em class="kt"> V </em> 0:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nl"><img src="../Images/fd9e9bcba6621720e9a63c828cb42b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*lNgDWba9PBXXnwGcEQ9RPg.png"/></div></figure><ul class=""><li id="2c85" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">其中<em class="kt"> x </em>是视频<em class="kt"> V </em>的矢量化形式。<em class="kt"> Sc </em> ( <em class="kt"> V </em>)为检测分数。</li><li id="4bcf" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">用<strong class="jn hj">表示<em class="kt"> Sc </em> ( <em class="kt"> V </em>)相对于<em class="kt"> V </em>在<em class="kt"> V </em> 0 </strong>点的导数为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nk"><img src="../Images/ad86a6a6390f5bde64c7363e12cca6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*-_3I4p8TZU6E3TM52DD4aQ.png"/></div></figure><ul class=""><li id="c688" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">给定属于事件类别<em class="kt"> c </em>的视频，其具有尺寸为<em class="kt"> p </em> × <em class="kt"> q </em>的<em class="kt"> k </em>个关键帧，计算空间和时间关键证据。</li><li id="73bc" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated"><strong class="jn hj">每个关键帧</strong>中每个像素的显著性得分可以计算为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nm"><img src="../Images/c7fb0bffeb6eaba55574e77a21ce4d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*fXOZH3KnHIVqIBmEIvHcJQ.png"/></div></figure><ul class=""><li id="3a5e" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">其中<em class="kt"> h </em> ( <em class="kt"> i </em>，<em class="kt"> j </em>，<em class="kt"> k </em>)是第<em class="kt"> k </em>关键帧的第<em class="kt"> i </em>行第<em class="kt"> j </em>列图像像素对应的<em class="kt"> wc </em>元素的索引。</li><li id="5147" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">因此，<strong class="jn hj">对于每个事件类别，可以为视频中的每个像素</strong>导出单个类别特定的显著性分数。</li><li id="64b2" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">在获得时空显著性图之后，<strong class="jn hj">对关键帧内所有像素的显著性分数进行平均</strong>，以获得<strong class="jn hj">关键帧级显著性分数</strong>。</li><li id="5023" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">然后，对关键帧级别的显著性分数进行<strong class="jn hj">排序</strong>以获得信息关键帧。</li><li id="8633" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">对于<strong class="jn hj">排名靠前的关键帧</strong>，使用<strong class="jn hj">显著性分数</strong>作为指导，应用<strong class="jn hj">图割</strong>算法<strong class="jn hj">分割空间显著区域</strong>。</li></ul></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="0cd2" class="lu lv hi bd jk lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">3.实验结果</h1><h2 id="2f0c" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated">3.1.资料组</h2><ul class=""><li id="c68f" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated">使用 NIST TRECVID 2014 多媒体事件检测数据集。</li><li id="d452" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">该数据集包含长度、质量和分辨率差异很大的无约束网络视频。</li><li id="b755" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">它还配有针对<strong class="jn hj"> 20 个事件类别的<strong class="jn hj">地面实况视频级注释</strong>。</strong></li><li id="c884" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">按照 100EX 评估程序，使用 3 个不同的分区进行评估:</li></ul><ol class=""><li id="8e1e" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg mq ld le lf bi translated"><strong class="jn hj">背景</strong>，包含不属于任何目标事件的大约<strong class="jn hj"> 5000 个背景视频。</strong></li><li id="48e5" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj"> 100EX </strong>，包含<strong class="jn hj">每个事件</strong>的 100 个正面视频，作为<strong class="jn hj">训练集</strong>。</li><li id="2a21" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj">包含<strong class="jn hj">23954 个视频</strong>的 MEDTest </strong>被用作<strong class="jn hj">测试集</strong>。</li></ol><h2 id="f4bf" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated">3.2.事件检测结果</h2><ul class=""><li id="2594" class="kx ky hi jn b jo ml jr mm ju mn jy mo kc mp kg lc ld le lf bi translated">最小归一化检测成本(MinNDC)和平均精度(AP)被用作每个事件的度量。</li><li id="4019" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">简而言之，不同的成本值被分配给漏检和虚警。</li><li id="b148" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">较低的 MinNDC 或较高的 AP 和 mAP 值指示较好的检测性能。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es nn"><img src="../Images/2a4c34063f004df2eb352d93fb7813c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zs4solIRhDSJtj9PXdPQ_g.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">事件检测结果与改进的密集轨迹 Fisher 向量(IDTFV)比较。</strong></figcaption></figure><ul class=""><li id="63c2" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">通过对所有事件进行平均，与最先进的 IDTFV 浅层特征相比，提议的基于 CNN 的 DevNet 在平均精度(mAP) <strong class="jn hj">方面有<strong class="jn hj"> 5.86%的改进</strong>。</strong></li></ul><h2 id="33c4" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated">3.3.证据重新计数结果</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es no"><img src="../Images/23114f95bff7da8fddc03b43b5d82b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*EPTm-nDP0aCGpEyfiveohA.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">证据质量与重新计票百分比的比较。</strong></figcaption></figure><ul class=""><li id="2275" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">使用了两个标准。</li></ul><ol class=""><li id="cd2b" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg mq ld le lf bi translated"><strong class="jn hj">证据质量</strong>，衡量本地化的关键证据能够在多大程度上让法官相信视频中发生了特定事件。</li><li id="d200" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg mq ld le lf bi translated"><strong class="jn hj">重新计数百分比</strong>，衡量视频片段相对于整个视频的紧凑程度。</li></ol><ul class=""><li id="6506" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">一些志愿者被要求担任评估者。评估者首先分别看到 1、5、10、25、50、75 和 100%的测试视频。他们投票决定显示的关键帧是否能让他们相信这是一个正面的范例。</li><li id="f5ee" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">如图所示，<strong class="jn hj"> DevNet 可以将重新计票百分比降低 15%到 25%，以获得与基线方法相同的证据质量。</strong>这证实了 DevNet 为用户快速准确地掌握视频事件的基本概念提供了相当好的证据。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es np"><img src="../Images/454117c9bb60562e399a858775ea2f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*tdpKLezLwaaSGtfj9H4JRQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">事件重新计数结果与基线方法的比较。t 表示时间关键证据，S 表示空间关键证据。</strong></figcaption></figure><ul class=""><li id="1261" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">上表总结了评估者的偏好。</li><li id="d45b" class="kx ky hi jn b jo lg jr lh ju li jy lj kc lk kg lc ld le lf bi translated">DevNet 对大多数事件来说更好。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="er es nq"><img src="../Images/8acfff11af47b7be107dc2e423d87801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9glO3FDyMUTf2XLUrqSAUg.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">DevNet 生成的事件重新计票结果。从左到右是顶部一个时间关键证据、空间显著图和空间关键证据。</strong></figcaption></figure><ul class=""><li id="1ae2" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">上面还显示了一些可视化结果。</li></ul></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h2 id="b25a" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated">参考</h2><p id="2028" class="pw-post-body-paragraph jl jm hi jn b jo ml ij jq jr mm im jt ju nr jw jx jy ns ka kb kc nt ke kf kg hb bi translated">[2015 CVPR][DevNet]<br/><a class="ae ix" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf" rel="noopener ugc nofollow" target="_blank">DevNet:用于多媒体事件检测和证据叙述的深度事件网络</a></p><h2 id="efb0" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated">视频分类</h2><p id="37db" class="pw-post-body-paragraph jl jm hi jn b jo ml ij jq jr mm im jt ju nr jw jx jy ns ka kb kc nt ke kf kg hb bi translated"><strong class="jn hj">2014</strong><a class="ae ix" href="https://sh-tsang.medium.com/paper-deep-video-large-scale-video-classification-with-convolutional-neural-network-video-585c36c4f042" rel="noopener">深度视频</a><strong class="jn hj">2015</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-devnet-deep-event-network-for-multimedia-event-detection-and-evidence-recounting-video-140ded103d01" rel="noopener">DevNet</a><a class="ae ix" href="https://sh-tsang.medium.com/paper-c3d-learning-spatiotemporal-features-with-3d-convolutional-networks-video-classification-72b49adb4081" rel="noopener">C3D</a><strong class="jn hj">2017</strong><a class="ae ix" href="https://sh-tsang.medium.com/paper-p3d-pseudo-3d-residual-networks-video-classification-action-recognition-d1dd13638d7c" rel="noopener">P3D</a></p><h2 id="2a60" class="mw lv hi bd jk mx my mz lz na nb nc md ju nd ne mf jy nf ng mh kc nh ni mj nj bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>