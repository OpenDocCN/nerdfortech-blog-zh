<html>
<head>
<title>Review — CoupleNet: Coupling Global Structure with Local Parts for Object Detection (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾— CoupleNet:将全局结构与局部部分耦合起来进行对象检测(对象检测)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850?source=collection_archive---------5-----------------------#2021-04-04">https://medium.com/nerd-for-tech/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850?source=collection_archive---------5-----------------------#2021-04-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7ab4" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">拥有本地和全球FCN分支机构，捕捉本地和全球信息，胜过<a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"> R-FCN </a>，<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快R-CNN </a>，<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>，&amp; <a class="ae ix" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank"> ION </a>。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/706c15c91842075f3995291c4e1bf68f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GRKfpy6PWpEuFlG3FaYMg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">通过结合局部和全局信息进行物体检测的玩具示例</strong></figcaption></figure><p id="41bc" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事里，回顾了中国科学院、中国科学院大学、南京审计大学、印第安纳大学的<strong class="jr hj"> CoupleNet:耦合全局结构与局部结构进行物体检测</strong>(couple net)。在本文中:</p><ul class=""><li id="94e2" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">区域提议网络(RPN)获得的对象提议被输入到由两个分支组成的<strong class="jr hj">耦合模块</strong>。</li><li id="9966" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">一个分支采用<strong class="jr hj">位置敏感感兴趣区域(PSRoI)池</strong>捕获<strong class="jr hj">物体的局部信息</strong>。</li><li id="7c1e" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">另一种采用<strong class="jr hj"> RoI汇集</strong>来编码<strong class="jr hj">全局和上下文信息</strong>。</li></ul><p id="22a3" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这是一篇发表在<strong class="jr hj"> 2017 ICCV </strong>的论文，引用超过<strong class="jr hj"> 130次</strong>。(<a class="li lj ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----d80150c5c850--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="1beb" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">概述</h1><ol class=""><li id="244c" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk mn la lb lc bi translated"><strong class="jr hj"> CoupleNet:网络架构</strong></li><li id="626f" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">实验结果</strong></li></ol></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="f826" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated"><strong class="ak"> 1。CoupleNet:网络架构</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mo"><img src="../Images/e81603e3c0f54bef048d997acc1dae39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XNSmruHpXqVSH9osnhzZxw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo"> CoupleNet:网络架构</strong></figcaption></figure><ul class=""><li id="7e44" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">ImageNet预训练<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> -101作为主干，去掉了最后的平均池层和fc层。</li><li id="4ffb" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">然后，每份提案都流向两个不同的分支机构:当地的FCN和全球的FCN。</li><li id="ac3e" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">最后，全局和局部FCN的输出耦合在一起作为对象的最终得分。</li></ul><h2 id="5a60" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">1.1.当地FCN</h2><ul class=""><li id="4d30" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">一组<strong class="jr hj">部分敏感得分图</strong>通过追加一个1×1卷积层与<strong class="jr hj"> <em class="nd"> k </em> ( <em class="nd"> C </em> + 1)通道</strong>，其中<em class="nd"> k </em>是指我们将物体分成<em class="nd"> k </em> × <em class="nd"> k </em>局部(<em class="nd">k</em>= 7)<em class="nd">C</em>+1是物体类别数加上背景。</li><li id="e459" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">对于每个类别，总共有<em class="nd"> k </em>个通道</strong>，每个通道负责对对象的特定部分进行编码。</li><li id="53fc" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">类别的最终得分是通过对k个回答进行投票来确定的。</li><li id="1c8a" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">平均池</strong>用于投票。然后，获得<strong class="jr hj"> a ( <em class="nd"> C </em> + 1)-d向量</strong>，其指示对象属于每个类别的概率。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ne"><img src="../Images/ef7d92b948b5fda40913a0eb3c105195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*oQATSgd8LAgK_GcQONObqg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">本地FCN使用的直观描述</strong></figcaption></figure><blockquote class="nf ng nh"><p id="37f2" class="jp jq nd jr b js jt ij ju jv jw im jx ni jz ka kb nj kd ke kf nk kh ki kj kk hb bi translated">如上图，例如:对于被截断的人，人们很难从全局描述中得到强有力的回应。<strong class="jr hj">局部FCN可以有效捕捉几个特定部位</strong>，比如人的鼻子、嘴巴等。</p></blockquote><h2 id="4cbd" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">1.2.全球FCN</h2><ul class=""><li id="7720" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">除了常规的RoI合并(全局FCN的黄色区域)之外，<strong class="jr hj">多插入一个RoI合并层(全局FCN的绿色区域)</strong>作为上下文区域，以提取对象的全局结构描述。</li><li id="6abe" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">具体来说，<strong class="jr hj">该上下文区域比原始提议的大小大两倍。</strong></li><li id="ac11" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">然后，从原始区域和上下文区域汇集的特征RoI被连接在一起。</li><li id="334d" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">核大小分别为<em class="nd"> k </em> × <em class="nd"> k </em>和1×1(<em class="nd">k</em>设为默认值7)的两个卷积层用于进一步抽象RoI的全局表示。</li><li id="3419" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">最后，1×1卷积的输出被送入分类器，其输出也是一个(<em class="nd"> C </em> + 1)-d向量。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nl"><img src="../Images/dd03a708ee4c7922027499d1635d881e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*wYhycVhB5Qj-BrUVYS0wSA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">全球FCN使用的直观描述</strong></figcaption></figure><blockquote class="nf ng nh"><p id="880b" class="jp jq nd jr b js jt ij ju jv jw im jx ni jz ka kb nj kd ke kf nk kh ki kj kk hb bi translated"><strong class="jr hj">对于那些具有简单空间结构并且在包围盒中包含大量背景</strong>的物体，例如餐桌，<strong class="jr hj">全局FCN有助于捕捉全局上下文</strong>以提高检测性能。</p></blockquote><h2 id="61ae" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">1.3.耦合结构</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nm"><img src="../Images/da65131c19b31cd98c99fbf5646d11b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*SUTmnADChLd9Xb9Woqh__g.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">不同归一化操作和耦合方法的效果。</strong></figcaption></figure><ul class=""><li id="6a9f" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">要合并本地FCN和全球FCN，有许多方法。</li><li id="91ab" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">发现使用<strong class="jr hj"> 1×1卷积来重新标度响应</strong>，比L2归一化好得多。</li><li id="3231" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">这是因为L2归一化减少了不同类别之间的输出差距，从而导致更小的得分差距，降低了准确性。</li><li id="af7b" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">元素式求和</strong>即使采用不同的归一化方法，也总能获得最佳性能。</li><li id="7abf" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于元素方面的产品，它甚至在训练期间也不稳定。</li><li id="27a0" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于逐元素最大值，它在某种程度上相当于网络中的集合模型。所以，作者也与合奏比较。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nn"><img src="../Images/b061dfef0133f7d2a057831300b1e6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*ifj01dzzYV-x5FLXt3g5Ow.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">情侣网vs模联</strong></figcaption></figure><ul class=""><li id="56a6" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">模型集合带来的提升(第4 &amp; 5行)不到1分。远远小于CoupleNet (81.7%)。</li><li id="95b9" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">另一方面，CoupleNet享受<strong class="jr hj">端到端</strong>培训，并且<strong class="jr hj">不需要培训多个型号</strong>，因此<strong class="jr hj">大大减少了培训时间</strong>。</li></ul><h2 id="1c17" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">1.4.复杂性</h2><ul class=""><li id="875a" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">整个网络是完全卷积的。</li><li id="a9db" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">全球分公司可以算是轻量级<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快的R-CNN </a>。RoI方式子网的深度仅为2。</li><li id="30d5" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">计算复杂度远小于深度为10的基于<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>的<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快R-CNN </a>系统中的子网。</li><li id="061a" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">因此，CoupleNet可以有效地执行推理，其运行速度比<a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"> R-FCN </a>稍慢，但比<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快的R-CNN </a>快得多。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="f3af" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">2.实验结果</h1><h2 id="5eb5" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">2.1.帕斯卡VOC 2007</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es no"><img src="../Images/f1ca9cecb95f01cdc55f872440bfe5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QYcp1Pq-pIuYPKnVvMT8QA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">与</strong> <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="bd jo">的对比更快R-CNN </strong> </a> <strong class="bd jo">与</strong> <a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="bd jo"> R-FCN </strong> </a> <strong class="bd jo">使用</strong><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd jo">ResNet</strong></a><strong class="bd jo">-101。</strong></figcaption></figure><ul class=""><li id="23f1" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">模型在VOC 2007 trainval和VOC 2012 trainval(“07+12”)的联合集上训练，并在VOC 2007测试集上评估。</li><li id="854f" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">没有上下文区域和没有多尺度训练的CoupleNet已经比有或没有多尺度训练的</strong> <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="jr hj">更快R-CNN </strong> </a> <strong class="jr hj">，和</strong><a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jr hj">R-FCN</strong></a><strong class="jr hj">。</strong></li><li id="97b8" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">结合上下文区域和多尺度训练，CoupleNet获得了更高的82.7%的mAP。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es np"><img src="../Images/b94aad77776cde5cbe8c232537664a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-laYhycniCFSnPC0qojow.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">PASCAL VOC 2007测试集的结果</strong></figcaption></figure><ul class=""><li id="9857" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj"> CoupleNet也优于其他SOTA方法</strong>，如<a class="ae ix" href="https://towardsdatascience.com/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------" rel="noopener" target="_blank"> ION </a>和<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>。</li></ul><h2 id="fb40" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">2.2.帕斯卡VOC 2012</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nq"><img src="../Images/96b83677757b4c5c58b82c51fa03154b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3itl9qMTfg2PKCYE9-58Rw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">PASCAL VOC 2012测试集的结果</strong></figcaption></figure><ul class=""><li id="bf7b" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">CoupleNet获得80.4%的顶图，比<a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"> R-FCN </a>高2.8分。</li></ul><blockquote class="nf ng nh"><p id="068b" class="jp jq nd jr b js jt ij ju jv jw im jx ni jz ka kb nj kd ke kf nk kh ki kj kk hb bi translated">不使用测试阶段的额外招数，<strong class="jr hj"> CoupleNet是第一个mAP高于80%的。</strong></p></blockquote><ul class=""><li id="11d4" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">下面显示了CoupleNet的一些检测示例:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nr"><img src="../Images/875eb982aac9e6c20b53576c6c1e1b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xquAzkTmJQstUVvcwzm7-A.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">PASCAL VOC 2012测试集上CoupleNet的检测实例</strong></figcaption></figure><h2 id="c0a4" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">2.3.可可女士</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ns"><img src="../Images/36fefc534e968fbbb3010e4aedda3b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*peL0aXPHKobWEk-34rQ3lw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">COCO 2015测试开发结果</strong></figcaption></figure><ul class=""><li id="03e3" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">CoupleNet在80k训练集和40k验证集的并集上进行训练，并在20k测试开发集上进行测试。</li><li id="4427" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">当在单个尺度中测试时，利用从{480，576，672，768，864}中随机采样的尺度进行多尺度训练。</li><li id="b1e8" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">单尺度CoupleNet已经取得了33.1%的成绩，领先</strong><a class="ae ix" href="https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jr hj">R-FCN</strong></a><strong class="jr hj">3.9个点。</strong></li><li id="121d" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">此外，多尺度训练进一步提高了34.4%的成绩。</li><li id="47bb" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">观察到数据集越有挑战性，提升越多(例如，VOC07的2.2%，VOC12的2.8%，以及COCO的4.5%)。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h2 id="faf9" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">参考</h2><p id="680e" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy nt ka kb kc nu ke kf kg nv ki kj kk hb bi translated">【2017 ICCV】【couple net】<br/><a class="ae ix" href="https://arxiv.org/abs/1708.02863" rel="noopener ugc nofollow" target="_blank">couple net:耦合全局结构和局部结构进行物体检测</a></p><h2 id="9e08" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">目标检测</h2><p id="4755" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy nt ka kb kc nu ke kf kg nv ki kj kk hb bi translated"><strong class="jr hj"> 2014 </strong> : [ <a class="ae ix" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------">过食</a>][<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------">R-CNN</a>]<br/><strong class="jr hj">2015</strong>:[<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快R-CNN </a> ] [ <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快R-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a><br/><strong class="jr hj">2016 [<a class="ae ix" href="https://towardsdatascience.com/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac?source=post_page---------------------------" rel="noopener" target="_blank">GBD-网/GBD-v1&amp;GBD-v2</a>][<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae ix" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a><br/><strong class="jr hj">2017</strong>:[<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a?source=post_page---------------------------">NoC</a>][<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------">TDM</a>[<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>[<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 9000] [</a><a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener">couple net</a>]<br/><strong class="jr hj">2018</strong>:[<a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolov 3</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">Cascade R-CNN</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>]<br/><strong class="jr hj">2019</strong>:[<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5">DCN v2</a></strong></p><h2 id="b5a8" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>