<html>
<head>
<title>Fine Tuning pretrained BERT for Sentiment Classification using Transformers in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 中的转换器微调预训练的用于情感分类的 BERT</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/fine-tuning-pretrained-bert-for-sentiment-classification-using-transformers-in-python-931ed142e37?source=collection_archive---------2-----------------------#2021-08-08">https://medium.com/nerd-for-tech/fine-tuning-pretrained-bert-for-sentiment-classification-using-transformers-in-python-931ed142e37?source=collection_archive---------2-----------------------#2021-08-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/b6933a073a37931f6d59d2aef157a725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*eWmEKPSrZIlLTU-LvApTYw.png"/></div></figure><h1 id="7283" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">情感分析</h1><p id="5502" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">情感分析是自然语言处理(NLP)的一个应用，用于发现用户评论、评论等的情感。在网上。如今，像脸书、推特这样的社交网站被广泛用于发布用户对不同事物的评论，比如电影、新闻、美食、时尚、政治等等。评论和意见在确定用户对特定实体的满意度方面起着重要作用。然后，这些用于找到极性，即正极、负极和中性。在这个项目中，讨论了一种对电影评论进行情感分析的方法。</p><h1 id="f74b" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">问题陈述</h1><p id="66d4" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">ABC 公司希望拥有一个高度可扩展的情感分析模型。因此，它决定超越他们的评论生态系统，并根据更多数据训练他们现有的模型。训练样本是评论和推文的混合。</p><h1 id="4748" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">属性描述:</h1><ul class=""><li id="a290" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">ID —唯一标识符</li><li id="4998" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">作者-用户 ID</li><li id="79f0" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">审查—不同类型</li><li id="9ddf" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">类别—代表各种情绪(<strong class="jm hj"> 0 </strong>:负面，<strong class="jm hj"> 1 </strong>:中性<strong class="jm hj"> 2 </strong>:正面)</li></ul><h1 id="02ed" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">必需的安装</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/57da2930027686cc196d578f335b1af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*Kig1wSN3RbzOxAcTUBpx4g.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated"><strong class="bd io">抱脸变压器</strong></figcaption></figure><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="bbe8" class="lk in hi lg b fi ll lm l ln lo">!pip install transformers</span></pre><h1 id="dc22" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">导入所需的包</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="e988" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">import</strong> <strong class="lg hj">pandas</strong> <strong class="lg hj">as</strong> <strong class="lg hj">pd</strong><br/><strong class="lg hj">import</strong> <strong class="lg hj">numpy</strong> <strong class="lg hj">as</strong> <strong class="lg hj">np</strong><br/><strong class="lg hj">import</strong> <strong class="lg hj">seaborn</strong> <strong class="lg hj">as</strong> <strong class="lg hj">sns</strong><br/><strong class="lg hj">import</strong> <strong class="lg hj">matplotlib.pyplot</strong> <strong class="lg hj">as</strong> <strong class="lg hj">plt</strong><br/>%matplotlib inline<br/><strong class="lg hj">import</strong> <strong class="lg hj">warnings</strong><br/>warnings.filterwarnings('ignore')</span></pre><h1 id="89bb" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">读取训练和测试数据样本</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="205a" class="lk in hi lg b fi ll lm l ln lo">train = pd.read_csv("/content/disk/MyDrive/Machine_Hack/train.csv")<br/>test = pd.read_csv("/content/disk/MyDrive/Machine_Hack/test.csv")<br/>sub = pd.read_csv("/content/disk/MyDrive/Machine_Hack/submission.csv")</span><span id="552c" class="lk in hi lg b fi lp lm l ln lo">train.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/d5d2df29a7b22cedb821866c3c557d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kw6Wf6TNsjlqtaUqblg6xw.png"/></div></div></figure><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="4f22" class="lk in hi lg b fi ll lm l ln lo">test.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/d7ad594821038c45a2592b953460084f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*mXE_elWQhibFTRGwsgQfFQ.png"/></div></figure><h1 id="43e0" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">训练样本中标签的分布</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/4c47c3ee49866e98b3749ea5c42bd5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*nvzF7NNaDqSdA0STIRntLA.png"/></div></figure><h1 id="3a84" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">提取要素和标注</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="caab" class="lk in hi lg b fi ll lm l ln lo">train_texts = train['Review'].values.tolist()<br/>train_labels = train['Sentiment'].values.tolist()<br/>test_texts = test['Review'].values.tolist()</span></pre><h1 id="acf6" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">将训练样本分成训练集和验证集</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="8404" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">from</strong> <strong class="lg hj">sklearn.model_selection</strong> <strong class="lg hj">import</strong> train_test_split<br/>train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2,random_state=42,stratify=train_labels)</span></pre><h1 id="31bd" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">微调定制模型的步骤</h1><ul class=""><li id="661c" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">准备数据集</li><li id="47b1" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">加载预训练的标记器，用数据集调用它</li><li id="d654" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">使用编码构建 Pytorch 数据集</li><li id="c505" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">负载预训练模型</li><li id="517f" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">加载训练器并训练它(或)使用本机 Pytorch 训练管道</li></ul><p id="f7e1" class="pw-post-body-paragraph jk jl hi jm b jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd mb kf kg kh hb bi translated">注意:这里我们使用了培训师</p><h1 id="49ca" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">导入所需的转换器库</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="8b02" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">import</strong> <strong class="lg hj">torch</strong><br/><strong class="lg hj">from</strong> <strong class="lg hj">torch.utils.data</strong> <strong class="lg hj">import</strong> Dataset<br/><strong class="lg hj">from</strong> <strong class="lg hj">transformers</strong> <strong class="lg hj">import</strong> DistilBertTokenizerFast,DistilBertForSequenceClassification<br/><strong class="lg hj">from</strong> <strong class="lg hj">transformers</strong> <strong class="lg hj">import</strong> Trainer,TrainingArguments</span></pre><h1 id="5bdd" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">设置模型名称</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="82fc" class="lk in hi lg b fi ll lm l ln lo">model_name  = 'distilbert-base-uncased'</span></pre><h1 id="6cb2" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">标记化</h1><p id="89b8" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">使用 tokenizer 对语料库进行编码。</p><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="419b" class="lk in hi lg b fi ll lm l ln lo">tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',num_labels=3)</span></pre><ul class=""><li id="00d6" class="ki kj hi jm b jn lx jr ly jv mc jz md kd me kh kn ko kp kq bi translated">这里标签的数量=3</li></ul><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="aa6a" class="lk in hi lg b fi ll lm l ln lo">train_encodings = tokenizer(train_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = 'pt')<br/>val_encodings = tokenizer(val_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = 'pt')<br/>test_encodings = tokenizer(test_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = 'pt')</span></pre><ul class=""><li id="1377" class="ki kj hi jm b jn lx jr ly jv mc jz md kd me kh kn ko kp kq bi translated">在 BERT 情况下，设置 truncation = True 将消除超过 max_length(512)的令牌</li><li id="8853" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">设置 padding =True 将使用空标记(即 0)填充长度小于 max_length 的文档，确保我们的所有序列都填充到相同的长度</li><li id="2418" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">设置 return_tensors = 'pt '会将编码返回为 pytorch 张量</li><li id="e2dd" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">这将允许我们同时向模型中输入一批序列。</li></ul><h1 id="1703" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">将我们的标签和编码转换成数据集对象</h1><ul class=""><li id="16e9" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">将标记化的数据包装到 torch 数据集中</li><li id="4ffd" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">在 PyTorch 中，这是通过子类化 torch.utils.data.Dataset 对象并实现 len 和 getitem 来实现的。</li></ul><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="041d" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">class</strong> <strong class="lg hj">SentimentDataset</strong>(torch.utils.data.Dataset):<br/>    <strong class="lg hj">def</strong> __init__(self, encodings, labels):<br/>        self.encodings = encodings<br/>        self.labels = labels<br/><br/>    <strong class="lg hj">def</strong> __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) <strong class="lg hj">for</strong> key, val <strong class="lg hj">in</strong> self.encodings.items()}<br/>        item['labels'] = torch.tensor(self.labels[idx])<br/>        <strong class="lg hj">return</strong> item<br/><br/>    <strong class="lg hj">def</strong> __len__(self):<br/>        <strong class="lg hj">return</strong> len(self.labels)<br/>## Test Dataset</span><span id="0374" class="lk in hi lg b fi lp lm l ln lo"><strong class="lg hj">class</strong> <strong class="lg hj">SentimentTestDataset</strong>(torch.utils.data.Dataset):<br/>    <strong class="lg hj">def</strong> __init__(self, encodings):<br/>        self.encodings = encodings<br/><br/>    <strong class="lg hj">def</strong> __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) <strong class="lg hj">for</strong> key, val <strong class="lg hj">in</strong> self.encodings.items()}<br/>        <strong class="lg hj">return</strong> item<br/>    <strong class="lg hj">def</strong> __len__(self):<br/>        <strong class="lg hj">return</strong> len(self.encodings)</span></pre><h1 id="ab4a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">Genearte 数据加载器</h1><ul class=""><li id="6f1a" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">将标记化的数据转换为 torch 数据集</li></ul><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="73f3" class="lk in hi lg b fi ll lm l ln lo">train_dataset = SentimentDataset(train_encodings, train_labels)<br/>val_dataset = SentimentDataset(val_encodings, val_labels)<br/>test_dataset = SentimentTestDataset(test_encodings)</span></pre><h1 id="ce4a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">定义一个简单的度量函数</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="df2b" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">from</strong> <strong class="lg hj">sklearn.metrics</strong> <strong class="lg hj">import</strong> accuracy_score, f1_score<br/><strong class="lg hj">def</strong> compute_metrics(p):<br/>    pred, labels = p<br/>    pred = np.argmax(pred, axis=1)<br/><br/>    accuracy = accuracy_score(y_true=labels, y_pred=pred)<br/>    <em class="mf">#recall = recall_score(y_true=labels, y_pred=pred)</em><br/>    <em class="mf">#precision = precision_score(y_true=labels, y_pred=pred)</em><br/>    f1 = f1_score(labels, pred, average='weighted')<br/><br/>    <strong class="lg hj">return</strong> {"accuracy": accuracy,"f1_score":f1}</span></pre><h1 id="e45a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">定义培训参数</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="ce7f" class="lk in hi lg b fi ll lm l ln lo">training_args = TrainingArguments(<br/>    output_dir='./res',          <em class="mf"># output directory</em><br/>    evaluation_strategy="steps",<br/>    num_train_epochs=5,              <em class="mf"># total number of training epochs</em><br/>    per_device_train_batch_size=32,  <em class="mf"># batch size per device during training</em><br/>    per_device_eval_batch_size=64,   <em class="mf"># batch size for evaluation</em><br/>    warmup_steps=500,                <em class="mf"># number of warmup steps for learning rate scheduler</em><br/>    weight_decay=0.01,               <em class="mf"># strength of weight decay</em><br/>    logging_dir='./logs4',            <em class="mf"># directory for storing logs</em><br/>    <em class="mf">#logging_steps=10,</em><br/>    load_best_model_at_end=<strong class="lg hj">True</strong>,<br/>)</span></pre><h1 id="7941" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">使用教练进行微调</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="6e2c" class="lk in hi lg b fi ll lm l ln lo">model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",num_labels=3)<br/><br/>trainer = Trainer(<br/>    model=model,<em class="mf"># the instantiated 🤗 Transformers model to be trained</em><br/>    args=training_args, <em class="mf"># training arguments, defined above</em><br/>    train_dataset=train_dataset,<em class="mf"># training dataset</em><br/>    eval_dataset=val_dataset , <em class="mf"># evaluation dataset</em><br/>    compute_metrics=compute_metrics,<br/>)<br/><br/>trainer.train()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/ec4ca08e4981ee6ae5ddeb338e1d714f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*N0di1LimmQR83zHSiCHVcQ.png"/></div></figure><h1 id="7a8b" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">基于验证数据集评估培训师</h1><p id="75b6" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">因为作为训练的一部分，load_best_model_at_end 被设置为 True，所以这将在完成训练时自动加载最佳模型。</p><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="c779" class="lk in hi lg b fi ll lm l ln lo">trainer.evaluate()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/dd32e4b33160ebbd791a30f6d305e0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2HZxmgus6PrMkbOw58ZgjA.png"/></div></figure><h1 id="175b" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">根据测试数据进行预测</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="dded" class="lk in hi lg b fi ll lm l ln lo">test[‘Sentiment’] = 0 test_texts = test[‘Review’].values.tolist() test_labels = test[‘Sentiment’].values.tolist() <br/>test_encodings = tokenizer(test_texts, truncation=<strong class="lg hj">True</strong>, padding=<strong class="lg hj">True</strong>,return_tensors = ‘pt’).to(“cuda”) <br/>test_dataset = SentimentDataset(test_encodings, test_labels)</span><span id="ee81" class="lk in hi lg b fi lp lm l ln lo">preds = trainer.predict(test_dataset=test_dataset)</span></pre><h1 id="7ff7" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">检索预测概率</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="caff" class="lk in hi lg b fi ll lm l ln lo">probs = torch.from_numpy(preds[0]).softmax(1)<br/><br/>predictions = probs.numpy()# convert tensors to numpy array</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mi"><img src="../Images/b5b5059bfaf504757da724bfcc4f28c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*SsuoQdBmK8PPuth0tOWp6w.png"/></div></div></figure><h1 id="ced4" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">将相关的预测概率转换成数据帧</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="3bfa" class="lk in hi lg b fi ll lm l ln lo">newdf = pd.DataFrame(predictions,columns=['Negative_0','Neutral_1','Positive_2'])<br/>new_df.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/f39bcdd224c930b56e1a934f26c3e652.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*1AN3RRIh0pPSPuhV7iSF0g.png"/></div></figure><h1 id="421f" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">定义函数以格式化预测标签</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="dde9" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">def</strong> labels(x):<br/>  <strong class="lg hj">if</strong> x == 0:<br/>    <strong class="lg hj">return</strong> 'Negative_0'<br/>  <strong class="lg hj">elif</strong> x == 1:<br/>    <strong class="lg hj">return</strong> 'Neutral_1'<br/>  <strong class="lg hj">else</strong>:<br/>    <strong class="lg hj">return</strong> 'Positive_2'<br/><br/>results = np.argmax(predictions,axis=1)<br/>test['Sentiment'] = results<br/>test['Sentiment'] = test['Sentiment'].map(labels)<br/>test.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es mk"><img src="../Images/b92fe0979801d8754e6ee352df42382a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43vJsctUofBSnrSR2ovDgw.png"/></div></div></figure><h1 id="4617" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">想象一下预测</h1><pre class="kx ky kz la fd lf lg lh li aw lj bi"><span id="6500" class="lk in hi lg b fi ll lm l ln lo"><strong class="lg hj">import</strong> <strong class="lg hj">seaborn</strong> <strong class="lg hj">as</strong> <strong class="lg hj">sns</strong><br/>sns.countplot(x='Sentiment',data=test)</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/ae1c89719f8204887a85e357872e75a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*c4n82rpzHMSHpgjhsdR3Ew.png"/></div></figure><h1 id="7105" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">结论</h1><ul class=""><li id="4b73" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">在这里，我们已经在自定义数据集上使用 huggingface transformers 库训练了 BERT 模型</li><li id="572c" class="ki kj hi jm b jn kr jr ks jv kt jz ku kd kv kh kn ko kp kq bi translated">同样，我们也可以使用其他型号的变压器，如带有 GPT 2 的 GPT-2 变压器、带有 distilt for sequence classification 的 DistilBert 变压器等。</li></ul><h1 id="9649" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">推荐人:</h1><div class="mm mn ez fb mo mp"><a href="https://huggingface.co/transformers/training.html" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">微调预训练模型</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">PyTorch 和 TensorFlow 2.0 的最先进的自然语言处理。变形金刚提供了成千上万的…</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">huggingface.co</p></div></div><div class="my l"><div class="mz l na nb nc my nd ik mp"/></div></div></a></div><h2 id="0b87" class="lk in hi bd io ne nf ng is nh ni nj iw jv nk nl ja jz nm nn je kd no np ji nq bi translated"><a class="ae nr" href="https://machinehack.com/practice/sentiment_analysis_with_rnn/overview" rel="noopener ugc nofollow" target="_blank">数据</a></h2><p id="e5a6" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><a class="ae nr" href="https://www.linkedin.com/in/plaban-nayak-a9433a25/" rel="noopener ugc nofollow" target="_blank">联系我</a></p></div></div>    
</body>
</html>