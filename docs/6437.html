<html>
<head>
<title>Easy Guide to Transformer Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器型号简易指南</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/easy-guide-to-transformer-models-6b15c103bfcf?source=collection_archive---------1-----------------------#2022-02-24">https://medium.com/nerd-for-tech/easy-guide-to-transformer-models-6b15c103bfcf?source=collection_archive---------1-----------------------#2022-02-24</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><figure class="ev ex im in io ip er es paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="er es il"><img src="../Images/1a4073baa50162c3c4fc4384f9b7528f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lWeMU99YSCE4BPEPotLg_A.jpeg"/></div></div><figcaption class="iw ix et er es iy iz bd b be z dx translated">阿瑟尼·托古列夫在<a class="ae ja" href="https://unsplash.com/s/photos/transformer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="dd32" class="pw-post-body-paragraph jb jc ho jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy hh bi translated">在今天的机器学习文献中，没有办法绕过论文“注意力是你所需要的一切”的变压器模型(<a class="ae ja" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani et al. (2017) </a>)。尤其是在自然语言处理领域，本文首次描述的变换器模型(如<a class="ae ja" href="https://openai.com/blog/gpt-2-1-5b-release/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>或<a class="ae ja" href="https://openai.com/blog/gpt-2-1-5b-release/" rel="noopener ugc nofollow" target="_blank">伯特</a>)是必不可少的。在本文中，我们希望解释这篇被大量引用的论文的关键点，并展示由此产生的创新。</p></div></div>    
</body>
</html>