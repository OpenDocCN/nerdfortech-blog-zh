<html>
<head>
<title>Understanding RNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解RNN</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/understanding-rnn-91d548c86ac9?source=collection_archive---------6-----------------------#2021-02-04">https://medium.com/nerd-for-tech/understanding-rnn-91d548c86ac9?source=collection_archive---------6-----------------------#2021-02-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9185" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人类可以开始思考任何话题，但它总是建立在对话题的理解上。例如，如果你正在看一部电影，如果你想关联电影的每一个场景，你应该知道前一个场景中显示的上下文，这是你可以关联或理解整部电影的方法。</p><p id="9388" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于传统的神经网络可以理解任何时候的数据和数据的相关性，但它不能记住过去数据的上下文。这是传统神经网络的主要缺点。同样，当我们处理数据序列时，传统的神经网络无法解决这个问题。求解RNN是在1986年David Rumelhart工作的基础上引入的。霍普菲尔德网络——一种特殊的<strong class="ih hj">RNN</strong>——是约翰·霍普菲尔德在1982年发现的。1993年，一个神经历史压缩器系统解决了一个“非常深度学习”的任务，该任务需要在一个及时展开的<strong class="ih hj"> RNN </strong>中超过1000个后续层。</p><p id="adfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是递归神经网络？</strong></p><p id="d434" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN是一种使用具有内部存储器的时间前馈神经网络的架构。内存意味着每个节点的输出依赖于过去的计算。最后，在产生输出时，它使用当前输入处的输入以及从先前输入产生的输出。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/aa9883f2b8cac17cd6de407b0992c72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uU9Hd69sCK6hELOl"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">这张图片取自<a class="ae jt" href="https://stanford.edu/" rel="noopener ugc nofollow" target="_blank">https://stanford.edu</a></figcaption></figure><p id="eb5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在该图中，我们可以看到输入X1被采用，它产生输出y1。在下一阶段，输出y1被作为具有X2的输入，以产生输出y2。类似地，遵循这个过程，并且通过这种方式记住数据的上下文。现在，进入上面提到的网络中的网络部分。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ju"><img src="../Images/c9e600c7a142946da955853c910b2290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XnRgd3Wjge5OTcmt"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">这张图片取自<a class="ae jt" href="https://stanford.edu/" rel="noopener ugc nofollow" target="_blank">https://stanford.edu</a></figcaption></figure><p id="7708" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<em class="jv"> Wax </em>、<em class="jv">魏如萱</em>、<em class="jv"> Wya </em>、<em class="jv"> ba </em>、<em class="jv"> by </em>为暂时共享的权重和偏差，g1、g2为激活函数。</p><p id="f96f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个时间步长<em class="jv"> t </em>，激活<em class="jv"> a </em> &lt; <em class="jv"> t </em> &gt;和输出<em class="jv"> y </em> &lt; <em class="jv"> t </em> &gt;表示如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jw"><img src="../Images/05a9ef6a026d68c61e121a34839e5bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YCgj4r_OMkcqr8Z4xrf5rQ.png"/></div></div></figure><p id="8561" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<t-1>最初将为0。</t-1></p><p id="5c06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，一旦数据通过正向传播，我们就得到输出y <t>。在前向路径中不更新权重，因此为了在梯度下降中达到全局最小值，我们必须更新权重。一旦计算出y和y &lt; t &gt;之间的损耗，这可以在<strong class="ih hj">反向传播</strong>中完成。遵循相同的过程，并且通过这种方式学习数据序列。</t></p><p id="5cd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在递归神经网络的情况下，损失函数L按照下面的公式计算。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/2b319c5ebb682a47bfdc68e4aa079783.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*qlJF21ZSkD3v5zq_Y0DRvA.png"/></div></figure><p id="5a95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">反向传播</strong>中，损失L相对于权重矩阵<em class="jv"> W </em>的导数计算如下</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jy"><img src="../Images/86f588b8b330cd88749a05d8f0c72cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*3cIEus-N7aSA0_6BUbWnrA.png"/></div></figure><p id="8d4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">RNN的应用</strong></p><p id="a31a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去的几年中，将RNNs应用于各种问题取得了令人难以置信的成功。例如:语音识别、语言建模、翻译、图像字幕。</p><p id="f0b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与RNN的问题</p><p id="3f9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，RNN研究的是随时间推移的序列信息。例如，如果我们正在处理像“球在篮子里”这样的陈述。在这个语句中，我们可以看到，下一个词篮可以根据之前的内容进行预测。但如果我们处理类似“我擅长板球，这就是我入选球队的原因”这样的上下文。这里所选的词与好相关，两个词之间的距离很大。在这种情况下，RNN会因为<strong class="ih hj">消失梯度</strong>或<strong class="ih hj">爆炸梯度</strong>而失败。</p><p id="9306" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如在反向传播中，当数据从不同的隐藏层传递时，我们在神经元中使用了类似sigmoid或relu的激活函数。现在，在反向传播中计算的导数，对于sigmoid激活函数，在0–1之间。当它向第一层发展时，由于导数计算，它将首先接近于0(可以忽略不计)。这就是所谓的<strong class="ih hj">消失梯度</strong>问题。同样，如果我们使用relu激活函数，将会产生<strong class="ih hj">爆炸梯度</strong>问题。</p><p id="e817" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN的第二个问题是它不能为当前状态考虑未来的输入。此外，它的计算速度非常慢。</p><p id="cdb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在LSTM，双向LSTM，GRU的建筑中解决这些问题，这将在另一篇文章中解释。</p></div></div>    
</body>
</html>