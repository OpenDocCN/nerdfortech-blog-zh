<html>
<head>
<title>Dissecting Optimization Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">剖析优化算法</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/dissecting-optimization-algorithms-96c137f824a0?source=collection_archive---------25-----------------------#2021-05-20">https://medium.com/nerd-for-tech/dissecting-optimization-algorithms-96c137f824a0?source=collection_archive---------25-----------------------#2021-05-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b0b6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">学习算法幕后的一瞥。</h2></div><blockquote class="ix iy iz"><p id="028f" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">你有没有想过“过去十年发生了什么，让深度学习如此成功？”以及“为什么现在有如此多的炒作？”。</p></blockquote><p id="91dc" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">如果你看看深度学习的时间线，<a class="ae ka" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>和<a class="ae ka" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>的想法已经在1989-1991年左右的神经网络背景下讨论过，如果你看看它，反向传播只不过是应用链式法则的<a class="ae ka" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>和<a class="ae ka" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>本身可以追溯到1847年。鉴于所有这些都不是真正的新东西，它们都至少有30年的历史了。那么，“为什么在过去的十年里，突然有这么多关于深度学习的炒作和谈论？”</p><p id="b763" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">嗯，这种增长趋势背后的答案之一是提出更好的学习算法，这基本上是优化我们深度学习模型的训练。这里，有两个问题需要解决:</p><blockquote class="ix iy iz"><p id="2b76" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">你如何计算梯度？</p><p id="ed27" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">你如何使用渐变？</p></blockquote><p id="5b77" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">前者的答案是，</p><ul class=""><li id="897f" class="kb kc hi jd b je jf jh ji jx kd jy ke jz kf jw kg kh ki kj bi translated">一批</li><li id="bd5b" class="kb kc hi jd b je kk jh kl jx km jy kn jz ko jw kg kh ki kj bi translated">随机的</li><li id="eab1" class="kb kc hi jd b je kk jh kl jx km jy kn jz ko jw kg kh ki kj bi translated">小批量</li></ul><p id="3b73" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">而后者是通过，</p><ul class=""><li id="1a91" class="kb kc hi jd b je jf jh ji jx kd jy ke jz kf jw kg kh ki kj bi translated">香草梯度下降</li><li id="78ef" class="kb kc hi jd b je kk jh kl jx km jy kn jz ko jw kg kh ki kj bi translated">基于动量的梯度下降</li><li id="3b80" class="kb kc hi jd b je kk jh kl jx km jy kn jz ko jw kg kh ki kj bi translated">内斯特罗夫加速梯度下降</li><li id="d86f" class="kb kc hi jd b je kk jh kl jx km jy kn jz ko jw kg kh ki kj bi translated">阿达格拉德</li><li id="4193" class="kb kc hi jd b je kk jh kl jx km jy kn jz ko jw kg kh ki kj bi translated">RMSProp</li><li id="8e97" class="kb kc hi jd b je kk jh kl jx km jy kn jz ko jw kg kh ki kj bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li></ul><p id="beb7" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><em class="jc">现在，让我们更深入地了解这些学习算法。这里，我们正在考虑具有两个隐藏层的前馈神经网络的矢量化实现。</em></p><ol class=""><li id="7e5d" class="kb kc hi jd b je jf jh ji jx kd jy ke jz kf jw kp kh ki kj bi translated">香草梯度下降:</li></ol><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="a304" class="kz la hi kv b fi lb lc l ld le">if algo == 'GD':</span><span id="7c27" class="kz la hi kv b fi lf lc l ld le">self.grad(X, Y)</span><span id="9ffa" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="e7c8" class="kz la hi kv b fi lf lc l ld le">self.params["W"+str(i)] -= eta * (self.gradients["dW"+str(i)]/m)</span><span id="0c99" class="kz la hi kv b fi lf lc l ld le">self.params["B"+str(i)] -= eta * (self.gradients["dB"+str(i)]/m</span><span id="fc52" class="kz la hi kv b fi lf lc l ld le">```</span><span id="5f83" class="kz la hi kv b fi lf lc l ld le">%%time</span><span id="e750" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="3a1a" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=1, algo="GD", display_loss=True)</span><span id="17e2" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es lg"><img src="../Images/fc6b869404e57651c37f34e1379606c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*-CxZpuiOh-yWe4VGnLcCcw.png"/></div></figure><p id="a80f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">普通梯度下降算法给我们的精度是0.716，这不是最佳的。</p><p id="62e5" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">2.小批量梯度下降:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="dfde" class="kz la hi kv b fi lb lc l ld le">elif algo == "MiniBatch":</span><span id="17cd" class="kz la hi kv b fi lf lc l ld le">for k in range(0, m, mini_batch_size):</span><span id="f210" class="kz la hi kv b fi lf lc l ld le">self.grad(X[k:k+mini_batch_size], Y[k:k+mini_batch_size])</span><span id="191b" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="82db" class="kz la hi kv b fi lf lc l ld le">self.params["W"+str(i)] -= eta * (self.gradients["dW"+str(i)]/mini_batch_size)</span><span id="987e" class="kz la hi kv b fi lf lc l ld le">self.params["B"+str(i)] -= eta * (self.gradients["dB"+str(i)]/mini_batch_size)</span><span id="7b39" class="kz la hi kv b fi lf lc l ld le">```</span><span id="5d02" class="kz la hi kv b fi lf lc l ld le">%%time</span><span id="9675" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="3b2c" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=1, algo="MiniBatch", mini_batch_size=128, display_loss=True)</span><span id="1288" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es lk"><img src="../Images/3f78158bb97d6d8ea893e97c92731057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*KQ-o1SBF7s5Pgy4o_Idnnw.png"/></div></figure><p id="59c4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">正如你可以观察到的一样，对于相同数量的时期和学习率，Mini Batch给了我们0.90的准确率。获得的两个精度之间的巨大差异是因为普通梯度下降遍历所有数据值，然后进行一次更新，但在Mini Batch中，它查看128个值，然后进行更新。因此，实际上，如果我们有100个时期，模型将遍历数据100次，但会进行几次更新。此外，这些点彼此相距较远，因为在每个小批量之后进行的更新(即梯度更新)非常大，导致损耗下降较大。然而，在普通梯度下降中，当你查看整个数据时，数据值被平均化，从而进行较小的更新。</p><p id="085b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">那么，当我们进一步减少小批量时会发生什么呢？</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="8d2e" class="kz la hi kv b fi lb lc l ld le">%%time</span><span id="7a17" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="9ef2" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=1, algo="MiniBatch", mini_batch_size=8, display_loss=True)</span><span id="fbfa" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es ll"><img src="../Images/4d1fd70bf585f0d233705c1a1fc0d5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*Gch3dAlQITYoCzYU65kLKA.png"/></div></figure><p id="b939" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这里，有更多的振荡。这是批量较小的危险之一。获得的梯度下降将是非常嘈杂的样本。</p><p id="34a4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">3.基于动量的梯度下降；</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="b371" class="kz la hi kv b fi lb lc l ld le">elif algo == "Momentum":</span><span id="f0cb" class="kz la hi kv b fi lf lc l ld le">self.grad(X, Y)</span><span id="3fd5" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="243b" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_w"+str(i)] = gamma *self.update_params["v_w"+str(i)] + eta * (self.gradients["dW"+str(i)]/m)</span><span id="2ed3" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_b"+str(i)] = gamma *self.update_params["v_b"+str(i)] + eta * (self.gradients["dB"+str(i)]/m)</span><span id="c2f7" class="kz la hi kv b fi lf lc l ld le">self.params["W"+str(i)] -= self.update_params["v_w"+str(i)]</span><span id="e89f" class="kz la hi kv b fi lf lc l ld le">self.params["B"+str(i)] -= self.update_params["v_b"+str(i)]</span><span id="c6a7" class="kz la hi kv b fi lf lc l ld le">```</span><span id="209c" class="kz la hi kv b fi lf lc l ld le">%%time</span><span id="e22d" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="1f5e" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=1, algo="Momentum", gamma=0.5, display_loss=True)</span><span id="3848" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es lm"><img src="../Images/3e6bbc2ad29a5fbd3e73936bf005973e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*JxWUmf1w0oCQkkXBJXOkOQ.png"/></div></figure><blockquote class="ix iy iz"><p id="abe7" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">在基于动量的算法中，术语“伽马”控制算法的加速。</p></blockquote><p id="165f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">正如你所观察到的，算法进行得很顺利，但是做得还不够。那么，如果我们把动量项“γ”增加到0.99呢？</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="f199" class="kz la hi kv b fi lb lc l ld le">%%time</span><span id="3b2d" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="0247" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=1, algo="Momentum", gamma=0.99, display_loss=True)</span><span id="3832" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/c16587c4acff2575885c264f77573008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*nnysVo8wreo3c8gxv0NGdA.png"/></div></div></figure><p id="9fc6" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在这里，算法只是滑向错误的方向，而且太吵了。为了解决这个问题，我们使用了内斯特罗夫加速梯度下降法，它可以承受非常大的动量项。</p><p id="ebf5" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">4.内斯特罗夫加速梯度下降；</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="1b7e" class="kz la hi kv b fi lb lc l ld le">elif algo == "NAG":</span><span id="dba4" class="kz la hi kv b fi lf lc l ld le">temp_params = {}</span><span id="35c7" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="31e8" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_w"+str(i)] = gamma * self.prev_update_params["v_w"+str(i)]</span><span id="e9ee" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_b"+str(i)] = gamma * self.prev_update_params["v_b"+str(i)]</span><span id="faa3" class="kz la hi kv b fi lf lc l ld le">temp_params["W"+str(i)] = self.params["W"+str(i)] - self.update_params["v_w"+str(i)]</span><span id="6ace" class="kz la hi kv b fi lf lc l ld le">temp_params["B"+str(i)] = self.params["B"+str(i)] - self.update_params["v_b"+str(i)]</span><span id="4ab4" class="kz la hi kv b fi lf lc l ld le">self.grad(X, Y, temp_params)</span><span id="0ab6" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="21c0" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_w"+str(i)] = gamma * self.update_params["v_w"+str(i)] + eta * (self.gradients["dW"+str(i)]/m)</span><span id="1113" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_b"+str(i)] = gamma * self.update_params["v_b"+str(i)] + eta * (self.gradients["dB"+str(i)]/m)</span><span id="4795" class="kz la hi kv b fi lf lc l ld le">self.params["W"+str(i)] -= eta * (self.update_params["v_w"+str(i)])</span><span id="b9d0" class="kz la hi kv b fi lf lc l ld le">self.params["B"+str(i)] -= eta * (self.update_params["v_b"+str(i)])</span><span id="199a" class="kz la hi kv b fi lf lc l ld le">self.prev_update_params = self.update_params</span><span id="4283" class="kz la hi kv b fi lf lc l ld le">```</span><span id="d487" class="kz la hi kv b fi lf lc l ld le">%%time</span><span id="6743" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="e8ed" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=1, algo="NAG", gamma=0.9, display_loss=True)</span><span id="ec8d" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ls"><img src="../Images/b4df5085fc51ff24b42b1fbc225134b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*tvP7NhFJZr3sBq-iA6ErCA.png"/></div></div></figure><p id="599a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">NAG给我们的准确率是0.92！这是迄今为止我们观察到的最好结果。不过，当势头良好时，唠叨也没什么好处。</p><p id="686a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">5.阿达格拉德:</p><p id="3437" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在AdaGrad(自适应梯度)中，我们按照参数更新历史的比例衰减参数的学习速率(eta)。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="0c1a" class="kz la hi kv b fi lb lc l ld le">elif algo == "AdaGrad":</span><span id="72ed" class="kz la hi kv b fi lf lc l ld le">self.grad(X, Y)</span><span id="ab14" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="06bf" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_w"+str(i)] += (self.gradients["dW"+str(i)]/m)**2</span><span id="0688" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_b"+str(i)] += (self.gradients["dB"+str(i)]/m)**2</span><span id="e22b" class="kz la hi kv b fi lf lc l ld le">self.params["W"+str(i)] -= (eta/(np.sqrt(self.update_params["v_w"+str(i)])+eps)) * (self.gradients["dW"+str(i)]/m)</span><span id="0059" class="kz la hi kv b fi lf lc l ld le">self.params["B"+str(i)] -= (eta/(np.sqrt(self.update_params["v_b"+str(i)])+eps)) * (self.gradients["dB"+str(i)]/m)</span><span id="9783" class="kz la hi kv b fi lf lc l ld le">```</span><span id="d797" class="kz la hi kv b fi lf lc l ld le">%%time</span><span id="bfb6" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="c64f" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=.1, algo="AdaGrad", display_loss=True)</span><span id="6415" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es lt"><img src="../Images/468a26034d0f42f7879479613020bb8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*uYQ1JP3ztd23tG2sbwgQyQ.png"/></div></figure><p id="0450" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">正如你所观察到的，最初算法开始时选择了一个不好的初始点，但它在20次迭代内很快就自我修正了。</p><p id="9ea1" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">使用AdaGrad的优点是对应于稀疏特征的参数得到更好的更新，但缺点是随着有效学习率分母的增长，学习率急剧下降。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="18be" class="kz la hi kv b fi lb lc l ld le">%%time</span><span id="83f2" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="f9a9" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=.1, algo="AdaGrad", display_loss=True)</span><span id="152b" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es ls"><img src="../Images/e64aa71203272fbb3f558578ab3d1899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*matrFwp_KVfo_0jYN9REdg.png"/></div></figure><p id="fb19" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">正如你所观察到的，AdaGrad在学习率很低的情况下无法工作，因为它给出了0.63的准确度分数。为了解决这个问题，使用了RMSProp。</p><p id="b12a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">6.RMSProp:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="99d6" class="kz la hi kv b fi lb lc l ld le">elif algo == "RMSProp":</span><span id="aab2" class="kz la hi kv b fi lf lc l ld le">self.grad(X, Y)</span><span id="669b" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="53e2" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_w"+str(i)] = beta*self.update_params["v_w"+str(i)] +(1 - beta)*((self.gradients["dW"+str(i)]/m)**2)</span><span id="6fb6" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_b"+str(i)] = beta*self.update_params["v_b"+str(i)] +(1 - beta)*((self.gradients["dB"+str(i)]/m)**2)</span><span id="eb85" class="kz la hi kv b fi lf lc l ld le">self.params["W"+str(i)] -= (eta/(np.sqrt(self.update_params["v_w"+str(i)]+eps)))*(self.gradients["dW"+str(i)]/m)</span><span id="3573" class="kz la hi kv b fi lf lc l ld le">self.params["B"+str(i)] -= (eta/(np.sqrt(self.update_params["v_b"+str(i)]+eps)))*(self.gradients["dB"+str(i)]/m)</span><span id="13f7" class="kz la hi kv b fi lf lc l ld le">```</span><span id="a386" class="kz la hi kv b fi lf lc l ld le">%%time</span><span id="c432" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="7219" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=.1, algo="RMSProp", beta=0.9, display_loss=True)</span><span id="2bee" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es lu"><img src="../Images/a5ad8af96f6a5f9d59dc28e292d85987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*sUc6pr1jbFxH5l5BwmXMjw.png"/></div></figure><p id="76e6" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这里，具有较小学习速率的问题被反驳，但是如果我们增加eta的值，它将开始振荡，因为没有一阶项或动量项来抵消振荡。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="e4c8" class="kz la hi kv b fi lb lc l ld le">%%time</span><span id="2d66" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="48c2" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=.9, algo="RMSProp", beta=0.9, display_loss=True)</span><span id="01dc" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es ls"><img src="../Images/040af8107f9878f196730033de339937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*EWgijhCM6Q5jRMSivMdyIA.png"/></div></figure><p id="45d4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">7.亚当:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="b012" class="kz la hi kv b fi lb lc l ld le">elif algo == "Adam":</span><span id="ddcf" class="kz la hi kv b fi lf lc l ld le">self.grad(X, Y)</span><span id="f77c" class="kz la hi kv b fi lf lc l ld le">num_updates=0</span><span id="19d9" class="kz la hi kv b fi lf lc l ld le">for i in range(1, self.num_layers+1):</span><span id="73cb" class="kz la hi kv b fi lf lc l ld le">num_updates += 1</span><span id="e121" class="kz la hi kv b fi lf lc l ld le">self.update_params["m_w"+str(i)] = beta1*self.update_params["m_w"+str(i)] + (1-beta1) * (self.gradients["dW"+str(i)]/m)</span><span id="4e18" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_w"+str(i)] = beta2*self.update_params["v_w"+str(i)] + (1-beta2) * ((self.gradients["dW"+str(i)]/m)**2)</span><span id="4704" class="kz la hi kv b fi lf lc l ld le">m_w_hat = self.update_params["m_w"+str(i)]/(1-np.power(beta1, num_updates))</span><span id="5d89" class="kz la hi kv b fi lf lc l ld le">v_w_hat = self.update_params["v_w"+str(i)]/(1-np.power(beta2, num_updates))</span><span id="ed4b" class="kz la hi kv b fi lf lc l ld le">self.params["W"+str(i)] -= (eta/np.sqrt(v_w_hat+eps))*m_w_hat</span><span id="59e2" class="kz la hi kv b fi lf lc l ld le">self.update_params["m_b"+str(i)] = beta1*self.update_params["m_b"+str(i)] + (1-beta1) * (self.gradients["dB"+str(i)]/m)</span><span id="5af4" class="kz la hi kv b fi lf lc l ld le">self.update_params["v_b"+str(i)] = beta2*self.update_params["v_b"+str(i)] + (1-beta2) * ((self.gradients["dB"+str(i)]/m)**2)</span><span id="9d76" class="kz la hi kv b fi lf lc l ld le">m_b_hat = self.update_params["m_b"+str(i)]/(1-np.power(beta1, num_updates))</span><span id="100b" class="kz la hi kv b fi lf lc l ld le">v_b_hat = self.update_params["v_b"+str(i)]/(1-np.power(beta2, num_updates))</span><span id="f8e4" class="kz la hi kv b fi lf lc l ld le">self.params["B"+str(i)] -= (eta/np.sqrt(v_b_hat+eps))*m_b_hat</span><span id="b6a6" class="kz la hi kv b fi lf lc l ld le">```</span><span id="1d19" class="kz la hi kv b fi lf lc l ld le">%%time</span><span id="fdc3" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="9065" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=.9, algo="Adam", beta=0.9, display_loss=True)</span><span id="37fe" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lv"><img src="../Images/5f161c17375e5ca4411a9c48a15bd6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*7tjmbbnvJzPiMc1g4SlouQ.png"/></div></div></figure><p id="db1b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">AdaGrad和RMSProp的所有局限性都被Adam反制了。Adam是深度学习中应用最广泛的学习算法。唯一的缺点是它比较慢，因为涉及到更多的计算。原因是它确实进行了计算量很大的偏差校正。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="8104" class="kz la hi kv b fi lb lc l ld le">%%time</span><span id="22e4" class="kz la hi kv b fi lf lc l ld le">model = FFSN_Vectorised(W1, W2)</span><span id="954f" class="kz la hi kv b fi lf lc l ld le">model.fit(X_train, y_OH_train, epochs=100, eta=.1, algo="Adam", beta=0.9, display_loss=True)</span><span id="f9d5" class="kz la hi kv b fi lf lc l ld le">print_accuracy()</span></pre><figure class="kq kr ks kt fd lh er es paragraph-image"><div class="er es lw"><img src="../Images/8769a059e44bfef90f8e71e02b9ba036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*6QBzecvVgJUgZZQmQcS4aw.png"/></div></figure><p id="d4de" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><em class="jc">鳍</em></p></div></div>    
</body>
</html>