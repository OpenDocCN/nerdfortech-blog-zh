<html>
<head>
<title>“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” Paper Summary &amp; Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“一幅图像值 16x16 个字:大规模图像识别的变形金刚”论文摘要和分析</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-paper-summary-3a387e71880a?source=collection_archive---------10-----------------------#2021-03-15">https://medium.com/nerd-for-tech/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-paper-summary-3a387e71880a?source=collection_archive---------10-----------------------#2021-03-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3fc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文:<a class="ae jd" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.11929.pdf</a></p><p id="9547" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">讨论由<a class="ae jd" href="https://github.com/VictorButoi" rel="noopener ugc nofollow" target="_blank">Victor Butoi</a>&amp;<a class="ae jd" href="https://github.com/cjw322" rel="noopener ugc nofollow" target="_blank">Cora Wu</a>主持，智能系统子团队</p><h1 id="eb03" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">文件的目标</h1><p id="cc40" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated"><strong class="ih hj">论文在攻克什么问题？<br/> </strong> <em class="kh">大规模图像识别</em>试图解决将 Transformer 架构应用于计算机视觉任务的问题，以减轻该领域对 CNN 的严重依赖。该论文认为，这种转变将产生与传统 CNN 相当的结果，同时需要较少的计算资源来训练。</p><p id="9523" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">这个问题的相关背景是什么？<br/> </strong>变压器已经广泛用于 NLP 任务，例如当前最先进的模型伯特、GPT 及其变体。在图像任务中使用变形金刚也有一些其他的工作，但是它们通常是非常昂贵的。</p><h1 id="55c5" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">论文投稿</h1><p id="42ef" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">论文提出了什么方法来解决这个问题？<br/> 为了调整图像输入以适应变换器的输入，本文将 2D 图像整形为一系列扁平的 2D 面片。一个可学习的嵌入被预先添加到嵌入的补丁序列中。这个标记的作用与 BERT 的[class]标记相似。然后将位置嵌入添加到补片嵌入中，以保留位置信息。</p><p id="5586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">变压器编码器由多头自关注和 MLP 块的交替层组成。变换器编码器的输出状态用作图像表示。在预训练和微调期间，一个分类头，MLP，被连接到变压器编码器的输出。在预训练期间，MLP 有一个隐藏层，而在微调期间，它是用单层实现的。</p><p id="8de7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Vision Transformer (ViT)在大型数据集上进行了预训练，然后针对较小的下游任务进行了微调。通过移除预训练的预测头并用零初始化的前馈层代替它来进行微调。</p><p id="6975" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">论文的投稿与之前的相关作品有何不同？<br/> </strong>这并不是第一篇将变形金刚应用于 CV 的论文。脸书其实已经发布了一款<a class="ae jd" href="https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/" rel="noopener ugc nofollow" target="_blank"> DETR(检测变形金刚)</a>；然而，它们与 CNN 一起使用，而不是单独使用。本文是 CV 独立变压器的成功应用。对于每个主要的贡献，它有如下不同:</p><ul class=""><li id="6292" class="ki kj hi ih b ii ij im in iq kk iu kl iy km jc kn ko kp kq bi translated"><strong class="ih hj">计算时间更少的准确性</strong>:与<a class="ae jd" href="https://arxiv.org/abs/1911.04252v4" rel="noopener ugc nofollow" target="_blank">吵闹的学生</a>相比，ViT 将训练时间减少了约 5 倍(训练时间的 20%)(尽管它达到了与表 2 所示大致相同的准确性)。</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kr"><img src="../Images/904fca6bb6e1efb0cf32ff59d2994bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TWNtr9Vo_9stFWZNk32akQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated"><a class="ae jd" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.11929.pdf</a></figcaption></figure><ul class=""><li id="0959" class="ki kj hi ih b ii ij im in iq kk iu kl iy km jc kn ko kp kq bi translated"><strong class="ih hj">无卷积</strong>:理论上，MLP 比 CNN 模型表现更好。然而，数据一直是影响 MLP 模型性能的一大障碍。由 CNN 强加的感应偏倚极大地推进了 CV 领域，并且通过作者使用的大数据集，他们能够克服对感应偏倚的需要。变压器与传统的 MLP 略有不同，其核心机制是<strong class="ih hj">自我关注</strong>。这使得变压器能够理解输入之间的关系。当在 NLP 中使用时，它以双向方式计算单词之间的关系，这意味着顺序不像单向 RNN 那样严格。</li><li id="ab03" class="ki kj hi ih b ii lh im li iq lj iu lk iy ll jc kn ko kp kq bi translated"><strong class="ih hj">变压器的功效</strong>:该论文通过观察注意力头的输出来分析 ViT 的内部表现(类似于 BERTology 论文)。论文发现，该模型可以使用位置嵌入对不同斑块之间的距离进行编码。该论文还发现，ViT 甚至在较低的层内整合了来自整个图像的信息，并陈述如下:“我们发现一些头部关注已经在最低层中的大部分图像，这表明该模型确实使用了全局整合信息的能力。”此外，他们还对模型性能进行了定量分析，并对模型的注意力地图和焦点进行了定性可视化。</li></ul><p id="ead8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文是如何评估其结果的？<br/> 提出的方法在三个不同的数据集上进行:Imagenet (1k 类和 21k 类)、JFT (18k 类)和 VTAB。通过少量拍摄或微调精度来测量结果，微调精度表示在数据集上微调模型后的精度，少量拍摄精度表示在训练和评估图像子集后的精度。</p><p id="7a0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们将 transformer 模型与流行的图像分类基准进行了比较，如 Big Transfer 和 Noisy Student。在这篇论文中，他们在 BERT 的基础上配置了 ViT，并通过用组规范化替换批量规范化以及采用标准化卷积来改进迁移学习来修改 Resnet。</p><p id="397c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，本文对自监督训练 ViT 进行了初步研究，结果表明，通过自监督预训练，与从头训练相比，准确率提高了 2%。</p><h1 id="71d4" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">论文限制、进一步研究和/或潜在应用</h1><p id="62f3" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">这篇论文介绍了 ViT:使用视觉转换器，而不是 CNN 或混合方法来完成图像任务。结果很有希望，但还不完整，因为除了分类之外，基于视觉的任务(如检测和分割)的性能还不存在。此外，与 Vaswani 等人(2017 年)不同，与 CNN 相比，变压器的性能改善更加有限。作者假设，进一步的预训练可以提高性能，因为与其他先进模型相比，ViT 是相对可扩展的。</p><p id="92d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再者，<a class="ae jd" href="https://arxiv.org/pdf/2001.08361.pdf" rel="noopener ugc nofollow" target="_blank">卡普兰等人。艾尔。</a>与 NLP 中的 LSTMs 相比，主要为转换器提供缩放定律，证明转换器可以缩放到更大的数据集。与 CNN 相比，看看变压器是否表现出类似的特性将是有趣的。如果是这样的话，那么很明显，基于变压器的技术也将成为 CV 中的 SOTA。</p><p id="40a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终，这些结果表明，变形金刚有可能成为一种通用模型，能够在广泛的人工任务领域进行学习，并享受以超大规模扩展数据的能力。这个愿景还没有到来，可能永远不会到来；如果是这样，这篇论文将被认为是未来的先兆。</p></div></div>    
</body>
</html>