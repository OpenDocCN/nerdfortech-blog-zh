<html>
<head>
<title>“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” Paper Summary &amp; Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“一幅图像值16x16个字:大规模图像识别的变形金刚”论文摘要和分析</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-paper-summary-3a387e71880a?source=collection_archive---------10-----------------------#2021-03-15">https://medium.com/nerd-for-tech/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-paper-summary-3a387e71880a?source=collection_archive---------10-----------------------#2021-03-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3fc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文:<a class="ae jd" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.11929.pdf</a></p><p id="9547" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">讨论由<a class="ae jd" href="https://github.com/VictorButoi" rel="noopener ugc nofollow" target="_blank">Victor Butoi</a>&amp;<a class="ae jd" href="https://github.com/cjw322" rel="noopener ugc nofollow" target="_blank">Cora Wu</a>主持，智能系统子团队</p><h1 id="eb03" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">文件的目标</h1><p id="cc40" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated"><strong class="ih hj">论文在攻克什么问题？<br/> </strong> <em class="kh">大规模图像识别</em>试图解决将Transformer架构应用于计算机视觉任务的问题，以减轻该领域对CNN的严重依赖。该论文认为，这种转变将产生与传统CNN相当的结果，同时需要较少的计算资源来训练。</p><p id="9523" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">这个问题的相关背景是什么？<br/> </strong>变压器已经广泛用于NLP任务，例如当前最先进的模型伯特、GPT及其变体。在图像任务中使用变形金刚也有一些其他的工作，但是它们通常是非常昂贵的。</p><h1 id="55c5" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">论文投稿</h1><p id="42ef" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">论文提出了什么方法来解决这个问题？<br/> 为了调整图像输入以适应变换器的输入，本文将2D图像整形为一系列扁平的2D面片。一个可学习的嵌入被预先添加到嵌入的补丁序列中。这个标记的作用与BERT的[class]标记相似。然后将位置嵌入添加到补片嵌入中，以保留位置信息。</p><p id="5586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">变压器编码器由多头自关注和MLP块的交替层组成。变换器编码器的输出状态用作图像表示。在预训练和微调期间，一个分类头，MLP，被连接到变压器编码器的输出。在预训练期间，MLP有一个隐藏层，而在微调期间，它是用单层实现的。</p><p id="8de7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Vision Transformer (ViT)在大型数据集上进行了预训练，然后针对较小的下游任务进行了微调。通过移除预训练的预测头并用零初始化的前馈层代替它来进行微调。</p><p id="6975" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">论文的投稿与之前的相关作品有何不同？<br/> </strong>这并不是第一篇将变形金刚应用于CV的论文。脸书其实已经发布了一款<a class="ae jd" href="https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/" rel="noopener ugc nofollow" target="_blank"> DETR(检测变形金刚)</a>；然而，它们与CNN一起使用，而不是单独使用。本文是CV独立变压器的成功应用。对于每个主要的贡献，它有如下不同:</p><ul class=""><li id="6292" class="ki kj hi ih b ii ij im in iq kk iu kl iy km jc kn ko kp kq bi translated"><strong class="ih hj">计算时间更少的准确性</strong>:与<a class="ae jd" href="https://arxiv.org/abs/1911.04252v4" rel="noopener ugc nofollow" target="_blank">吵闹的学生</a>相比，ViT将训练时间减少了约5倍(训练时间的20%)(尽管它达到了与表2所示大致相同的准确性)。</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kr"><img src="../Images/904fca6bb6e1efb0cf32ff59d2994bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TWNtr9Vo_9stFWZNk32akQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated"><a class="ae jd" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.11929.pdf</a></figcaption></figure><ul class=""><li id="0959" class="ki kj hi ih b ii ij im in iq kk iu kl iy km jc kn ko kp kq bi translated"><strong class="ih hj">无卷积</strong>:理论上，MLP比CNN模型表现更好。然而，数据一直是影响MLP模型性能的一大障碍。由CNN强加的感应偏倚极大地推进了CV领域，并且通过作者使用的大数据集，他们能够克服对感应偏倚的需要。变压器与传统的MLP略有不同，其核心机制是<strong class="ih hj">自我关注</strong>。这使得变压器能够理解输入之间的关系。当在NLP中使用时，它以双向方式计算单词之间的关系，这意味着顺序不像单向RNN那样严格。</li><li id="ab03" class="ki kj hi ih b ii lh im li iq lj iu lk iy ll jc kn ko kp kq bi translated"><strong class="ih hj">变压器的功效</strong>:该论文通过观察注意力头的输出来分析ViT的内部表现(类似于BERTology论文)。论文发现，该模型可以使用位置嵌入对不同斑块之间的距离进行编码。该论文还发现，ViT甚至在较低的层内整合了来自整个图像的信息，并陈述如下:“我们发现一些头部关注已经在最低层中的大部分图像，这表明该模型确实使用了全局整合信息的能力。”此外，他们还对模型性能进行了定量分析，并对模型的注意力地图和焦点进行了定性可视化。</li></ul><p id="ead8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文是如何评估其结果的？<br/> 提出的方法在三个不同的数据集上进行:Imagenet (1k类和21k类)、JFT (18k类)和VTAB。通过少量拍摄或微调精度来测量结果，微调精度表示在数据集上微调模型后的精度，少量拍摄精度表示在训练和评估图像子集后的精度。</p><p id="7a0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们将transformer模型与流行的图像分类基准进行了比较，如Big Transfer和Noisy Student。在这篇论文中，他们在BERT的基础上配置了ViT，并通过用组规范化替换批量规范化以及采用标准化卷积来改进迁移学习来修改Resnet。</p><p id="397c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，本文对自监督训练ViT进行了初步研究，结果表明，通过自监督预训练，与从头训练相比，准确率提高了2%。</p><h1 id="71d4" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">论文限制、进一步研究和/或潜在应用</h1><p id="62f3" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">这篇论文介绍了ViT:使用视觉转换器，而不是CNN或混合方法来完成图像任务。结果很有希望，但还不完整，因为除了分类之外，基于视觉的任务(如检测和分割)的性能还不存在。此外，与Vaswani等人(2017年)不同，与CNN相比，变压器的性能改善更加有限。作者假设，进一步的预训练可以提高性能，因为与其他先进模型相比，ViT是相对可扩展的。</p><p id="92d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再者，<a class="ae jd" href="https://arxiv.org/pdf/2001.08361.pdf" rel="noopener ugc nofollow" target="_blank">卡普兰等人。艾尔。</a>与NLP中的LSTMs相比，主要为转换器提供缩放定律，证明转换器可以缩放到更大的数据集。与CNN相比，看看变压器是否表现出类似的特性将是有趣的。如果是这样的话，那么很明显，基于变压器的技术也将成为CV中的SOTA。</p><p id="40a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终，这些结果表明，变形金刚有可能成为一种通用模型，能够在广泛的人工任务领域进行学习，并享受以超大规模扩展数据的能力。这个愿景还没有到来，可能永远不会到来；如果是这样，这篇论文将被认为是未来的先兆。</p></div></div>    
</body>
</html>