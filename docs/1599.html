<html>
<head>
<title>Review — SEFCNN: A Switchable Deep Learning Approach for In-loop Filtering in Video Coding (HEVC Filtering)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述— SEFCNN:一种用于视频编码中环路滤波的可切换深度学习方法(HEVC滤波)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-sefcnn-a-switchable-deep-learning-approach-for-in-loop-filtering-in-video-coding-hevc-ebaff10dd535?source=collection_archive---------11-----------------------#2021-03-27">https://medium.com/nerd-for-tech/review-sefcnn-a-switchable-deep-learning-approach-for-in-loop-filtering-in-video-coding-hevc-ebaff10dd535?source=collection_archive---------11-----------------------#2021-03-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="81d8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">用网络概念<a class="ae ix" href="https://towardsdatascience.com/review-vdsr-super-resolution-f8050d49362f?source=post_page---------------------------" rel="noopener" target="_blank"> VDSR </a>和<a class="ae ix" href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=post_page---------------------------" rel="noopener" target="_blank">T3】SENet</a><strong class="ak">，</strong>胜过<a class="ae ix" rel="noopener" href="/@sh.tsang/review-vrcnn-variable-filter-size-residue-learning-cnn-codec-post-processing-4a8a337ea73c"> VRCNN </a>，<a class="ae ix" rel="noopener" href="/@sh.tsang/review-rhcnn-residual-highway-convolutional-neural-network-codec-post-processing-f9b617795a61"> RHCNN </a>，和<a class="ae ix" rel="noopener" href="/@sh.tsang/review-mlsdrn-multi-channel-long-short-term-dependency-residual-network-codec-filtering-d97537ca8e45"> MLSDRN </a></h2></div><p id="c801" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">在</span>这篇论文中，<strong class="ja hj">对杭州师范大学、Visionular Inc .和长安大学提出的用于视频编码中的环路内滤波的可切换深度学习方法</strong>(SEFCNN)进行了评述。在本文中:</p><ul class=""><li id="8ca0" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">挤压激励滤波CNN</strong>，设计有两个子网:<strong class="ja hj">特征提取(FEX)子网</strong>和<strong class="ja hj">特征增强(FEN)子网</strong>。</li><li id="3531" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">针对不同类型的帧</strong>，使用FEX和分<strong class="ja hj">训练不同的模型</strong>。</li><li id="3218" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">最后，<strong class="ja hj">提出了一种自适应增强机制</strong>，该机制<strong class="ja hj">可在基于CNN的方法和传统方法</strong>之间切换。</li></ul><p id="d69d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这是<strong class="ja hj"> 2020 TCVST </strong>中的一篇论文，其中TCVST的<strong class="ja hj">高影响因子为4.133 </strong>。(<a class="kr ks ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----ebaff10dd535--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="0828" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">概述</h1><ol class=""><li id="1304" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt lx kj kk kl bi translated"><strong class="ja hj"> SEFCNN:网络架构</strong></li><li id="4bbd" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">模型训练策略</strong></li><li id="ecef" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">实验结果</strong></li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="abe4" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated"><strong class="ak"> 1。SEFCNN:网络架构</strong></h1><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es ly"><img src="../Images/70e26573050f81f23b90082656903b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJNEJce6T729y0vrpK5rxw.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc"> SEFCNN:网络架构</strong></figcaption></figure><ul class=""><li id="3742" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在H.265/HEVC中，SEFCNN是一个可选的环路滤波器。</li><li id="9973" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">SEFCNN由两个子网组成，即低层特征提取(FEX)网和高层特征增强(FEN)网。</li><li id="cbc8" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">每个子网都可以单独调用和训练。</strong></li><li id="aa09" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">长同一性跳跃连接</strong>然后被直接添加到残差以生成输出图像。</li></ul><h2 id="0088" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">1.1.子网FEX</h2><ul class=""><li id="2e42" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">该网络的灵感来自于<a class="ae ix" href="https://towardsdatascience.com/review-vdsr-super-resolution-f8050d49362f?source=post_page---------------------------" rel="noopener" target="_blank"> VDSR </a>的成功。</li><li id="d5dd" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">输入数据<strong class="ja hj">经过<em class="nc"> N </em>叠加层</strong>转换成高级特征。</li><li id="1955" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">对于每个卷积层，我们将内核大小设置为3×3，并使用64个滤波器。</li></ul><h2 id="c090" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">1.2.子网FEN</h2><ul class=""><li id="2e43" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">在FEN中，左分支用于剩余学习，右分支用作跳过连接。</li><li id="1dec" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">在左分支的开始，3个卷积层被级联以获得信道的高级特征。</li><li id="10ef" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">然后，执行源于<a class="ae ix" href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="ja hj"> SENet </strong> </a>的<strong class="ja hj">挤压激励(SE)模块</strong>，以进一步增强网络的代表性。</li><li id="e714" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">因此，它包括两个步骤，挤压和激励。</li><li id="d2ab" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">通过对输入<em class="nc"> U </em>应用全局平均池(GAP)来采用挤压步骤:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nd"><img src="../Images/3e5fc82da89559ce25799698b8613f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*esduCtkNFtWfNJ99iHuXgw.png"/></div></figure><ul class=""><li id="ee6a" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">接下来，激励过程被设计成通过调整有用通道的相应权重参数来强调有用通道。2卷积层首先被引入用于非线性映射。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ne"><img src="../Images/9c051c970e371932d4f2b3a7af7caedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*GlEptvbNoGsegTg14sLaQw.png"/></div></figure><ul class=""><li id="4d52" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">最后，输入<em class="nc"> U </em>的每个通道由<em class="nc"> sk </em>进行加权和重新校准。</li></ul><blockquote class="nf ng nh"><p id="cf05" class="iy iz nc ja b jb jc ij jd je jf im jg ni ji jj jk nj jm jn jo nk jq jr js jt hb bi translated"><strong class="ja hj">SE块可以有效整理和强化信息丰富的特征图。</strong></p></blockquote><ul class=""><li id="731d" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">(如果有兴趣，请随时访问<a class="ae ix" href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=post_page---------------------------" rel="noopener" target="_blank"> SENet </a>。)</li><li id="c4bd" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">最后，添加正确的分支:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nl"><img src="../Images/d96000ff81264c40947474ff03723669.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*JD0ejOnK_a_NlsuziH8PjQ.png"/></div></figure><ul class=""><li id="b3b4" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">其中<em class="nc"> bk </em>通过1×1卷积计算。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="2343" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated"><strong class="ak"> 2。模型训练策略</strong></h1><h2 id="c90e" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">2.1.不同QP的特定型号</h2><ul class=""><li id="4c96" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">具有较高QP的重建帧通常质量较低，并且包含更多伪像。</li><li id="0f32" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">换句话说，不同的模型被训练用于不同的QP水平。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nm"><img src="../Images/d0bad7f73b8b66e922fecd1279f29740.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*XsNVZsKS6TMXIWUXik5MKA.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">BD-率(%) </strong></figcaption></figure><ul class=""><li id="76b6" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">全球模型</strong>:所有qp的一个模型。</li><li id="0f64" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">独立车型</strong>:一个车型对应一个QP，优于全球车型。</li></ul><h2 id="191d" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">2.2.不同QP的分级CNN结构</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nn"><img src="../Images/2c057eb93500b4b6c5b1bd3c732d64c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*rxZMW-9LB2jPpWoeiMNDdg.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">BD-比率(%) </strong></figcaption></figure><ul class=""><li id="b6a6" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">简而言之，根据上述结果，在<strong class="ja hj">高比特率场景</strong>中<strong class="ja hj"> QP等于22或27，仅涉及子网FEX。</strong></li><li id="8c5c" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">在<strong class="ja hj">低比特率场景</strong>，其中<strong class="ja hj"> QP等于32或37，启动整个SEFCNN。</strong></li><li id="5570" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">相对于SEFCNN，子网FEX具有更少的层数和参数，有效地降低了计算负担。</li></ul><h2 id="ec88" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">2.3.不同帧类型的分层CNN模型</h2><ul class=""><li id="4801" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">I帧的特点是纹理和方向。</li><li id="0e33" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">相反，<strong class="ja hj">从运动估计中获得P和B帧的预测样本。</strong></li><li id="87e0" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">除了预测值，<strong class="ja hj">I、P和B帧的残差值也具有不同的特征</strong>，因为不同的帧类型在前向/反向变换和量化过程中采用不同的编码工具。显然，它们在环路滤波问题上的拟合函数是不同的。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es no"><img src="../Images/4a22045d10ea91edbcbd78d68728cd72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*eXXtrh6phPjo10cMvVc_CA.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">QP 37的比特率和PSNR</strong></figcaption></figure><ul class=""><li id="fbbf" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">为P帧训练单独的模型实现了平均0.036 dB的PSNR增益</strong>和<strong class="ja hj">，相应的比特率降低了1.851% </strong>，而对于<strong class="ja hj"> B帧</strong>，<strong class="ja hj">增益为0.055 dB，比特率也略有下降。</strong></li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es np"><img src="../Images/3b2e613143e4fae6c4d4389f22f82561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*N8R_nrmGzKJ8CS6hvI11zg.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">QP 27的比特率和PSNR</strong></figcaption></figure><ul class=""><li id="f067" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在QP = 27时，共享单一模型更好。</li></ul><blockquote class="nf ng nh"><p id="4a5a" class="iy iz nc ja b jb jc ij jd je jf im jg ni ji jj jk nj jm jn jo nk jq jr js jt hb bi translated"><strong class="ja hj">当QP 37或32岁时，为了追求更高的性能，为每种镜架类型训练了一个单独的模特。当QP是27或22时，I模型在I、P和B帧之间共享。</strong></p></blockquote><h2 id="9587" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">2.4.CU级可切换增强</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nq"><img src="../Images/1a06ac506398acf30f10de9b0e768b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*c9JWmH0jlekHe6trVXoUZA.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">CU级可切换增强</strong></figcaption></figure><ul class=""><li id="affd" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">基本思想仅仅是增强将来很少被参考的帧或者帧内预测样本没有被增强的区域。</li><li id="d9ac" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">sef CNN的使用取决于各种配置下帧顺序中的相对时间位置。</strong> SEFCNN基于大量结果启用。更多细节请参考论文。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="437f" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated"><strong class="ak"> 3。实验结果</strong></h1><h2 id="e361" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">3.1.与<a class="ae ix" rel="noopener" href="/@sh.tsang/review-vrcnn-variable-filter-size-residue-learning-cnn-codec-post-processing-4a8a337ea73c"> VRCNN </a>的比较</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nr"><img src="../Images/255f27047e3e1e396bed74256c2b9fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XR_eVti11KoMif0ifP6f2w.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">BD-率(%) </strong></figcaption></figure><ul class=""><li id="638e" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">在AI配置中，SEFCNN获得了9.96%的BD-rate降低，而</strong><a class="ae ix" rel="noopener" href="/@sh.tsang/review-vrcnn-variable-filter-size-residue-learning-cnn-codec-post-processing-4a8a337ea73c"><strong class="ja hj">VR CNN</strong></a><strong class="ja hj">只有3.03%的增益。</strong></li><li id="8dfc" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">对于帧间编码，SEFCNN实现了8.04%和7.60%的BD-rate减少，并且<a class="ae ix" rel="noopener" href="/@sh.tsang/review-vrcnn-variable-filter-size-residue-learning-cnn-codec-post-processing-4a8a337ea73c"> VRCNN </a>的对应值在LDP和RA中分别是3.38%和4.85%。</li></ul><h2 id="3ef7" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">3.2.与<a class="ae ix" rel="noopener" href="/@sh.tsang/review-rhcnn-residual-highway-convolutional-neural-network-codec-post-processing-f9b617795a61"> RHCNN </a>的比较</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ns"><img src="../Images/f1b8118dd26a287d3734ab4def2dd499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*J6c6SU2YvAjNMIjT2JyamQ.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">QP的比特率和PSNR = 37</strong></figcaption></figure><ul class=""><li id="9d91" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj"> SEFCNN比</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-rhcnn-residual-highway-convolutional-neural-network-codec-post-processing-f9b617795a61"> <strong class="ja hj"> RHCNN </strong> </a>分别高出0.306 dB、0.154 dB、0.181 dB，更别提比特率的降低了。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nt"><img src="../Images/015d398982e238f0c1ac6e2ce1674603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23uAn9CHmZKJjcbxsMfAEw.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">使用HM-12.0的BD-Rate(%)</strong></figcaption></figure><ul class=""><li id="d682" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">同样，提出的SEFCNN优于RHCNN。</li></ul><h2 id="940b" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">3.3.与<a class="ae ix" rel="noopener" href="/@sh.tsang/review-mlsdrn-multi-channel-long-short-term-dependency-residual-network-codec-filtering-d97537ca8e45"> MLSDRN </a>的比较</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nu"><img src="../Images/a8929221474eb25a8b323ff37424c71d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*1s2b3VTRLX5sPBd5Txlpyw.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">使用HM-7.0的BD-速率(%)</strong></figcaption></figure><ul class=""><li id="f404" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">再次，提出的<strong class="ja hj"> SEFCNN优于</strong><a class="ae ix" rel="noopener" href="/@sh.tsang/review-mlsdrn-multi-channel-long-short-term-dependency-residual-network-codec-filtering-d97537ca8e45"><strong class="ja hj">MLSDRN</strong></a>【40】。</li></ul><h2 id="9207" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">3.4.复杂性分析</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nv"><img src="../Images/3367f1e9e6959507b96d2ca1cc1440f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*3ACjIduolh4vOLk2bzPRZA.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">编码器复杂度分析</strong></figcaption></figure><ul class=""><li id="fbd7" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在低视频分辨率下，如416×240，SEFCNN的编码时间不超过HM16.9的两倍。</li><li id="5231" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">当视频分辨率提高到1280×720时，帧内和帧间编码的额外运行时间分别不超过38%和19%。</strong></li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nw"><img src="../Images/d007a47a8bc85a6b6a2ed2c6f84bd0c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*1A08-lkCqMAm8C5cZTVjFg.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated"><strong class="bd lc">解码器复杂度分析</strong></figcaption></figure><ul class=""><li id="82d6" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">还有如上所述的解码时间测量。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="adfd" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">文中有大量的实验。如果有兴趣，请随时访问该文件。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="632d" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">参考</h2><p id="988e" class="pw-post-body-paragraph iy iz hi ja b jb ls ij jd je lt im jg jh nx jj jk jl ny jn jo jp nz jr js jt hb bi translated">【2020 TCS vt】【sef CNN】<br/><a class="ae ix" href="https://ieeexplore.ieee.org/document/8801877" rel="noopener ugc nofollow" target="_blank">一种用于视频编码中环路内滤波的可切换深度学习方法</a></p><h2 id="ebe1" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated">编解码器过滤</h2><p id="641e" class="pw-post-body-paragraph iy iz hi ja b jb ls ij jd je lt im jg jh nx jj jk jl ny jn jo jp nz jr js jt hb bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(能)(说)(什)(么)(话)(,)(我)(们)(还)(不)(能)(说)(出)(什)(么)(话)(,)(我)(们)(还)(不)(能)(说)(出)(什)(么)(话)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(,)(我)(们)(还)(不)(能)(说)(什)(么)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(,)(她)(们)(还)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(,)(她)(们)(还)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(。</p><h2 id="4e2e" class="mo lb hi bd lc mp mq mr lg ms mt mu lk jh mv mw lm jl mx my lo jp mz na lq nb bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>