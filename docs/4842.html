<html>
<head>
<title>Building a Search Engine Scraper with Streamlit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Streamlit 构建搜索引擎刮刀</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/building-a-search-engine-scraper-with-streamlit-b616e5bd293c?source=collection_archive---------1-----------------------#2021-08-10">https://medium.com/nerd-for-tech/building-a-search-engine-scraper-with-streamlit-b616e5bd293c?source=collection_archive---------1-----------------------#2021-08-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3f0c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">Web 抓取是一种通过最少的人工努力从 web 上获取数据的有效技术</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b613a21f234d68bb4c66fd6a26e91f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pjuZu6g9HJUoMBZZz5D_A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><h2 id="506c" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">网页抓取</h2><p id="b029" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">Web 抓取使我们能够以最少的人工努力从 web 上获取数据。然而，这种技术必须小心使用，以免降低网站的性能。在继续之前，我们将讨论一些提示，以保护我们自己不被网站列入黑名单，从而降低其性能。</p><ol class=""><li id="8127" class="le lf hi kn b ko lg kr lh jy li kc lj kg lk ld ll lm ln lo bi translated">我们不应该像 DDoS 攻击那样让网站的服务器超载，这是不道德的做法。如果我们让一个网站的服务器超载，我们可能会被列入黑名单。我们可以使用 Python 中的<strong class="kn hj">时间</strong>包的<strong class="kn hj"> sleep() </strong>函数，在我们的请求之间给出随机暂停。</li><li id="2dfc" class="le lf hi kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">我们必须不断更换用户代理。我们可以维护一个用户代理的 csv 文件，并经常切换它们。我们将在本文后面讨论更多关于用户代理的内容。</li><li id="cf1a" class="le lf hi kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">我们必须继续保存收集到的数据。这可以确保当网站将我们列入黑名单时，我们的数据不会丢失。我们可以从最后保存的检查点重新开始擦除，而不是重新开始。</li><li id="dc6a" class="le lf hi kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">如果可能的话，我们可以使用 Python 的<strong class="kn hj"> selenium </strong>包，它使我们能够模仿人类的活动，如滚动、点击等。这可能会降低我们被列入黑名单的几率。</li><li id="cbf6" class="le lf hi kn b ko lp kr lq jy lr kc ls kg lt ld ll lm ln lo bi translated">我们必须仔细检查并遵守网站的“robots.txt ”,以识别禁止访问的页面以及不允许访问网站的用户代理。我们可以使用“domain/robots.txt”访问“robots.txt”。例如，我们可以使用<a class="ae lu" href="http://bing.com/robots.txt" rel="noopener ugc nofollow" target="_blank">bing.com/robots.txt</a>访问 bing.com 的 robots.txt。</li></ol><h2 id="d927" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">搜索引擎刮刀</h2><p id="3584" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">在本文中，我们将使用 BeautifulSoup 和 Streamlit 构建一个搜索引擎 scraper*。该应用程序将搜索字符串作为输入，从 Bing 的第一页抓取搜索结果并显示出来。该应用程序还会返回搜索结果的数据框。下面是 Streamlit 应用程序的 Python 代码，并使用注释进行了清楚的解释。</p><p id="24b9" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj"> *本应用程序仅用于说明网页抓取的过程。强烈建议使用搜索引擎的 API 来提取搜索结果。网络抓取应该只是最后的手段。</strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="c596" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">上述代码可以通过在本地机器的终端中执行以下命令来运行。</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="7487" class="jn jo hi mb b fi mf mg l mh mi">streamlit run filename.py</span></pre><p id="fe35" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj"> filename.py </strong>是包含上述代码的 python 脚本文件。下面是我们执行命令后得到的输出截图。在这种情况下，包含 Python 脚本的文件的名称是<strong class="kn hj">bing _ scrape _ streamlit . py</strong>。因此，运行代码的命令如下所示。</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="03e3" class="jn jo hi mb b fi mf mg l mh mi">streamlit run bing_scrape_streamlit.py</span></pre><p id="46fc" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">下面是在本地机器的终端中执行上述命令后，应用程序在本地浏览器中的输出。我们还可以在输出的底部看到结果的数据框。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/e93380ad1e80b89d3a0687cc28e95508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XCovyuKIlTXU40yMZoiaCg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><h2 id="ab52" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">提取的搜索结果有什么用？</h2><ul class=""><li id="550c" class="le lf hi kn b ko kp kr ks jy mk kc ml kg mm ld mn lm ln lo bi translated">要查找公司列表的网站，假设(公司的)第一个搜索结果主要是公司的网站。这可以通过在公司列表上循环代码并存储从 API 请求中抓取的结果/响应来实现。大多数 API 也支持批量请求。然而，这可能需要一些进一步的验证，因为不是每个第一搜索结果都包含一个公司的网站。</li><li id="96b8" class="le lf hi kn b ko lp kr lq jy lr kc ls kg lt ld mn lm ln lo bi translated">找到人们/公司的社交媒体页面的 URL。这可以通过在搜索字符串中包含一个人/公司的名称和所需社交媒体网站的名称来实现。网站搜索可能更有帮助。</li><li id="b272" class="le lf hi kn b ko lp kr lq jy lr kc ls kg lt ld mn lm ln lo bi translated">查找与某公司相关的最新消息。</li></ul><p id="4f84" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">上面提到的所有任务都需要大量的手动工作，这些工作随着要搜索的实体的大小而增加。因此，web 抓取/API 在很大程度上减少了这种手动工作。然而，需要人在回路中来验证结果。</p><p id="04ee" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">我们将讨论一些在网络抓取初学者的头脑中可能出现的问题。</p><p id="d716" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj"> 1。什么是 requests.get()？</strong></p><p id="9644" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">requests 包使我们能够用 Python 处理 HTTP 请求。它的<strong class="kn hj"> get() </strong>函数向 web 服务器发送查询并返回响应。</p><p id="0f50" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj"> 2。什么是用户代理？</strong></p><p id="abe7" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">用户代理描述正在使用的浏览器和操作环境。这使得网络服务器能够发送最适合我们的浏览器/设备的内容。移动设备上的浏览器与 Windows 系统上的 Google Chrome 有不同的用户代理。切换用户代理保护我们不被列入黑名单，因为 web 服务器从多个设备获得请求。</p><p id="bab3" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj"> 3。什么是 BeautifulSoup？</strong></p><p id="e3d4" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">BeautifulSoup 解析 HTML/XML 文档，使我们能够从网页中提取数据。我们必须将请求响应的内容/文本传递给 BeautifulSoup 类进行解析。然后，我们可以提取网页的 HTML 脚本的元素/标签。</p><p id="8876" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj"> 4。我们如何选择正确的 HTML 标签？</strong></p><p id="b939" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">我们可以通过检查网页的源代码来识别正确的标签。<strong class="kn hj">“inspect”</strong>功能在所有流行的网络浏览器中都可用。我们将讨论如何<strong class="kn hj">‘检查’</strong>一个网页的源代码。</p><p id="99ce" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj">第一步:</strong>我们将右键单击网页中我们想要抓取的元素，并选择<strong class="kn hj">‘Inspect’</strong>。对于搜索引擎抓取器，我们希望抓取一个搜索结果的标题、URL 和描述。因此，我们将右键单击第一个搜索结果中的 URL。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mo"><img src="../Images/aaaa6fe7d14f49779bc9a0e579c196cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7OMdnWC6qbKO6-g81MwEuQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="9c21" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated"><strong class="kn hj">第二步:</strong>我们必须选择一个标签(父标签)，嵌入所有我们想要刮取的标签(子标签)。例如，我们需要抓取搜索结果，所以我们将选择一个父标签，嵌入附加到单个搜索结果的所有标签。在下图中，&lt; li class="b_algo" &gt;嵌入了附加到单个搜索结果的所有标签。现在，我们可以通过单独处理每个&lt; li class="b_algo" &gt;(搜索结果)来从&lt; li class="b_algo" &gt;中提取每个子标签。</p><p id="aa20" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">我们不应该只选择<h2>(搜索结果的标题)或</h2><p>(搜索结果的描述)。这可能会选取页面中除了与搜索结果相关联的那些之外的其他</p><h2>和</h2><p>。这也可能使我们很难将一个列表</p><h2>映射到一个列表</h2><p>。</p></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/21ec21b935b36ef12e2210483a6520d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFP7XJumjKusPI8xaMigPg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="26ee" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">因此，网络抓取帮助我们减少从网络上获取数据所需的人工劳动。如果网站的结构保持不变，我们可能会多次使用相同的代码从网站中提取数据。从网页中提取 HTML 脚本的代码对于所有网站都是相似的。不同的是从 HTML 脚本中清理和提取所需数据的过程。</p><p id="4628" class="pw-post-body-paragraph kl km hi kn b ko lg ij kq kr lh im kt jy lv kv kw kc lw ky kz kg lx lb lc ld hb bi translated">然而，我强烈推荐使用搜索引擎 API 来提取搜索结果。网络抓取应该只是最后的手段。即使你选择了网络抓取，也必须在不降低网站性能的情况下进行，并且只能用于提取公开信息。</p></div></div>    
</body>
</html>