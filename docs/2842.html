<html>
<head>
<title>Linear Regression — Icebreaker to Machine Learning Algorithms.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归——机器学习算法的破冰船。</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/linear-regression-icebreaker-to-machine-learning-algorithms-b5f680d19d4d?source=collection_archive---------24-----------------------#2021-05-21">https://medium.com/nerd-for-tech/linear-regression-icebreaker-to-machine-learning-algorithms-b5f680d19d4d?source=collection_archive---------24-----------------------#2021-05-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a624" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是爬机器学习算法阶梯的第一步。线性回归属于监督学习，我们必须训练线性回归模型来预测数据。</p><h2 id="ba36" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">简介</strong>:</h2><ul class=""><li id="9771" class="jy jz hi ih b ii ka im kb iq kc iu kd iy ke jc kf kg kh ki bi translated">19世纪，弗朗西斯·高尔顿正在研究父母和孩子之间的关系</li><li id="08d1" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">他说父母和孩子之间有关联和因果关系。</li></ul><p id="46f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目标是通过最小化垂直距离，即预测值和实际值之间的误差，找到最佳拟合线。</p><p id="c3a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归用于寻找目标和一个或多个预测值之间的线性关系。有两种类型的线性回归-简单和多重。</p><p id="04f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→输入、预测值或独立变量X</p><p id="fcd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→输出、响应或因变量y。</p><p id="3919" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要确定最佳拟合线:</p><ul class=""><li id="c90f" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">它必须通过质心(X的平均值= Y的平均值)</li><li id="2d30" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">它应该有更少的误差。</li><li id="c019" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">然后我们应该找到斜率(m)和截距(b)。</li></ul><h2 id="6223" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">简单线性回归:</h2><p id="7783" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">简单线性回归对于寻找两个连续变量之间的关系很有用。一个是预测变量或自变量，另一个是响应变量或因变量。如果一个变量可以被另一个精确地表达，那么两个变量之间的关系就是确定的。例如，使用摄氏温度可以准确预测华氏温度。统计关系在确定两个变量之间的关系时并不准确。比如身高和体重的关系。</p><p id="d7a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">核心思想是获得最适合数据的线。最佳拟合线是总预测误差(所有数据点)尽可能小的线。误差是点到回归线的距离。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/5506bda0d9521957aab18ff18771410a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-MxKoiWxJXLfExpvbT1eQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图1:简单的线性回归</figcaption></figure><p id="21a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lk">实时示例:</em> </strong></p><p id="dfac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有一个数据集，其中包含有关“学习小时数”和“获得分数”之间关系的信息。已经观察了许多学生，记录了他们的学习时间和成绩。这将是我们的训练数据。目标是设计一个模型，如果给定学习的小时数，它可以预测分数。使用训练数据，获得将给出最小误差的回归线。这个线性方程然后被用于任何新的数据。也就是说，如果我们给出一个学生学习的小时数作为输入，我们的模型应该以最小的误差预测他们的分数。</p><p id="8724" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单线性回归方程Y (Pred) = m (x) + b</p><p id="9855" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中m =斜率，x =预测值，b =截距。</p><p id="76f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">得出斜率(m):</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ll"><img src="../Images/6f298bb512028d0984fce3d3573043f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LXQk7mCEX_wQG4AVlQtGOw.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图2:计算简单线性回归的斜率(m)和截距(b)的步骤。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lm"><img src="../Images/6548fc8fc28b70aa6b1a23249c89ab5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/0*a9vLHo5O8I_F7RE9.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图3:截距计算</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ln"><img src="../Images/49eb93c315f83305b5a8a6171dcc4eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/0*n7FHbsWkSDBzkMIv.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图4:斜率计算</figcaption></figure><p id="aad0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lk">探坡(‘B1’或‘m’):</em></strong></p><ul class=""><li id="4e9b" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">如果斜率&gt; 0，则X(预测值)和y(目标值)具有正相关关系。也就是X的增加会增加y</li><li id="448a" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">如果斜率&lt; 0, then X (predictor) and y (target) have a negative relationship. That is increase in X decreases y.</li><li id="0611" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">If slope = 0, then X (predictor) and y (target) have no relationship.</li></ul><p id="7d3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lk">探索y轴截距(‘B0’或‘c’):</em></strong></p><ul class=""><li id="2ccd" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">如果模型不包括x=0，那么预测将变得没有意义，只有b0。例如，我们有一个关于身高(x)和体重(y)的数据集。取x=0(即身高为0)，将使方程只有b0值，这是完全没有意义的，因为在实时身高和体重永远不会为零。这是由于考虑了超出其范围的模型值。</li><li id="376a" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">如果模型包括值0，那么“b0”将是x=0时所有预测值的平均值。但是，将所有预测变量设置为零通常是不可能的。</li><li id="4741" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">b0值保证剩余具有平均值零。如果没有' b0 '项，那么回归将被强制越过原点。回归系数和预测都会有偏差。</li></ul><h2 id="cc4e" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">多元线性回归:</h2><p id="132e" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">多元线性回归对于找出n个自变量和一个因变量之间的关系很有用。以便找到优化的斜率和截距，即实际值和预测值之间误差较小的值。为了找到优化的斜率和截距值，使用线性回归算法的梯度下降心。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lo"><img src="../Images/2ac2cded2ec33b73f5be627239b04865.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7uqWXtRkvnQyJtxoLe42Q.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图5:多元线性回归公式。</figcaption></figure><p id="a5d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lk">渐变下降:</em> </strong></p><p id="2391" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是一种寻找函数最小值的一阶迭代优化算法。它采用递归方法来寻找最佳斜率(m)和截距(b)。成本函数(误差)应该减少。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lp"><img src="../Images/e0a967351ed793f9e222136b4f47845c.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*tD7k9QCX7QdQFurcy8jVIw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图6:成本函数(误差)</figcaption></figure><ul class=""><li id="f509" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">现在，我们要用偏导数得出最佳斜率和截距。</li><li id="ccad" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">偏导数是一种在将其他变量视为常数的同时，对一个独立变量的函数进行微分的方法。它被表示为</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lq"><img src="../Images/5daa96d7c711f0327d0135a1da16386f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08NlnPV7SdRVk5iCT0BA-w.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图7:偏导数公式。</figcaption></figure><ul class=""><li id="4948" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">当我们在斜率和成本函数之间绘制图形时，梯度下降曲线就出现了。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lr"><img src="../Images/28cc25f478dfe3a6ae206269d8236be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_o1qsIWMGbTI1OTrLb2lw.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图8:梯度下降。</figcaption></figure><ul class=""><li id="503b" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">偏导数方程w.r.t斜率和截距。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ls"><img src="../Images/43bcf056e7727e57eadb49dcd2989f63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zLpTsho7f9HsIXdSS90JA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图9:偏导数</figcaption></figure><ul class=""><li id="14cb" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">使用梯度下降计算最小误差的示例。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lt"><img src="../Images/317e12e018a08abd4398dee2bf4c4852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*937xFsEa1_I_Z6JONHctYg.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图10:示例问题。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lu"><img src="../Images/d1aafaf0eef701cfeba00e9711dad61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4sjzslGt6-FlQoj4O5sQg.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图11:计算新截距的公式。</figcaption></figure><ul class=""><li id="c0b8" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">为了计算新的截距值，必须从步长中减去old_b。</li></ul><p id="dbf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→步长=偏导数*学习率</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lv"><img src="../Images/c8d56b5f87ce1fb5b82b62220c66c080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*vdZ5wafNWXpPxxNTAlqVcA.jpeg"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图12:扩展的公式。</figcaption></figure><ul class=""><li id="aeaf" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">学习率意味着移动的距离，偏导数意味着移动的方向。</li><li id="7b16" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">如果我们有更大的学习率值，它将偏离更多，所以选择最小值是好的。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lt"><img src="../Images/c679758da4b1c89c2c7f099de7c4e9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5I-LuC6wQSbcN60KMJbmg.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图13:计算优化截距的步骤。</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lw"><img src="../Images/7f4058545b53ad1569cf95627baa9dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mjpS02-S0GQVb6ZKo3XPWA.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图14:优化的截距值。</figcaption></figure><ul class=""><li id="f787" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">我们必须迭代这个过程，直到具有低误差的全局最小点，但是误差不会为零。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lx"><img src="../Images/268a9f116198148df5676d9b89945251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yk1E-fMnShTFmXBmJUGZQg.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图15:寻找梯度下降的程序。</figcaption></figure><p id="92ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降代码:<a class="ae ly" href="https://github.com/Rishikumar04/Data-Science-Training/blob/main/Linear_Regression/01_Gradient%20Descent.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/rishikumar 04/Data-Science-Training/blob/main/Linear _ Regression/01 _ Gradient % 20 decusing . ipynb</a></p><p id="ef6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的假设:</p><ul class=""><li id="4a1a" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">线性关系</li><li id="b786" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">没有或很少多重共线性</li><li id="81ea" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">同方差性</li><li id="da34" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">残差的正态性。</li></ul><p id="4a38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lk">线性关系:</em> </strong></p><p id="08d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→X和y之间应该有关系，要么是正关系，要么是负关系。</p><p id="cf26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→线性回归对异常值敏感。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lz"><img src="../Images/d1787ef750ae94250539a87ff312c892.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*DhKFk_2lI0M1NHWGDw5cpw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图16:散点图。</figcaption></figure><ul class=""><li id="a19f" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">自变量(X)和因变量(y)之间的正线性关系。</li></ul><p id="15b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lk">多重共线性:</em> </strong></p><ul class=""><li id="92cf" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">多重共线性表示两个独立变量之间有很强的关系。</li><li id="6cda" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">如果有关系，那么系数会有一些变化，方差会增加。因此，在将数据输入模型之前，必须从数据集中移除多重共线性。</li></ul><p id="3645" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="lk"/></strong></p><p id="632b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Homo =相同，Scedasticity =方差</p><ul class=""><li id="a4ae" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">残差(误差)应该具有相同的方差。</li><li id="d488" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">分布在数据上是一样的。</li><li id="4dae" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">散点图是要绘制预测值及其误差，结果图应具有相同的误差方差。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ma"><img src="../Images/6ba271fcaa08e81ecfaba5b5f5999c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LZsb6EQb_3zT7IWBB4e2Q.jpeg"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图17:同方差和异方差</figcaption></figure><p id="fa2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lk">残差中的正态:</em> </strong></p><ul class=""><li id="3cff" class="jy jz hi ih b ii ij im in iq ko iu kp iy kq jc kf kg kh ki bi translated">误差应该服从正态分布。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mb"><img src="../Images/ffa7010f354ad455175c780423cc0422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*4xYQPeEEJhHL8JmtyfF1ww.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图18:说明残差正态性的KDE图。</figcaption></figure><p id="c90b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的其他求解实例:<a class="ae ly" href="https://github.com/Rishikumar04/Data-Science-Training/tree/main/Linear_Regression" rel="noopener ugc nofollow" target="_blank">https://github . com/rishikumar 04/Data-Science-Training/tree/main/Linear _ Regression</a>。</p><p id="12a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个由资料汇编而成的教育帖(如何塞·波尔蒂利亚的Udemy课程，我的导师穆罕默德·伊姆兰(<a class="ae ly" href="https://www.linkedin.com/in/imohamedimran/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/imohamedimran/</a>)等)..)在我的旅途中帮助了我。</p><p id="52d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。</p></div></div>    
</body>
</html>