<html>
<head>
<title>Introduction to structured streaming with apache-spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">apache-spark结构化流简介</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/introduction-to-structured-streaming-with-apache-spark-7079a25b0d1a?source=collection_archive---------0-----------------------#2021-11-28">https://medium.com/nerd-for-tech/introduction-to-structured-streaming-with-apache-spark-7079a25b0d1a?source=collection_archive---------0-----------------------#2021-11-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f493" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是结构化流</strong></p><p id="0ff5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结构化流是一个建立在Apache sparkSQL引擎之上的<strong class="ih hj">流处理框架，因为它使用spark中现有的dataframe APIs，几乎所有熟悉的操作都支持流。结构化流是通过检查点和预写日志实现的容错流机制。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/e3d50d83b71a9f9e902b1adb427d63a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IyFEpsA3qg_mxDHHLCoqEA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">火花结构化流</figcaption></figure><blockquote class="jt ju jv"><p id="ff9b" class="if ig jw ih b ii ij ik il im in io ip jx ir is it jy iv iw ix jz iz ja jb jc hb bi translated"><!-- -->结构化流背后的主要思想是数据被连续地附加到一个表中，用最简单的方式来说，结构化流就是你的数据帧“但是流”。</p></blockquote><p id="c34f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">流式处理使我们能够构建连续的应用程序，这些应用程序与数据实时交互。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ka"><img src="../Images/4ec63f5f99f2b411565c0946c53b4451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UCCQbXtBQ33Tk34rvPyQeQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作为流数据帧的结构化流</figcaption></figure><p id="ec41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结构化流的核心概念</strong></p><p id="fe64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">转换</strong></p><p id="992a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了少数例外，spark dataframe API中几乎所有可用的转换(聚合、连接)在结构化流中也是可用的。只有一个可用的动作是流的开始。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="7be5" class="kg kh hi kc b fi ki kj l kk kl">/*Prepare the schema for the data by reading the corresponding json file<br/>* */<br/><strong class="kc hj">val</strong> static <strong class="kc hj">=</strong> spark.read.json("/data/activity-data/")<br/><strong class="kc hj">val</strong> dataSchema <strong class="kc hj">=</strong> static.schema<br/>/*maxFilesPerTrigger is given a low value to specify how quickly spark is trying<br/>to read the data in the file.<br/>* */<br/><strong class="kc hj">val</strong> streaming <strong class="kc hj">=</strong> spark.readStream.schema(dataSchema)<br/>  .option("maxFilesPerTrigger", <strong class="kc hj">1</strong>).json("/data/activity-data")</span></pre><p id="f342" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的例子中，我们试图从一个带有给定模式的JSON文件进行流式传输。<strong class="ih hj"> maFilePerTrigger </strong>表示您希望每个触发器传输多少条记录。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="93e7" class="kg kh hi kc b fi ki kj l kk kl">/*Just like batch data all the operation in spark streaming are lazy only evaluated at the time of action<br/>* */<br/> <strong class="kc hj">val</strong> activityCounts<strong class="kc hj">=</strong>streaming.groupBy("gt").count()<br/><br/>/*Write result to memory<br/>  queryName: activity_counts<br/>  outputMode: complete(All the rows will be written to the output sink)<br/><br/>* */<br/><strong class="kc hj">val</strong> activityQuery <strong class="kc hj">=</strong> activityCounts.writeStream.queryName("activity_counts")<br/>  .format("memory").outputMode("complete")<br/>  .start()<br/></span></pre><p id="285d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在从JSON文件流式传输之后，一个<strong class="ih hj"> groupBy </strong>操作被应用于流式数据，并且该查询被写入内存，我们将在本文后面查看用于写入的各种输出位置(接收器)和模式。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="abd0" class="kg kh hi kc b fi ki kj l kk kl">/*wait for the termination of the query<br/>  * */<br/>  activityQuery.awaitTermination()<br/><br/>  <strong class="kc hj">for</strong>( i <strong class="kc hj">&lt;-</strong> <strong class="kc hj">1</strong> to <strong class="kc hj">5</strong> ) {<br/>    spark.sql("SELECT * FROM activity_counts").show()<br/>    <strong class="kc hj">Thread</strong>.sleep(<strong class="kc hj">1000</strong>)<br/>  }</span></pre><p id="f16c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查询将继续流入内存，直到调用终止，<strong class="ih hj"> awaitTermination </strong>()将等待查询终止。在调用awaitTermination函数之前，流过程不会真正开始，但为了简单起见，我们将跳过<strong class="ih hj"> </strong>中的<strong class="ih hj"> awaitTermination() </strong>其余代码片段。</p><p id="a730" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">聚合和连接</strong></p><p id="5e15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark结构化流支持聚合和连接操作，类似于spark dataframe API，让我们看一个例子。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="9bdd" class="kg kh hi kc b fi ki kj l kk kl">/*Aggregations<br/>  Cube:<br/>      Create a multidimensional cube for the current Dataset using       the specified columns,<br/>      so we can run aggregation on them.<br/>  * */<br/><strong class="kc hj">val</strong> deviceModelStats <strong class="kc hj">=</strong> streaming.cube("sample1").avg()<br/>.writeStream.queryName("device_counts").format("memory").outputMode("complete")<br/>  .start()</span></pre><p id="e20f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多维数据集</strong>函数创建一个具有给定列名的多维数据集，因此我们可以在其上运行聚合。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="074c" class="kg kh hi kc b fi ki kj l kk kl">/*Joins<br/><br/>  * */<br/>  <strong class="kc hj">val</strong> historicalAgg <strong class="kc hj">=</strong> static.groupBy("sample1").avg()<br/>  <strong class="kc hj">val</strong> deviceModelStatsJoins <strong class="kc hj">=</strong> streaming<br/>    .cube("sample1").avg()<br/>    .join(historicalAgg, <strong class="kc hj">Seq</strong>("sample1")   .writeStream.queryName("device_counts").format("memory").outputMode("complete")<br/>    .start(</span></pre><p id="4bbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据来源</strong></p><p id="2823" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在结构化流中，数据源是从中读取数据的源。下面是一些数据源的示例。</p><ul class=""><li id="035c" class="km kn hi ih b ii ij im in iq ko iu kp iy kq jc kr ks kt ku bi translated">阿帕奇卡夫卡</li><li id="10c0" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">分布式文件系统HDFS或s3 spark中的文件将不断读取新文件。</li><li id="8d34" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">测试用插座</li></ul><p id="9c4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据接收器</strong></p><p id="a6fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接收器是结构化流的目的地，以下是接收器的示例。</p><ul class=""><li id="d395" class="km kn hi ih b ii ij im in iq ko iu kp iy kq jc kr ks kt ku bi translated">阿帕奇卡夫卡</li><li id="a9bf" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">几乎任何格式的文件(csv、parquet、json)</li><li id="3f10" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">记忆汇</li></ul><p id="ca7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出模式</strong></p><p id="617b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如何将数据写入输出接收器。支持的模式有。</p><p id="7797" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Append </strong>:将传入的数据追加到现有的表中。追加模式类似于向现有表追加新行。</p><p id="1ad5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">完成</strong>:在完成模式下，全部输出将被重写。</p><p id="98ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">更新</strong>:类似于完成模式，但是在更新模式下只有那些有变化的行会受到影响。</p><p id="d687" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">触发</strong></p><p id="9015" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">触发器指定何时寻找新的记录集，默认情况下，spark会在完成新的数据集后寻找新的记录集。Spark还支持基于时间的触发器(基于事件的触发器)。</p><p id="e880" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基于事件时间的处理</strong></p><p id="6be9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark支持基于事件时间的处理，即根据记录中的事件时间戳处理数据。</p><p id="5581" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">水印</strong></p><p id="675d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">watermark指定spark流预期在事件时间内多晚看到数据。</p><blockquote class="jt ju jv"><p id="93da" class="if ig jw ih b ii ij ik il im in io ip jx ir is it jy iv iw ix jz iz ja jb jc hb bi translated">水印长度表示他们需要多长时间记住数据的长度。</p></blockquote><p id="7892" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">阅读和写作卡夫卡的作品</strong></p><p id="8dde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache kafka是一个针对数据流的分布式发布和订阅系统，它允许您以容错方式发布和订阅像消息队列一样的记录。kafka将记录存储在被称为主题的类别中，主题中的每个记录由键、值和时间戳组成。记录存储的位置称为<strong class="ih hj">偏移</strong>。</p><p id="ffcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">从卡夫卡主题中读取数据</strong></p><p id="1a16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要从kafka主题中读取数据，必须选择以下模式之一，<strong class="ih hj">分配，</strong>订阅，<strong class="ih hj">订阅模式</strong>。</p><ul class=""><li id="e7b2" class="km kn hi ih b ii ij im in iq ko iu kp iy kq jc kr ks kt ku bi translated">分配:这是一种细粒度的阅读kafka主题的方式，不仅指定了主题，还指定了主题分区。这可以以json字符串的形式给出。</li></ul><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="bc03" class="kg kh hi kc b fi ki kj l kk kl">{"topicA": [0,1],"topicB":[2,4]}</span></pre><ul class=""><li id="f0dc" class="km kn hi ih b ii ij im in iq ko iu kp iy kq jc kr ks kt ku bi translated">另外两种方法是通过指定主题列表来读取卡夫卡的主题</li></ul><p id="d5c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">开始和结束偏移</strong></p><p id="7357" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查询开始的起始点，可以使用json字符串来指定。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="d5b6" class="kg kh hi kc b fi ki kj l kk kl">{"topicA": {"0":23,"1":-1},"topicB":{"0":-2}}</span></pre><p id="cd5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的json字符串中-2指定最早的，而-1指定最新的偏移量。这仅适用于新的流式查询开始时，rest将从查询停止的地方继续。</p><p id="38d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">失败数据丢失</strong></p><p id="2bdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当数据丢失时是否使查询失败。由于主题丢失或偏移量超出范围，查询可能会失败。默认值将为真。</p><p id="2970" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> maxOffsetperTrigger </strong></p><p id="0e0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个触发器要读取的偏移量。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="d3df" class="kg kh hi kc b fi ki kj l kk kl"><strong class="kc hj">val</strong> df1 <strong class="kc hj">=</strong> spark.readStream.format("kafka").<br/>    option("kafka.bootstrap.servers", "host1:port1,host2:port2")<br/>    .option("subscribe", "topic1")<br/>    .load()</span></pre><p id="5132" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将由以下格式组成</p><ul class=""><li id="55bc" class="km kn hi ih b ii ij im in iq ko iu kp iy kq jc kr ks kt ku bi translated">关键字:二进制</li><li id="82c2" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">值:二进制</li><li id="eab4" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">主题:字符串</li><li id="5da1" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">分区:int</li><li id="43b6" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">偏移:长型</li><li id="ff89" class="km kn hi ih b ii kv im kw iq kx iu ky iy kz jc kr ks kt ku bi translated">时间戳:长</li></ul><blockquote class="jt ju jv"><p id="0fd6" class="if ig jw ih b ii ij ik il im in io ip jx ir is it jy iv iw ix jz iz ja jb jc hb bi translated">kafka的输出将以某种方式序列化，使用spark dataframe API或用户定义的函数将其转换为所需的格式。</p></blockquote><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="bae6" class="kg kh hi kc b fi ki kj l kk kl"><strong class="kc hj">val</strong> df2<strong class="kc hj">=</strong>df1.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")</span></pre><p id="f4e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">将数据写入卡夫卡主题</strong></p><p id="4774" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将数据写入kafka主题也称为发布。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="2912" class="kg kh hi kc b fi ki kj l kk kl">df2.writeStream<br/>.format("kafka")<br/>.option("kafka.bootstrap.servers", "host1:port1,host2:port2")<br/>.option("checkpointLocation", "/to/HDFS-compatible/dir")<br/>.start()</span></pre><p id="c9b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">何时输出数据</strong></p><p id="e6c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">触发器可用于提供向接收器输出数据的时间间隔。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="4717" class="kg kh hi kc b fi ki kj l kk kl"><strong class="kc hj">import</strong> <strong class="kc hj">org.apache.spark.sql.streaming.Trigger</strong><br/>activityCounts.writeStream.trigger(<strong class="kc hj">Trigger</strong>.<strong class="kc hj">ProcessingTime</strong>("100 seconds"))<br/>.format("console").outputMode("complete").start()</span></pre><p id="50ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">触发一次</strong>只能用来输出一次数据。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="8660" class="kg kh hi kc b fi ki kj l kk kl"><em class="jw">activityCounts</em>.writeStream.trigger(Trigger.<em class="jw">Once</em>())<br/> .format("console").outputMode("complete").start()</span></pre></div></div>    
</body>
</html>