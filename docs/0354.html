<html>
<head>
<title>A Real-Time Object Detection model using YOLOv3 algorithm for non-GPU Computers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向非 GPU 计算机的 YOLOv3 算法实时目标检测模型</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/a-real-time-object-detection-model-using-yolov3-algorithm-for-non-gpu-computers-8941a20b445?source=collection_archive---------2-----------------------#2020-10-22">https://medium.com/nerd-for-tech/a-real-time-object-detection-model-using-yolov3-algorithm-for-non-gpu-computers-8941a20b445?source=collection_archive---------2-----------------------#2020-10-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="b83b" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">合著者<a class="jh ji ge" href="https://medium.com/u/b4c7c4df291a?source=post_page-----8941a20b445--------------------------------" rel="noopener" target="_blank"> Dhrumilparikh </a></p></blockquote><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jj"><img src="../Images/c85d0df9b253d6b6ee934e827220e3f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gKJLZuZO4RnqpF9DLXg-Ug.png"/></div></div></figure><blockquote class="if ig ih"><p id="425d" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">你只看一次(YOLO)是一个最先进的，实时对象检测系统。YOLOv3 速度极快，精度极高。</p></blockquote><p id="c95c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated">YOLOv3 是一种新兴的对象检测模型，用于运行在图形处理单元(GPU)不足的笔记本电脑或台式机上。<strong class="il hj"> </strong>“你只看一次”v3 (YOLOv3)模型广泛应用于基于深度学习的物体检测方法<strong class="il hj">。</strong>该模型是在<strong class="il hj"> COCO 数据集</strong>上准备的，实现了 57.9%的平均精度(mAP)。YOLOv3 在非 GPU 计算机上的运行速度约为 20 FPS。</p><p id="8bc4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated">YOLO 已经经历了三次迭代，每一次都是对前一次的逐步改进。你可以检查每一篇文章:</p><ul class=""><li id="a765" class="jy jz hi il b im in iq ir jv ka jw kb jx kc jg kd ke kf kg bi translated"><a class="ae kh" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.02640.pdf</a></li><li id="f581" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated"><a class="ae kh" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a></li><li id="6485" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated">约洛夫 3:<a class="ae kh" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></li></ul><p id="359e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><strong class="il hj">先决条件</strong></p><ul class=""><li id="6cf0" class="jy jz hi il b im in iq ir jv ka jw kb jx kc jg kd ke kf kg bi translated">OpenCV</li><li id="f381" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated">NumPy</li><li id="f550" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated">YOLO 的配置文件和权重文件</li><li id="1db2" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated">熊猫</li><li id="36be" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated">COCO 名称文件</li><li id="6b56" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated">Python(3.0 或以上版本)</li></ul><p id="7f8d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><strong class="il hj">什么是 YOLO？</strong></p><p id="052c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated">YOLO 是一个聪明的<strong class="il hj">卷积神经网络(CNN) </strong>用于实时进行对象检测。该算法将单个神经网络应用于整个图像，然后将图像分成多个区域，并预测每个区域的边界框和概率。这些边界框由预测概率加权。图像的高分区域被认为是检测。</p><p id="c381" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated">与基于分类器的系统相比，我们的模型有几个优点。它在测试时查看整个图像，因此它的预测由图像中的全局上下文提供信息。它还可以通过单个网络评估进行预测，不像 R-CNN 这样的系统需要成千上万的单个图像。这使得它非常快，比 R-CNN 快 1000 多倍，比快速 R-CNN 快 100 倍。</p><p id="8e3d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated">约洛夫 3 的建筑:</p><figure class="jk jl jm jn fd jo er es paragraph-image"><div class="er es kn"><img src="../Images/4bd06719fb86d3ff11535b31ce52a851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*sT0XPVWWUMNdllDcJt1wCA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated"><a class="ae kh" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><ul class=""><li id="ce25" class="jy jz hi il b im in iq ir jv ka jw kb jx kc jg kd ke kf kg bi translated">每个边界框包含 5 个元素:(<em class="ik"> x，y，w，h </em>)和一个<strong class="il hj">框置信度得分</strong>。置信度得分反映了框包含对象的可能性以及边界框的精确度。然后用图像的宽度和高度归一化边框宽度<em class="ik"> w </em>和高度<em class="ik"> h </em>。<em class="ik"> x </em>和<em class="ik"> y </em>是相对于相应单元格的偏移量。因此，<em class="ik"> x，y，w </em>和<em class="ik"> h </em>都在 0 和 1 之间。每个单元格有 20 个条件类概率。<strong class="il hj">条件类别概率</strong>是检测到的物体属于特定类别的概率(每个单元每个类别一个概率)。因此，YOLO 的预测具有(S，S，B×5 + C) = (7，7，2×5 + 20) = (7，7，30)的形状。</li><li id="0e44" class="jy jz hi il b im ki iq kj jv kk jw kl jx km jg kd ke kf kg bi translated">YOLO 的主要概念是建立一个 CNN 网络来预测(7，7，30)张量。它使用 CNN 网络将空间维度降低到 7×7，每个位置有 1024 个输出通道。YOLO 使用两个完全连接的图层执行线性回归，以进行 7×7×2 边界框预测(下图中)。要进行最终预测，请将那些具有高框置信度得分(大于 0.50)的预测作为最终预测。</li></ul><p id="f636" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><strong class="il hj">实施:</strong></p><ol class=""><li id="9516" class="jy jz hi il b im in iq ir jv ka jw kb jx kc jg ks ke kf kg bi translated">我们需要在任何 python IDE 中创建一个 python 文件，并导入所有必需的包。</li></ol><pre class="jk jl jm jn fd kt ku kv kw aw kx bi"><span id="ef0f" class="ky kz hi ku b fi la lb l lc ld">import cv2<br/>import numpy as np<br/>import time</span></pre><p id="d90d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><strong class="il hj"> 2。加载 YOLOv3 的训练权重和配置文件。</strong></p><pre class="jk jl jm jn fd kt ku kv kw aw kx bi"><span id="1637" class="ky kz hi ku b fi la lb l lc ld">net = cv2.dnn.readNet('yolov3.weights','yolov3.cfg')</span><span id="2705" class="ky kz hi ku b fi le lb l lc ld">classes = []</span><span id="57a7" class="ky kz hi ku b fi le lb l lc ld">with open('coco.names','r') as f:</span><span id="03b1" class="ky kz hi ku b fi le lb l lc ld">classes = f.read().splitlines()</span><span id="e1d8" class="ky kz hi ku b fi le lb l lc ld">cap = cv2.VideoCapture(0)</span></pre><p id="a0e1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><strong class="il hj"> 3。现在，我们将图像分成多个区域并预测边界框。然后，它将制作区域置信度大于 0.5 的盒子。</strong></p><pre class="jk jl jm jn fd kt ku kv kw aw kx bi"><span id="c799" class="ky kz hi ku b fi la lb l lc ld">font = cv2.FONT_HERSHEY_DUPLEX</span><span id="9474" class="ky kz hi ku b fi le lb l lc ld">starting_time = time.time()</span><span id="558d" class="ky kz hi ku b fi le lb l lc ld">frame_id = 0</span><span id="9a7c" class="ky kz hi ku b fi le lb l lc ld">while True:</span><span id="d597" class="ky kz hi ku b fi le lb l lc ld">   _, img = cap.read()</span><span id="6bc2" class="ky kz hi ku b fi le lb l lc ld">   height, width, _ = img.shape</span><span id="a3e2" class="ky kz hi ku b fi le lb l lc ld">   frame_id += 1</span><span id="4dc0" class="ky kz hi ku b fi le lb l lc ld">   blob = cv2.dnn.blobFromImage(img, 1/255, (416,416), (0,0,0),       swapRB = True, crop = False)</span><span id="548b" class="ky kz hi ku b fi le lb l lc ld">   net.setInput(blob)</span><span id="de6f" class="ky kz hi ku b fi le lb l lc ld">   output_layers_names = net.getUnconnectedOutLayersNames()</span><span id="c83b" class="ky kz hi ku b fi le lb l lc ld">   layersOutputs = net.forward(output_layers_names)</span><span id="8784" class="ky kz hi ku b fi le lb l lc ld">   boxes = []</span><span id="55e6" class="ky kz hi ku b fi le lb l lc ld">   confidences = []</span><span id="045e" class="ky kz hi ku b fi le lb l lc ld">   class_ids = []</span><span id="6cd3" class="ky kz hi ku b fi le lb l lc ld">   for output in layersOutputs:</span><span id="362f" class="ky kz hi ku b fi le lb l lc ld">      for detection in output:</span><span id="a092" class="ky kz hi ku b fi le lb l lc ld">         scores = detection[5:]</span><span id="a388" class="ky kz hi ku b fi le lb l lc ld">         class_id = np.argmax(scores)</span><span id="7ada" class="ky kz hi ku b fi le lb l lc ld">         confidence = scores[class_id]</span><span id="a884" class="ky kz hi ku b fi le lb l lc ld">         if confidence &gt; 0.5:</span><span id="f178" class="ky kz hi ku b fi le lb l lc ld">            center_x = int(detection[0]*width)</span><span id="6e9b" class="ky kz hi ku b fi le lb l lc ld">            center_y = int(detection[1]*height)</span><span id="59d4" class="ky kz hi ku b fi le lb l lc ld">            w = int(detection[2]*width)</span><span id="4a38" class="ky kz hi ku b fi le lb l lc ld">            h = int(detection[3]*height)</span><span id="7d35" class="ky kz hi ku b fi le lb l lc ld">            x = int(center_x - w/2)</span><span id="c404" class="ky kz hi ku b fi le lb l lc ld">            y = int(center_y - h/2)</span><span id="b86b" class="ky kz hi ku b fi le lb l lc ld">            boxes.append([x, y, w, h])</span><span id="2b43" class="ky kz hi ku b fi le lb l lc ld">            class_ids.append(class_id)</span><span id="f93d" class="ky kz hi ku b fi le lb l lc ld">            confidences.append((float(confidence)))</span><span id="a944" class="ky kz hi ku b fi le lb l lc ld">    print(len(boxes))</span><span id="0fe7" class="ky kz hi ku b fi le lb l lc ld">    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)</span><span id="bbb5" class="ky kz hi ku b fi le lb l lc ld">    colors = np.random.uniform(0, 255, size = (len(boxes), 3))</span></pre><p id="4e1e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><strong class="il hj"> 4。这将制作不同颜色的矩形盒子，并通过将物体与预先训练的重量进行比较来给出标签，就这样。尽情享受吧！！</strong></p><pre class="jk jl jm jn fd kt ku kv kw aw kx bi"><span id="1b9b" class="ky kz hi ku b fi la lb l lc ld">    for i in range(len(boxes)):</span><span id="b644" class="ky kz hi ku b fi le lb l lc ld">       if i in indexes:</span><span id="debb" class="ky kz hi ku b fi le lb l lc ld">          x, y , w, h = boxes[i]</span><span id="443c" class="ky kz hi ku b fi le lb l lc ld">          label = str(classes[class_ids[i]])</span><span id="112a" class="ky kz hi ku b fi le lb l lc ld">          confidence = str(round(confidences[i],2))</span><span id="64b7" class="ky kz hi ku b fi le lb l lc ld">          color = colors[i]</span><span id="c9ef" class="ky kz hi ku b fi le lb l lc ld">          cv2.rectangle(img, (x,y), (x+w,y+h), color, 8)</span><span id="20cd" class="ky kz hi ku b fi le lb l lc ld">          cv2.putText(img, label + " " + confidence, (x, y+20), font, 1, (255,255,255) , 2)</span><span id="cbad" class="ky kz hi ku b fi le lb l lc ld">    elapsed_time = time.time() - starting_time</span><span id="d3f3" class="ky kz hi ku b fi le lb l lc ld">    fps = frame_id / elapsed_time</span><span id="3e43" class="ky kz hi ku b fi le lb l lc ld">    cv2.putText(img, "FPS: " + str(fps), (10,30), font, 1, (0,0,0), 1)</span><span id="8c9b" class="ky kz hi ku b fi le lb l lc ld">    cv2.imshow('Output',cv2.resize(img,(700, 500)))  # cv2.resize(img,(600, 400))</span><span id="efed" class="ky kz hi ku b fi le lb l lc ld">    key = cv2.waitKey(1)  #0</span><span id="79ad" class="ky kz hi ku b fi le lb l lc ld">    #if the 'c' key is pressed, stop the loop</span><span id="972e" class="ky kz hi ku b fi le lb l lc ld">    if key == ord('c'):</span><span id="7a68" class="ky kz hi ku b fi le lb l lc ld">       break</span><span id="f6b9" class="ky kz hi ku b fi le lb l lc ld">cap.release()</span><span id="218a" class="ky kz hi ku b fi le lb l lc ld">cv2.destroyAllWindows()</span></pre><h2 id="a62d" class="ky kz hi bd lf lg lh li lj lk ll lm ln jv lo lp lq jw lr ls lt jx lu lv lw lx bi translated"><strong class="ak">结果会是这样的:</strong></h2><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es ly"><img src="../Images/2d65b2ba0fb429293962ff744352a35f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwjE6Ho5WjeieFY9rfQYuw.png"/></div></div></figure><figure class="jk jl jm jn fd jo er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lz"><img src="../Images/f47111831a872d2ac50e213d87abb914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r05bp1Tu3NjeNsodOFTxVg.png"/></div></div></figure><h2 id="482d" class="ky kz hi bd lf lg lh li lj lk ll lm ln jv lo lp lq jw lr ls lt jx lu lv lw lx bi translated"><strong class="ak"> <em class="ma">概要:</em> </strong></h2><p id="5c2e" class="pw-post-body-paragraph ii ij hi il b im mb io ip iq mc is it jv md iw ix jw me ja jb jx mf je jf jg hb bi translated">在这个模型中，<strong class="il hj"> YOLOv3 </strong>实现了将物体检测带到非 GPU 计算机上的目标。该模型还可以从记录的图像或视频中检测物体。此外，YOLOv3 还在物体探测领域做出了贡献。此外，YOLOv3 表明浅层网络对于轻量级实时对象检测网络具有巨大的潜力。在非 GPU 计算机上以 20 FPS 的速度运行，对于这样一个小系统来说是非常有前途的。此外，该模型 YOLOv3 表明，当涉及到较小的浅层网络时，批量规范化的使用应该受到质疑。轻量级实时对象检测领域的进展是使对象检测在日常实例中可用的最后前沿。</p><h2 id="78f6" class="ky kz hi bd lf lg lh li lj lk ll lm ln jv lo lp lq jw lr ls lt jx lu lv lw lx bi translated">参考资料:</h2><p id="5f74" class="pw-post-body-paragraph ii ij hi il b im mb io ip iq mc is it jv md iw ix jw me ja jb jx mf je jf jg hb bi translated"><a class="ae kh" href="https://www.youtube.com/watch?v=1LCb1PVqzeY" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=1LCb1PVqzeY</a></p><div class="mg mh ez fb mi mj"><a href="https://pjreddie.com/darknet/yolo/" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hj fi z dy mo ea eb mp ed ef hh bi translated">YOLO:实时目标检测</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">你只看一次(YOLO)是一个最先进的，实时对象检测系统。在 Pascal Titan X 上，它处理…</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">pjreddie.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx jt mj"/></div></div></a></div><p id="476b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><a class="ae kh" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" rel="noopener" target="_blank">https://towards data science . com/yolo-v3-object-detection-53 FB 7d 3 bfe 6 b</a></p><p id="681a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><strong class="il hj"> <em class="ik">项目编码:</em> </strong></p><p id="71c0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jv iv iw ix jw iz ja jb jx jd je jf jg hb bi translated"><a class="ae kh" href="https://github.com/Krutarth08/Realtime_Object_Detection" rel="noopener ugc nofollow" target="_blank">https://github.com/Krutarth08/Realtime_Object_Detection</a></p></div></div>    
</body>
</html>