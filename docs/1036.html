<html>
<head>
<title>NLP Zero to One : Sparse Document Representations (Part 2/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 零到一:稀疏文档表示(第 2/30 部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-sparse-document-representations-part-2-30-d7ce30b96d63?source=collection_archive---------9-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-sparse-document-representations-part-2-30-d7ce30b96d63?source=collection_archive---------9-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="d5c3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">词汇袋和 TFIDF</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/5a158a965ec37ebe4d328d8c840a8898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INbeLwgYDUwS4ddzsGgR5w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="953d" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="dace" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">NLP 工程师经常处理文档或文本的语料库。原始文本不能直接输入机器学习算法。开发一些方法以计算机/算法理解的方式表示这些文档是非常重要的，例如数字向量。这些方法也被称为特征提取方法或特征编码。在这个博客中，我们将学习和实现 3 个非常重要的特征提取方法。</p><h1 id="706d" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">1.词汇袋</h1><p id="b391" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">这是非常灵活、直观和最容易的特征提取方法。文本/句子被表示为唯一单词的计数列表，因此这种方法也被称为计数矢量化。为了对我们的文档进行矢量化，我们所要做的就是计算每个单词出现的次数。</p><p id="6158" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">因为词袋模型基于出现来加权词。实际上，像“是”、“该”、“和”这样最常见的词没有任何价值。在计数矢量化之前，停用词(在本系列的博客中介绍过)被移除。</p><h2 id="a3ce" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">例子..</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lu"><img src="../Images/bb87cf7effda02f010ce1abb4d70f96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*cyMePM4ZBa28GN0vtzcoRw.png"/></div></div></figure><p id="4654" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">词汇是这些文档中唯一单词的总数。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="1f6f" class="lg jo hi lw b fi ma mb l mc md">Vocabulary: [‘dog’, ‘a’, ‘live’, ‘in’, ‘home’, ‘hut’, ‘the’, ‘is’]</span></pre><h2 id="d0e2" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">密码</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/164f9509e5e2fe109304bdc79dd7f500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*MXDrdGHolwrgjQx4_-l7SA.png"/></div></figure><h2 id="1cb1" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">缺点..</h2><p id="04fa" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">该模型只关心已知单词是否出现在文档中，而不关心它在文档中的位置。显然，当文档中的单词的顺序或结构被丢弃时，通过简单地使用文档向量来表示整个文档会有显著的信息损失，但是这对于许多计算语言学应用来说已经足够了。它在计算上更简单，并且在定位或上下文信息不相关时被积极地使用。</p><h1 id="db3f" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">2.TF-IDF(术语频率-逆文档频率)</h1><p id="de28" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">TFI-DF 是一种方法，它提供了一种给予更少的单词更大权重的方式。为了更好地理解，我们将尝试分别理解 TF-IDF 中的两个定义。</p><h2 id="c88c" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">词频:tf(t，d)</h2><p id="6225" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">这总结了给定单词在文档中出现的频率。它衡量一个单词在文档中出现的频率。有两种流行的方法来表示这一点。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="9e10" class="lg jo hi lw b fi ma mb l mc md">1. Term frequency adjusted for document length: <strong class="lw hj">tf(<em class="me">t</em>,<em class="me">d</em>) = ( number of times term t appear in document d )÷ (number of words in d)</strong></span><span id="f914" class="lg jo hi lw b fi mf mb l mc md">2. logarithmically scaled frequency: <strong class="lw hj">tf(<em class="me">t</em>,<em class="me">d</em>) = log (1 + number of times term t appear in document d )</strong></span></pre><h2 id="a518" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">例子..</h2><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="b0fe" class="lg jo hi lw b fi ma mb l mc md">doc1 = ‘a dog live in home’</span></pre><blockquote class="mg mh mi"><p id="ac6d" class="kf kg me kh b ki lb ij kk kl lc im kn mj ld kq kr mk le ku kv ml lf ky kz la hb bi translated"><strong class="kh hj"> tf( </strong> <em class="hi">狗，doc1 </em> <strong class="kh hj"> ) = </strong> 1/5。<em class="hi">(按方法 1)<br/></em><strong class="kh hj">TF(</strong><em class="hi">dog，doc1</em><strong class="kh hj">)= 1+log(1)</strong><em class="hi">。(根据方法 2) </em></p></blockquote><h2 id="1743" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">反向文档频率:idf</h2><p id="fc46" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">IDF 是衡量术语重要性的指标。它是文档总数与带有术语 t 的文档数的对数比例。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mm"><img src="../Images/1f4517edadd2a122560af425ac946e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*bsmF8bvqoA2Ougqa_Njkug.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:维基</figcaption></figure><p id="73c1" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated"><strong class="kh hj">分子:</strong>文档总数<br/> <strong class="kh hj">分母:</strong>带术语的文档总数</p><h2 id="f074" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">例子..</h2><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="76bc" class="lg jo hi lw b fi ma mb l mc md">D = [ ‘a <strong class="lw hj">dog</strong> live in home’, ‘a <strong class="lw hj">dog</strong> live in the hut’, ‘hut is <strong class="lw hj">dog</strong> home’ ]   <br/>D is the corpus</span></pre><blockquote class="mg mh mi"><p id="cb66" class="kf kg me kh b ki lb ij kk kl lc im kn mj ld kq kr mk le ku kv ml lf ky kz la hb bi translated"><strong class="kh hj"> idf( </strong> dog，D <strong class="kh hj"> ) </strong> = log(文档总数(3) /带有术语“dog”的文档总数(3) ) = log(3/3) = log(1) = 0</p></blockquote><h2 id="36b8" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">TFIDF: tf x idf</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mn"><img src="../Images/481b1a9204927749b25a63d0c2ac678c.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*cLAAYotAsYebjA_Mwu4rHg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:维基</figcaption></figure><p id="3b5d" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">我们现在可以计算文档中每个术语的 TF-IDF 分数。分数暗示了这个词的重要性。正如你在上面的例子中看到的。如果术语“dog”出现在所有文档中，则该单词的逆文档频率将为零，因此 TFIDF 分数将为零。这基本上意味着，如果同一个单词出现在所有文档中，那么它就没有相关性。</p><h2 id="56aa" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">密码..</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mo"><img src="../Images/2fb1f1617fdde653bbabcb9ae877e0df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPyf8-iYxapAlDwSL7Sc_Q.png"/></div></div></figure><h2 id="eefe" class="lg jo hi bd jp lh li lj jt lk ll lm jx ko ln lo jz ks lp lq kb kw lr ls kd lt bi translated">缺点..</h2><p id="c68e" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">TF-IDF 使得特征提取比仅计数词袋模型中呈现的文档中的术语的实例数量更健壮。但是它没有解决 BoW 模型的主要缺点，TF-IDF 模型仍然抛弃了文档中单词的顺序或结构。</p><h1 id="3db1" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">注意:</h1><p id="1f3f" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated"><strong class="kh hj"> <em class="me">稀疏:</em> </strong>由于大多数文档通常会使用语料库中使用的单词的非常小的子集，因此得到的矩阵将具有许多为零的特征值(通常超过 99%)。NLP 实践者通常应用主成分分析(PCA)来降低维数。<br/> <strong class="kh hj">朴素贝叶斯模型:</strong>一种过度简化的假设模型，朴素贝叶斯分类器在许多现实世界中工作得相当好，著名的有<strong class="kh hj"> BoW </strong>模型或<strong class="kh hj"> TF-IDF </strong>的文档分类</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="763c" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">上一篇:<a class="ae mq" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-basics-part-1-30-35c3f6bc7097?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零对一:基础知识(Part 1/30) </strong> </a> <br/>下一篇:<a class="ae mq" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-deep-learning-theory-basics-part-3-30-baa8cbbe271d?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零对一:深度学习理论基础知识(Part 3/30) </strong> </a></p></div></div>    
</body>
</html>