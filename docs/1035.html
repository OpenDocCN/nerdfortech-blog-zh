<html>
<head>
<title>NLP Zero to One: Basics (Part 1/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 零到一:基础(第 1/30 部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-basics-part-1-30-35c3f6bc7097?source=collection_archive---------8-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-basics-part-1-30-35c3f6bc7097?source=collection_archive---------8-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6117" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">自然语言和计算语言学。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/0b2cce41e23108671ca76b8aec688add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rqYkFqvQdjcpRYLQJsHXmg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="76bd" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="4ca3" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated"><em class="lb">自然语言处理</em> (NLP)是计算方法的应用，不仅从文本中提取信息，还在此基础上模拟不同的应用。所有基于语言的文本都有系统的结构或规则，这通常被称为形态学，例如“jump”的过去式总是“jumped”。对人类来说，这种形态学上的理解是显而易见的。</p><p id="a473" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">在这篇介绍性的 NLP 博客中，我们将看到不同的方法来确定语言的形态结构和规则。</p><h1 id="bea2" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">标记化</h1><p id="1f57" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">将文本分割成相关单词的任务称为标记化。</p><p id="1be1" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">在最简单的形式中，可以通过使用空白分割文本来实现标记化。NLTK 提供了一个名为<em class="lb"> word_tokenize() </em>的函数，用于将字符串拆分成令牌。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="ce08" class="lm jo hi li b fi ln lo l lp lq">text = 'we will look into the core components that are relevant to language in computational linguistics'</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/9fd497a8282fbdd1efe5c50042f2d711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5yG2ijC8bAKFU0kMItvVA.png"/></div></div></figure><p id="cf6a" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">但是简单的符号化并不是一直都有效。在单词之间包含标点符号的复杂单词的情况下(例如:what's)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ls"><img src="../Images/bb171476270fdbedde8fe9a479dd2f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjwtKUBle-FaNgx8InpkTw.png"/></div></div></figure><p id="6618" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">如果我们想保留带标点符号的单词，简单的方法是我们可以用空格把文本分成单词，不用任何标点符号。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/a0dadca286e7e3ac95c68dcc4d804831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yH1kiSx8izAyboWWKgF6aw.png"/></div></div></figure><h1 id="1218" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">词干和引理满足</h1><p id="9c5a" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">将每个单词还原到其词根的任务。例如，“Walk”是单词“Walks”、“Walking”、“Walked”的词根。通常词根可能比时态本身包含更多的意义。所以在 NLP 任务中，提取文本中单词的词根是非常重要的。</p><p id="a908" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">词干有助于减少文档中的词汇，从而节省大量计算。同样，在分类这样的任务中，一旦使用词干，单词的时态就变得无关紧要了。</p><p id="7025" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">最流行的方法是<em class="lb">波特词干算法</em>。它是一种后缀剥离算法，不依赖于由词形变化和词根关系组成的查找表。为提取词根建立了一些简单的规则。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/25aa282358aa7019a416ce7df2385e08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQn5jj9FBUQimMx1ZfJjkA.png"/></div></div></figure><p id="b399" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">词干匹配与词干匹配非常相似，因为它删除了词尾变化和后缀来将单词转换成它们的词根。意义和上下文可能在词干分析中丢失，词条满足保留了上下文。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/acd40039347aa3c325f74d01cb578fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DrJUsQEzJn6GabLZu8tNfw.png"/></div></div></figure><h2 id="d1bf" class="lm jo hi bd jp lu lv lw jt lx ly lz jx ko ma mb jz ks mc md kb kw me mf kd mg bi translated">其他重要..</h2><p id="2acc" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated"><strong class="kh hj">规范化大小写:</strong>将所有单词转换成一个大小写是很常见的。<br/> <strong class="kh hj">停用词:</strong>停用词是在对文本数据进行提取/建模的过程中不起作用的那些词，因为它们是最常见的词，例如:“<em class="lb"> the </em>”、“<em class="lb"> a </em>”、“<em class="lb"> is </em>”。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/41009338db1ffe74c7d5212831a1e7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LubckEXkOXqNmt7na_ISLw.png"/></div></div></figure><h1 id="d13b" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">注意</h1><p id="5228" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated"><strong class="kh hj">数据清洗:</strong>在对文本数据应用复杂的计算方法之前，我们需要理解和清洗数据。这些技术帮助我们用先进的 DNN 和自然语言处理技术为文本建模做好准备。</p><p id="0b7f" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">接下来:<a class="ae mh" href="https://kowshikchilamkurthy.medium.com/nlp-theory-and-code-sparse-document-representations-part-2-20-bdbd89c567a" rel="noopener"> NLP 理论和代码:稀疏文档表示(第 2/40 部分)</a></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="2d1b" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><strong class="kh hj">下一个</strong>:</p><p id="9905" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><a class="ae mh" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-sparse-document-representations-part-2-30-d7ce30b96d63?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零对一:稀疏文档表示(第 2/30 部分)</strong> </a></p></div></div>    
</body>
</html>