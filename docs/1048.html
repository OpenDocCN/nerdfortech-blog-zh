<html>
<head>
<title>NLP Zero to One: Transformers (Part 13/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 从零到一:变压器(第 13/30 部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-transformers-part-13-30-5cd5a3ddd93b?source=collection_archive---------21-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-transformers-part-13-30-5cd5a3ddd93b?source=collection_archive---------21-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a367" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">标度点积注意力，多头自我注意力，</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/5b5ffebd095245a2080f8499166a82ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29A5XGFJEsZHhs2p0mb0Tg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="4f1c" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="ff00" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">Transformers 是一种新颖的神经架构，被证明是最近在机器学习翻译方面取得的成功。像编码器-解码器模型一样，Transformer 是一种使用编码器和解码器将一个序列转换成另一个序列的架构。与之前基于 RNN 的序列到序列模型的不同之处在于 transformer 不使用任何递归网络(GRU、LSTM 等)。)既不是编码器也不是解码器。因此，变压器消除了在编码器和解码器网络中使用 RNN 连接的需要。</p><p id="d80b" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">transformer 的想法是，不使用 RNN 来累积内存，而是直接在输入序列上使用<strong class="kh hj">多头注意力</strong>。这使得它类似于允许并行执行计算的前馈网络。只有不含 RNN 的注意机制才能显著提高翻译任务和其他任务的成绩。</p><h1 id="2bac" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">自我关注..</h1><p id="f333" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">自我注意的想法是我们想要模拟序列中的每个单词是如何被给定序列中的所有其他单词影响的。所以这个想法是得到自我关注权重"<strong class="kh hj"><em class="lg"/></strong><em class="lg"/>，它告诉我们序列中的单词是如何受到序列中所有其他单词的影响的。设感兴趣的词用 Q 表示，其他词依次用 k 表示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lh"><img src="../Images/54e6e49b21e1211ed22d89fbcc6cfca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*5HtNUlK8irB6MR8iKi8osw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">自我关注权重，由作者生成</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/06c2cb7e1ba84fcd2ac437f5ad9ab337.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*YMx7RO8nMn0H8FxMWlDWzA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">标度点积注意力</figcaption></figure><p id="9cf9" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">SoftMax 函数被应用于自我关注权重"<strong class="kh hj"><em class="lg"/></strong><em class="lg">"</em>以具有 0 和 1 之间的分布。然后，将计算出的自我关注权重“<strong class="kh hj"><em class="lg">”</em></strong><em class="lg">”</em>应用于表示为“<strong class="kh hj"> V </strong>的序列中的所有单词。</p><h1 id="98b0" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">多头自我关注..</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lj"><img src="../Images/acf6df1b9d46515264515a676d53d69f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KXXUKLynFkaNEyg87Du30A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">左:比例点积注意，右:多头注意，Ref: [1]</figcaption></figure><p id="a184" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">这种自我注意机制被并行成多个并排的机制，如右图所示。自我注意机制通过 Q、K 和 v 的线性投影重复多次。这种自我注意的并行版本被称为多头自我注意。这允许系统从 Q、K 和 V 的不同表示中学习，这对模型是有益的。这些线性表示是通过将 Q、K 和 V 乘以在训练期间学习的权重矩阵 W 来完成的[2]。</p><h1 id="c401" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">变形金刚(电影名)..</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lk"><img src="../Images/e6e397a5c6da34a833161937101f09d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*Iku3mgitQwvwZvH4_9nr4g.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">变压器架构</figcaption></figure><p id="54ad" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">在这一节中，让我们试着理解转换器架构。正如我们已经介绍过的它由注意机制组成，没有任何 RNN(递归神经网络)。编码器和解码器都由重复的多头自关注机制组成。这种多头自我注意机制的重复在图中被描述为<em class="lg"> Nx </em>。</p><p id="38f6" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">变压器架构中有几个重要的组件和细节。让我们简单讨论一下:</p><ol class=""><li id="edf3" class="ll lm hi kh b ki lb kl lc ko ln ks lo kw lp la lq lr ls lt bi translated"><strong class="kh hj">位置编码:</strong>我们不能直接使用字符串，所以输入和输出序列被输入到一个嵌入层，并把序列转换成一个 n 维空间。</li><li id="eea8" class="ll lm hi kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated"><strong class="kh hj">输出(右移):</strong>理解我们为什么需要将输出右移非常重要。我们将需要有很多句子对，我们可以开始训练我们的模型。假设我们想从英语翻译成泰卢固语。我们的编码输入将是一个英语句子，解码器的输入将是一个泰卢固语句子<strong class="kh hj">右移</strong>。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/0408244f05c54bac79c6583901a18ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WsU-csUhQqP0x5jc6L00g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">比较基于 RNN 和基于变压器的 E-D 模型的解码器设置，由作者生成</figcaption></figure><p id="2332" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">在基于 RNN 的编码器-解码器中，我们不必给出移位的输出语句，该模型简单地使用时间步长“<strong class="kh hj">t”</strong>的预测作为时间步长“<strong class="kh hj">t+1”</strong>的解码器的输入。如果你观察到这种转变已经在基于 RNN 的编码器-解码器内部发生。<br/>在变压器中，我们采用顺序模型，因此在时间步长“<strong class="kh hj"> t </strong>”时解码器的输入应独立于时间步长“<strong class="kh hj"> t-1”时发生的情况。</strong>因此，通过将解码器输入移动一个位置，我们的模型需要预测位置<em class="lg"> i. </em>的目标单词/字符</p><p id="8282" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated"><strong class="kh hj"> 3。预测:</strong>预测与我们训练模型的方式略有不同，因为预测时我们不会有输出序列。显然，我们将不得不使用解码器的输出作为每一步，并使用它作为解码器的输入。我们需要多次运行解码器来完成输出序列的预测。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="eb3c" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">上一篇:<a class="ae mb" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-attention-mechanism-part-12-30-c5c36670c81f?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零对一:注意机制(第 12/30 部分)</strong> </a></p><p id="a103" class="pw-post-body-paragraph kf kg hi kh b ki lb ij kk kl lc im kn ko ld kq kr ks le ku kv kw lf ky kz la hb bi translated">接下来:<a class="ae mb" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-bert-part-14-40-691ef069712f?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP 零对一:BERT (Part 14/40) </strong> </a></p></div></div>    
</body>
</html>