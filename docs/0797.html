<html>
<head>
<title>Optimizers in Neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络优化器</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/optimizers-in-neural-networks-b962a233051e?source=collection_archive---------8-----------------------#2021-02-14">https://medium.com/nerd-for-tech/optimizers-in-neural-networks-b962a233051e?source=collection_archive---------8-----------------------#2021-02-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/67099398a249f07d3ca8bbb45605dc35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gy3ogGtzUednSmRF8Ud6Eg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">尼古拉斯·瑞恩在<a class="ae hv" href="https://unsplash.com/s/photos/slope?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><p id="19ab" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">优化器是用于修改属性(如权重和学习率)以最小化损失的方法/算法。</p><h2 id="8952" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">基于梯度</h2><p id="f894" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated"><strong class="ix hz">批量梯度下降</strong> —回归分类&amp;</p><p id="285e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它计算损失函数相对于整个训练数据集的参数的梯度。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="fbb1" class="jt ju hy ky b fi lc ld l le lf">for i in range(epochs):<br/>    param_gradient = evaluate_gradient(loss_function, data, params)<br/>    params = params - learning_rate * param_gradient</span></pre><blockquote class="lg lh li"><p id="a4da" class="iv iw lj ix b iy iz ja jb jc jd je jf lk jh ji jj ll jl jm jn lm jp jq jr js hb bi translated">缓慢的<br/>计算繁重的<br/>内存密集型</p></blockquote><p id="a4d8" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">随机梯度下降</strong></p><p id="44b1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在 SGD 中，每个训练示例和标签都会发生参数更新。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a7ca" class="jt ju hy ky b fi lc ld l le lf">for i in range(epochs):<br/>    np.random.shuffle(data)<br/>    for sample in data:<br/>        params_gradient = evaluate_gradient(loss_function, sample, params)<br/>        params = params - learning_rate * params_gradient</span></pre><blockquote class="lg lh li"><p id="817f" class="iv iw lj ix b iy iz ja jb jc jd je jf lk jh ji jj ll jl jm jn lm jp jq jr js hb bi translated">快速<br/>高方差<br/>内存高效</p></blockquote><p id="9b0c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">小批量新币</strong></p><p id="19d6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">小批量梯度下降结合了批量和 SGD 的优点。在这种情况下，参数更新发生在 n 个训练样本的每个小批量(32，64…通常是 2 的幂)中。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="9e76" class="jt ju hy ky b fi lc ld l le lf">for i in range(epochs):<br/>    np.random.shuffle(data)<br/>    for batch in get_batches(data, batch_size=64):<br/>       params_gradient = evaluate_gradient(loss_function, batch, params)<br/>       params = params - learning_rate * params_gradient</span></pre><blockquote class="lg lh li"><p id="1829" class="iv iw lj ix b iy iz ja jb jc jd je jf lk jh ji jj ll jl jm jn lm jp jq jr js hb bi translated">快速<br/>较小方差<br/>内存效率高</p></blockquote><p id="e7b2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上述三个优化器中，我们有一个恒定的学习率。这些学习率也会影响表现。下图总结了不同值的学习率和损失之间的关系。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es ln"><img src="../Images/2b968d15fdc741649e4eef5c14370cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*roEyfDy-ts8_SwyXV_f_Gw.png"/></div></figure><p id="1882" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以有一些优化器可以动态调整学习速度。让我们讨论这些。</p><h2 id="0440" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">基于动量</h2><p id="f3ea" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">动量是一种有助于在相关方向加速 SGD 并抑制振荡的方法。</p><p id="869d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> Adagrad(自适应梯度算法)</strong></p><p id="6923" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">学习率与参数的所有先前梯度的平方和成反比。因此，较大的过去梯度值将导致较低的学习率。而如果过去的梯度和很小，学习率就会很高。</p><p id="ea3d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> RMSprop(均方根传播)</strong></p><p id="c720" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RMSprop 将学习率除以梯度平方的指数衰减平均值。[γ = 0.9，学习率= 0.001]</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="55ec" class="jt ju hy ky b fi lc ld l le lf">eps , gamma = 1e-8, 0.999</span><span id="da37" class="jt ju hy ky b fi lo ld l le lf">for epoch in range(epochs):   <br/>    <br/>    for batch in get_batches(data, batch_size=64):<br/>        params_gradient = evaluate_gradient(loss_function, batch, params)<br/>        expected_grad = gamma * expected_grad + (1 - gamma) * np.square(params_gradient)<br/>        RMS_grad = np.sqrt(expected_grad + eps)<br/>        params = params -(eta/RMS_grad) * grad</span></pre><p id="edf1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">亚当(自适应矩估计)</strong></p><p id="5b6f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为每个网络权重保持一个学习速率，并随着学习的进行而调整。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="005e" class="jt ju hy ky b fi lc ld l le lf">alpha, beta1, beta2, epsilon  = 0.01, 0.9, 0.999, 1e-8<br/>m_t, v_t, t = 0, 0, 0</span><span id="13bb" class="jt ju hy ky b fi lo ld l le lf">while (1):<br/> t+=1<br/> param_gradient = evaluate_gradient(loss_function, data, params)<br/> m_t = beta1*m_t + (1-beta1)*param_gradient<br/> v_t = beta2*v_t + (1-beta2)*(param_gradient**2)<br/> m_cap = m_t/(1-(beta1**t))<br/> v_cap = v_t/(1-(beta2**t))       <br/> params = params - (alpha*m_cap)/(math.sqrt(v_cap)+epsilon)</span></pre><blockquote class="lg lh li"><p id="ab68" class="iv iw lj ix b iy iz ja jb jc jd je jf lk jh ji jj ll jl jm jn lm jp jq jr js hb bi translated">计算量小内存效率高超级参数几乎不需要调整。</p></blockquote></div></div>    
</body>
</html>