<html>
<head>
<title>Encoder-Decoder model for Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器翻译的编解码模型</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/encoder-decoder-model-for-machine-translation-8a90be12ac32?source=collection_archive---------2-----------------------#2021-02-18">https://medium.com/nerd-for-tech/encoder-decoder-model-for-machine-translation-8a90be12ac32?source=collection_archive---------2-----------------------#2021-02-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0a8c273598cef770e2afb353aa71d1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8cAuFh6z3drWqa_a.png"/></div></div></figure><p id="257b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我将尝试解释序列到序列模型，即编码器-解码器。最初，这个模型是为机器翻译开发的，但后来它也用于许多其他应用，如文本摘要、问答和视频字幕等。</p><p id="1ca0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">读者应该知道神经网络，激活函数，损失函数，优化，RNN，LSTM，GRU，双向 LSTM。如果你对<a class="ae jo" href="https://jaimin-ml2001.medium.com/understanding-rnn-91d548c86ac9?source=your_stories_page-------------------------------------" rel="noopener"> RNN </a>、<a class="ae jo" href="https://jaimin-ml2001.medium.com/what-is-lstm-peephole-lstm-and-gru-77470d84954b?source=your_stories_page-------------------------------------" rel="noopener"> LSTM、GRU </a>或<a class="ae jo" href="https://jaimin-ml2001.medium.com/what-does-it-mean-by-bidirectional-lstm-63d6838e34d9?source=your_stories_page-------------------------------------" rel="noopener">双向 LSTM </a>不了解，可以看看我以前的文章。</p><p id="e3b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">序列到序列模型可用于以下应用。</p><ol class=""><li id="fbd7" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated">机器翻译:在 2016 年发布的一篇<a class="ae jo" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中，谷歌提到了序列对序列是如何超越机器翻译中所有以前的方法的</li><li id="0f36" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">语音识别:同样的模型可以用于语音识别，谷歌在另一篇<a class="ae jo" href="https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0233.PDF" rel="noopener ugc nofollow" target="_blank">论文</a>中提到过。</li><li id="1741" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">图像字幕:-2015 年，谷歌提到了一篇研究论文<a class="ae jo" href="https://arxiv.org/pdf/1505.00487.pdf" rel="noopener ugc nofollow" target="_blank"/>在图像和视频字幕中实现了序列对序列模型。</li></ol><p id="a613" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编解码器架构</strong></p><p id="e8c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在 2014 年<a class="ae jo" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">的一篇研究论文中</a> google 提到了序列对序列模型以及它如何用于不同大小的输入和输出数据。例如，如果我们想将英语单词转换成印地语，我们可以看到输入了 4 个单词，但输出了 6 个单词。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kd"><img src="../Images/34b486b00ef5881a17c5572c3ff4d0ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5hVlZQp5fP0dUxXikw46g.png"/></div></div></figure><p id="bf57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种问题中，传统的 LSTM 体系结构由于需要翻译的顺序而无法工作。这就是为解决这类问题引入序列对序列模型的原因。</p><p id="75ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器和解码器的结构包括三个部分。编码器、编码器矢量和解码器。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/00ada28920bc22439e0d6a49fbef4e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9AIZx8XZci2sHb_q.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated"><a class="ae jo" href="http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/" rel="noopener ugc nofollow" target="_blank">http://www . wild ml . com/2016/04/deep-learning-for-chatbots-part-1-introduction/</a></figcaption></figure><p id="6703" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编码器</strong></p><p id="dc8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器单元是简单的 RNN 单元(可以使用 LSTM 或 GRU 以获得更好的性能),它接受输入向量。在每一个时间戳，输入被视为单个字向量，但是输出不是在每一个状态被获取。每个编码器单元的输出被拒绝，并且内部状态被用于生成编码器向量。隐藏状态可以通过这个公式来计算。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/22d8d37b074ffbcd611f684db69be764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*clXXL8cX3MFnHDDg.png"/></div></div></figure><p id="212f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是应用于 t-1 处的隐藏状态 h 和 t 处的输入 x 的权重的简单计算。现在，让我们看看编码器的内部。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/745bab6f90fa617db35261fbfc03fe44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8X_VhM72epSFJ0gJpzkiIQ.png"/></div></div></figure><p id="4e20" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">LSTM/GRU 细胞一次接受一个输入，通过这种方式我们接受序列作为输入。X1，X2，X3 …Xm 是输入，Y1，Y2，Y3..Ym 是架构中显示的输出。而 h0，c0…hm，cm 是将被传送到解码器的内部状态。其中 h 是隐藏状态，c 是权重，两者都可以传送到解码器。根据需要，我们还可以在该架构中采用双向或堆叠式 LSTM。这里，我们不考虑输出 Y0…Ym，但我们将 htct 作为编码器向量，并将其传递给解码器。隐藏状态可以用下面的公式计算。它是权重和先前输出以及当前输入的简单相加。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/b02c6be7c7b3193d6fc695b7460a831c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b0yb2ix2ugq64WEl.png"/></div></div></figure><p id="4813" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们考虑和举例，以便更好地理解。这里我们以向量的形式传递文本(在 pic 中显示的是原始文本)。向量可以是嵌入、word2vec 或 one hot 表示的形式。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/4a0e9df0413e11a7140b3d86c0099f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z4Y9hOUkUkDG1Itg0fxkiQ.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">编码器的英文文本输入</figcaption></figure><p id="66d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，我们可以看到 X1 = I，X2 = like，X3 = NLP，X4 是句子的结尾。所有这些文本数据都以向量的形式提供给编码器输入。</p><p id="ba0d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编码器矢量</strong></p><ul class=""><li id="a017" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn kp jv jw jx bi translated">这是从模型的编码器部分产生的隐藏状态(h4)。如上图所示，并给出了公式。</li><li id="21f6" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn kp jv jw jx bi translated">这个向量按顺序存储信息。例如，根据 LSTM 的属性，它存储珍贵状态的信息，在 h4 状态，它封装所有输入元素的信息，以便帮助解码器做出准确的预测。</li></ul><p id="d2a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">解码器</strong></p><ul class=""><li id="cdda" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn kp jv jw jx bi translated">与编码器相同，解码器也由 RNN/LSTM 系列组成。这里我们在每个时间戳 t 取一个输出。</li><li id="db9d" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn kp jv jw jx bi translated">每个 RNN 单元用先前的输出 y_t-1 和隐藏状态 ht-1 产生输出 y_t。这可以用下面的公式计算</li></ul><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/692eea80a3fa893283bb91963e8e3d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OiiYKn1sVoZcgPY4.png"/></div></div></figure><ul class=""><li id="dc13" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn kp jv jw jx bi translated">用 ht 和 y_t-1 计算输出 y_t。所以我们可以用下面的公式计算 y_t。此外，正如在编码器解码器的<a class="ae jo" href="https://arxiv.org/pdf/1911.09886.pdf" rel="noopener ugc nofollow" target="_blank">研究论文中提到的，我们使用 softmax 函数来确定输出，因为这种预测是概率性的。</a></li></ul><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/97abc26b752b52eaac03caf1f7d20b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BHazBCeNZTZNWDUq.png"/></div></div></figure><p id="6868" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">继续上面提到的例子，嵌入向量作为解码器的输入给出。为了理解句子的开始和结束，我们将在训练数据集中在句子的开始处添加 start_ 并在句子的结束处添加 _end。所以我们的训练数据会是这样的。</p><p id="bee0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">X = "我爱 NLP "</p><p id="d11b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Y = "开始 _मुझेएनएलपीपसंदहै_ 结束"</p><p id="0953" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">视觉上，这可以显示如下。作为编码器向量的输入和隐藏状态被提供给解码器输入。解码器从 Start_ 开始解码句子的开头，并将句子转换成印地语。解码器将理解带有 _End 单词的句子的结尾。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/7117e68c0dc71a44fa455a64adb090bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ENhLYP1ktXK-0ZRgp0-SaQ.png"/></div></div></figure><p id="773e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在每个阶段，在实际和预测字<strong class="is hj"> y </strong>和<strong class="is hj"> ŷ.之间计算损失</strong>基于计算的损失，权重在反向传播中得到更新，最终句子将被转换成印地语。</p><p id="fdd2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">用 Keras 实现编码器解码器</strong></p><p id="156b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我已经从<a class="ae jo" href="https://www.cfilt.iitb.ac.in/~parallelcorp/iitb_en_hi_parallel/" rel="noopener ugc nofollow" target="_blank">这里</a>获取了用于训练的数据集。代码可以在<a class="ae jo" href="https://github.com/Jaiminml/Encoder-Decoder.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="e2b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入所有需要的库</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="3e5f" class="ky kz hi ku b fi la lb l lc ld">import numpy as np</span><span id="324a" class="ky kz hi ku b fi le lb l lc ld">import tensorflow as tf</span><span id="1c3e" class="ky kz hi ku b fi le lb l lc ld">from tensorflow import keras</span><span id="8365" class="ky kz hi ku b fi le lb l lc ld">import pandas as pd<br/>batch_size = 64 # Batch size for training.</span><span id="6bff" class="ky kz hi ku b fi le lb l lc ld">epochs = 100 # Number of epochs to train for.</span><span id="422a" class="ky kz hi ku b fi le lb l lc ld">latent_dim = 256 # Latent dimensionality of the encoding space.</span><span id="1476" class="ky kz hi ku b fi le lb l lc ld">num_samples = 10000 # Number of samples to train on.</span><span id="a861" class="ky kz hi ku b fi le lb l lc ld"># Path to the data txt file on disk.</span></pre><p id="2a1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入印地语和英语句子的数据集</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="896a" class="ky kz hi ku b fi la lb l lc ld">hindi_sen = open('IITB.en-hi.hi').read().split("\n")[:-1]</span><span id="6d8b" class="ky kz hi ku b fi le lb l lc ld">eng_sen = open('IITB.en-hi.en').read().split("\n")[:-1]</span></pre><p id="d68f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建一组输入和目标字符，其中我们将采用在印地语和英语句子中使用独特字符。</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="b932" class="ky kz hi ku b fi la lb l lc ld"># Vectorize the data.</span><span id="121d" class="ky kz hi ku b fi le lb l lc ld">input_characters = set()</span><span id="5d60" class="ky kz hi ku b fi le lb l lc ld">target_characters = set()</span><span id="0461" class="ky kz hi ku b fi le lb l lc ld">for char in input_text:</span><span id="1f1d" class="ky kz hi ku b fi le lb l lc ld">    for chars in char:</span><span id="4e11" class="ky kz hi ku b fi le lb l lc ld">       if chars not in input_characters:</span><span id="33db" class="ky kz hi ku b fi le lb l lc ld">           input_characters.add(chars)</span><span id="6e45" class="ky kz hi ku b fi le lb l lc ld">for char in target_text:</span><span id="269d" class="ky kz hi ku b fi le lb l lc ld">    for chars in char:</span><span id="db37" class="ky kz hi ku b fi le lb l lc ld">       if chars not in target_characters:</span><span id="cdef" class="ky kz hi ku b fi le lb l lc ld">          target_characters.add(chars)</span><span id="bb0e" class="ky kz hi ku b fi le lb l lc ld">input_characters = sorted(list(input_characters))</span><span id="ad94" class="ky kz hi ku b fi le lb l lc ld">target_characters = sorted(list(target_characters))</span><span id="5b24" class="ky kz hi ku b fi le lb l lc ld">num_encoder_tokens = len(input_characters)</span><span id="f087" class="ky kz hi ku b fi le lb l lc ld">num_decoder_tokens = len(target_characters)</span><span id="d646" class="ky kz hi ku b fi le lb l lc ld">max_encoder_seq_length = max([len(txt) for txt in input_text])</span><span id="6bb2" class="ky kz hi ku b fi le lb l lc ld">max_decoder_seq_length = max([len(txt) for txt in target_text])</span><span id="170a" class="ky kz hi ku b fi le lb l lc ld">print(“Number of samples:”, len(input_text))</span><span id="f154" class="ky kz hi ku b fi le lb l lc ld">print(“Number of unique input tokens:”, num_encoder_tokens)</span><span id="1e7a" class="ky kz hi ku b fi le lb l lc ld">print(“Number of unique output tokens:”, num_decoder_tokens)</span><span id="4776" class="ky kz hi ku b fi le lb l lc ld">print(“Max sequence length for inputs:”, max_encoder_seq_length)</span><span id="2681" class="ky kz hi ku b fi le lb l lc ld">print(“Max sequence length for outputs:”, max_decoder_seq_length)</span></pre><p id="bb79" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出将是这样的，在这方面，我们将采取的句子数量，最大句子长度和输入/输出令牌，其中将包含唯一的字母。</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="e6d6" class="ky kz hi ku b fi la lb l lc ld">Number of samples: 10000<br/>Number of unique input tokens: 93<br/>Number of unique output tokens: 154<br/>Max sequence length for inputs: 233<br/>Max sequence length for outputs: 346</span></pre><p id="d1dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，将数据转换成字典，字典将从独特的字符中提取数据，并给它一个数值。</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="6b10" class="ky kz hi ku b fi la lb l lc ld">input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])</span><span id="6afe" class="ky kz hi ku b fi le lb l lc ld">target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])</span></pre><p id="0adb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出应该是这样的。</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="3629" class="ky kz hi ku b fi la lb l lc ld">‘A’: 31, ‘B’: 32, ‘C’: 33, ‘D’: 34, ‘E’: 35, ‘F’: 36, ‘G’: 37, ‘H’: 38, ‘I’: 39, ‘J’: 40, ‘K’: 41, ‘L’: 42, ‘M’: 43, ’N’: 44,</span></pre><p id="feff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下一步是嵌入数据。该步骤将获取编码器输入、解码器输入和解码器输出数据。我们将根据输入和输出序列长度来确定维度。此外，数据将被热编码，并在值不存在的地方用 0 填充。</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="02da" class="ky kz hi ku b fi la lb l lc ld">encoder_input_data = np.zeros(</span><span id="9432" class="ky kz hi ku b fi le lb l lc ld">(len(input_text), max_encoder_seq_length, num_encoder_tokens), dtype="float32"</span><span id="ff0d" class="ky kz hi ku b fi le lb l lc ld">)</span><span id="aaa1" class="ky kz hi ku b fi le lb l lc ld">decoder_input_data = np.zeros(</span><span id="a2b1" class="ky kz hi ku b fi le lb l lc ld">(len(input_text), max_decoder_seq_length, num_decoder_tokens), dtype="float32"</span><span id="2233" class="ky kz hi ku b fi le lb l lc ld">)</span><span id="0242" class="ky kz hi ku b fi le lb l lc ld">decoder_target_data = np.zeros(</span><span id="cb0d" class="ky kz hi ku b fi le lb l lc ld">(len(input_text), max_decoder_seq_length, num_decoder_tokens), dtype="float32"</span><span id="9e22" class="ky kz hi ku b fi le lb l lc ld">)</span><span id="d560" class="ky kz hi ku b fi le lb l lc ld">for i, (input_texts, target_texts) in enumerate(zip(input_text, target_text)):</span><span id="994a" class="ky kz hi ku b fi le lb l lc ld">  for t, char in enumerate(input_texts):</span><span id="3187" class="ky kz hi ku b fi le lb l lc ld">    encoder_input_data[i, t, input_token_index[char]] = 1.0</span><span id="3ef9" class="ky kz hi ku b fi le lb l lc ld">  encoder_input_data[i, t + 1 :, input_token_index[" "]] = 1.0</span><span id="9e98" class="ky kz hi ku b fi le lb l lc ld">  for t, char in enumerate(target_texts):</span><span id="84b6" class="ky kz hi ku b fi le lb l lc ld"># decoder_target_data is ahead of decoder_input_data by one timestep</span><span id="790d" class="ky kz hi ku b fi le lb l lc ld">     decoder_input_data[i, t, target_token_index[char]] = 1.0</span><span id="5d4d" class="ky kz hi ku b fi le lb l lc ld">     if t &gt; 0:</span><span id="f072" class="ky kz hi ku b fi le lb l lc ld"># decoder_target_data will be ahead by one timestep</span><span id="98fc" class="ky kz hi ku b fi le lb l lc ld"># and will not include the start character.</span><span id="baab" class="ky kz hi ku b fi le lb l lc ld">        decoder_target_data[i, t - 1, target_token_index[char]] = 1.0</span><span id="f693" class="ky kz hi ku b fi le lb l lc ld">   decoder_input_data[i, t + 1 :, target_token_index[" "]] = 1.0</span><span id="97d8" class="ky kz hi ku b fi le lb l lc ld">   decoder_target_data[i, t:, target_token_index[" "]] = 1.0</span></pre><p id="5b15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们将定义编码器和解码器模型。其中包含来自前一步骤的嵌入数据、LSTM 层和密集层。在这里，根据架构，我们不会考虑编码器上的输出，我们只会从编码器获取状态 h 和 c，并将其传递给解码器。</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="b801" class="ky kz hi ku b fi la lb l lc ld"># Define an input sequence and process it.</span><span id="8967" class="ky kz hi ku b fi le lb l lc ld">encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))</span><span id="a87b" class="ky kz hi ku b fi le lb l lc ld">encoder = keras.layers.LSTM(latent_dim, return_state=True)</span><span id="49d9" class="ky kz hi ku b fi le lb l lc ld">encoder_outputs, state_h, state_c = encoder(encoder_inputs)</span><span id="2926" class="ky kz hi ku b fi le lb l lc ld"># We discard `encoder_outputs` and only keep the states.</span><span id="f1c1" class="ky kz hi ku b fi le lb l lc ld">encoder_states = [state_h, state_c]</span><span id="154b" class="ky kz hi ku b fi le lb l lc ld"># Set up the decoder, using `encoder_states` as initial state.</span><span id="5b9d" class="ky kz hi ku b fi le lb l lc ld">decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))</span><span id="411d" class="ky kz hi ku b fi le lb l lc ld"># We set up our decoder to return full output sequences,</span><span id="63b0" class="ky kz hi ku b fi le lb l lc ld"># and to return internal states as well. We don't use the</span><span id="270a" class="ky kz hi ku b fi le lb l lc ld"># return states in the training model, but we will use them in inference.</span><span id="46fe" class="ky kz hi ku b fi le lb l lc ld">decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)</span><span id="4a3f" class="ky kz hi ku b fi le lb l lc ld">decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)</span><span id="f5f1" class="ky kz hi ku b fi le lb l lc ld">decoder_dense = keras.layers.Dense(num_decoder_tokens, activation="softmax")</span><span id="b5ab" class="ky kz hi ku b fi le lb l lc ld">decoder_outputs = decoder_dense(decoder_outputs)</span><span id="1d7d" class="ky kz hi ku b fi le lb l lc ld"># Define the model that will turn</span><span id="f529" class="ky kz hi ku b fi le lb l lc ld"># `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`</span><span id="84d1" class="ky kz hi ku b fi le lb l lc ld">model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)</span></pre><p id="4fb2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们可以训练模型了。这里的优化函数是 rmsprop，我已经训练了模型多达 100 个时期。</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="574e" class="ky kz hi ku b fi la lb l lc ld">model.compile(</span><span id="ff74" class="ky kz hi ku b fi le lb l lc ld">optimizer="rmsprop", loss="categorical_crossentropy", metrics=["accuracy"]</span><span id="f5c0" class="ky kz hi ku b fi le lb l lc ld">)</span><span id="52f6" class="ky kz hi ku b fi le lb l lc ld">model.fit(</span><span id="792b" class="ky kz hi ku b fi le lb l lc ld">[encoder_input_data, decoder_input_data],</span><span id="a286" class="ky kz hi ku b fi le lb l lc ld">decoder_target_data,</span><span id="12e3" class="ky kz hi ku b fi le lb l lc ld">batch_size=batch_size,</span><span id="c25b" class="ky kz hi ku b fi le lb l lc ld">epochs=epochs,</span><span id="b03b" class="ky kz hi ku b fi le lb l lc ld">validation_split=0.2,</span><span id="2b84" class="ky kz hi ku b fi le lb l lc ld">)</span><span id="4123" class="ky kz hi ku b fi le lb l lc ld"># Save model</span><span id="d801" class="ky kz hi ku b fi le lb l lc ld">model.save("s2s")</span></pre><p id="6fbd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练 100 个纪元后的结果是</p><pre class="ke kf kg kh fd kt ku kv kw aw kx bi"><span id="bd74" class="ky kz hi ku b fi la lb l lc ld">Epoch 93/100<br/>125/125 [==============================] - 11s 86ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.0399 - val_accuracy: 0.9903<br/>Epoch 94/100<br/>125/125 [==============================] - 11s 87ms/step - loss: 0.0232 - accuracy: 0.9928 - val_loss: 0.0399 - val_accuracy: 0.9903<br/>Epoch 95/100<br/>125/125 [==============================] - 11s 87ms/step - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.0400 - val_accuracy: 0.9904<br/>Epoch 96/100<br/>125/125 [==============================] - 11s 86ms/step - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.0397 - val_accuracy: 0.9902<br/>Epoch 97/100<br/>125/125 [==============================] - 11s 86ms/step - loss: 0.0228 - accuracy: 0.9929 - val_loss: 0.0392 - val_accuracy: 0.9905<br/>Epoch 98/100<br/>125/125 [==============================] - 11s 86ms/step - loss: 0.0230 - accuracy: 0.9929 - val_loss: 0.0421 - val_accuracy: 0.9897<br/>Epoch 99/100<br/>125/125 [==============================] - 11s 86ms/step - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.0397 - val_accuracy: 0.9904<br/>Epoch 100/100<br/>125/125 [==============================] - 11s 87ms/step - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.0391 - val_accuracy: 0.9906</span><span id="a703" class="ky kz hi ku b fi le lb l lc ld">WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.<br/>WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.INFO:tensorflow:Assets written to: s2s/assetsINFO:tensorflow:Assets written to: s2s/assets</span></pre><h1 id="3a82" class="lf kz hi bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">接下来是什么:</h1><p id="36d0" class="pw-post-body-paragraph iq ir hi is b it mc iv iw ix md iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">在本系列的下一部分，我们将把注意力用于英语到印地语的翻译。</p><p id="d9d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">参考文献</strong></p><ol class=""><li id="87bf" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated"><a class="ae jo" href="https://keras.io/examples/nlp/lstm_seq2seq/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/nlp/lstm_seq2seq/</a></li><li id="b9ef" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated"><a class="ae jo" href="https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72" rel="noopener" target="_blank">https://towards data science . com/NLP-sequence-to-sequence-networks-part-1-processing-text-data-d141a 5643 b 72</a></li><li id="de38" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated"><a class="ae jo" href="https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1" rel="noopener" target="_blank">https://towards data science . com/NLP-sequence-to-sequence-networks-part-2-seq 2 seq-model-encoder decoder-model-6c 22 e 29 FD 7 e 1</a></li></ol></div></div>    
</body>
</html>