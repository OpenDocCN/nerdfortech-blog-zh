<html>
<head>
<title>One-shot image classification by meta learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于元学习的一次性图像分类</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585?source=collection_archive---------8-----------------------#2021-04-10">https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585?source=collection_archive---------8-----------------------#2021-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="24f3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">简要背景</h2></div><p id="d92e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇博客是作为哥伦比亚大学Parijat Dube教授讲授的class COMSE6998实用深度学习系统性能系列研讨会的一部分发表的。我和我的朋友阿克塞尔查阅了4篇关于一次性学习的研究论文。在这篇博客中，我将总结其中的两篇论文。其他两篇论文可以参考他的<a class="ae jt" rel="noopener" href="/@ax2146/one-shot-learning-a23c29052d32">博客</a>。</p><h1 id="f254" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">介绍</h1><p id="b316" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">一次性学习是一种分类任务，其中模型必须通过仅查看每个类别的一个或几个训练示例来预测测试图像的类别。人类非常擅长一次性学习。年仅3岁的孩子只要看几个例子就能把猫和大象区分开来。我们能让机器以类似的方式学习吗？</p><p id="82c1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一次性学习在签名验证和人脸识别等任务中很有用，在这些任务中，我们有许多类，但每个类的示例很少。此外，在这些任务中，类的数量可以保持变化，并且每次遇到新的类时训练模型是不可行的。例如，假设我们在一家公司有100名员工，我们想建立一个面部识别算法。我们可以训练一个在输出层有100个神经元的CNN来完成这个任务。当一个新员工加入组织时会发生什么？我们将不得不用101个班级重新训练这个模型。</p><p id="600e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了在一次性学习中减轻这一点，我们建立了一个模型来生成图像的嵌入/表示，该嵌入/表示可用于测试期间的匹配，而不是建立一个模型来分类到一个确定的类别中。在这篇博文中，我们将会看到用于一次性学习任务的两种深度学习方法。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="a52e" class="ju jv hi bd jw jx ky jz ka kb kz kd ke io la ip kg ir lb is ki iu lc iv kk kl bi translated">预赛</h1><p id="ecb6" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">n路k-shot分类的<strong class="iz hj">任务是对来自n个不同类别</strong>的实例进行分类，并为每个类别提供分类器k个实例。</p><h2 id="be8d" class="ld jv hi bd jw le lf lg ka lh li lj ke jg lk ll kg jk lm ln ki jo lo lp kk lq bi translated">Omniglot数据集</h2><p id="19b5" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">这个数据集由50个不同的字母组成，每个字符有20个手工绘制的样本。我们将在这个博客中介绍的模型的性能将在这个数据集上进行测量。对于一次性任务，模型将在没有共同字母的集合上训练、验证和测试，以使结果不会有偏差。</p><figure class="ls lt lu lv fd lw er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es lr"><img src="../Images/a5a71db59d763cca289a4918f565af12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmQwbgVRVCD0FORYD8tIcA.png"/></div></div><figcaption class="md me et er es mf mg bd b be z dx translated">Omniglot数据集</figcaption></figure><h2 id="5963" class="ld jv hi bd jw le lf lg ka lh li lj ke jg lk ll kg jk lm ln ki jo lo lp kk lq bi translated">元学习</h2><p id="895d" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">在少镜头分类的元学习中，我们试图在训练模型时模拟推理过程，即在训练期间，我们将把小批量的一些图像视为训练图像，而将其他图像视为测试图像，基于所选测试图像的损失来进行参数优化。</p><p id="0bf1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们不会深入元学习的细节。<a class="ae jt" href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#training-in-the-same-way-as-testing" rel="noopener ugc nofollow" target="_blank">【5】</a>很好地概述了元学习的方法。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="2350" class="ju jv hi bd jw jx ky jz ka kb kz kd ke io la ip kg ir lb is ki iu lc iv kk kl bi translated">匹配网络</h1><p id="6d57" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">一次性分类模型的目标是学习训练图像的表示，通过计算相似性得分，可以利用该训练图像来对测试图像进行分类。Oriol Vinyals等人<a class="ae jt" href="https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>提出的匹配网络考虑了整个支持集(训练集)来计算两幅图像之间的关系得分和相似度。</p><p id="fb16" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型被训练成将作为图像和标签集合的支持集S = {x_i，y_i}映射到分类器，使得对于任何输入x和可能输出标签的向量y，分类器给出不同标签上的分布:</p><figure class="ls lt lu lv fd lw er es paragraph-image"><div class="er es mh"><img src="../Images/26341d705b6105b30b1a3b6b768e46d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/0*hK3EWoEDvJWq2VlQ"/></div></figure><figure class="ls lt lu lv fd lw er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mi"><img src="../Images/e14521f465623e7f6eb194177eafcfa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jreCxeXsAruMRMm6.png"/></div></div><figcaption class="md me et er es mf mg bd b be z dx translated">匹配网络架构</figcaption></figure><p id="d7b2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">S的每个例子被参数θ的相同函数g嵌入，同时输入经历其嵌入。输入x和支持集x_i的每个例子之间的相似性通过具有余弦相似性的注意力核来测量:</p><figure class="ls lt lu lv fd lw er es paragraph-image"><div class="er es mj"><img src="../Images/6ec876cfa76f9ab66c3250861d42515d.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/1*Crie-T2Gb4f_19X33WdHsA.gif"/></div></figure><p id="d173" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">概率分布将通过对各项求和来给出，就像上图中描述的那样。</p><h1 id="5877" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">匹配网络的嵌入</h1><p id="7800" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">作者提出了两种类型的匹配网络嵌入。</p><h2 id="8972" class="ld jv hi bd jw le lf lg ka lh li lj ke jg lk ll kg jk lm ln ki jo lo lp kk lq bi translated">简单嵌入</h2><p id="23b3" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">在简单版本中，嵌入函数是以单个数据样本作为输入的神经网络。我们可以设f=g。</p><h2 id="b21a" class="ld jv hi bd jw le lf lg ka lh li lj ke jg lk ll kg jk lm ln ki jo lo lp kk lq bi translated">全上下文嵌入</h2><p id="0cba" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">匹配网络模型进一步提出通过除了原始输入之外还将整个支持集S作为输入来增强嵌入函数，使得可以基于与其他支持样本的关系来调整所学习的嵌入。</p><ul class=""><li id="f118" class="mk ml hi iz b ja jb jd je jg mm jk mn jo mo js mp mq mr ms bi translated">gθ使用双向LSTM在整个支持集s的上下文中编码<strong class="iz hj"> x </strong> i</li><li id="e700" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js mp mq mr ms bi translated">fθ使用LSTM对测试样本<strong class="iz hj"> x </strong>进行编码，并关注支持集s的“读取”</li></ul><p id="3da8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">全上下文嵌入的工作原理:-</p><ol class=""><li id="c32b" class="mk ml hi iz b ja jb jd je jg mm jk mn jo mo js my mq mr ms bi translated">首先，测试样本通过CNN嵌入层，提取该图像的特征(f’(x))</li><li id="403f" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js my mq mr ms bi translated">然后，作为隐藏状态的一部分，利用支持集上的阅读注意力向量来训练LSTM</li><li id="2aba" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js my mq mr ms bi translated">如果我们做k步“读”，最终f(x，S)=h_k。</li></ol><figure class="ls lt lu lv fd lw er es paragraph-image"><div class="er es mz"><img src="../Images/30aafb5f15cb9a8cdaac2b484cd37c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*J5A4KdbJQawkVzHRkgWPUw.png"/></div><figcaption class="md me et er es mf mg bd b be z dx translated">全上下文嵌入</figcaption></figure><p id="0f75" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种嵌入方法被称为“全上下文嵌入(FCE)”。有趣的是，它确实有助于提高困难任务(mini ImageNet上的少数镜头分类)的性能，但对简单任务(Omniglot)没有影响。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="6bdf" class="ju jv hi bd jw jx ky jz ka kb kz kd ke io la ip kg ir lb is ki iu lc iv kk kl bi translated">记忆匹配网络</h1><p id="5d53" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">在2018年的一篇论文中<a class="ae jt" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Memory_Matching_Networks_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">【4】</a>Qi Cai和他的团队提出了记忆匹配网络，该网络使用记忆块来存储训练集后的信息，并利用这些记忆块来获得关系分数。</p><p id="6551" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面显示的工作流程解释了内存匹配网络。</p><figure class="ls lt lu lv fd lw er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es na"><img src="../Images/262d35e6999155d6ddc2d5e806d22ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*omlyD2jbbL4KFTuC"/></div></div><figcaption class="md me et er es mf mg bd b be z dx translated">内存匹配网络架构</figcaption></figure><ol class=""><li id="b300" class="mk ml hi iz b ja jb jd je jg mm jk mn jo mo js my mq mr ms bi translated">存储器模块具有读控制器和写控制器。每当新的图像被添加到训练集中时，我们使用写控制器来更新存储器模块。</li><li id="d5f6" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js my mq mr ms bi translated">作为上下文学习器，来自记忆槽的嵌入被输入到一个递归神经网络(RNNs)中，以<strong class="iz hj">为未标记图像</strong>预测CNNs的参数。</li><li id="e41b" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js my mq mr ms bi translated">支持集(g)的嵌入通过存储模块的读取控制器获得，并且未标记图像(f)的嵌入通过CNN层获得。</li><li id="ef08" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js my mq mr ms bi translated">使用g和f之间的相似性分数来预测标签</li></ol><p id="2560" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">MM网和匹配网的区别在于CNN参数的<strong class="iz hj">上下文学习</strong>使其对一次性任务更有用。</p><h2 id="6e8f" class="ld jv hi bd jw le lf lg ka lh li lj ke jg lk ll kg jk lm ln ki jo lo lp kk lq bi translated">内存匹配网络性能</h2><figure class="ls lt lu lv fd lw er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es nb"><img src="../Images/827a468cecf4c97affe9de9eb61114e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_ToLmrwc0IfJXBCH.png"/></div></div><figcaption class="md me et er es mf mg bd b be z dx translated">准确度五路一次分类</figcaption></figure><p id="44b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上表中，我们可以看到，内存匹配网络在复杂的mini Imagenet集上的性能优于关系网和匹配Met，但在简单的Omniglot集上的性能略差于关系网。因此，记忆匹配更适合处理困难的一次性任务。</p><h1 id="5cf1" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">结论</h1><ul class=""><li id="44e8" class="mk ml hi iz b ja km jd kn jg nc jk nd jo ne js mp mq mr ms bi translated">一次性学习是一个非常重要和活跃的研究领域。近年来出现了许多新技术。</li><li id="7e8a" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js mp mq mr ms bi translated">一次性学习还有很大的提升空间。所有这些技术在omniglot集合上都表现良好，但在更复杂的集合上表现不佳。此外，这些模型对其他变化也很敏感。例如，如果一幅图像中的人戴着帽子、围巾或眼镜，而另一幅图像中的人没有戴，则准确性会大大降低。</li><li id="2912" class="mk ml hi iz b ja mt jd mu jg mv jk mw jo mx js mp mq mr ms bi translated">最后，元学习方法(学会学习)带来了良好的泛化能力，对于一次性学习任务非常有用。</li></ul><h1 id="a9d0" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">参考</h1><p id="a579" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">[1]用于一次性图像识别的连体神经网络，Gregory Koch等人:【http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf T4】</p><p id="9728" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[2]一击学习的匹配网络，Oriol viny als &amp; al:<a class="ae jt" href="https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://proceedings . neur IPS . cc/paper/2016/file/90e 1357833654983612 FB 05 E3 EC 9148 c-paper . pdf</a></p><p id="90b4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[3]高光谱图像少镜头分类的深度关系网络，高魁良等:<a class="ae jt" href="https://www.mdpi.com/2072-4292/12/6/923/htm" rel="noopener ugc nofollow" target="_blank"/></p><p id="2e4c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[4]单镜头图像识别的内存匹配网络，齐蔡等:<a class="ae jt" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Memory_Matching_Networks_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ cvpr _ 2018/papers/Cai _ Memory _ Matching _ Networks _ CVPR _ 2018 _ paper . pdf</a></p><p id="e18f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[5]元学习:学会快速学习</p><p id="abd9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae jt" href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#training-in-the-same-way-as-testing" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/11/30/meta-learning . html # training-in-the-same-way-as-testing</a></p></div></div>    
</body>
</html>