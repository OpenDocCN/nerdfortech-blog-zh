<html>
<head>
<title>Linear discriminant analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性判别分析</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/linear-discriminant-analysis-c24d9729d3d2?source=collection_archive---------7-----------------------#2021-05-01">https://medium.com/nerd-for-tech/linear-discriminant-analysis-c24d9729d3d2?source=collection_archive---------7-----------------------#2021-05-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fc69" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">减少维度并增加类别分离</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/4ee6fdc9db78fa204867264cf3759e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*LUJGKVj75PZqA82Kw0svUw.png"/></div></figure><h1 id="cee6" class="jf jg hi bd jh ji jj jk jl jm jn jo jp io jq ip jr ir js is jt iu ju iv jv jw bi translated">介绍</h1><p id="e3ce" class="pw-post-body-paragraph jx jy hi jz b ka kb ij kc kd ke im kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">线性判别分析(LDA)是一种相当简单的方法，用于寻找特征的线性组合，这些特征区别性地表征相同类别中的成员，同时分离不同类别(<a class="ae kt" href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" rel="noopener ugc nofollow" target="_blank">来源</a>)。本教程简要介绍了使用LDA的动机，展示了如何计算LDA的步骤，并用python实现了计算。此处提供了示例<a class="ae kt" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/linear%20discriminant%20analysis.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="01f8" class="jf jg hi bd jh ji jj jk jl jm jn jo jp io jq ip jr ir js is jt iu ju iv jv jw bi translated">皱胃向左移</h1><p id="f17f" class="pw-post-body-paragraph jx jy hi jz b ka kb ij kc kd ke im kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">简而言之，LDA试图同时实现两件事:</p><ul class=""><li id="3012" class="ku kv hi jz b ka kw kd kx kg ky kk kz ko la ks lb lc ld le bi translated">尽可能将同一类别的样本分组(减少类别内差异)</li><li id="92c6" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">尽可能将不同类别的样本分开(增加类别间方差)</li></ul><p id="317f" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">该属性非常有用，因为它能够对数据进行聚类、对样本进行分类和/或降低样本的维数。另一种流行的降维算法主成分分析(PCA)只关心解释整个数据的尽可能多的方差，而不考虑任何关于类成员和类之间/类内方差的信息。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/899e6182e9c3214689a3e7218fbd367f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EziHrbB_KN3pyZTk.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated"><a class="ae kt" href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="79c5" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">从上图可以看出，PCA是以较低维度表示整个数据的一个很好的选择(例如，为了节省内存，找到大多数方差描述特征)，另一方面，LDA是有用的聚类/分类。</p><h2 id="ab2c" class="lw jg hi bd jh lx ly lz jl ma mb mc jp kg md me jr kk mf mg jt ko mh mi jv mj bi translated">参数学习</h2><p id="4d66" class="pw-post-body-paragraph jx jy hi jz b ka kb ij kc kd ke im kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">LDA模型由一个矩阵<code class="du mk ml mm mn b">v</code>组成，该矩阵将数据投影到具有最大类间分离和最小类内分离的低维空间中。为了计算<code class="du mk ml mm mn b">v</code>，我们需要(<a class="ae kt" href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener ugc nofollow" target="_blank">来源</a>):</p><ul class=""><li id="3301" class="ku kv hi jz b ka kw kd kx kg ky kk kz ko la ks lb lc ld le bi translated"><strong class="jz hj">计算类内和类间</strong> <a class="ae kt" href="https://en.wikipedia.org/wiki/Scatter_matrix" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hj">散布矩阵</strong> </a>。示例实现使用<code class="du mk ml mm mn b">pandas DataFrame</code>。类内散布矩阵捕获关于每个类内数据散布的信息。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="e7c9" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">类间散布矩阵旨在收集类之间如何散布的信息。它有类似于类内散布矩阵的python实现。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mo mp l"/></div></figure><ul class=""><li id="1adc" class="ku kv hi jz b ka kw kd kx kg ky kk kz ko la ks lb lc ld le bi translated"><strong class="jz hj">计算散布矩阵的特征向量和特征值。</strong>由于我们有关于类之间和类内部数据分布的信息，我们可以用它来找到一个矩阵<code class="du mk ml mm mn b">v</code>，使类之间分布最大化，类内分布最小化。我们需要最大化以下标准(<a class="ae kt" href="https://arxiv.org/pdf/1903.11240.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>):</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mq"><img src="../Images/840b54a06a874b529e4954a0eb727e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:110/1*enbcq-WNMv_oANRYu4WDPw.gif"/></div></figure><p id="38a6" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">为了找到这样的<code class="du mk ml mm mn b">v</code>,我们可以以下面的形式解决广义特征值问题(<a class="ae kt" href="https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec11lda.pdf" rel="noopener ugc nofollow" target="_blank">源</a>和<a class="ae kt" href="https://www.csd.uwo.ca/~oveksler/Courses/CS434a_541a/Lecture8.pdf" rel="noopener ugc nofollow" target="_blank">源</a>用于更详细的推导):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/8704ed579b58100bf5b68d4077d05a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/1*9xcTM5k6VL3fOEep7tHNFQ.gif"/></div></figure><p id="885e" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">其中:</p><ul class=""><li id="35e4" class="ku kv hi jz b ka kw kd kx kg ky kk kz ko la ks lb lc ld le bi translated">v包含特征向量</li><li id="8d61" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">λ包含特征值</li></ul><p id="f412" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">因此，寻找这些值简化为寻找以下矩阵的特征向量和特征值(<a class="ae kt" href="https://www.csd.uwo.ca/~oveksler/Courses/CS434a_541a/Lecture8.pdf" rel="noopener ugc nofollow" target="_blank">源</a>):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/656f2292ce0b30ce5785fafda7704f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:102/1*BhJPh1GpV29BPtaWFYIUbw.gif"/></div></figure><p id="c936" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">通过寻找特征向量，我们将找到新的子空间的轴，在那里我们的生活变得更简单:类更加分离，类内的数据具有更低的方差。</p><p id="0697" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">python中的计算非常简单:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mo mp l"/></div></figure><ul class=""><li id="b819" class="ku kv hi jz b ka kw kd kx kg ky kk kz ko la ks lb lc ld le bi translated"><strong class="jz hj">对特征值排序，选择前n个</strong>。我们将只保留信息最丰富的轴(可以作为模型的参数给出)。特征值对于寻找最具信息性的轴很方便:</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="9f2d" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">注意，特征值的数量取决于类别的数量和特征的数量。最多n可以是min(n_classes -1，n_features) ( <a class="ae kt" href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p><ul class=""><li id="c962" class="ku kv hi jz b ka kw kd kx kg ky kk kz ko la ks lb lc ld le bi translated"><strong class="jz hj">创建一个包含映射到n个特征值的特征向量的新矩阵</strong> <code class="du mk ml mm mn b"><strong class="jz hj">v</strong></code> <strong class="jz hj">。</strong>这样我们就可以创建一个矩阵来改变数据的基础。</li><li id="51ef" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated"><strong class="jz hj">取数据与</strong> <code class="du mk ml mm mn b"><strong class="jz hj">v</strong></code> <strong class="jz hj">矩阵的点积，得到新的特征。</strong>这种方式将数据转换到空间中，使得类内和类间的分离具有所需的属性。</li></ul><p id="6d24" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">执行LDA的简单实现:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="78c4" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">更详细的教程可以在这里找到<a class="ae kt" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/linear%20discriminant%20analysis.ipynb" rel="noopener ugc nofollow" target="_blank">。一旦我们训练了矩阵<code class="du mk ml mm mn b">v</code>，我们就可以用它将测试数据转换到更低维度的空间。</a></p><h1 id="d31a" class="jf jg hi bd jh ji jj jk jl jm jn jo jp io jq ip jr ir js is jt iu ju iv jv jw bi translated">例子</h1><p id="43c1" class="pw-post-body-paragraph jx jy hi jz b ka kb ij kc kd ke im kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">为了展示LDA的有用性，让我们使用wine <a class="ae kt" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank">数据集</a>。完整的例子可见<a class="ae kt" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/linear%20discriminant%20analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="fb4f" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">在读取数据后，我使用主成分分析(PCA)来绘制使用2个成分的数据。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mt"><img src="../Images/4c7580eeff769619ee23e64991c3dbfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*BYs5oxg0yEsVqGuSge_9QA.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">二维葡萄酒数据主成分分析</figcaption></figure><p id="5dd1" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">从图中我们可以看出，PCA关心的是总方差。成分0比成分1解释了更多的差异。它不考虑类成员的信息，甚至不考虑类的数量。它适用于表示大部分数据差异。对于LDA，我们有一个不同的画面。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/57872df966dafe910aea83d244b96056.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*tfR8POWsx8fJmIgcTSuqXg.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">二维葡萄酒数据LDA</figcaption></figure><p id="c21d" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">LDA使用关于类的信息，并做它应该做的事情:减少类内部的差异，增加类之间的距离。现在很容易将不同类的例子分成不同的组。这对于从整体上表示数据差异可能不是很有用。</p><p id="c385" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">我们可以从分类中看出LDA的有用性。如果我们使用用于训练-测试分割的多个种子来训练具有和不具有LDA的模型，我们可以评估LDA平均来说是否增加了准确性。为了比较，我将在一个例子中使用PCA来转换特征。完整示例可在<a class="ae kt" href="https://github.com/RRisto/learning/blob/master/linear_algebra_learn/linear_discriminant_analysis/linear%20discriminant%20analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="d48a" class="pw-post-body-paragraph jx jy hi jz b ka kw ij kc kd kx im kf kg lk ki kj kk ll km kn ko lm kq kr ks hb bi translated">f1-无LDA时的平均得分为0.978，有LDA时为0.985。这不是一个巨大的增长，但平均来说仍然提高了分类的准确性。使用PCA，我们的F1分数非常低，只有0.660。</p><h1 id="b9dc" class="jf jg hi bd jh ji jj jk jl jm jn jo jp io jq ip jr ir js is jt iu ju iv jv jw bi translated">结论</h1><p id="3e97" class="pw-post-body-paragraph jx jy hi jz b ka kb ij kc kd ke im kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">LDA有助于通过增加不同类别之间的分离来降低数据维度。算法背后的计算并不复杂。尽管如此，它仍然是数据科学中使用的有用方法，并且可以用于例如分类管道中。</p><h1 id="cc65" class="jf jg hi bd jh ji jj jk jl jm jn jo jp io jq ip jr ir js is jt iu ju iv jv jw bi translated">参考</h1><ul class=""><li id="a4c4" class="ku kv hi jz b ka kb kd ke kg mv kk mw ko mx ks lb lc ld le bi translated">特征值和广义特征值问题:教程，<a class="ae kt" href="https://arxiv.org/pdf/1903.11240.pdf" rel="noopener ugc nofollow" target="_blank"> ArXiv </a></li><li id="eecf" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated"><a class="ae kt" href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" rel="noopener ugc nofollow" target="_blank">线性判别分析，维基百科</a></li><li id="d9f7" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">线性判别分析–一点一点，<a class="ae kt" href="https://sebastianraschka.com/" rel="noopener ugc nofollow" target="_blank"> sebastianraschka </a></li><li id="fddf" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">线性判别分析<a class="ae kt" href="https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec11lda.pdf" rel="noopener ugc nofollow" target="_blank">圣何塞州立大学</a></li><li id="87e6" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">模式识别-第八讲，<a class="ae kt" href="https://www.csd.uwo.ca/~oveksler/Courses/CS434a_541a/Lecture8.pdf" rel="noopener ugc nofollow" target="_blank">奥尔加·维克斯列尔教授</a></li><li id="0b86" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">散布矩阵，<a class="ae kt" href="https://en.wikipedia.org/wiki/Scatter_matrix" rel="noopener ugc nofollow" target="_blank">维基百科</a></li><li id="091b" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">sklearn.discriminant _ analysis。线性判别分析，<a class="ae kt" href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html" rel="noopener ugc nofollow" target="_blank">skict-learn</a></li><li id="d161" class="ku kv hi jz b ka lf kd lg kg lh kk li ko lj ks lb lc ld le bi translated">葡萄酒数据集，<a class="ae kt" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库</a></li></ul></div></div>    
</body>
</html>