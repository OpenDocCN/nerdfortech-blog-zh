<html>
<head>
<title>Concept of Machine Learning | Ridge Regression or L2 Regularization, Lasso Regression or L1 Regularization.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的概念|岭回归或 L2 正则化、拉索回归或 L1 正则化。</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/concept-of-machine-learning-ridge-regression-or-l2-regularization-58a5286e7375?source=collection_archive---------18-----------------------#2021-05-03">https://medium.com/nerd-for-tech/concept-of-machine-learning-ridge-regression-or-l2-regularization-58a5286e7375?source=collection_archive---------18-----------------------#2021-05-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="560d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正则化是避免训练数据过度拟合的重要概念，尤其是当训练和测试数据变化很大时。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/945ceca53554dd53926934e3713354ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*nHgtmYqQGiM6_D_TU75J8g.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jp">正规化</strong></figcaption></figure><p id="1c97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正则化是通过向 RSS 添加一个“惩罚”项来计算的，以实现测试数据的<em class="jq">较小方差</em>。</p><h2 id="278d" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">岭回归:</h2><p id="657d" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">RSS 通过添加 b 的系数的平方和来修改。</p><p id="5550" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设下面的方程是回归模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kq"><img src="../Images/2b93f6a0e315b0c2e4a19d6c7f9b32fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*0SZ9Wc109NEur3xk.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jp">多元回归模型</strong></figcaption></figure><p id="562b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中 B0，B1……为参数，1，x1，x2……为特征，曲线为 n 维。</p><p id="3533" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">岭回归方程是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kr"><img src="../Images/516a7e614b5bad2a969a281a42364117.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*3U-fChrT-wEAw4zptF1WHQ.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jp">岭回归</strong></figcaption></figure><p id="9d2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ks" href="https://datascience.stackexchange.com/questions/93927/why-we-take-alphasumbj%c2%b2-as-penalty-in-ridge-regression" rel="noopener ugc nofollow" target="_blank">但问题是我们为什么要拿 alpha*sum(Bj)做惩罚？</a></p><h2 id="96dd" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">在矩阵表示法中，岭回归成本将被写成:</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kt"><img src="../Images/8936828d0e288071e4c11fa1377b1208.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*dXC8gxSm0dOH5g9J2SQSEA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">矩阵表示法中的岭回归</figcaption></figure><h2 id="376c" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">查找 B(系数) :</h2><p id="3ac0" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">计算 RSS 的梯度</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ku"><img src="../Images/d6dd59edfb7b579857c68b496b3e3391.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Ug2n6kM-7oe3xbEk5Zwrnw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jp">渐变</strong></figcaption></figure><p id="bded" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">方法 1: </strong>梯度下降法:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/8ce94545bdad873b9dcb7e7666316f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*XkGr-jRpZCYmKXiOnXOCMg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">梯度下降法</figcaption></figure><p id="d6e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">方法 2: </strong>设置梯度= 0</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kw"><img src="../Images/f0d98488a16c0795c8b0aed9856c24e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*oqcM3De5NQSV_Ai_jMJE3A.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">梯度= 0</figcaption></figure><h2 id="ff02" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">套索回归:</h2><p id="54f6" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">通过添加 b 的系数的模来修改 RSS</p><p id="9d88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">拉索回归方程是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kx"><img src="../Images/0fa20a345426b4a96c236f1796ac13c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*pG_alev8MIASaUo7BEQhGA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">拉索回归方程</figcaption></figure><h2 id="a8f4" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">选择λ(α)的值:</h2><p id="b795" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">请注意:在文章的这一行之前，我把罚分作为α，在这之后，你会看到我已经讨论了λ。这些α和λ是相同的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/fdaaba4c351960a23ab1885f2b34f44f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_bqgX42dglS7vrnWyhBrA.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">资料组</figcaption></figure><p id="58e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的测试集是指不在数据集中的数据。</p><p id="b3cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将把数据分成训练集和测试集，我们的目标是最小化测试误差。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ld"><img src="../Images/2915092dcb30c63c5fb21e7161735b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*vPNOyr5VIplSgk_Df74c9w.png"/></div></figure><p id="6738" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们的目标是使测试误差最小，我们以测试误差最小的方式取λ的值。</p><p id="347c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是可能有许多测试集。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es le"><img src="../Images/9636f75609f602ec03b80b185f9daeb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*Bt4efYt81zV0KauATCLBww.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">列车测试分离。</figcaption></figure><p id="71bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">哪个测试集？</p><h2 id="fa64" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">k 倍验证:</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lf"><img src="../Images/6e8df4d0aed39d1771c3ccd66763969d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*ItMQCOe3gNF96JRfoAJAuw.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">n 个数据分成 K 个集合。</figcaption></figure><p id="41b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们将 N 个数据分成 K 个集合。并且每个集合具有 N/K 个数据。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lg"><img src="../Images/6aa73c08f1053da6abfed58057232fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*tGmChb8VUSLZo6XdfiQjTA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">履历</figcaption></figure><p id="f8f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择λ使 CV(λ)最小。</p><p id="ef3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">K 值→ </strong> 5 或 10</p><h2 id="d44c" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">全文系列:</h2><div class="lh li ez fb lj lk"><a href="https://ujjwalkar.netlify.app/post/concept-of-machine-learning-tutorial-series/" rel="noopener  ugc nofollow" target="_blank"><div class="ll ab dw"><div class="lm ab ln cl cj lo"><h2 class="bd hj fi z dy lp ea eb lq ed ef hh bi translated">机器学习的概念文章系列| Ujjwal Kar</h2><div class="lr l"><h3 class="bd b fi z dy lp ea eb lq ed ef dx translated">回归入门|使用梯度下降的简单线性回归优化…</h3></div><div class="ls l"><p class="bd b fp z dy lp ea eb lq ed ef dx translated">ujjwalkar.netlify.app</p></div></div><div class="lt l"><div class="lu l lv lw lx lt ly jj lk"/></div></div></a></div></div></div>    
</body>
</html>