<html>
<head>
<title>Review — I3D: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset (Video Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">I3D 评论:行动认可？新模型和动力学数据集(视频分类)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset-video-classification-a7535aa8bf48?source=collection_archive---------8-----------------------#2021-06-27">https://medium.com/nerd-for-tech/review-quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset-video-classification-a7535aa8bf48?source=collection_archive---------8-----------------------#2021-06-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="56c5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用膨胀的<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7"> GoogLeNet/Inception-V1 </a>进行双流 3D ConvNets，表现优于<a class="ae ix" href="https://sh-tsang.medium.com/paper-deep-video-large-scale-video-classification-with-convolutional-neural-network-video-585c36c4f042" rel="noopener"> <strong class="ak">深度视频</strong> </a> <strong class="ak">，</strong> <a class="ae ix" href="https://sh-tsang.medium.com/review-two-stream-convnet-spatial-and-temporal-networks-video-classification-10d3f848c0fa" rel="noopener"> <strong class="ak">双流 ConvNet </strong> </a> <strong class="ak">，</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-tsn-temporal-segment-network-video-classification-16a2819462f5" rel="noopener"><strong class="ak">TSN</strong></a><strong class="ak">，</strong> <a class="ae ix" href="https://sh-tsang.medium.com/paper-c3d-learning-spatiotemporal-features-with-3d-convolutional-networks-video-classification-72b49adb4081" rel="noopener"> <strong class="ak"> C3D </strong> </a>等。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/d2e9d9cd2699b240ed82f03bebf7957b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVEGo3gzu9Z45p0l-bR0fg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">这些演员是要亲吻对方，还是他们已经这样做了？</figcaption></figure><p id="c6b0" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事里，<strong class="jr hj"> Quo Vadis，动作识别？回顾了 DeepMind 和牛津大学的一个新模型和动力学数据集</strong>(I3D)。在本文中:</p><ul class=""><li id="58d7" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">双流膨胀 3D ConvNet (I3D) </strong>基于 2D ConvNet 膨胀设计:<strong class="jr hj">过滤器和池内核扩展到 3D。</strong></li><li id="f8a8" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">学习无缝时空特征</strong>，同时利用成功的 ImageNet 架构设计，甚至它们的参数。</li></ul><p id="5f12" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这是一篇发表在<strong class="jr hj"> 2017 CVPR </strong>的论文，引用超过<strong class="jr hj"> 3000 次</strong>。(<a class="li lj ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----a7535aa8bf48--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="b3b9" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">概述</h1><ol class=""><li id="5d7e" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk mn la lb lc bi translated"><strong class="jr hj">现有网络架构</strong></li><li id="3cd5" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj"/><strong class="jr hj">【I3D】</strong></li><li id="5fda" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">提议的双流膨胀 3D ConvNets (I3D) </strong></li><li id="0386" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">动力学人体动作视频数据集</strong></li><li id="a885" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk mn la lb lc bi translated"><strong class="jr hj">实验结果</strong></li></ol></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="44fa" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated"><strong class="ak"> 1。先前的网络架构</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mo"><img src="../Images/d5b32cb600de67a62393a06fbae7a9ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkQJrvTo3S94NzlJKiX1Rw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">现有网络架构</strong></figcaption></figure><h2 id="539f" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated"><strong class="ak"> 1.1。天真的方法</strong></h2><ul class=""><li id="8610" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">这可以通过使用它们从每一帧中独立提取特征，然后在整个视频中汇集它们的预测来实现。</li><li id="f006" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">这符合单词袋图像建模方法的<strong class="jr hj">精神，但尽管在实践中很方便，但它存在完全<strong class="jr hj">忽略时间结构</strong>的问题(例如，模型可能无法区分开门和关门)。这就是故事顶部第一个数字中的问题。</strong></li></ul><h2 id="7e88" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">1.2.(a) ConvNet+LSTM</h2><ul class=""><li id="3cdc" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">为了处理上述问题，<strong class="jr hj">在</strong> <a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7"> <strong class="jr hj">盗梦空间-V1 </strong> </a>的最后一个平均池层后增加了一个 <a class="ae ix" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> <strong class="jr hj">批量归一化</strong> </a>的 LSTM 层，有 512 个隐藏单元。</li><li id="8d12" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">分类器顶部增加一个全连通层</strong>。</li></ul><h2 id="cd14" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated"><strong class="ak"> 1.3。(b) 3D-ConvNet </strong></h2><ul class=""><li id="1b03" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">本文实现的<strong class="jr hj">是</strong><a class="ae ix" href="https://sh-tsang.medium.com/paper-c3d-learning-spatiotemporal-features-with-3d-convolutional-networks-video-classification-72b49adb4081" rel="noopener"><strong class="jr hj">C3D</strong></a><strong class="jr hj"/>的一个小变种，它有<strong class="jr hj"> 8 个卷积层，5 个池层，2 个全连通层</strong>在顶部。</li><li id="9955" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">该模型的输入是 112 × 112 像素裁剪的短 16 帧剪辑，与原始实施方案相同。</li><li id="f1df" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">与[29]不同的是，<a class="ae ix" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">批量归一化</a>在所有卷积和全连接层之后使用。</li></ul><h2 id="faa2" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">1.4.(c) (d)使用 2D-康文网络/三维康文网络的双流网络</h2><ul class=""><li id="d3ef" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/review-two-stream-convnet-spatial-and-temporal-networks-video-classification-10d3f848c0fa" rel="noopener"/></li><li id="e24f" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">最近的一个扩展【8】</strong>在最后一个网络卷积层之后融合空间和流流。</li><li id="0a2c" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">在本文中，作者用<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7">盗梦空间-V1 </a>作为主干来近似他们。</li><li id="d0ac" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">网络的输入是相隔 10 帧采样的<strong class="jr hj"> 5 个连续的 RGB 帧</strong>，以及<strong class="jr hj">相应的光流片段。</strong></li><li id="6536" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7">盗梦空间-V1 </a>最后一个平均池层之前的空间和运动特征<strong class="jr hj">(5×7×7 特征网格，对应时间、x、y 维度)经过一个 3 × 3 × 3 的 3D 卷积层</strong>和 512 个输出通道<strong class="jr hj">，之后是 3×3×3 的 3D max-pooling 层</strong>和最后的全连接层<strong class="jr hj">。</strong></li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="83f6" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">2.<strong class="ak">膨胀 3D 网络技术(I3D) </strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nd"><img src="../Images/f6127e9752b7bd9ffe13e70f8e00ea84.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*qvFGRmnkyXUmmySA_W2iFw.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">提议的膨胀 3D ConvNet (I3D) </strong></figcaption></figure><ul class=""><li id="744c" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">要将 2D ConvNet 转换成 3D 对应物，需要一些技巧或需要注意一些问题。</li></ul><h2 id="39f9" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">2.1.膨胀 2D ConvNets 到三维</h2><ul class=""><li id="e6ee" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">从 2D 架构开始，膨胀所有的过滤器和池内核，过滤器通常是方形的，并使它们成为立方的— <em class="ne"> N </em> × <em class="ne"> N </em>过滤器变成<em class="ne">N</em>×<em class="ne">N</em>×<em class="ne">N</em>。</li></ul><h2 id="ca76" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">2.2.从 2D 滤镜引导 3D 滤镜</h2><ul class=""><li id="07c9" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">通过将图像重复复制到视频序列中，可以将其转换为(无聊的)视频。</li><li id="6c1d" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">由于线性，这可以通过沿时间维度重复 2D 滤波器的权重 N 次来实现，并且通过除以 N 来重新调整它们。</li><li id="62a7" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">通过这样做，逐点非线性图层以及平均和最大池图层的输出与 2D 的情况相同。</li></ul><h2 id="a95d" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">2.3.感受野在空间、时间和网络深度上的增长</h2><ul class=""><li id="cffb" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">然而，当考虑时间时，对称的<strong class="jr hj">感受野</strong>不一定是最佳的——这应当<strong class="jr hj">取决于帧速率和图像尺寸。</strong></li><li id="0bc5" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">如果它在时间上相对于空间增长过快，它可能会合并来自不同对象的边缘，破坏早期特征检测，而如果<strong class="jr hj">它增长过慢，它可能无法很好地捕捉场景动态。</strong></li><li id="9936" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">对于本文中的模型，输入视频以每秒 25 帧进行处理；发现<strong class="jr hj">在前两个 max-pooling 层不执行临时池是有帮助的。</strong></li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="d4c8" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">3.提议的双流膨胀三维飞机(I3D)</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nf"><img src="../Images/3a87537c12507001cffc7fe9412bcfd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ab76Q3eRUOOuIX87hs-GZg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">膨胀的</strong> <a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7"> <strong class="bd jo">盗梦空间——V1</strong></a><strong class="bd jo">架构(左)及其详细的盗梦子模块(右)。</strong></figcaption></figure><ul class=""><li id="366e" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">以上显示了充气的</strong> <a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7"> <strong class="jr hj">盗梦空间-V1 </strong> </a> <strong class="jr hj">及其对应的盗梦模块。</strong></li><li id="4cf7" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">一个 I3D 网络针对<strong class="jr hj"> RGB </strong>输入进行训练，另一个针对<strong class="jr hj">流量</strong>输入进行训练。</li><li id="b6de" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">发现具有双流配置是有价值的，其中一个 I3D 网络在 RGB 输入上训练，另一个在携带优化的平滑流信息的流输入上训练。</li><li id="8d2a" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">两个网络<strong class="jr hj">被分别训练</strong>和<strong class="jr hj">在测试时平均它们的预测。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ng"><img src="../Images/122fd0e0e3adaa9a12415e30ba70700a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_O1BLbWlGgPW4aG45qvSA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">模型的参数数量和时间输入大小</strong></figcaption></figure><ul class=""><li id="71a8" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">最后，对上述模型进行了测试。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="9af7" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">4.动力学人体动作视频数据集</h1><h2 id="2df2" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">4.1.Kintetics</h2><ul class=""><li id="3923" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">在该数据集中，动作类列表包括:<strong class="jr hj">个人动作(单数)</strong>，例如:画画、喝酒、大笑、出拳；<strong class="jr hj">人与人之间的动作</strong>，例如拥抱、亲吻、握手；<strong class="jr hj">人-物动作</strong>，如拆礼物、割草、洗碗。</li><li id="ca85" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">数据集有<strong class="jr hj"> 400 个人体动作类</strong>，每个类有<strong class="jr hj"> 400 个或更多的剪辑</strong>，每个剪辑来自一个独特的视频。<strong class="jr hj">剪辑持续 10s 左右</strong>，没有未剪辑的视频。</li><li id="b70a" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">测试装置</strong>包括<strong class="jr hj">每类</strong>100 个夹子。(该数据集的更多详细信息见[16]。)</li></ul><h2 id="c881" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">4.2.微观动力学</h2><ul class=""><li id="6a4c" class="ku kv hi jr b js mi jv mj jy mk kc ml kg mm kk kz la lb lc bi translated">在本文中，<strong class="jr hj">是一个比全动力学</strong>更小的数据集，称为 miniKinetics。</li><li id="ef52" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">这是数据集的早期版本，只有<strong class="jr hj">213 个类，共有 12 万个剪辑跨越三个分割</strong>，一个用于<strong class="jr hj">训练</strong>，每类<strong class="jr hj">150–1000 个剪辑</strong>，一个用于<strong class="jr hj">验证</strong>，每类<strong class="jr hj"> 25 个剪辑</strong>，一个用于<strong class="jr hj">测试</strong>，每类<strong class="jr hj"> 75 个剪辑</strong>。</li><li id="1ccf" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">MiniKinetics 支持更快的实验，并且在完整动力学数据集之前可用。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="3353" class="lr ls hi bd jo lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">5.实验结果</h1><h2 id="edb2" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">5.1.不同的模型架构</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nh"><img src="../Images/f373051515292f11218edebf37f2feab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7b9-u_OqXrpqu5UOlsmYg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">架构对比:(左)在 UCF-101 的 split 1 上进行训练和测试；(中)在 HMDB-51 的 split 1 上进行训练和测试；(右)微型动力学的培训和测试。</strong></figcaption></figure><ul class=""><li id="0651" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">可以看出，<strong class="jr hj">所提出的新 I3D 模型在所有数据集中表现最佳，具有 RGB、flow 或 RGB+flow 模态。</strong></li></ul><h2 id="a201" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">5.2.使用微型动力学进行预训练的效果</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ni"><img src="../Images/ee40328e7ed5fc068151ac04aed87bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWG-RzcXPal1rMCYiXjjYA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">在 UCF-101 和 HMDB-51 测试集上的性能(两个测试集的分割 1 ),适用于在 miniKinetics 上进行了预训练的架构</strong></figcaption></figure><ul class=""><li id="bb6b" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">原</strong>:UCF-101/HMDB-51 次列车</li><li id="d8a1" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">固定</strong>:来自 miniKinetics 的特性，最后一层在 UCF-101 / HMDB-51 上训练。</li><li id="2c8f" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated"><strong class="jr hj">全英尺</strong>:微型动力学预训练，在 UCF-101 / HMDB-51 上进行端到端微调</li><li id="c387" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">δ表示原始分类与全英尺和固定的最佳分类之间的错误分类差异百分比。</li><li id="0f64" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">明显的结果是<strong class="jr hj">所有架构都受益于固定或全英尺</strong>的预培训。</li></ul><h2 id="d62c" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">5.3.SOTA 比较</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nj"><img src="../Images/ffed8b1826a051ebe19148175080d9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*_YjVEKVK4bGQZxkzXZ6MiA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">与最先进的 UCF-101 和 HMDB-51 数据集进行比较，平均分为三次。</strong></figcaption></figure><ul class=""><li id="8db1" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">单独的所提出的<strong class="jr hj"> RGB-I3D 或 RGB-流模型中的任一个，当在动力学上被预训练时，通过任何模型或模型组合胜过所有先前公开的性能，例如</strong> <a class="ae ix" href="https://sh-tsang.medium.com/paper-deep-video-large-scale-video-classification-with-convolutional-neural-network-video-585c36c4f042" rel="noopener"> <strong class="jr hj">深度视频</strong> </a> <strong class="jr hj">(深度网络)</strong> <a class="ae ix" href="https://sh-tsang.medium.com/review-two-stream-convnet-spatial-and-temporal-networks-video-classification-10d3f848c0fa" rel="noopener"> <strong class="jr hj">双流 ConvNet </strong> </a> <strong class="jr hj">、</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-tsn-temporal-segment-network-video-classification-16a2819462f5" rel="noopener"><strong class="jr hj">TSN</strong></a><strong class="jr hj">、</strong> <a class="ae ix" href="https://sh-tsang.medium.com/paper-c3d-learning-spatiotemporal-features-with-3d-convolutional-networks-video-classification-72b49adb4081" rel="noopener"> <strong class="jr hj"/></a></li><li id="2a22" class="ku kv hi jr b js ld jv le jy lf kc lg kg lh kk kz la lb lc bi translated">提议的<strong class="jr hj">组合双流架构</strong>大大扩大了相对于以前模型的优势，使 UCF-101 的<strong class="jr hj">总体性能达到 97.9，HMDB-51 的</strong>达到 80.2，与以前最好的模型相比<strong class="jr hj">分别对应 57%和 33%的错误分类减少</strong>。</li></ul></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h2 id="7388" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">参考</h2><p id="4546" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy nk ka kb kc nl ke kf kg nm ki kj kk hb bi translated">【2017 CVPR】【I3D】<br/><a class="ae ix" href="https://arxiv.org/abs/1705.07750" rel="noopener ugc nofollow" target="_blank">Quo Vadis，动作识别？新模型和动力学数据集</a></p><h2 id="9682" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated">视频分类/动作识别</h2><p id="5b67" class="pw-post-body-paragraph jp jq hi jr b js mi ij ju jv mj im jx jy nk ka kb kc nl ke kf kg nm ki kj kk hb bi translated"><strong class="jr hj">2014</strong><a class="ae ix" href="https://sh-tsang.medium.com/paper-deep-video-large-scale-video-classification-with-convolutional-neural-network-video-585c36c4f042" rel="noopener">深度视频</a><a class="ae ix" href="https://sh-tsang.medium.com/review-two-stream-convnet-spatial-and-temporal-networks-video-classification-10d3f848c0fa" rel="noopener">双流 ConvNet</a><strong class="jr hj">2015</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-devnet-deep-event-network-for-multimedia-event-detection-and-evidence-recounting-video-140ded103d01" rel="noopener">DevNet</a><a class="ae ix" href="https://sh-tsang.medium.com/paper-c3d-learning-spatiotemporal-features-with-3d-convolutional-networks-video-classification-72b49adb4081" rel="noopener">C3D</a><strong class="jr hj">2016</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-tsn-temporal-segment-network-video-classification-16a2819462f5" rel="noopener">TSN</a><strong class="jr hj">2017</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-temporal-modeling-approaches-for-large-scale-youtube-8m-video-understanding-video-6a9af865e7fc" rel="noopener">时态建模方法</a><a class="ae ix" href="https://sh-tsang.medium.com/review-revisiting-the-effectiveness-of-off-the-shelf-temporal-modeling-approaches-for-26ac83422242" rel="noopener">4 时态建模方法【T21</a></p><h2 id="08d5" class="mp ls hi bd jo mq mr ms lw mt mu mv ma jy mw mx mc kc my mz me kg na nb mg nc bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>