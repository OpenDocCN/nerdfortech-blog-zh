<html>
<head>
<title>How to initialize weights in Neural Network?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中权重如何初始化？</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/how-to-initialize-weights-in-neural-network-3b0126c1f775?source=collection_archive---------1-----------------------#2022-04-06">https://medium.com/nerd-for-tech/how-to-initialize-weights-in-neural-network-3b0126c1f775?source=collection_archive---------1-----------------------#2022-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="acf8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">关于三种最流行的权重初始化方法的直观而简单的教程可以帮助您为您的项目选择正确的方法。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f3e3827b732b63089ed54af19b69ad7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wURbXmB8NPudrju0"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae jn" href="https://unsplash.com/@kierinsight?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kier In Sight </a>的照片</figcaption></figure><h1 id="37fe" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">介绍</h1><p id="b45b" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">权重初始化是一个模型设计参数，错误的选择可能会减慢或停止收敛。你可以把它看作损失函数景观的起点。</p><p id="4f07" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">一个直观的猜测是从 0 开始，但它也导致 0 梯度，因此根本没有学习。</p><p id="9a08" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">另一种选择是从分布中随机抽取点。在这篇博客中，我们将坚持高斯分布有两个参数，均值和方差。这些值定义了分布的范围。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/801740e7cfab7224bc20c1db8f1b1d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*elO4XhXdhXEJprsUCn5zQA.png"/></div></div></figure><h2 id="ba64" class="li jp hi bd jq lj lk ll ju lm ln lo jy kp lp lq ka kt lr ls kc kx lt lu ke lv bi translated">这都是关于方差的</h2><p id="52f4" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">假设我们有一个简单的感知器，我们传递输入 x。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lw"><img src="../Images/7eab0d9a3d72545768365c96db349630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4d79NiAMp9cUKRpqc5IPJw.png"/></div></div></figure><p id="0ec5" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">我们对每个权重的方差也进行了求和，这意味着它随着每一层而增长。这个总和输出有一个更大的<strong class="ki hj">值分布</strong>。</p><p id="d7e3" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">让我们在概念上的 10 层神经网络上测试一下。在现实生活中，训练这样的模型需要时间和计算能力。我们可以简单假设通过十层“网络”执行十次矩阵乘法，测量<strong class="ki hj">层</strong>之间的<strong class="ki hj">均值和方差</strong>。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lx ly l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">如果你想看完整的代码，可以看看我的 Github</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/1f92a56307b37e221e691e22ede97c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qny5htmpHMgdQrsCf9c5bQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">标题中的平均值和方差</figcaption></figure><p id="112e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">如你所见，<strong class="ki hj">方差已经在第二层展开</strong>。绘制其余的值会使图表不可读。</p><p id="677c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">主要的一点是，巨大的值在网络中是不受欢迎的。它们会使模型<strong class="ki hj">变慢</strong>并可能导致<strong class="ki hj">爆炸梯度</strong>问题。因此，我们希望在所有层中保持相同的分布。</p><h1 id="7052" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">乐村化</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/c3d92472dbe56cd2a69076f6bd89efb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6lEZ5y0M9cdwfocuvylCdw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">n-输入——输入数量</figcaption></figure><p id="bf85" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这是第一次尝试在整个网络中保持相同的方差。这是 PyTorch 目前使用的默认方法。我们只需根据进入该层的输入数量缩小初始化规模。</p><p id="7995" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><em class="mb">我们的网络表现:</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/c86bfe10cda29f8327665b9a7199072e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFv03qBKHiC0Q1A-5VpnuQ.png"/></div></div></figure><p id="fc51" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">正如你所看到的，我们在整个模型中保持了相当相似的分布，但是我们忽略了一个重要的因素。</p><p id="f915" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">激活函数和反向传播如何影响方差？</p><h1 id="de9c" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">Xavier 初始化</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/ed1a0d8764169d34c7533ded3a6d62a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pt3GbaClomgJwqMy97_JsA.png"/></div></div></figure><p id="21a4" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">Xavier 2010 年的论文讨论了激活函数和反向传播对整个网络方差的影响。他们计算出正切<strong class="ki hj">后的方差减少</strong>，并且<strong class="ki hj">梯度</strong> <strong class="ki hj">在反向传播中消失</strong>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/7dacc667c15e6b2d384669ebe0b37a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J16aeIL8ZAEozY6AeJYTVg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">[1]</figcaption></figure><p id="6c7f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">为了解决这个问题，他们引入了一种新的初始化方法，为对称点为 0 的激活函数创建，就像 Tanh 一样。并且还归一化了反向传播梯度的方差。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/1fda3af6ce2a34875a60f8b0118149c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bGgKG6rne2m3sHwCoQnUAg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">[1]</figcaption></figure><p id="da99" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">如果你想更深入地挖掘统计数据以及它们是如何产生的，我推荐这篇<a class="ae jn" href="https://pouannes.github.io/blog/initialization/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">博文</strong> </a>。</p><p id="b16b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><em class="mb">我们的网络性能:</em></p><div class="iy iz ja jb fd ab cb"><figure class="mg jc mh mi mj mk ml paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/a30c3f2672a57df44d3071dc37182840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kKzR1_Q2mZNC7l4640dFaw.png"/></div></figure><figure class="mg jc mh mi mj mk ml paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/a031da7294724309d600ba370fe32aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kaDoIesI4SOleXwM06BaVQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx mm di mn mo translated">左:前 6 层，右:后 4 层</figcaption></figure></div><p id="7773" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">好的，所以只有当<strong class="ki hj">的输入和输出数量等于</strong>时，网络才保持相似的分布。</p><p id="9587" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">解释就是 Xavier 论文中使用的<strong class="ki hj">网络</strong>。他们的前馈模型每层有 1000 个隐藏单元。因此，他们取得的结果看起来很好，很顺利。</p><p id="fbcd" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><em class="mb">那 ReLU 呢？</em></p><p id="e79e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">由于 ReLU 是一个非对称函数，它在 Xavier 初始化中表现不佳。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/755e85d4a3a487b21c8b229c3dc722fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sBbeX04R-1tibS4tWtOCVw.png"/></div></div></figure><p id="4c1b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">因此，在 AlexNet(2012)之后，大多数模型使用 0 均值和 0.01 标准差执行权重初始化。缺点开始出现在更深的模型中，更容易出现消失/爆炸梯度问题。最好的例子是 VGG19 模型，它必须使用来自预训练的 11 层浅层网络的权重。</p><h1 id="22f1" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">He 初始化</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/078af574aa9bac53c5e2502a64deeeb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-PC1gSaVfPF_Vxd5nQ-oA.png"/></div></div></figure><p id="ff07" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在 2015 年的一篇论文中，Kaimar 分析了 ReLU 函数对输出方差的影响，并提出了一种新的初始化。</p><p id="b249" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><em class="mb">我们的网络表现:</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/041542fd62abb7777a14cc70d1535aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RZFvZOG4pafj8n9ceLoXWQ.png"/></div></div></figure><p id="604a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">自从深度学习开始深入到 30 层以上。我还将我们的模型扩展到 50 层，并进行了测试。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/55145098809d93d3ac4c592accda4c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ekLnCU1u_ca2okC1YSIKSw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">50 层网络</figcaption></figure><p id="101b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">如您所见，初始化在所有层中保持相对相似的均值和方差。由于 ReLU 的非对称特性，均值明显偏移。</p><blockquote class="mt mu mv"><p id="a639" class="kg kh mb ki b kj lc ij kl km ld im ko mw le kr ks mx lf kv kw my lg kz la lb hb bi translated">初始化用于训练图像识别纸<em class="hi"> </em>深度残差学习中的<strong class="ki hj">结果</strong>。</p></blockquote><h1 id="7cfc" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">讨论和结论</h1><p id="932e" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">自 2015 年放开批量归一化后，权重初始化的意义降低。批量标准化缩小了每一层中的激活，这加速了计算并减少了爆炸梯度的问题。因此，<em class="mb">小心初始化</em>解决的一些问题得到了解决。</p><p id="0d3b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">然而，Yang 和 Schoenholz[2]表明，Xavier 和 He 都不具有 ResNet 的最优方差，并且初始化应该依赖于深度。这证明了权重初始化仍然是一个活跃的研究领域。</p><p id="6301" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这篇博客的目的是介绍不同的初始化技术，并提供更多关于神经网络如何工作的内幕。希望我的解释和可视化能帮助你理解它，并激发你进一步探索这个话题。</p><p id="0ba1" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">查看我的<a class="ae jn" href="https://maciejbalawejder.medium.com/" rel="noopener"> <strong class="ki hj">中型</strong> </a>和<a class="ae jn" href="https://github.com/maciejbalawejder" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj"> Github </strong> </a> <strong class="ki hj"> </strong>简介如果你想看我的其他项目。</p><h1 id="a275" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated"><strong class="ak">参考</strong></h1><p id="2dc3" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated"><a class="ae jn" href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>了解训练深度前馈神经网络的难度</p><p id="5971" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1712.08969.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>平均场残差网络:混沌边缘</p></div></div>    
</body>
</html>