<html>
<head>
<title>Review — Model Distillation: Distilling the Knowledge in a Neural Network (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾—模型提取:提取神经网络中的知识(图像分类)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-model-distillation-distilling-the-knowledge-in-a-neural-network-image-classification-48ce0c81618a?source=collection_archive---------12-----------------------#2021-02-28">https://medium.com/nerd-for-tech/review-model-distillation-distilling-the-knowledge-in-a-neural-network-image-classification-48ce0c81618a?source=collection_archive---------12-----------------------#2021-02-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0412" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用蒸馏获得较小的模型。在JFT数据集上对AlexNet进行更快的训练。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es iy"><img src="../Images/35bb396b9d24f93ceae611bd2b6e705b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*urh1QpVMnrqRRwu0"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">蒸馏温度更高</strong></figcaption></figure><p id="3b62" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">在</span>这个故事中，简要回顾了谷歌公司的<strong class="jn hj">在神经网络</strong>中提取知识。这是辛顿教授的论文。</p><blockquote class="kq kr ks"><p id="3ede" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated">模型集成是提高模型性能的一种简单方法。然而，这可能是计算昂贵的，尤其是如果单个模型是大型神经网络。</p></blockquote><ul class=""><li id="1eba" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">在本文中，模型集合中的知识被提取到单个模型中。</li></ul><p id="8062" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">这是一篇发表在<strong class="jn hj"> 2014 NIPS </strong>的论文，引用次数超过<strong class="jn hj"> 5000次</strong>。(<a class="lg lh ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----48ce0c81618a--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="47af" class="lp lq hi bd jk lr ls lt lu lv lw lx ly io lz ip ma ir mb is mc iu md iv me mf bi translated">概述</h1><ol class=""><li id="e44b" class="kx ky hi jn b jo mg jr mh ju mi jy mj kc mk kg ml ld le lf bi translated"><strong class="jn hj">模型蒸馏的更高温度</strong></li><li id="5e0e" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg ml ld le lf bi translated"><strong class="jn hj">实验结果</strong></li></ol></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="742b" class="lp lq hi bd jk lr ls lt lu lv lw lx ly io lz ip ma ir mb is mc iu md iv me mf bi translated">1.模型蒸馏的更高温度</h1><h2 id="d422" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">1.1.软目标的温度更高</h2><ul class=""><li id="1b91" class="kx ky hi jn b jo mg jr mh ju mi jy mj kc mk kg lc ld le lf bi translated">神经网络通常通过使用“softmax”输出层来产生类别概率，该输出层通过将<em class="kt"> zi </em>与其他逻辑进行比较，将为每个类别计算的逻辑<em class="kt"> zi </em>转换成概率<em class="kt"> qi </em>:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nf"><img src="../Images/c42cf148581ec9e1309e80a774e97290.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*0i0yHiFbZYw4z4gnfPLXlw.png"/></div></figure><ul class=""><li id="43a0" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">其中<em class="kt"> T </em>是通常设置为1的温度。</li></ul><blockquote class="kq kr ks"><p id="ba1f" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated"><strong class="jn hj">使用较高的值为<em class="hi"> T </em>在类上产生较柔和的概率分布。</strong>这很有用，因为<strong class="jn hj">学习函数的大部分信息存在于软目标中非常小概率的比率中。</strong></p></blockquote><ul class=""><li id="283f" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">例如，一个版本的2可能被给定10^-6是3的概率，而10^−9是7的概率，而对于另一个版本，可能正好相反。这是定义数据的丰富相似性结构的有价值的信息(即，它表明哪些2看起来像3，哪些看起来像7)。</li><li id="f3b0" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated"><strong class="jn hj">通过在转移集上训练知识，并对转移集中的每个案例使用软目标分布(<em class="kt">T</em>T&gt;1)</strong>，将知识转移到提取的模型，该转移集是通过使用softmax中具有高温的笨重模型产生的。</li><li id="e6cc" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">当训练蒸馏模型时使用相同的高温，但是在它已经被训练之后，它使用温度<em class="kt"> T </em>为1。</li></ul><h2 id="7fbf" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">1.2.梯度的计算</h2><ul class=""><li id="bb25" class="kx ky hi jn b jo mg jr mh ju mi jy mj kc mk kg lc ld le lf bi translated">转移集中的每个案例相对于提取模型的每个logit，<em class="kt"> zi </em>贡献一个<strong class="jn hj">交叉熵梯度</strong>，<strong class="jn hj"> <em class="kt"> dC </em> / <em class="kt"> dzi </em>。</strong></li><li id="be47" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">如果笨重模型具有产生软目标概率<em class="kt"> pi </em>的逻辑值<em class="kt"> vi </em>，并且转移训练在温度<em class="kt"> T </em>下完成，则梯度由下式给出:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ng"><img src="../Images/f22bc9f4713b7cc8418f6e8561904450.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*ncydSLCjmof7NkBev8rM9A.png"/></div></figure><ul class=""><li id="443e" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">如果温度与logits值相比较高，则可近似为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nh"><img src="../Images/88ea4935f7e04ec3acd69161db46aa21.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*qVPyje77HTNrDx99ItTUig.png"/></div></figure><ul class=""><li id="0dd8" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">假设逻辑<em class="kt"> z </em>和<em class="kt"> v </em>已经过零均值化:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ni"><img src="../Images/134b6b6676344f14d8c6d8932522c842.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*BWLsa_n-hXqvtLSjYVXN1w.png"/></div></figure><ul class=""><li id="4e16" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">梯度可以进一步简化为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nj"><img src="../Images/0d22ce53d06a57905c9b61a929add8ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*xvjx8xtjPM7HVhh5XKadZA.png"/></div></figure><ul class=""><li id="b522" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">后来发现，当提取的模型太小而不能捕获笨重模型中的所有知识时，中间温度工作得最好。</li></ul></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="be7b" class="lp lq hi bd jk lr ls lt lu lv lw lx ly io lz ip ma ir mb is mc iu md iv me mf bi translated"><strong class="ak"> 2。实验结果</strong></h1><h2 id="11c6" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">2.1.MNIST</h2><ul class=""><li id="6a94" class="kx ky hi jn b jo mg jr mh ju mi jy mj kc mk kg lc ld le lf bi translated">具有<strong class="jn hj">两个隐藏层</strong>的单个大型神经网络<strong class="jn hj"> 1200在所有60，000个训练案例上校正线性隐藏单元</strong>。<a class="ae ix" href="https://sh-tsang.medium.com/paper-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-image-classification-a74b369b4b8e" rel="noopener"> <strong class="jn hj">辍</strong> </a> <strong class="jn hj"> </strong>用。本网实现了<strong class="jn hj"> 67测试错误</strong>。</li><li id="950e" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">一个更小的网用<strong class="jn hj">两个隐层</strong>对<strong class="jn hj"> 800个线性隐单元</strong>和<strong class="jn hj">不正则化</strong>实现了<strong class="jn hj"> 146个错误</strong>。</li><li id="6ec5" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">如果在20 的<strong class="jn hj">温度下，仅通过增加匹配由大网产生的软目标的额外任务来调整小网，它实现了<strong class="jn hj"> 74个测试误差</strong>。</strong></li></ul><blockquote class="kq kr ks"><p id="aa6a" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated">这表明软目标可以将大量知识转移到提取的模型中。</p></blockquote><ul class=""><li id="8959" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">当蒸馏网络在其两个隐藏层中的每一层都有300个或更多的单元时，所有高于8的温度都给出了相当相似的结果。但是，当这一数字急剧减少到每层30个单位时，在2.5到4的范围内的温度比更高或更低的温度要好得多。</li></ul><h2 id="896d" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">2.2.语音识别</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nk"><img src="../Images/bfdac6d1d4921c40060cf19b0647a59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*JYApWjO6KPX11hxjqO_Ctw.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">帧分类准确率和误字率(WER) </strong></figcaption></figure><ul class=""><li id="daa6" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">使用具有<strong class="jn hj"> 8个隐藏层</strong>的架构，每个隐藏层包含<strong class="jn hj"> 2560个校正线性单元</strong>和具有14，000个标签(HMM目标ht)的最终softmax层。</li><li id="1c7e" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">输入是26帧40个Mel缩放的滤波器组系数，每帧提前10ms，我们预测第21帧的HMM状态。</li><li id="3675" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated"><strong class="jn hj">参数</strong>总数约为<strong class="jn hj"> 85M </strong>。</li><li id="d911" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">为了训练DNN声学模型，我们使用了大约2000小时的口语数据，这产生了大约7亿个训练样本。该系统在我们的开发集上实现了58.9% 的<strong class="jn hj">帧精度和10.9% </strong>的<strong class="jn hj">误码率(WER)。</strong></li><li id="e903" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated"><strong class="jn hj">由于目标函数中的不匹配，集成对WER的最终目标给出了较小的改进</strong>(在23K字测试集上)，但是再次地，<strong class="jn hj">由集成实现的WER的改进被转移到提取的模型。</strong></li></ul><h2 id="da45" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">2.3.JFT</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nl"><img src="../Images/6e810f69df5a5d87172d27ac530c5889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*pPD-bEjqq8Z5gP8krnfi_g.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">JFT开发集上的分类准确率(前1)</strong></figcaption></figure><ul class=""><li id="cfa1" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">JFT是一个谷歌内部数据集，有1亿张带标签的图片，有1.5万个标签。</li><li id="171b" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated"><a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"> AlexNet </a>需要用6个月的时间进行培训。等待几年来训练一个模型的集合不是一个选项。</li><li id="9ca0" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated"><strong class="jn hj">一种方法是使用“专家”模型，每个模型都在</strong>数据上进行训练，这些数据在来自类的<strong class="jn hj">易混淆子集的示例中高度丰富(就像不同类型的蘑菇)。</strong></li><li id="a453" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">培养61名专科模特，每名300节课。</li><li id="6ee0" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">在测试时，我们可以使用通才模型的预测来决定哪些专家是相关的，并且只需要运行这些专家。</li></ul><blockquote class="kq kr ks"><p id="fd33" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated">这个想法是，当我们有更多的专家负责某一特定类别时，准确性可以得到提高。同时，训练时间可以更短，因为训练独立的专家模型非常容易并行化。</p></blockquote><ul class=""><li id="dfd7" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">请随意阅读报纸了解更多细节。</li></ul><h2 id="3295" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">2.4.作为正则化的软目标</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="er es nm"><img src="../Images/addf174d4845fbbccea10d8eddbad3ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6QlsdqeQeg7Sm5BzZfgCA.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">帧分类准确率和误字率(WER) </strong></figcaption></figure><ul class=""><li id="74ae" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">许多有用的信息可以在软目标中携带，这些信息不可能用单个硬目标进行编码。</li><li id="c7a8" class="kx ky hi jn b jo mm jr mn ju mo jy mp kc mq kg lc ld le lf bi translated">只有3%的数据(大约2000万个例子)，用硬目标训练基线模型会导致严重的过度拟合。</li></ul><blockquote class="kq kr ks"><p id="a340" class="jl jm kt jn b jo jp ij jq jr js im jt ku jv jw jx kv jz ka kb kw kd ke kf kg hb bi translated"><strong class="jn hj">软目标允许新模型仅从3%的训练集中进行推广。</strong></p></blockquote><ul class=""><li id="fc54" class="kx ky hi jn b jo jp jr js ju kz jy la kc lb kg lc ld le lf bi translated">软目标是通过在全训练集上训练获得的。</li></ul></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h2 id="d740" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">参考</h2><p id="ac25" class="pw-post-body-paragraph jl jm hi jn b jo mg ij jq jr mh im jt ju nr jw jx jy ns ka kb kc nt ke kf kg hb bi translated">【2014 NIPS】【蒸馏】<br/> <a class="ae ix" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">在神经网络中蒸馏知识</a></p><h2 id="5c7c" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated">图像分类</h2><p id="abc1" class="pw-post-body-paragraph jl jm hi jn b jo mg ij jq jr mh im jt ju nr jw jx jy ns ka kb kc nt ke kf kg hb bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(什)(么)(情)(感)(呢)(?)(我)(们)(都)(不)(知)(道)(了)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(什)(么)(情)(感)(呢)(?)(我)(们)(还)(没)(有)(什)(么)(好)(感)(,)(我)(们)(就)(没)(有)(什)(么)(情)(感)(,)(我)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(强)(的)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)( )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(都)(不)(想)(要)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(了)(,)(我)(们)(还)(没)(想)(到)(这)(个)(人)(,)(我)(们)(还)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(都)(不)(想)(到)(这)(里)(去)(了)(。 )(我)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。 )(我)(们)(都)(不)(想)(到)(这)(里)(来)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(不)(想)(要)(到)(这)(里)(去)(,)(我)(们)(都)(不)(想)(到)(这)(里)(去)(了)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(。</p><h2 id="9970" class="mr lq hi bd jk ms mt mu lu mv mw mx ly ju my mz ma jy na nb mc kc nc nd me ne bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>