<html>
<head>
<title>Popular Machine Learning Optimization Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">流行的机器学习优化算法</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/popular-machine-learning-optimization-algorithms-f24830dc28b4?source=collection_archive---------3-----------------------#2022-08-26">https://medium.com/nerd-for-tech/popular-machine-learning-optimization-algorithms-f24830dc28b4?source=collection_archive---------3-----------------------#2022-08-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b6307640ee9ba8e33be317049ea75fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RKZSSrVboVWNfkWc"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">康尼·施耐德在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="a6cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">机器学习中的优化是在给定一组输入的情况下寻找正确预测的迭代过程。在每次迭代中，目标是减少预测值和实际值之间的误差，也就是所谓的基本真实值。深度学习中的优化有几种实现方式。一个好的损耗优化方法应该尽可能地收敛到全局最小值，并且避免像平台、鞍点和局部最小值(如果有的话)这样的陷阱。在向前传递之后，需要优化算法来更新模型权重。梯度下降是所述优化的一个流行例子。梯度下降的一些实现包括 Momentum 或 AdaMax。一些梯度下降方法是:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/39e087d85059e236fef1b37d285602be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*lO8nFidhuY_omhf7OXzOiQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">函数空间中的一个鞍点，由于有几个极小值，它会给优化带来问题。Nicoguaro，CC BY 3.0，通过维基共享</figcaption></figure><p id="b6c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">香草渐变下降</strong></p><p id="8780" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">普通梯度下降法对于简单的凸函数非常有效，但是由于所有边上的梯度都没有进一步降低，它们往往会卡在平台或可能的局部最小值附近。下降也不是平滑的，有时会以之字形移动，这减慢了收敛。其他实现有助于避免这些问题。</p><p id="04c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">公式:</p><ul class=""><li id="4ac9" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><em class="kh"> delta = —学习率*梯度</em></li><li id="39a4" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">θ+=δ</em></li></ul><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/4e0d751a195a64bdf1680768f24c5aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PeOJ6WvJDKtFVe-pLg1Xhw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">梯度下降</figcaption></figure><p id="05ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">气势</strong></p><p id="81f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">动量是普通梯度下降法的一种改进，它在函数的步长上增加了一些动量。这种势头有助于越过某些高原和局部最小值。这是通过将前一步的移动添加到当前计算的步来实现的。这个想法类似于将一个碗滚下碗，而不是停在底部，而是基于其初始动量滚过中心，并慢慢停留在碗的底部。动量有助于减少梯度下降的振荡，并平滑向最小值的移动。衰减率乘以前面的步骤有助于衡量前面的步骤要添加到当前步骤中的量。衰减率为 0 类似于具有大量摩擦的普通网络，而衰减率为 1 时，总是添加上一步，这是一个没有摩擦的永无止境的运动。通常使用 0.8 到 0.9 的衰减率值，这通常会给机芯一些摩擦力，并提供足够的动量来加快步伐，但会逐渐稳定下来。</p><p id="a9c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">公式:</p><ul class=""><li id="e309" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><em class="kh">梯度总和=梯度+先前梯度总和*衰减率</em></li><li id="5039" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">delta =-learning _ rate *</em><strong class="ix hj"><em class="kh">sum _ of _ gradient</em></strong></li><li id="db50" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">θ+=</em><strong class="ix hj"><em class="kh"/></strong></li></ul><p id="fe5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">动量公式的另一个版本是添加(1-decay_rate)来缩放梯度:</p><ul class=""><li id="a3f6" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><em class="kh">梯度总和=</em>(1-衰减率)* <em class="kh">梯度+先前梯度总和*衰减率</em></li></ul><p id="3d18" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">无论选择哪个公式版本，都必须选择适当的学习率超参数，以便以更平滑的方式将梯度移向最小值。</p><p id="de18" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> AdaGrad(自适应渐变)</strong></p><p id="3915" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">动量的问题是，它可能很容易通过最小值，或者可能首先选择最陡的路径而忽略其他可能的路径，尽管最陡的路径不一定是到达最小值的最快路径。AdaGrad 通过引入梯度平方和来惩罚在相同方向上探索下降。探索的特征梯度越多，其平方和就越高，最终其步长就越小。这对于稀疏要素来说非常有用，否则这些要素的下降幅度可能会很小。每个特征参数都有自己的下降学习率，因此该算法并不严重依赖学习率超参数。阿达格拉德的缺点是，由于平方和分数的快速增加，下降速度最终会慢到可以忽略不计。</p><p id="b0e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">公式:</p><ul class=""><li id="8a5d" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><em class="kh">sum _ of _ gradient _ squared = previous _ sum _ of _ gradient _ squared+梯度</em></li><li id="5b51" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">delta =-学习率*梯度</em><strong class="ix hj"><em class="kh">/sqrt(sum _ of _ gradient _ squared)</em></strong></li><li id="528a" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">θ</em><strong class="ix hj"><em class="kh">+=δ</em></strong></li></ul><p id="87a5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> RMSProp(均方根传播)</strong></p><p id="3868" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RMSProp 是 Adagrad 的改进版本，它在梯度平方的和中添加了一个衰减因子。它优先考虑当前梯度步骤，同时减少过去梯度的累积效应。这有助于加快收敛速度，因为与阿达格拉德相反，下降速度不断增加。例如，0.9 的衰减率将(1–0.9)标度应用于当前的平方梯度，与 Adagrad 相比增加了 10%。</p><p id="34ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">公式；</p><ul class=""><li id="3ec8" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><em class="kh">sum _ of _ gradient _ squared = previous _ sum _ of _ gradient _ squared * decay _ rate+渐变* (1- decay_rate) </em></li><li id="5d43" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh"/><strong class="ix hj"><em class="kh">=</em></strong><em class="kh">-learning _ rate</em><strong class="ix hj"><em class="kh">* gradient/sqrt(sum _ of _ gradient _ squared)</em></strong></li><li id="aa03" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">θ</em><strong class="ix hj"><em class="kh">+=δ</em></strong></li></ul><p id="90e2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> ADAM(自适应矩估计)</strong></p><p id="5469" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Adam 优化器使用下降法的最佳部分，是目前最流行的选择。它实现了动量(速度)和 RMSProp 算法(稀疏特征下降)。衰减参数β1 应用于梯度的总和(一阶矩),通常设定为约 0.9。第二衰减参数β2 也应用于通常设置为大约 0.999 的平方梯度的和。</p><p id="bc93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">公式:</p><ul class=""><li id="f5bd" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><em class="kh">sum _ of _ gradient = previous _ sum _ of _ gradient * beta 1+gradient *(1—beta 1)</em>【动量】</li><li id="8dfe" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">sum _ of _ gradient _ squared = previous _ sum _ of _ gradient _ squared * beta 2+gradient *(1-beta 2)</em>【rms prop】</li><li id="7196" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">delta =-learning _ rate *</em><strong class="ix hj"><em class="kh">sum _ of _ gradient</em></strong><em class="kh">/sqrt(</em><strong class="ix hj"><em class="kh">sum _ of _ gradient _ squared</em></strong><em class="kh">)</em></li><li id="bf68" class="jy jz hi ix b iy ki jc kj jg kk jk kl jo km js kd ke kf kg bi translated"><em class="kh">θ+=δ</em></li></ul><p id="5a29" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">结论</strong></p><p id="bd5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望这篇文章为你找到适合你的机器学习模型的优化技术提供了一些基本的解释。请在评论区留下你的想法和问题。谢谢你</p></div></div>    
</body>
</html>