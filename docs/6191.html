<html>
<head>
<title>Regularization in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的正则化</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/regularization-in-machine-learning-5e1adcacef6?source=collection_archive---------5-----------------------#2022-01-13">https://medium.com/nerd-for-tech/regularization-in-machine-learning-5e1adcacef6?source=collection_archive---------5-----------------------#2022-01-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6bed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练你的神经网络或任何机器学习模型时，主要关注的是避免过度拟合。如果过度拟合，模型将不够准确，因为它太努力地试图捕捉训练数据集中存在的噪声。噪声是数据点，并不真正代表数据的真实属性，而是随机的。学习这样的数据点会导致模型过度拟合的高风险。</p><p id="6083" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">避免<strong class="ih hj">过拟合</strong>的技术有很多种。其中之一是使用交叉验证，这有助于估计测试集的误差，并决定哪些参数最适合您的模型，这也称为超调。另一种技术是正则化，我们将在本文中讨论。</p><h1 id="0bd0" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">定义</h1><p id="8274" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated"><strong class="ih hj">正则化</strong>是一种通过在给定训练集上适当拟合函数来减少误差的方法，同时避免模型的过度拟合。</p><h1 id="6677" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">避免过度拟合的常用技术</h1><ol class=""><li id="7747" class="kg kh hi ih b ii kb im kc iq ki iu kj iy kk jc kl km kn ko bi translated"><strong class="ih hj"> L2 正规化— </strong>这是最常见的正规化形式。它对目标函数计算中所有参数的平方值进行惩罚。对于网络中的每个权重<em class="kp"> w </em>，项<em class="kp"> (λw^2)/2 </em>被添加到目标中，其中λ是正则化强度。常见的是在前面看到 1/2 的因子，因为这一项相对于参数<em class="kp"> w </em>的梯度仅仅是<em class="kp"> λw </em>而不是<em class="kp"> 2λw </em>。它严重地惩罚了大的权重向量，而偏爱较小的权重向量。</li></ol><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kq"><img src="../Images/cb51d9ee02d1f16f3632041c3ed7f52d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3rjanoWgYy0gT4_fijpG-g.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">正则化目标函数</figcaption></figure><p id="be4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的等式中，L 是任何损失函数，F 表示 Frobenius 范数。</p><p id="518c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj"> L1 正则化— </strong>这是正则化的另一种常见形式，其中对于每个权重<em class="kp"> w，</em>项<em class="kp"> λ|w| </em>被添加到目标中<em class="kp"> </em>。同样，我们可以结合 L1 正则化和 L2 正则化:<em class="kp"> λ1|w|+λ2w^2 </em>。具有 L1 正则化的神经元最终仅使用其最重要输入的稀疏子集，并且变得对“噪声”输入几乎不变。一般来说，L2 正则化可以预期比 L1 给出更好的性能。</p><p id="afe5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.<strong class="ih hj">退出</strong>——这是一种极其有效、简单且最近推出的正规化技术。它是通过仅保持神经元以某种概率<em class="kp"> p </em>活动，或者在训练时将其设置为零来实现的。因为丢失层的输出是随机二次抽样的，所以在训练过程中会降低网络的容量。它可以在网络中的任何或所有隐藏层以及输入层上实现。它不能用于输出图层。</p><p id="618f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.<strong class="ih hj">数据扩充</strong> —减少过度拟合的另一种方法是增加数据大小。在机器学习问题中，我们无法增加训练数据的大小，因为标记的数据太昂贵了。但是在图像的情况下，我们可以通过翻转、旋转、缩放或移动图像来增加数据集的大小，以创建新的样本。</p><p id="00bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.<strong class="ih hj">提前停止</strong> —这是一种交叉验证策略，我们将训练集的一部分单独作为验证集。当我们看到验证集上的性能越来越差时，我们会立即停止对模型的训练。这就是所谓的提前停止。</p><h1 id="8290" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><ul class=""><li id="4ac1" class="kg kh hi ih b ii kb im kc iq ki iu kj iy kk jc lg km kn ko bi translated">过度拟合发生在更复杂的神经网络模型中(即具有许多层或许多神经元的网络)</li><li id="4693" class="kg kh hi ih b ii lh im li iq lj iu lk iy ll jc lg km kn ko bi translated">通过使用 L1 和 L2 正则化以及丢弃，可以降低神经网络的复杂度</li><li id="3f87" class="kg kh hi ih b ii lh im li iq lj iu lk iy ll jc lg km kn ko bi translated">L1 正则化迫使权重参数变为零</li><li id="7daa" class="kg kh hi ih b ii lh im li iq lj iu lk iy ll jc lg km kn ko bi translated">L2 正则化迫使权重参数趋向于零(但从不精确为零)</li><li id="5bd1" class="kg kh hi ih b ii lh im li iq lj iu lk iy ll jc lg km kn ko bi translated">较小的权重参数使得一些神经元的贡献可以忽略不计→神经网络变得不太复杂→较少过拟合</li><li id="4916" class="kg kh hi ih b ii lh im li iq lj iu lk iy ll jc lg km kn ko bi translated">在丢失期间，一些神经元以随机概率 p 被去激活→神经网络变得不那么复杂<strong class="ih hj"> </strong> →更少的过拟合</li></ul></div></div>    
</body>
</html>