<html>
<head>
<title>Machine Learning: Feature Selection and Extraction with Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:特征选择和提取及实例</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/machine-learning-feature-selection-and-extraction-with-examples-80e3e2c2e1a1?source=collection_archive---------9-----------------------#2021-04-20">https://medium.com/nerd-for-tech/machine-learning-feature-selection-and-extraction-with-examples-80e3e2c2e1a1?source=collection_archive---------9-----------------------#2021-04-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a4fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简介</strong></p><p id="dfb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">花更多的时间和精力去理解你正在处理的数据集总是值得的。在没有深入理解数据集的情况下选择机器学习算法是盲目的，并且很可能以沮丧和浪费时间而告终。</p><p id="f969" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集清洗、特征选择和特征提取是实现这种理解的步骤。</p><p id="b6e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">功能选择</strong></p><p id="1696" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习是从给定的特征集中提取目标相关信息。在给定特征数据集和目标的情况下，只有那些特征可以对机器学习过程中相关的目标做出贡献。无关特征不仅浪费计算资源，而且引入不必要的噪声。<a class="ae jd" href="https://justinzads.medium.com/data-engineering-a-feature-selection-example-with-the-iris-dataset-11f0554e4b00" rel="noopener">本文描述了一个特征选择的例子。</a></p><p id="bb0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相关性分析是消除不相关特征的关键。以下是标准:</p><ul class=""><li id="f252" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">要素数据集不应是常数，也不应具有特定的变量级别。</li><li id="014f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">特征应该与目标相关联，否则它对目标估计没有任何贡献</li><li id="fb5f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">特征不应该高度相关，或者其中一个特征不能提供比其他特征更多的信息。它只能在这一点上增加采样噪声。</li></ul><p id="5faa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sklearn模块中有几个工具可用于此，请参考本文了解更多详细信息:</p><p id="592d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/feature _ selection . html</a></p><p id="193b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，Lasso回归还可以在模型训练过程中消除不相关的特征，但它仅限于线性估计。</p><p id="9882" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特征提取</strong></p><p id="eea8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这离特征选择又进了一步。为了使机器学习有效和响应，我们期望更小的特征维空间，并且每个特征维空间对估计目标有更多的贡献。特征提取是具有新特征集的变换，其中新的特征集</p><ul class=""><li id="9404" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">尺寸较小</li><li id="56ce" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">与目标有最大相关性</li></ul><p id="476a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于线性系统，PCA、ICA、Maniford是典型的算法。对于非线性系统，各种基于Maniford的算法，核心ICA。对于文本、图像数据集，它们通常具有大的特征维数，并且特征高度相关，其中基于深度学习的嵌入或基于CNN、RNN的算法非常适合。</p><p id="02bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">值得一提的是，在大多数情况下，特征提取是核心机器学习本身的一部分。是否在单独的处理管道中进行特征提取取决于数据收集、存储和处理基础设施，也取决于工程和商业要求。</p><p id="a5f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">流形示例</p><p id="8442" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个例子中，我们使用了来自sklearn模块的著名数字数据集。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="33e6" class="kb kc hi jx b fi kd ke l kf kg">from sklearn import datasets<br/>digits = datasets.load_digits()<br/>plt.hist(digits.target, histtype = 'barstacked', rwidth=0.8)</span><span id="ab6e" class="kb kc hi jx b fi kh ke l kf kg">for key in digits.keys():print(key)<br/>print(digits.data.shape)<br/>print(digits.feature_names)</span></pre><p id="4225" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们对其进行归一化，并应用高斯神经网络估计量。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="b91d" class="kb kc hi jx b fi kd ke l kf kg">from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.metrics import accuracy_score, confusion_matrix<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="c25a" class="kb kc hi jx b fi kh ke l kf kg">norm_model = MinMaxScaler()<br/>data_norm = norm_model.fit_transform(digits.data)</span><span id="97ec" class="kb kc hi jx b fi kh ke l kf kg">X_train, X_test, y_train, y_test = train_test_split(data_norm, digits.target, train_size = 0.7, random_state = 41)</span><span id="be42" class="kb kc hi jx b fi kh ke l kf kg">bay = GaussianNB()<br/>bay.fit(X_train, y_train)<br/>y_model1 = bay.predict(X_test)</span><span id="46e8" class="kb kc hi jx b fi kh ke l kf kg">print(accuracy_score(y_test, y_model1))<br/>mat1 = confusion_matrix(y_test, y_model1)<br/>sns.heatmap(mat1, annot=True, cbar = False)</span></pre><figure class="js jt ju jv fd kj er es paragraph-image"><div class="er es ki"><img src="../Images/51a31a19fcbf2d85641ba10e93118c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*z2NJQRvxOYKE0_0-"/></div></figure><p id="8b3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有80%的准确率，这对于这个简单而快速的高斯来说已经不错了。</p><p id="d96f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们要添加特征提取，以降低特征维数并提高准确性。让我们先查看可视化数据集。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="0724" class="kb kc hi jx b fi kd ke l kf kg">fig, axes = plt.subplots(10, 10, figsize=(5, 5),<br/>                         subplot_kw={'xticks':[], 'yticks':[]},<br/>                         gridspec_kw=dict(hspace=0.1, wspace=0.1))</span><span id="cc19" class="kb kc hi jx b fi kh ke l kf kg">for i, ax in enumerate(axes.flat):<br/>    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')<br/>    ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')</span></pre><figure class="js jt ju jv fd kj er es paragraph-image"><div class="er es km"><img src="../Images/542352d8e003ed29bbbca14406a1d786.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*c8Sg3Nu4LceG-6fL"/></div></figure><p id="e25f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们从数字图像中看到的，点聚集在一起形成一个数字。流形似乎很适合这种情况。让我们使用Isomap将特征维数降低到15。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="eacd" class="kb kc hi jx b fi kd ke l kf kg">from sklearn.manifold import Isomap<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.metrics import accuracy_score, confusion_matrix</span><span id="5c62" class="kb kc hi jx b fi kh ke l kf kg">model = make_pipeline(Isomap(n_components = 15), GaussianNB())<br/>model.fit(X_train, y_train)<br/>y_model = model.predict(X_test)</span><span id="6d9b" class="kb kc hi jx b fi kh ke l kf kg">print(accuracy_score(y_test, y_model))</span><span id="2820" class="kb kc hi jx b fi kh ke l kf kg">mat = confusion_matrix(y_test, y_model)<br/>sns.heatmap(mat, annot=True, cbar = False)</span></pre><figure class="js jt ju jv fd kj er es paragraph-image"><div class="er es kn"><img src="../Images/379355f493802f2e53d89596394ab7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/0*P0jLUiJ9bStj11ex"/></div></figure><p id="6016" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，特征维数从48下降到15，同时估计精度从80%提高到97%。</p><p id="e1e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，超参数n_components = 15是用GridSearchCV()选择的。代码如下:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="7285" class="kb kc hi jx b fi kd ke l kf kg">from sklearn.model_selection import GridSearchCV</span><span id="c5c2" class="kb kc hi jx b fi kh ke l kf kg">model = make_pipeline(Isomap(), GaussianNB())<br/>grid = GridSearchCV(model, param_grid = {'isomap__n_components':[2,5,7,9,10,12,15,20,30]}, cv = 7)<br/>grid.fit(X_train, y_train)</span><span id="560b" class="kb kc hi jx b fi kh ke l kf kg">print(grid.best_params_)</span><span id="38f0" class="kb kc hi jx b fi kh ke l kf kg">{'isomap__n_components': 15}</span></pre><figure class="js jt ju jv fd kj er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/9b904ffdd4a2aeac7e40f7240f7d7f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uBwJYcstQmA1XIKG"/></div></div></figure><p id="e6da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图表显示，在过度拟合之前，15是一个最佳数字。</p><p id="6699" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> VAE例子</strong></p><p id="e2d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习模型对线性和非线性数据都有效。对于高度相关的特征集(如文本、图像), RNN可以通过学习特征之间的模式来显著降低特征维数。</p><p id="8b19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个例子中，3张肖像照片被压缩成4-D矢量，然后被精确地恢复成图像。为了使它更有趣，我用VAE模型代替。VAEM模型不是输出AEM模型中的向量，而是输出高斯分布，可以从该分布中对向量进行采样。通过从输出分布中采样矢量，并将矢量解码回图像，我们可以看到图像从一个到另一个的过渡。</p><p id="277c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是张量板标量的训练曲线:</p><figure class="js jt ju jv fd kj er es paragraph-image"><div class="er es kt"><img src="../Images/9916f9fa7056496d2c0e8888887036bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/0*XC7pZnSvV-cj61g_"/></div></figure><figure class="js jt ju jv fd kj er es paragraph-image"><div class="er es kt"><img src="../Images/a74f30cc74cf596e399a918e6ea26e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/0*CyYJsJeKhi-y1t0g"/></div></figure><p id="abe2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经过训练，我们有以下矢量解码成图像。</p><p id="3e65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原始图像→矢量→解码图像:</p><figure class="js jt ju jv fd kj er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ku"><img src="../Images/6d61942b7547c78541e06f80f7e896a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qJs1KWxF1PKrtVy5"/></div></div></figure><p id="fb3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于模型是基于相同的3幅图像训练的，这种极端的结果没有实用价值。它只是在这里展示了深度学习在特征减少领域可以走多远。</p><p id="870f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有趣的是，我们可以对(z_mean，z_log_var)分布上的向量进行采样，以获得一些混合图像:</p><figure class="js jt ju jv fd kj er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kv"><img src="../Images/4fda19cc240a0b8dc326ea3ec3c6ecac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r4GIsx5UKcirYKrz"/></div></div></figure><p id="6e65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">项目笔记本可在<a class="ae jd" href="https://github.com/justinzh/AIPortraits/blob/main/Jmodel.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得</p></div></div>    
</body>
</html>