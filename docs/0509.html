<html>
<head>
<title>Apache Spark — Visual Intro!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark —视觉介绍！！</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/apache-spark-visual-intro-9eb3fd2709f9?source=collection_archive---------0-----------------------#2021-01-05">https://medium.com/nerd-for-tech/apache-spark-visual-intro-9eb3fd2709f9?source=collection_archive---------0-----------------------#2021-01-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/748dd9f036971bb9f9c33ceedd823641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdseeK69MKaybgWto_YWQg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">照片由<a class="ae hv" href="https://unsplash.com/@ayooj?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Ayooj Rangaraj </a>在<a class="ae hv" href="https://unsplash.com/s/photos/spark?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><div class=""/><blockquote class="iv"><p id="3f42" class="iw ix hy bd iy iz ja jb jc jd je jf dx translated"><strong class="ak"> Apache Spark </strong>是用于大规模数据处理的统一计算引擎/框架。</p></blockquote><h2 id="ff63" class="jg jh hy bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">介绍</h2><figure class="kf kg kh ki fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ke"><img src="../Images/543fda8cf3d7b70f7ba4984d060eaafd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GLgWfE7NViaRMxuZvCb40w.png"/></div></div></figure><h2 id="14f2" class="jg jh hy bd ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jz kn kb kc kd bi translated">火花执行模型</h2><blockquote class="iv"><p id="6b77" class="iw ix hy bd iy iz ko kp kq kr ks jf dx translated">Spark应用程序有两个组件——驱动程序和执行程序</p></blockquote><figure class="ku kv kw kx ky hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kt"><img src="../Images/6a8fd6d4c56feb9397a2a5e0415221ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_JrEf9DEYuYtDa96Nrg_w.png"/></div></div></figure><p id="c9cd" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated">在我们继续查看spark应用程序和集群之间的交互是如何发生的之前，让我们理解驱动程序和执行程序的主要职责。</p><p id="ae82" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated"><strong class="lb hz">驱动程序:</strong></p><ol class=""><li id="9e7b" class="lt lu hy lb b lc ld lg lh jr lv jv lw jz lx jf ly lz ma mb bi translated">从集群管理器计算应用程序的资源需求</li><li id="1efe" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">管理作业的生命周期和动态资源需求</li><li id="9cc8" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">在执行者之间分配工作</li><li id="a552" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">对节点或执行器故障做出反应</li><li id="164a" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">监控和跟踪执行者的进度</li><li id="fbcc" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">将响应发送给用户</li><li id="9313" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">这是创建SparkContext的地方。</li><li id="8ef1" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">它为WebUI提供作业的元数据。</li></ol><figure class="kf kg kh ki fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/b4a59df2b9977a74beeb61d62f213b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*At7TQMMc1HF7xyRDWvXYig.png"/></div></div></figure><p id="4538" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated"><strong class="lb hz">执行者:</strong></p><ol class=""><li id="3a86" class="lt lu hy lb b lc ld lg lh jr lv jv lw jz lx jf ly lz ma mb bi translated">执行司机分配的任务</li><li id="d7c8" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">向驾驶员报告状态和进度</li></ol><p id="7fed" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated">我们现在对驱动者和执行者做什么有了一个概念，让我们看看他们如何通过与集群交互来实现他们的目标。</p><blockquote class="iv"><p id="a12c" class="iw ix hy bd iy iz ko kp kq kr ks jf dx translated">spark应用程序的部署模式指定了驱动程序进程相对于集群的运行位置。</p></blockquote><figure class="ku kv kw kx ky hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/d0272739e1711af6318bf8bafad7c80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vwB-p7JqKJCK6W-PZqY48g.png"/></div></div></figure><ol class=""><li id="6205" class="lt lu hy lb b lc ld lg lh jr lv jv lw jz lx jf ly lz ma mb bi translated">向集群提交Spark应用程序以及所有配置</li><li id="c80f" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">集群管理器要求节点管理器根据可用资源和提供的配置启动驱动程序进程</li><li id="c6a3" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">驱动程序进程请求附加资源(基于提供的配置的执行者)</li><li id="f47a" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">集群管理器要求节点管理器分配执行器进程</li><li id="ac18" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">集群管理器通知驱动程序执行器的位置</li><li id="e7df" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated">驱动程序向执行者发送任务</li></ol><h2 id="c589" class="jg jh hy bd ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jz kn kb kc kd bi translated">火花编程模型</h2><figure class="kf kg kh ki fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/5baa410bbafd26625056acfd69845734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XS-FiMcQT_ZbXoVP0rhS9Q.png"/></div></div></figure><p id="059a" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated">如上图所示，Spark应用程序分为三个部分:</p><ol class=""><li id="0aa4" class="lt lu hy lb b lc ld lg lh jr lv jv lw jz lx jf ly lz ma mb bi translated"><strong class="lb hz">作业</strong>—Spark应用中的一个动作</li><li id="6161" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated"><strong class="lb hz">阶段</strong> —应用程序中的洗牌依赖</li><li id="bfaf" class="lt lu hy lb b lc mc lg md jr me jv mf jz mg jf ly lz ma mb bi translated"><strong class="lb hz">任务</strong> —节点上的本地计算</li></ol><h2 id="3ca3" class="jg jh hy bd ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jz kn kb kc kd bi translated">结构化API</h2><figure class="kf kg kh ki fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/01bdb83587f01d77e25c3b96967de009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dj3Jh7yQoIz4eLp2EG6SLQ.png"/></div></div></figure><p id="ff0d" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated"><strong class="lb hz"> DataFrame </strong>是分布在集群中运行的一组执行器上的数据的逻辑表示。数据帧的物理数据保存在每个执行器内存的分区中。这允许并行处理数据。</p><figure class="kf kg kh ki fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/ccc8224771588d1008a89d58df2ad589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBd8m_fu9AvvAzWuPOOzvQ.png"/></div></div></figure><h2 id="d7f6" class="jg jh hy bd ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jz kn kb kc kd bi translated">懒惰评估</h2><p id="29b5" class="pw-post-body-paragraph kz la hy lb b lc mm le lf lg mn li lj jr mo ll lm jv mp lo lp jz mq lr ls jf hb bi translated">惰性求值是一种将计算推迟到实际需要结果时进行的技术。由于我们不断推迟计算，我们只能读取一次数据，并在内存中执行所有计算。通过避免不必要的计算，这种方法还能够有效地使用内存。懒求值有以下优点。</p><p id="4d2d" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated"><strong class="lb hz">查询优化— </strong>惰性评估允许Spark通过改变计算顺序来优化执行计划。Spark能够将操作组合在一起，减少数据传递的次数。</p><pre class="kf kg kh ki fd mr ms mt bn mu mv bi"><span id="6f6a" class="mw jh hy ms b be mx my l mz na">df = spark.read.parquet("employee")<br/><br/><br/>#Transformations<br/><br/>df_ordered = df.orderBy("Salary")                      #1<br/>df_sal = df_ordered.filter("Salary &gt; 60000")           #2<br/>df_exp = df_sal.filter("Experience_Years &gt; 5")         #3<br/>df_age = df_exp.filter("Age &gt; 30")                     #4<br/>df_pune_grp = df_age.groupBy("Gender").sum("Salary")   #5 </span></pre><p id="db75" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated">在上面的代码片段中，由于惰性评估，所有三个过滤器都在一次通过中应用，order by在过滤器之后完成，因此速度更快(因为要排序的数据更少)。</p><p id="c23d" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated"><strong class="lb hz">缓存— </strong>惰性评估允许Spark将数据缓存在内存中并在其上执行计算，而不是从磁盘中读取。此外，它允许Spark跳过不必要的中间数据。</p><p id="c1e4" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated"><strong class="lb hz">容错— </strong>由于惰性评估需要跟踪转换，使用沿袭Spark的副作用是可以重新计算失败的任务，而不是重新计算整个流水线。</p><h2 id="272e" class="jg jh hy bd ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jz kn kb kc kd bi translated">摄取数据</h2><p id="ae47" class="pw-post-body-paragraph kz la hy lb b lc mm le lf lg mn li lj jr mo ll lm jv mp lo lp jz mq lr ls jf hb bi translated">在spark中，我们有DataFrame Reader API，它允许我们将各种来源的数据读入spark生态系统。该API具有以下公共签名。</p><pre class="kf kg kh ki fd mr ms nb nc aw nd bi"><span id="93ff" class="jg jh hy ms b fi ne nf l ng na">spark.read.format("csv") \ <br/>          .options("mode", "FAILFAST") \<br/>          .schema(fileschema) \ <br/>          .load()</span></pre><blockquote class="nh ni nj"><p id="1e4e" class="kz la nk lb b lc ld le lf lg lh li lj nl lk ll lm nm ln lo lp nn lq lr ls jf hb bi translated"><strong class="lb hz">格式</strong> — csv、json、avro、parquet、社区格式<br/> <strong class="lb hz">选项</strong> —模式(FAILFAST、drop formattered、PERMISSIVE)、路径<br/> <strong class="lb hz">模式</strong> — Explcit、Implicit、InferSchema</p></blockquote><h2 id="cdc6" class="jg jh hy bd ji jj kj jl jm jn kk jp jq jr kl jt ju jv km jx jy jz kn kb kc kd bi translated">写入数据</h2><p id="7b05" class="pw-post-body-paragraph kz la hy lb b lc mm le lf lg mn li lj jr mo ll lm jv mp lo lp jz mq lr ls jf hb bi translated">与数据帧读取器API类似，Spark也有数据帧写入器API。该API具有以下公共签名。</p><pre class="kf kg kh ki fd mr ms nb nc aw nd bi"><span id="a94f" class="jg jh hy ms b fi ne nf l ng na">DataFrame.write.format("csv") \ <br/>          .options("path", "/data/output/") \<br/>          .mode("overwrite") \ <br/>          .save()</span></pre><blockquote class="nh ni nj"><p id="b1d2" class="kz la nk lb b lc ld le lf lg lh li lj nl lk ll lm nm ln lo lp nn lq lr ls jf hb bi translated"><strong class="lb hz">格式</strong> — csv、json、avro、parquet、community格式<br/> <strong class="lb hz">选项</strong> — path、maxRecordPerFile <br/> <strong class="lb hz">模式</strong> —追加、覆盖、errorIfExists、忽略<br/> <strong class="lb hz">分区依据— </strong>决定物理数据布局<strong class="lb hz"> <br/>桶依据— </strong>给定列将数据存储在固定数量的桶中<strong class="lb hz"> <br/>保存表</strong></p></blockquote><p id="432a" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated">在这篇博客中，我已经介绍了基本的构建模块。在下一篇博客中，我将会写一些PySpark的代码示例。</p><p id="ff65" class="pw-post-body-paragraph kz la hy lb b lc ld le lf lg lh li lj jr lk ll lm jv ln lo lp jz lq lr ls jf hb bi translated">快乐火花！！</p></div></div>    
</body>
</html>