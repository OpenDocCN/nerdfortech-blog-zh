<html>
<head>
<title>Build a system to identify fake news articles!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立识别假新闻文章的系统！</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/build-a-system-to-identify-fake-news-articles-6604968043cb?source=collection_archive---------10-----------------------#2021-05-12">https://medium.com/nerd-for-tech/build-a-system-to-identify-fake-news-articles-6604968043cb?source=collection_archive---------10-----------------------#2021-05-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7147" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用文本分析和经典的机器学习方法解决现实世界的问题！</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/70b1d40cabf20d41212b1ff1a1feca84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*FiFh8Zmgp9yxopWHwuiftw.jpeg"/></div></figure><p id="54f3" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi kb translated">与报纸、杂志、广播和电视等传统大众媒体相比，假新闻的普遍传播是社交网络传播新闻扩张的副作用。人类分辨真假事实的效率低下暴露了假新闻对逻辑真理、民主、新闻和政府机构公信力的威胁。</p><p id="09d8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">信息的真实性是其完整性的重要组成部分。对假新闻的打击使得应用层对社交网络信息和数据消费的完整性和准确性检查变得不可分割。虚假内容的披露意味着处理和网络资源的浪费。此外，它还对所提供服务的信息完整性和可信度构成了严重威胁。因此，不真实信息的分享关系到应用于新闻传播的<strong class="jh hj">信任质量(QoT) </strong>，指的是用户对特定来源的内容的信任程度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kk"><img src="../Images/4b5765a1b56d93e3568b9d5168dca976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rAcw9qCGjz-gfD7t.jpg"/></div></div></figure><p id="ec16" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这篇文章中，我们调查了用自然语言预处理数据、矢量化、降维、机器学习和信息检索质量评估的方法。我们也将假新闻的识别置于语境中。</p><h2 id="0e69" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">商业问题</h2><p id="2f1a" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">开发一个机器学习程序来识别一篇文章何时可能是假新闻。</p><h2 id="dcbd" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">用户指南</h2><p id="85cf" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated"><a class="ae lp" href="https://github.com/Priyanka-Dandale/Fake-News-Kaggle-Competition" rel="noopener ugc nofollow" target="_blank"> <strong class="jh hj"> Colab 笔记本</strong> </a>可以在我的<a class="ae lp" href="https://github.com/Priyanka-Dandale/Fake-News-Kaggle-Competition" rel="noopener ugc nofollow" target="_blank"> GitHub </a>仓库中获得这个真实世界的用例！</p><h2 id="a0da" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">理解数据</h2><p id="5880" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">您可以从<a class="ae lp" href="https://www.kaggle.com/c/fake-news/data" rel="noopener ugc nofollow" target="_blank"><strong class="jh hj"><em class="lq">Kaggle</em></strong></a><strong class="jh hj"><em class="lq">下载用例的数据集。</em>T19】</strong></p><p id="c4bf" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">什么是假新闻？— </em> </strong>假新闻一词最初指的是在相关新闻的幌子下传播的虚假且经常是耸人听闻的信息。然而，这个词的用法已经发生了变化，现在被认为是社交媒体上传播虚假信息的同义词。值得注意的是，根据谷歌趋势，“假新闻”一词在 2017 年非常流行。</p><blockquote class="lr ls lt"><p id="04dd" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated">假新闻被定义为故意和明显虚假的新闻，或任何呈现为事实上不正确的新闻的信息，旨在误导新闻消费者相信它是真实的。</p></blockquote><h2 id="2828" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated"><strong class="ak">数据概述</strong></h2><p id="d75c" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">数据将在一个文件<strong class="jh hj"><em class="lq"/></strong>中总共有 20800 个<strong class="jh hj"><em class="lq"/></strong>个观测值或行数。它包含<strong class="jh hj"> <em class="lq"> 5 </em>列</strong> : id(新闻文章的唯一 id)、title(新闻文章的标题)、author(新闻文章的作者)、text(文章的正文；可能是不完整的)、标签(将物品标记为潜在不可靠的标签)，用 1(不可靠或伪造)或 0(可靠)表示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es lx"><img src="../Images/9576b55420c2bca8e62e862cd65933e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5PZRbWjhe8Qm9mejifI1Qg.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">前 10 个数据观察</figcaption></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="4a22" class="kp kq hi md b fi mh mi l mj mk">import pandas as pd<br/>df = pd.read_csv(r"/content/train.csv",error_bad_lines=False)</span><span id="1568" class="kp kq hi md b fi ml mi l mj mk"># error_bad_lines=False -- cause the offending lines to be skipped.</span><span id="4342" class="kp kq hi md b fi ml mi l mj mk">print(df.shape)</span><span id="4d1a" class="kp kq hi md b fi ml mi l mj mk"># first 10 lines of the data</span><span id="3bac" class="kp kq hi md b fi ml mi l mj mk">print(df.head(10))</span></pre><h2 id="2a9d" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">ML 问题的类型</h2><p id="25e5" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">这是一个<strong class="jh hj"> <em class="lq">二元分类问题</em> </strong>，对于一篇给定的新闻文章，我们需要预测它是否可靠。</p><h2 id="5b5b" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">基本探索性数据分析</h2><p id="a2a5" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">训练的观测值总数是多少？</em></strong><em class="lq"/>——<em class="lq">20800</em>。</p><p id="23b9" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">数据中是否有缺失值？</em> </strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mm"><img src="../Images/674c7d689e23f13185b7a2454292dc14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*KcyTnI9tXfKtII3DfI98Eg.png"/></div></figure><ul class=""><li id="5925" class="mn mo hi jh b ji jj jl jm jo mp js mq jw mr ka ms mt mu mv bi translated">缺失值的百分比更小，因此我们可以丢弃那些观察值。</li></ul><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="5e7b" class="kp kq hi md b fi mh mi l mj mk"># Count of missing values</span><span id="7f9d" class="kp kq hi md b fi ml mi l mj mk">print(pd.DataFrame(df.isnull().sum(),columns=['Count of missing values']))</span><span id="a6ca" class="kp kq hi md b fi ml mi l mj mk"># Percentage of missing values</span><span id="b532" class="kp kq hi md b fi ml mi l mj mk">print(pd.DataFrame(round(df.isnull().sum()*100/df.shape[0],2),columns=['Percentage of missing values']))</span><span id="6e1c" class="kp kq hi md b fi ml mi l mj mk"># Drop missing observations<br/>df.dropna(inplace=True)<br/>print(df.shape)</span><span id="1515" class="kp kq hi md b fi ml mi l mj mk">#resetting the index after dropping missing rows<br/>df.reset_index(inplace=False,drop=True)</span><span id="f702" class="kp kq hi md b fi ml mi l mj mk">df.head(10)</span></pre><p id="803b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">阶级标签的分布是怎样的？</em> </strong></p><p id="2418" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">不可靠文章(假或 1) </strong>的数量是 7924 篇，<strong class="jh hj">可靠文章(0) </strong>的数量是 10361 篇，数据不符合不平衡情况，但几乎 43%的文章是假的！😥</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es mw"><img src="../Images/40dd874e7c3dfe4adab49322a4dcab8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*15UgzWjxoVsIXKUtSm6QjQ.png"/></div></div></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="fc3a" class="kp kq hi md b fi mh mi l mj mk">import seaborn as sns<br/>sns.countplot(x = df['label']);</span><span id="f0b1" class="kp kq hi md b fi ml mi l mj mk">print("1 : Unreliable or fake")<br/>print("0 : Reliable")<br/>print("\nDistribution of labels is:")<br/>print(df.label.value_counts());</span><span id="db8c" class="kp kq hi md b fi ml mi l mj mk">print(round(df.label.value_counts(normalize=True),2)*100);</span></pre><p id="ce50" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">假新闻文章的常见作者有哪些？</em> </strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mx"><img src="../Images/833a0c87060094f0f2be0be5130f27a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*TNdyVY2D4uTH1NyOnLqGzQ.png"/></div></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="3861" class="kp kq hi md b fi mh mi l mj mk">from wordcloud import WordCloud, STOPWORDS<br/>from os import path<br/>import matplotlib.pyplot as plt</span><span id="5231" class="kp kq hi md b fi ml mi l mj mk">df_unreliable = df[df['label'] == 1]<br/>df_reliable = df[df['label'] == 0]</span><span id="e9cf" class="kp kq hi md b fi ml mi l mj mk">textp_w = df_unreliable['author']<br/>stopwords = set(STOPWORDS)</span><span id="a444" class="kp kq hi md b fi ml mi l mj mk">print ("Word Cloud for Authors of Fake news articles:")<br/>wordcloud = WordCloud().generate(' '.join(textp_w))<br/>plt.figure(figsize=(10,10))</span><span id="456e" class="kp kq hi md b fi ml mi l mj mk"># Generate plot<br/>plt.imshow(wordcloud)<br/>plt.axis("off")<br/>plt.show()</span></pre><p id="a92d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">哪些是可靠新闻文章的常见作者？</em>T44】</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/5d1ba9b40de14976c00fd43f4fd3be60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*Xs0gVzw1D4gTRNHmU8a9JA.png"/></div></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="6a28" class="kp kq hi md b fi mh mi l mj mk">textp_w = df_reliable['author']<br/>stopwords = set(STOPWORDS)</span><span id="e9ed" class="kp kq hi md b fi ml mi l mj mk">print ("Word Cloud for Authors of Reliable news articles:")<br/>wordcloud = WordCloud().generate(' '.join(textp_w))<br/>plt.figure(figsize=(10,10))</span><span id="43f3" class="kp kq hi md b fi ml mi l mj mk"># Generate plot<br/>plt.imshow(wordcloud)<br/>plt.axis("off")<br/>plt.show()</span></pre><h2 id="3984" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">数据预处理</h2><p id="5cca" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">为什么我们需要将文本数据转换成数值数据？</em>T48】</strong></p><ul class=""><li id="294f" class="mn mo hi jh b ji jj jl jm jo mp js mq jw mr ka ms mt mu mv bi translated">计算机只理解数字。</li><li id="6b42" class="mn mo hi jh b ji mz jl na jo nb js nc jw nd ka ms mt mu mv bi translated">一旦我们将文本转换成向量，我们就可以利用线性代数的优点。</li></ul><p id="c734" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在我们继续之前，让我们先了解一下本帖中会经常用到的两个术语。</p><ul class=""><li id="2715" class="mn mo hi jh b ji jj jl jm jo mp js mq jw mr ka ms mt mu mv bi translated"><strong class="jh hj">文档</strong> -它只是一个包含文本数据的文件。就数据集而言，每个记录或数据点都可以被视为一个文档。</li><li id="c19d" class="mn mo hi jh b ji mz jl na jo nb js nc jw nd ka ms mt mu mv bi translated"><strong class="jh hj">文集</strong> -一组文档被称为一个文集。就数据集而言，整个数据点或整个数据集可以被认为是一个语料库。</li></ul><p id="2c2e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们可以走了！！</p><p id="d916" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">文本预处理的第一步是从“标题”特征中去除特殊字符、词干和停用词。如果你想了解如何在文本预处理上执行上述步骤，那么请查阅<a class="ae lp" href="https://github.com/Priyanka-Dandale/Fake-News-Kaggle-Competition" rel="noopener ugc nofollow" target="_blank"> <strong class="jh hj"> Colab 笔记本</strong> </a> <strong class="jh hj">。我已经用一个例子解释了这些步骤！</strong></p><blockquote class="lr ls lt"><p id="8395" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated">注意:对于预处理和模型构建，我只考虑了“标题”和“标签”特性。您还可以包括其他功能。我尝试过保留“文本”,但是性能指标是不可靠的！</p></blockquote><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="a867" class="kp kq hi md b fi mh mi l mj mk">import nltk<br/>nltk.download('stopwords')<br/>import re<br/>from nltk.corpus import stopwords</span><span id="fd32" class="kp kq hi md b fi ml mi l mj mk">STOP_WORDS = stopwords.words("english")<br/>from tqdm import tqdm</span><span id="f8c9" class="kp kq hi md b fi ml mi l mj mk">def preprocess(documents):<br/>    corpus=[]<br/>    sentences=[]<br/>    for i in tqdm(range(0,len(documents))):<br/>         news = re.sub('[^a-zA-Z]',' ', documents['title'][i])<br/>         news = news.lower()<br/>         tokens = news.split()<br/>         words = [PorterStemmer().stem(token) for token in tokens if    <br/>         token not in set(stopwords.words('english'))]</span><span id="1dfc" class="kp kq hi md b fi ml mi l mj mk">         sentences =' '.join(words)<br/>         corpus.append(sentences)<br/>   return corpus<br/>corpus = preprocess(df)</span></pre><p id="102f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">执行上述预处理步骤后，我们需要将数据转换为数字。为此，我们将使用简单的<a class="ae lp" href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/" rel="noopener ugc nofollow" target="_blank"><strong class="jh hj"/><strong class="jh hj">【弓形】</strong> </a>的方法。</p><h2 id="8e52" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">基于词包的特征提取</h2><p id="95e2" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">单词包是描述单词在文档中出现的文本表示。它涉及两件事:</p><ul class=""><li id="55bc" class="mn mo hi jh b ji jj jl jm jo mp js mq jw mr ka ms mt mu mv bi translated">已知单词的词汇表。</li><li id="3f86" class="mn mo hi jh b ji mz jl na jo nb js nc jw nd ka ms mt mu mv bi translated">已知单词存在的一种度量。</li></ul><p id="7dc2" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">它被称为单词的<strong class="jh hj"><em class="lq">包</em>，因为任何关于文档中单词的顺序或结构的信息都被丢弃了。该模型只关心已知单词是否出现在文档中，而不关心它在文档中的位置。</strong></p><blockquote class="lr ls lt"><p id="9d82" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated">句子和文档的一个非常常见的特征提取过程是单词袋方法。在这种方法中，我们查看文本中单词的直方图，即将每个单词计数视为一个特征。</p></blockquote><p id="94f1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们从下面的例子中了解一下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ne"><img src="../Images/cf772e42877e101fdc78a593de20f576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bmmBrp3-uVhm1uPO.png"/></div></div></figure><p id="ec14" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这里，cat、sat、in、hat、with 是 3 个文档的 6 个特征，条目是这些特征的频率。很简单。不是吗？</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="8e1d" class="kp kq hi md b fi mh mi l mj mk">from sklearn.feature_extraction.text import CountVectorizer</span><span id="417b" class="kp kq hi md b fi ml mi l mj mk">cv = CountVectorizer(max_features=5000,ngram_range=(1,3))<br/>X = cv.fit_transform(corpus).toarray()</span><span id="43e3" class="kp kq hi md b fi ml mi l mj mk">print(X.shape)<br/>print(cv.get_featurbe_names()[0:7])</span></pre><p id="9089" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将从弓上获得 5000 个特征。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/7c57dcd9cdca327f03ebcee547f21a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*BQ9d6K-AOOPCUcJ5gAmIaQ.png"/></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">7 大功能名称</figcaption></figure><h2 id="6299" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated"><strong class="ak">列车测试分割</strong></h2><p id="3ba2" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">这里，我们将以 75:25 的比例将我们的数据分成训练验证(测试)。列车数据中的数据点数:<em class="lq"> 21141 </em>和验证数据中的数据点数:<em class="lq"> 7048 </em>。</p><p id="5b1f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在将数据分成训练和验证之后，我们将得到下面的类标签分布，这表明数据不遵循不平衡标准。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ng"><img src="../Images/12c197e7811476832a617c628bb352b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lEVaCBUWmEzu0p22ClFXCw.png"/></div></div></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="12fd" class="kp kq hi md b fi mh mi l mj mk">y_true = df['label']</span><span id="82bd" class="kp kq hi md b fi ml mi l mj mk">from sklearn.model_selection import train_test_split</span><span id="c485" class="kp kq hi md b fi ml mi l mj mk">X_train,X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.25, random_state=42)</span><span id="d678" class="kp kq hi md b fi ml mi l mj mk">print("Number of data points in train data :",X_train.shape)<br/>print("Number of data points in validation data :",X_test.shape)</span><span id="1c0d" class="kp kq hi md b fi ml mi l mj mk">from collections import Counter<br/>print("-"*10, "Distribution of output variable in train data", "-"*10)</span><span id="bcea" class="kp kq hi md b fi ml mi l mj mk">train_distr = Counter(y_train)<br/>train_len = len(y_train)</span><span id="2dc9" class="kp kq hi md b fi ml mi l mj mk">print("Class 0: ",round(int(train_distr[0])*100/train_len,2),"Class 1: ", round(int(train_distr[1])*100/train_len,2))</span><span id="a445" class="kp kq hi md b fi ml mi l mj mk">print("-"*10, "Distribution of output variable in validation data", "-"*10)</span><span id="64e4" class="kp kq hi md b fi ml mi l mj mk">test_distr = Counter(y_test)<br/>test_len = len(y_test)</span><span id="20e2" class="kp kq hi md b fi ml mi l mj mk">print("Class 0: ",round(int(test_distr[0])*100/test_len,2), "Class 1: ",round(int(test_distr[1])*100/test_len,2))</span></pre><h2 id="dc2f" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">随机基线模型</h2><p id="ea3a" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">基线预测算法提供了一组预测，您可以像对问题的任何预测一样对这些预测进行评估，例如分类准确性或损失。随机预测算法预测在训练数据中观察到的随机结果。这意味着随机模型随机预测标签 0 或 1。当评估所有其他机器学习算法时，这些算法的分数提供了所需的比较点…</p><blockquote class="lr ls lt"><p id="32dd" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated"><strong class="jh hj">使用随机模型的验证数据的准确度为 50%。</strong>大量提高精度的示波器！</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/fd2e3ed2f4ebbac65548108f5ee200df.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*WUqQoEVo29o2nRRb1J3erw.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/d3d04cbf006fe9ea127a15c1f2f64bb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*7E-L90-3okKrmM_FC7Vngg.png"/></div></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="9fa5" class="kp kq hi md b fi mh mi l mj mk">from sklearn import metrics<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics.classification import accuracy_score<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="52e1" class="kp kq hi md b fi ml mi l mj mk"># we create a output array that has exactly same size as the CV data</span><span id="07e2" class="kp kq hi md b fi ml mi l mj mk">predicted_y = np.zeros((test_len,2))<br/>for i in range(test_len):<br/>    rand_probs = np.random.rand(1,2)<br/>    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])</span><span id="820f" class="kp kq hi md b fi ml mi l mj mk">predicted_y =np.argmax(predicted_y, axis=1)</span><span id="dfb1" class="kp kq hi md b fi ml mi l mj mk">print("Accuracy on Validation Data using Random Model",round(accuracy_score(y_test, predicted_y),2))<br/>plot_confusion_matrix(y_test, predicted_y)</span></pre><h2 id="0ae6" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">多项式朴素贝叶斯分类器</h2><p id="6a51" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">多项式朴素贝叶斯算法是一种概率学习方法，主要用于自然语言处理。该算法基于贝叶斯定理，并预测文本(如一封电子邮件或一篇报纸文章)的标签。它计算给定样本中每个标签的概率，然后将概率最高的标签作为输出。</p><p id="1b76" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">朴素贝叶斯分类器是许多算法的集合，其中所有算法共享一个共同的原则，即被分类的每个特征与任何其他特征都不相关。一个特征的存在与否不影响另一个特征的存在与否。</p><blockquote class="lr ls lt"><p id="4b74" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated"><strong class="jh hj">使用 MNB 模型对列车数据的准确率为 92% </strong></p><p id="1e24" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated"><strong class="jh hj">使用 MNB 模型验证数据的准确率为 90% </strong></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nj"><img src="../Images/1f11c78c966e7e0b19623b7cabd767ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*ytIFlBZJ9iCEooGyHVTXMQ.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nk"><img src="../Images/1752b03445de5609fff0ef22642c382b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PYyeSqz8QmX8arKTTBJkag.png"/></div></div></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="e296" class="kp kq hi md b fi mh mi l mj mk">from sklearn.naive_bayes import MultinomialNB</span><span id="10ed" class="kp kq hi md b fi ml mi l mj mk">classifier=MultinomialNB()<br/>classifier.fit(X_train,y_train)</span><span id="5ffa" class="kp kq hi md b fi ml mi l mj mk">predicted_y_train = classifier.predict_proba(X_train)<br/>predicted_y_train = np.argmax(predicted_y_train,axis=1)</span><span id="710e" class="kp kq hi md b fi ml mi l mj mk">print("Accuracy on Train Data using NB Model",round(metrics.accuracy_score(y_train, predicted_y_train),2))</span><span id="eae5" class="kp kq hi md b fi ml mi l mj mk">predict_y = classifier.predict_proba(X_test)<br/>predicted_y =np.argmax(predict_y,axis=1)</span><span id="7e9a" class="kp kq hi md b fi ml mi l mj mk">#print("Total number of validation data points :", len(predicted_y))</span><span id="52dd" class="kp kq hi md b fi ml mi l mj mk">print("Accuracy on Validation Data using NB Model",round(metrics.accuracy_score(y_test,predicted_y),2))</span><span id="febd" class="kp kq hi md b fi ml mi l mj mk"># Go to Colab notebook for plot_confusion_matrix function<br/>plot_confusion_matrix(y_test, predicted_y)</span></pre><h2 id="3e63" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">被动主动分类器</h2><p id="4cf8" class="pw-post-body-paragraph jf jg hi jh b ji lk ij jk jl ll im jn jo lm jq jr js ln ju jv jw lo jy jz ka hb bi translated">被动攻击算法一般用于大规模学习。这是少数在线学习算法之一。在在线 ML 算法中，输入数据按顺序出现，ML 模型逐步更新。这在存在大量数据的情况下非常有用，并且由于数据的庞大，训练数据在计算上是不可行的。我们可以简单地说，在线学习算法将获得一个训练样本，更新分类器，然后丢弃该样本。一个很好的例子是检测 Twitter 上的假新闻，那里每秒钟都有新数据被添加进来。</p><p id="1994" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> <em class="lq">这个算法是怎么实现的？</em> </strong></p><p id="41d6" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">简单来说:</p><ul class=""><li id="9406" class="mn mo hi jh b ji jj jl jm jo mp js mq jw mr ka ms mt mu mv bi translated"><strong class="jh hj">被动:</strong>为了正确预测，保持模型，不做任何改变。</li><li id="16c2" class="mn mo hi jh b ji mz jl na jo nb js nc jw nd ka ms mt mu mv bi translated"><strong class="jh hj">积极的:</strong>对于不正确的预测，对模型做出改变可能是对模型的某种改变，可以纠正它。</li></ul><p id="dd1c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">要了解更多关于这种算法背后的数学知识，我推荐观看由 Victor Lavrenko 博士制作的关于这种算法工作的精彩视频。</p><blockquote class="lr ls lt"><p id="93d6" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated"><strong class="jh hj">使用被动回归模型对列车数据的准确率为 100% </strong></p><p id="aba8" class="jf jg lq jh b ji jj ij jk jl jm im jn lu jp jq jr lv jt ju jv lw jx jy jz ka hb bi translated"><strong class="jh hj">被动回归模型验证数据的准确率为 96% </strong></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/2f27617b46329862218fbe5cef660cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*EWOcezZgJWnGDAEI6mZMBg.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nm"><img src="../Images/c80bf872122b5129e6594ee19a73901e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5RGS1fcz81Q93706wmZMw.png"/></div></div></figure><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="9fa9" class="kp kq hi md b fi mh mi l mj mk">from sklearn.linear_model import PassiveAggressiveClassifier</span><span id="9515" class="kp kq hi md b fi ml mi l mj mk">linear_clf=PassiveAggressiveClassifier()</span><span id="5364" class="kp kq hi md b fi ml mi l mj mk"># Fitting model<br/>linear_clf.fit(X_train, y_train)</span><span id="5011" class="kp kq hi md b fi ml mi l mj mk">predictions=linear_clf.predict(X_test)<br/>predicted_y_train = linear_clf.predict(X_train)</span><span id="09db" class="kp kq hi md b fi ml mi l mj mk">print("Accuracy on Train Data using PassiveAgressive Model",round(metrics.accuracy_score(y_train, predicted_y_train),2))</span><span id="f3a7" class="kp kq hi md b fi ml mi l mj mk">pred = linear_clf.predict(X_test)</span><span id="c0bc" class="kp kq hi md b fi ml mi l mj mk">print("Total number of validation data points :", len(predicted_y))<br/>print("Accuracy on Validation Data using PassiveAgressive Model",round(metrics.accuracy_score(y_test,pred),2))</span><span id="402e" class="kp kq hi md b fi ml mi l mj mk">plot_confusion_matrix(y_test, pred)</span></pre><p id="5767" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">感谢你阅读❤</p><h2 id="07e3" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">参考</h2><ul class=""><li id="74d9" class="mn mo hi jh b ji lk jl ll jo nn js no jw np ka ms mt mu mv bi translated"><strong class="jh hj">文字袋模型</strong>简介作者— <a class="ae lp" href="https://machinelearningmastery.com/author/jasonb/" rel="noopener ugc nofollow" target="_blank"> <strong class="jh hj">杰森·布朗利</strong> </a></li></ul><h2 id="c3f2" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jo la lb lc js ld le lf jw lg lh li lj bi translated">资源</h2><ul class=""><li id="b805" class="mn mo hi jh b ji lk jl ll jo nn js no jw np ka ms mt mu mv bi translated"><strong class="jh hj"> Kaggle 竞赛—</strong><a class="ae lp" href="https://www.kaggle.com/c/fake-news/data" rel="noopener ugc nofollow" target="_blank"><strong class="jh hj">https://www.kaggle.com/c/fake-news/data</strong></a><strong class="jh hj">。</strong></li></ul><p id="1de8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi">__________________________________________________________________</p><p id="0398" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于任何建议或疑问，请在下面留下您的评论，并关注更新。</p><p id="a8f8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你喜欢这篇文章，请点击👏图标来支持它。这将有助于其他媒体用户找到它。分享一下，让别人也能看！</p><p id="1830" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">快乐学习！😊</p></div></div>    
</body>
</html>