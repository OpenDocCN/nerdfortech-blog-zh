<html>
<head>
<title>Regularization — Tackling Overfitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化—解决过度拟合问题</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/regularization-tackling-overfitting-228134de7cf5?source=collection_archive---------10-----------------------#2021-02-15">https://medium.com/nerd-for-tech/regularization-tackling-overfitting-228134de7cf5?source=collection_archive---------10-----------------------#2021-02-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/c2a1e71265adc8d894f34e8778263abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sYrH_81DaSDPr3QyEbmnKA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">照片由<a class="ae hv" href="https://unsplash.com/@2mduffel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">马克·达弗尔</a>在<a class="ae hv" href="https://unsplash.com/s/photos/rules-and-regulations?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><div class=""/><p id="b18e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">正则化</strong>是一种惩罚复杂模型的原则，这样它们可以更好地推广。它可以防止过度拟合。在这个博客中，我们将访问常见的正则化技术。</p><blockquote class="jt"><p id="ae3b" class="ju jv hy bd jw jx jy jz ka kb kc js dx translated">你的神经网络只和你输入的数据一样好。T9】</p></blockquote><h2 id="c805" class="ke kf hy bd kg kh ki kj kk kl km kn ko jg kp kq kr jk ks kt ku jo kv kw kx ky bi translated">数据扩充</h2><p id="c658" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">深度学习神经网络的性能通常会随着可用数据量的增加而提高。但是我们通常没有大量的数据。数据扩充是一种从现有训练数据中人工创建新训练数据的技术。根据我们何时应用这些变换，我们有两种类型的增强:</p><p id="ab0e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">在线— </strong>预先执行所有必要的转换</p><p id="4811" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">离线— </strong>在将小批量产品送入 ML 模型之前，对其进行转换</p><p id="d323" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">图像数据增强的例子</strong></p><blockquote class="le lf lg"><p id="fb7a" class="iv iw lh ix b iy iz ja jb jc jd je jf li jh ji jj lj jl jm jn lk jp jq jr js hb bi translated">PCA <br/>翻转、旋转<br/>裁剪、缩放</p></blockquote><p id="c988" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有条件的 GANs 或风格转移也可以用来生成更多的数据。</p><p id="570c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">自然语言处理数据扩充示例</strong></p><blockquote class="le lf lg"><p id="5469" class="iv iw lh ix b iy iz ja jb jc jd je jf li jh ji jj lj jl jm jn lk jp jq jr js hb bi translated">同义词替换<br/>随机插入/删除<br/>单词嵌入</p></blockquote><p id="7fe2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">数值数据示例</strong></p><blockquote class="le lf lg"><p id="844e" class="iv iw lh ix b iy iz ja jb jc jd je jf li jh ji jj lj jl jm jn lk jp jq jr js hb bi translated">重击</p></blockquote><h2 id="dafb" class="ke kf hy bd kg kh ll kj kk kl lm kn ko jg ln kq kr jk lo kt ku jo lp kw kx ky bi translated">拒绝传统社会的人</h2><p id="4681" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">Dropout 是一种正则化技术，在训练期间将随机选择的神经元的激活值归零。在神经网络中，每一层都实现了丢失。不同的图层类型应有不同的漏失。</p><p id="df8c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意:退出将增加收敛所需的迭代次数。但是由于它减少了计算，每次迭代都更快。</p><h2 id="ce3c" class="ke kf hy bd kg kh ll kj kk kl lm kn ko jg ln kq kr jk lo kt ku jo lp kw kx ky bi translated">全体</h2><p id="3e0c" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">打包——结合优秀的学习者，平滑他们的预测</p><p id="6e5d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Bagging 是一种减少预测方差的方法，它通过使用重复组合从数据集生成用于训练的附加数据，以产生多组原始数据。通过增加训练集的大小，您只是减少了方差，将预测调整到预期的结果。《出埃及记》<strong class="ix hz">随机森林</strong></p><p id="0c1c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">提升——将弱学习者组合成强学习者</p><p id="31ec" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">升压是减少偏差的一种方式。子集的创建不是随机的，而是取决于先前模型的性能。每个新的子集都包含被以前的模型错误分类的元素。《出埃及记》<strong class="ix hz"> XGBoost </strong></p><h2 id="4f4a" class="ke kf hy bd kg kh ll kj kk kl lm kn ko jg ln kq kr jk lo kt ku jo lp kw kx ky bi translated">提前停止</h2><p id="bde0" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">在模型建立期间，在每个时期之后，在单独的验证数据集上评估模型。如果验证数据集上的模型的验证损失(或任何其他度量)开始下降，则训练过程停止。这个过程叫做提前停止。如果模型在当前数据集上的性能比前一时期更好，则我们保存模型权重。最后，选择具有最佳性能的权重。这里重要的一点是选择最能定义模型性能的正确的性能指标。</p><p id="bcb3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">早停可以通用。</p><h2 id="bffb" class="ke kf hy bd kg kh ll kj kk kl lm kn ko jg ln kq kr jk lo kt ku jo lp kw kx ky bi translated">添加噪声</h2><p id="ea72" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">噪声的添加具有正则化效果，并且反过来提高了模型的鲁棒性。噪声(高斯噪声)阻止模型记忆训练样本。一般在输入层加噪声，但不限于。它可以随机添加到网络的其他部分。</p><blockquote class="le lf lg"><p id="1695" class="iv iw lh ix b iy iz ja jb jc jd je jf li jh ji jj lj jl jm jn lk jp jq jr js hb bi translated"><strong class="ix hz">激活</strong> —在深度网络中有用<br/> <strong class="ix hz">权重— </strong>对 RNNs 有用。<br/> <strong class="ix hz">梯度</strong> —适用于深度全连接网络</p></blockquote><p id="718a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意:在添加噪声之前，应该对相关参数进行缩放。</p><h2 id="f416" class="ke kf hy bd kg kh ll kj kk kl lm kn ko jg ln kq kr jk lo kt ku jo lp kw kx ky bi translated"><strong class="ak">批量归一化</strong></h2><p id="8e10" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">隐藏层激活的值在训练过程中会发生变化。在批处理规范化中，我们通过使用当前批处理中值的平均值和标准差(或方差)来规范化每个图层的输入。基本上，我们正在进行标准化，不仅是在开始，而是在整个网络。每个小批量使用其平均值和标准偏差进行缩放。这给每个层引入了一些噪声，提供了一种正则化效果。</p><h2 id="0596" class="ke kf hy bd kg kh ll kj kk kl lm kn ko jg ln kq kr jk lo kt ku jo lp kw kx ky bi translated">L1(拉索)正则化</h2><p id="4e92" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">当要素数量较多时，L1 正则化是一个很好的选择，L1 提供了一个稀疏的解决方案，即移除要素。它在计算上不太昂贵。</p><h2 id="ce43" class="ke kf hy bd kg kh ll kj kk kl lm kn ko jg ln kq kr jk lo kt ku jo lp kw kx ky bi translated">L2(岭)正则化</h2><p id="41b8" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">相互依赖倾向于增加系数方差，使系数不可靠/不稳定，这损害了模型的通用性。性别和怀孕特征对是相互依赖特征的一个例子。L2 减少了这些估计的方差，抵消了相互依存的影响。</p></div></div>    
</body>
</html>