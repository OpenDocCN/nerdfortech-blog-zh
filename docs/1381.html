<html>
<head>
<title>K-Means Clustering using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means使用Python进行聚类</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/k-means-clustering-using-python-2150769bd0b9?source=collection_archive---------4-----------------------#2021-03-16">https://medium.com/nerd-for-tech/k-means-clustering-using-python-2150769bd0b9?source=collection_archive---------4-----------------------#2021-03-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7855" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以前我写过一个流行的监督学习算法，线性回归，在今天的帖子里我会写关于无监督学习算法，K-means算法。K-Means是一种简单且广泛使用的聚类算法。</p><p id="5455" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K-Means通常用于将特定的数据分组到某些类中，聚类本身属于无监督学习算法，这意味着在我们创建模型时，没有以前被标记的数据，也没有训练过程。</p><p id="0e68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K-means算法的步骤简单描述如下:</p><ol class=""><li id="9cc1" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">决定将用于对数据分组的簇k的数量</li><li id="cbda" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">随机选择每个簇的中心/质心，显然，需要选择的质心的数量是基于你在步骤1中决定的簇的数量</li><li id="cc0c" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">计算所有数据到所有k形心/中心聚类之间的距离</li><li id="2d03" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">将数据分配给与质心距离最近的聚类</li><li id="a3ca" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">通过计算每个聚类的平均值来决定新的质心</li><li id="0280" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">从步骤3开始重复该过程，直到质心值没有变化，或者当数据保持在同一聚类中时(数据没有移动到其他聚类)</li></ol><p id="f0a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果有多个变量将被用作聚类的特征，为了计算数据和质心之间的距离，我们可以使用欧几里德距离公式，如果你对它感兴趣，可以谷歌一下，互联网上有很多资源，因为这里我不会解释欧几里德公式。</p></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><p id="0035" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，我将使用爱尔兰样本数据集，就像往常一样，我将只使用google colab作为编码环境。我强烈推荐你使用google colab来练习python数据科学和分析，因为它非常轻便，易于使用。</p><p id="de5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们开始吧…</p><p id="a16a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们需要导入项目所需的所有库。在这里，我导入熊猫、seaborn、NumPy和matplotlib。当我们使用python时，这些库非常常见。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="c725" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#import library</strong><br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>import numpy as np</span></pre><p id="ad4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步我将使用。熊猫图书馆的read_csv函数。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="354b" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#read Iris dataset</strong><br/>data = pd.read_csv("/content/Iris.csv")<br/>data</span></pre><p id="e0d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们可以看到数据由150行和6列组成。为了对数据进行聚类，我们将只使用特性列，有SepalLengthCm、SepalWidthCm、PetalLengthCm和PetalWidthCm。我们将放弃物种栏。</p><figure class="jy jz ka kb fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kn"><img src="../Images/0fd60edb50ae6da835a43894c20ae0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*jLvANeiwLtPD3Xb4B7aXMQ.jpeg"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">爱尔兰数据集</figcaption></figure><p id="4cae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了只获取特征数据，我将使用。iloc函数和我会将新的数据帧存储在一个名为kmeans_data的变量中。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="b989" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#get the feature columns only</strong><br/>kmeans_data = data.iloc[:,1:5]<br/>kmeans_data</span></pre><p id="91b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们最终只得到特性列。</p><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es kz"><img src="../Images/3f9c1ad7e994c95ce33a5f2d3726d589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*tJwGg2jAOOKIAL3RsLjJGQ.jpeg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">爱尔兰数据集(仅要素)</figcaption></figure><p id="4e79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考上一节中的K-Means算法步骤，我们需要做的第一步是选择聚类数，我们通常称之为K。实际上有很多方法可以用来找到最佳的聚类数(K)，但在这篇文章中，我将只使用一种简单而流行的方法，称为Elbow方法。在肘形法中，选择的聚类数是图上肘形形成的位置。</p><p id="119c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们需要从上述特性中获取值，并以数组格式存储它们。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="88b2" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#get the value from the features in array format</strong><br/>features_value = kmeans_data.values<br/>features_value</span></pre><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es la"><img src="../Images/6b57c43bd8da3f232f6bca560a7a7fd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*_h9pXVfKLvquaT74QdMqgA.jpeg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">数组格式的特征值</figcaption></figure><p id="18db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在肘方法中，有两个度量用于定义k-最优值，它们是失真和惯性。失真是每个数据点到质心的距离的平方和的平均值，而惯性只是数据点到聚类中心/质心的距离的平方和。这里的距离可以是欧几里德距离。</p><p id="abeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们编写下面的代码来找出每个K数的惯性和失真。这里K值定义为从1到11。为了求惯性，我们可以用函数。惯性_。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="c2e5" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#finding the optimal number of k for clustering using elbow method</strong></span><span id="d3b6" class="kh ki hi kd b fi lb kk l kl km">from sklearn.cluster import KMeans<br/>inertia = []<br/>K = range(1,11)<br/>for k in K:<br/>    km = KMeans(n_clusters=k)<br/>    km = km.fit(features_value)<br/>    inertia.append(km.inertia_)<br/>    distortions.append(sum(np.min(cdist(features_value,<br/>    km.cluster_centers_, 'euclidean'),axis=1)) /<br/>    features_value.shape[0])</span></pre><p id="d5d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了最终决定K的最佳值，我们需要绘制从前面的代码中得到的数据。这里我将给出惯性计算的图片，惯性和扭曲实际上给出相同的结果。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="4760" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#Plotting the inertia result to find the elbow</strong></span><span id="6554" class="kh ki hi kd b fi lb kk l kl km">plt.plot(K, inertia, 'bx-')<br/>plt.xlabel('k')<br/>plt.ylabel('Sum_of_squared_distances')<br/>plt.title('Elbow Method For Optimal k')<br/>plt.show()</span></pre><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es lc"><img src="../Images/0645cb927674bfe961a61dac4a387d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*fsvxeraDUE5_hDc4WPbxjw.png"/></div></figure><p id="6fc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果很难确定图形的哪一部分是肘部，我们可以使用kneed定位器库。通过使用kneed定位器，我们最终发现当k = 3时，肘部被定位。因此，我们将用于构建K均值模型的聚类数是三个聚类。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="fdcc" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#finding the elbow using kneed locator</strong></span><span id="55d2" class="kh ki hi kd b fi lb kk l kl km">from kneed import KneeLocator<br/>kl = KneeLocator(range(1, 11), inertia, curve="convex", direction="decreasing")<br/>kl.elbow</span></pre><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es ld"><img src="../Images/23618e9ab295c9ef278a33abd6a7873b.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*5wONylwgYgRc9naZIf01wg.jpeg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">最佳聚类数</figcaption></figure><p id="18a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们可以像下面的代码一样应用K数= 3的K均值聚类。我们得到每个数据点的聚类，以numpy数组表示。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="afa0" class="kh ki hi kd b fi kj kk l kl km">kmeans = KMeans(n_clusters=3, random_state=100)<br/>kmeans.fit(features_value)<br/>y_kmeans = kmeans.predict(features_value)<br/>y_kmeans</span></pre><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es le"><img src="../Images/4e45b407e5d83f6f11d02013d7544976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*jAN63FaBhR1cdJ7ctmUPWw.jpeg"/></div></figure><p id="0d7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以添加一个新列来表示前一个数据框中每个数据的聚类，这里我将添加一个名为cluster的列。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="2b98" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#Add new column to store the cluster </strong><br/>kmeans_data['cluster'] = y_kmeans<br/>kmeans_data</span></pre><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es lf"><img src="../Images/77238c51b57d6c3175a34ecd577fd9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*N5lbETpp9ijAvyubu46d4w.jpeg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">具有簇列的新数据框</figcaption></figure><p id="6f40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在可视化集群之前，我想找到每个集群的质心。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="e005" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#find the centroid</strong></span><span id="75f0" class="kh ki hi kd b fi lb kk l kl km">centers = np.array(kmeans.cluster_centers_)<br/>centers<br/>centroid = pd.DataFrame(centers)<br/>centroid</span></pre><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es lg"><img src="../Images/19d7ec2e21af097362db4929e406f33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*hTjAtL7rHnjhV4Kux0hv7Q.jpeg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">每个聚类的质心</figcaption></figure><p id="a326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们创建可视化。在这里，我基于两个第一特征创建可视化，SepalWidth和SepalLength。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="9b37" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#last we will visualizing the clustering result using seaborn based on sepalwidth and sepalLegnth</strong></span><span id="af56" class="kh ki hi kd b fi lb kk l kl km">sns.scatterplot(x = knn_data.SepalLengthCm, y =knn_data.SepalWidthCm, s = 50, c = knn_data.cluster, marker = "o", hue = knn_data.cluster)<br/>sns.scatterplot(x = centers[:,0], y = centers[:,1], marker="o", color='r', s = 70, label="centroid")</span></pre><figure class="jy jz ka kb fd ko er es paragraph-image"><div class="er es lh"><img src="../Images/03eff3bfb4215bd34fb95ca8177817d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*V6jROgouTFLkbjYoTf2eMA.png"/></div></figure><p id="bbdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><p id="18f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae li" href="http://Reference Clustering" rel="noopener ugc nofollow" target="_blank">https://rubiks code . net/2020/10/05/回到机器学习基础知识聚类/ </a></p></div></div>    
</body>
</html>