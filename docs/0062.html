<html>
<head>
<title>Bias-Variance Tradeoff for Dummies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">假人的偏差-方差权衡</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/bias-variance-tradeoff-for-dummies-9f13147ab7d0?source=collection_archive---------1-----------------------#2019-10-30">https://medium.com/nerd-for-tech/bias-variance-tradeoff-for-dummies-9f13147ab7d0?source=collection_archive---------1-----------------------#2019-10-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6646af8950e3a22179a602a1885fb465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JX57lp9Yh12-7DsrX61tzQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">“简单的模型是最好的模型”——奥卡姆剃刀</figcaption></figure><h2 id="4c09" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">偏差和方差概述</h2><p id="002f" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km hb bi translated">偏差是模型为使目标函数更容易学习而做出的简化假设。偏差是一种以自我为中心的方法，它告诉我们决策与数据有多远。(通俗地说，偏见有它的自我，它不在乎数据)</p><p id="8dbd" class="pw-post-body-paragraph js jt hi ju b jv kn jx jy jz ko kb kc jf kp ke kf jj kq kh ki jn kr kk kl km hb bi translated">一般来说，线性算法有很高的偏向性，这使得它们学习起来很快，也更容易理解，但通常不太灵活。就预测性能而言，它们在无法满足简化假设的复杂问题上表现不佳。大量的偏差可能会导致数据拟合不足，因为如果我们有很高的偏差，它甚至会在不关心数据的情况下做出假设。</p><ul class=""><li id="5439" class="ks kt hi ju b jv kn jz ko jf ku jj kv jn kw km kx ky kz la bi translated"><strong class="ju hj">高偏差</strong>:暗示关于目标函数形式的更多假设。高偏差机器学习算法包括线性回归、线性判别分析和逻辑回归。</li><li id="ed0b" class="ks kt hi ju b jv lb jz lc jf ld jj le jn lf km kx ky kz la bi translated"><strong class="ju hj">低偏差</strong>:暗示对目标函数形式的假设较少。低偏差机器学习算法包括决策树、k近邻和支持向量机。</li></ul><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/3ebfa97fce31fd3f730cfe0bb396d708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tww_DrPhpsuNK6mg4xFGA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><em class="ll">图1:偏倚和方差的图解说明。<br/>来源:</em><a class="ae lm" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank"><em class="ll">http://scott.fortmann-roe.com/docs/BiasVariance.html</em></a></figcaption></figure><p id="2fcb" class="pw-post-body-paragraph js jt hi ju b jv kn jx jy jz ko kb kc jf kp ke kf jj kq kh ki jn kr kk kl km hb bi translated">方差是如果使用不同的训练数据，目标函数的估计将改变的量。它指的是算法对特定训练数据集的敏感度，即如果你在不同的训练数据集上训练，机器学习模型的预测会改变多少。(通俗地说，方差指的是紧跟数据点，即随波逐流)</p><p id="5230" class="pw-post-body-paragraph js jt hi ju b jv kn jx jy jz ko kb kc jf kp ke kf jj kq kh ki jn kr kk kl km hb bi translated">具有高方差的机器学习算法受到训练数据细节的强烈影响。高方差机器学习算法对数据非常敏感，这可能导致训练数据的过拟合。</p><ul class=""><li id="b335" class="ks kt hi ju b jv kn jz ko jf ku jj kv jn kw km kx ky kz la bi translated"><strong class="ju hj">高方差</strong>:表示目标函数的估计值随着训练数据集的变化而发生较大变化。高方差机器学习算法包括决策树、k近邻和支持向量机。</li><li id="d64f" class="ks kt hi ju b jv lb jz lc jf ld jj le jn lf km kx ky kz la bi translated"><strong class="ju hj">低方差</strong>:建议随着训练数据集的改变，对目标函数的估计进行小的改变。低方差机器学习算法包括线性回归、线性判别分析和逻辑回归。</li></ul><h2 id="2e16" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">偏差-方差二分法</h2><p id="8eb9" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km hb bi translated">这里的概念是，虽然增加机器学习模型的复杂性可能会提高对训练数据的拟合，但它不需要提高对训练数据(即新数据)的预测精度。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/611feced90586e711462e3dbb0a8f7fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*D3r_BiTnQMvSwnzf4PC6tw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><em class="ll">图2:偏差-方差权衡的图解说明(蓝色-训练拟合，红色-测试拟合)</em></figcaption></figure><p id="aafd" class="pw-post-body-paragraph js jt hi ju b jv kn jx jy jz ko kb kc jf kp ke kf jj kq kh ki jn kr kk kl km hb bi translated">从上图中，我们可以看到，随着模型复杂性的增加，通过密切跟踪训练数据集的拟合，测试数据的拟合变得更好。测试数据的预测误差随着模型复杂性的增加而减小，直到某一点(当达到模型的正确复杂性时预测误差最小，也称为“最佳点”)，然后预测误差随着模型复杂性的进一步增加而增加。</p><blockquote class="lo lp lq"><p id="434b" class="js jt lr ju b jv kn jx jy jz ko kb kc ls kp ke kf lt kq kh ki lu kr kk kl km hb bi translated">在这里，奥卡姆剃刀理论出现了，它说“没有必要，实体不应该繁殖”。在机器学习的背景下，这意味着与简单模型相比，不必要地增加模型复杂性将产生相对糟糕的结果。</p></blockquote><h2 id="25a8" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">例子</strong></h2><p id="23db" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km hb bi translated">偏见车将是一种不太关心数据而做出简化假设的车，无论我们以何种方式训练，它都不会有任何不同。但另一方面，如果我们制造一辆对数据有极强感知能力(即高方差)的汽车，那么它只能复制它以前见过的东西。这个模型的问题是，它在以前没有见过的情况下反应很差，因为它没有正确的偏见来归纳新的数据。</p><h2 id="3a51" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">结论</h2><p id="bce3" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km hb bi translated">实际上，我们希望在一定程度的偏差和方差之间找到一个中间点，称为偏差-方差权衡。最佳的机器学习模型应该具有一定的归纳权威，但同时，它应该非常开放地听取数据。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="97da" class="pw-post-body-paragraph js jt hi ju b jv kn jx jy jz ko kb kc jf kp ke kf jj kq kh ki jn kr kk kl km hb bi translated">如果你想了解更多，或者想让我写更多关于这个主题的东西，请随时联系…</p><p id="0369" class="pw-post-body-paragraph js jt hi ju b jv kn jx jy jz ko kb kc jf kp ke kf jj kq kh ki jn kr kk kl km hb bi translated">我的社交链接:<a class="ae lm" href="https://www.linkedin.com/in/shubhamsaboo/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae lm" href="https://twitter.com/Saboo_Shubham_" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae lm" href="https://github.com/Shubhamsaboo" rel="noopener ugc nofollow" target="_blank">Github</a></p><p id="d398" class="pw-post-body-paragraph js jt hi ju b jv kn jx jy jz ko kb kc jf kp ke kf jj kq kh ki jn kr kk kl km hb bi translated"><em class="lr">如果您喜欢这篇文章或觉得它有帮助，请花一分钟时间按一下鼓掌按钮，它会增加文章对其他媒体用户的可见性。</em></p><h2 id="d13d" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">参考资料:</h2><ul class=""><li id="aa44" class="ks kt hi ju b jv jw jz ka jf mc jj md jn me km kx ky kz la bi translated"><a class="ae lm" href="http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html" rel="noopener ugc nofollow" target="_blank">http://www . cs . Cornell . edu/courses/cs 4780/2018 fa/lectures/lecture note 12 . html</a></li><li id="ad18" class="ks kt hi ju b jv lb jz lc jf ld jj le jn lf km kx ky kz la bi translated"><a class="ae lm" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Bias % E2 % 80% 93 variance _ trade off</a></li></ul></div></div>    
</body>
</html>