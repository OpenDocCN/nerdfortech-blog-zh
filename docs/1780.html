<html>
<head>
<title>Review — CornerNet: Detecting Objects as Paired Keypoints (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾— CornerNet:将对象检测为成对的关键点(对象检测)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b?source=collection_archive---------2-----------------------#2021-04-06">https://medium.com/nerd-for-tech/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b?source=collection_archive---------2-----------------------#2021-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="401b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">检测死角，胜过<a class="ae ix" rel="noopener" href="/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4">屏蔽 R-CNN </a>，<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快 R-CNN </a>，<a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener"> CoupleNet </a>，<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank"> G-RMI </a>，<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>，<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------"> TDM </a>，<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">约洛夫 2 </a>，<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>，<a class="ae ix" href="https://sh-tsang.medium.com/review-grf-dsod-grf-ssd-improving-object-detection-from-scratch-via-gated-feature-reuse-495c11b627d3" rel="noopener"> GRF-DSOD </a>，<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>，<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a>，<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"/></h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/a86893ae11d1a80b2ae081ea9ff2ae50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U8rvAkbf45Uek-k9nYmtNQ.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo"> CornerNet 检测左上角和右下角</strong></figcaption></figure><p id="ddd4" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di">在</span>这个故事中，回顾了密歇根大学的<strong class="jr hj"> CornerNet:将对象检测为成对关键点</strong>(corner net)。在本文中:</p><ul class=""><li id="4fd6" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">对象边界框被检测为<strong class="jr hj">一对关键点，左上角和右下角</strong>，消除了设计现有单级检测器中常用的一组锚框的需要。</li></ul><p id="994b" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这是一篇发表在<strong class="jr hj"> 2018 ECCV </strong>的论文，被<strong class="jr hj">引用超过 900 次</strong>。(<a class="ld le ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----ffb23026291b--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @ Medium)</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="4693" class="lm ln hi bd jo lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">概述</h1><ol class=""><li id="468e" class="ku kv hi jr b js md jv me jy mf kc mg kg mh kk mi la lb lc bi translated"><strong class="jr hj"> CornerNet:网络架构</strong></li><li id="df57" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk mi la lb lc bi translated"><strong class="jr hj">角点检测(热图&amp;偏移)</strong></li><li id="9ba1" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk mi la lb lc bi translated"><strong class="jr hj">角落分组(嵌入)</strong></li><li id="3e1f" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk mi la lb lc bi translated"><strong class="jr hj">角落池</strong></li><li id="84ab" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk mi la lb lc bi translated"><strong class="jr hj">与最先进探测器的比较</strong></li></ol></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="6574" class="lm ln hi bd jo lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">1.CornerNet:网络架构</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mo"><img src="../Images/1861fb8f0cdbc0f78b29ce04a3e270dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*64PUWScdjXJeE3m30HS2hg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo"> CornerNet:网络架构</strong></figcaption></figure><ul class=""><li id="cc9a" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><a class="ae ix" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank"> <strong class="jr hj">纽维尔·ECCV‘16</strong></a>中使用的沙漏网络，原本用于人体姿态估计，作为<strong class="jr hj">骨干</strong>。</li><li id="3566" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">使用<a class="ae ix" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank">纽维尔·ECCV 的</a>作为主干可能是因为现在 CornerNet 将<strong class="jr hj">检测关键点</strong>，类似于人类姿势估计网络的目的。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mp"><img src="../Images/287e372cdbf4bd5463a1fda58d808b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N8oeJTzhkcZze0dqMYw2Mg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">覆盖在角落预测热图上的边界框预测的两个例子。左:</strong>左上角热点图<strong class="bd jo">，右:</strong>右下角热点图</figcaption></figure><ul class=""><li id="f8d1" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">每个关键点被预测为一个热图。</strong></li><li id="a81f" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated"><strong class="jr hj">这里，角点被视为关键点。</strong></li><li id="66f1" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">在<a class="ae ix" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank">纽维尔·ECCV 16</a>中，头部、肩膀、手掌等被视为关键点。</li><li id="8cc3" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">沙漏网络后面是<strong class="jr hj">两个预测模块。</strong></li><li id="ef37" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">一个模块用于<strong class="jr hj">左上角</strong>，另一个模块用于<strong class="jr hj">右下角</strong>。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mq"><img src="../Images/2b39fd693de39011bd9502fabdad62ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*pLCNtad-5vsNWQ09f-VzrA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">预测热图、嵌入和偏移的多个分支。</strong></figcaption></figure><ul class=""><li id="fdcd" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">每个模块都有自己的<strong class="jr hj">角池模块</strong>，如上所示，从沙漏网络中汇集要素，然后<strong class="jr hj">预测热图、嵌入和偏移。</strong></li><li id="7092" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">沙漏网络的<strong class="jr hj">深度</strong>为<strong class="jr hj"> 104 </strong>。</li><li id="07c4" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">与许多其他先进的检测器不同，只有来自整个网络最后一层的特征用于进行预测。</li><li id="72fb" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">全部训练损失为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mr"><img src="../Images/97fc7e3b2de0a67e7ea904bfd91f4725.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*RqoXaCRAhjh066_VWp30xA.png"/></div></figure><ul class=""><li id="c59d" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">其中<strong class="jr hj"> <em class="ms"> Ldet </em>为热图的检测损失，<em class="ms"> Lpull </em>和<em class="ms"> Lpush </em>为嵌入损失，<em class="ms"> Loff </em>为偏移损失。</strong>这些损失将在下文详细描述。</li><li id="3870" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">α和<em class="ms"> β </em>设置为 0.1，<em class="ms"> γ </em>设置为 1。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="91aa" class="lm ln hi bd jo lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">2.拐角检测(热图和偏移)</h1><ul class=""><li id="d66d" class="ku kv hi jr b js md jv me jy mf kc mg kg mh kk kz la lb lc bi translated">每套热图有<em class="ms"> C </em>个通道，其中<em class="ms"> C </em>为类别数，大小为<em class="ms"> H </em> × <em class="ms"> W </em>。</li><li id="19f9" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">设<em class="ms"> pcij </em>为预测热图中类别<em class="ms"> c </em>在位置(<em class="ms"> i </em>，<em class="ms"> j </em>)处的得分，设<em class="ms"> ycij </em>为用非标准化高斯增强的“真实”热图，类似于人的姿态估计。</li><li id="0c91" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">在<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a>中使用焦点损失，检测损失<em class="ms"> Ldet </em>为:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mt"><img src="../Images/21b24f43f72da322cd189f25727a41f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*lxLmvkp8HDGjdmBXIYvwXQ.png"/></div></figure><ul class=""><li id="ecb0" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">其中<em class="ms"> N </em>是一幅图像中物体的数量，<em class="ms"> α </em> =2，<em class="ms"> β </em> =4。</li><li id="8939" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">将热图中的位置重新映射到输入图像，可能会损失一些精度，这可能会极大地影响小边界框与其地面事实的 IoU。</li><li id="1792" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">因此，在将角点位置重新映射到输入分辨率之前，会预测位置偏移来稍微调整角点位置。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mu"><img src="../Images/9cf870c399afebaf7067e99eca2dd314.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*ExtENEraV8m1YxjZDYP86w.png"/></div></figure><ul class=""><li id="7ed8" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">其中<em class="ms"> ok </em>为偏移量，<em class="ms"> xk </em>和<em class="ms"> yk </em>为转角<em class="ms"> k </em>的<em class="ms"> x </em>和<em class="ms"> y </em>坐标。</li><li id="a737" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">使用平滑 L1 损耗，如在快速 R-CNN 中，在地面真实角点位置:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mv"><img src="../Images/34cb3470c79dfc2f7c0cd342edded1c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*gLWpY8_VBb547louT8KEZA.png"/></div></figure></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="67a0" class="lm ln hi bd jo lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">3.拐角分组(嵌入)</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mw"><img src="../Images/07b6471e78d3bcb28c97c47a5de739d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*4L3LNsRDVxJH4Mc8ZNMeaA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">网络被训练来预测属于同一对象的角的相似嵌入。</strong></figcaption></figure><ul class=""><li id="006b" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">受<a class="ae ix" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank">纽维尔·ECCV‘16</a>中的关联嵌入的启发，<strong class="jr hj"> CornerNet 为每个检测到的角预测一个嵌入向量，这样，如果左上角和右下角属于同一个边界框，它们的嵌入之间的距离应该很小。</strong></li><li id="30b7" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">嵌入的实际值并不重要。<strong class="jr hj">只有嵌入之间的距离被用来对角进行分组。</strong></li><li id="b669" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">设<em class="ms"> etk </em>为物体<em class="ms"> k </em>左上角的嵌入，<em class="ms"> ebk </em>为右下角的嵌入。</li><li id="3232" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated"><strong class="jr hj">“拉”损失</strong>用于训练网络<strong class="jr hj">将拐角</strong>和<strong class="jr hj">分组，“推”损失</strong>用于<strong class="jr hj">分离拐角</strong>:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mx"><img src="../Images/2442c575003ea5c61dca1f91830a884a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*xnCU_4-mBofBv16o-xLkUQ.png"/></div></figure><ul class=""><li id="0ace" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">其中<em class="ms"> ek </em>是<em class="ms"> etk </em>和<em class="ms"> ebk </em>的平均值。Δ=1.</li><li id="2882" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">与偏移损耗类似，损耗仅应用于地面真实角点位置。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es my"><img src="../Images/5f96212c3e089adf530a33248d36750f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mtOW_KcHzZKfQJRkThyHGA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">用事实代替预测的误差分析</strong></figcaption></figure><ul class=""><li id="785f" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">单独使用地面实况热图将 AP 从 38.5%提高到 74.0%。</li><li id="0048" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">用地面实况偏移替换预测偏移，AP 进一步增加 13.1%到 87.1%。</li><li id="6002" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">这表明，尽管在检测和分组角点方面还有很大的改进空间，但主要的瓶颈是检测角点。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="00ff" class="lm ln hi bd jo lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">3.角落联营</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mz"><img src="../Images/2bc8483e1d2e2afb42f186007b8b4123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQCsb2FRq48cDY0CGuweMg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">通常没有局部证据来确定包围盒拐角的位置</strong></figcaption></figure><ul class=""><li id="f3a7" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">如上所示，通常没有局部视觉证据来证明拐角的存在。</li><li id="3746" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">角点池的提出是为了通过编码明确的先验知识来更好地定位角点。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mx"><img src="../Images/6c3e54dba0ee7fc77c6d2fe68daf2aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*gBYTnXtUdTMwnLMJ3AdXfA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">角落汇集</strong></figcaption></figure><ul class=""><li id="6aca" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">它可以用下面的公式表示:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es na"><img src="../Images/9a7dbecf58b499edea600fef9dd276e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*3n8jewmEEUe6kyIRIguAOA.png"/></div></figure><ul class=""><li id="c4d8" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">下面举例说明:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nb"><img src="../Images/760ea0a9da64280214a518a7af2a2b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*bMvvK7nMMNGbJZFrl0-YDA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">角池示例</strong></figcaption></figure><ul class=""><li id="a8e7" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">左上角的池层可以非常高效地实现。</li><li id="a15f" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated"><strong class="jr hj">横向最大池从右向左扫描特征图，纵向最大池从下向上扫描特征图。</strong></li><li id="9c14" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">然后添加两个最大汇集特征图。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nc"><img src="../Images/30fe403ceb52f72a2b4b8304565b77bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5yk9tdPgE9sCbFDebqhZg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated"><strong class="bd jo">角落里汇集着对田蜜女士的验证。</strong></figcaption></figure><ul class=""><li id="2ffc" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated">有了角池，就有了显著的提高:在 AP 上提高了 2.0%，在 AP50 上提高了 2.1%，在 AP75 上提高了 2.2%。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="e7c6" class="lm ln hi bd jo lo lp lq lr ls lt lu lv io lw ip lx ir ly is lz iu ma iv mb mc bi translated">4.<strong class="ak">与最先进探测器的比较</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nd"><img src="../Images/0b226873814ba6b3dfd9a993d3e53248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c5937aeD619hYHqiLkBhFA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx translated">CornerNet 与其他公司在 COCO 测试开发上的对比。</figcaption></figure><ul class=""><li id="d5e2" class="ku kv hi jr b js jt jv jw jy kw kc kx kg ky kk kz la lb lc bi translated"><strong class="jr hj">经过多尺度评测，CornerNet 达到了 42.1% </strong>的 AP，优于现有的一阶段方法，如<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">约洛夫 2 </a>、<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>、<a class="ae ix" href="https://sh-tsang.medium.com/review-grf-dsod-grf-ssd-improving-object-detection-from-scratch-via-gated-feature-reuse-495c11b627d3" rel="noopener"> GRF-DSOD </a>、<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>、<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a>、<a class="ae ix" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener"> RefineDet </a>。</li><li id="da1f" class="ku kv hi jr b js mj jv mk jy ml kc mm kg mn kk kz la lb lc bi translated">CornerNet 也与级联 R-CNN 等两阶段方法相竞争，并且优于<a class="ae ix" rel="noopener" href="/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4">屏蔽 R-CNN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快 R-CNN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a>和 CoupleNet。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="7e20" class="ne ln hi bd jo nf ng nh lr ni nj nk lv jy nl nm lx kc nn no lz kg np nq mb nr bi translated">参考</h2><p id="5ebd" class="pw-post-body-paragraph jp jq hi jr b js md ij ju jv me im jx jy ns ka kb kc nt ke kf kg nu ki kj kk hb bi translated">【2018 ECCV】【corner net】<br/><a class="ae ix" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">corner net:将物体检测为成对的关键点</a></p><h2 id="ae7b" class="ne ln hi bd jo nf ng nh lr ni nj nk lv jy nl nm lx kc nn no lz kg np nq mb nr bi translated">目标检测</h2><p id="4755" class="pw-post-body-paragraph jp jq hi jr b js md ij ju jv me im jx jy ns ka kb kc nt ke kf kg nu ki kj kk hb bi translated"><strong class="jr hj"> 2014 </strong> : [ <a class="ae ix" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------">过食</a>][<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------">R-CNN</a>]<br/><strong class="jr hj">2015</strong>:[<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快 R-CNN </a> ] [ <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快 R-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a><br/><strong class="jr hj">2016 [<a class="ae ix" href="https://towardsdatascience.com/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac?source=post_page---------------------------" rel="noopener" target="_blank">GBD-网/GBD-v1&amp;GBD-v2</a>][<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae ix" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a><br/><strong class="jr hj">2017</strong>:[<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a?source=post_page---------------------------">NoC</a>][<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------">TDM</a>[<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>[<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 9000] [</a><a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener">couple net</a>]<br/><strong class="jr hj">2018</strong>:[<a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolov 3</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">Cascade R-CNN</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener">refined et</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">corner net</a>]【T78</strong></p><h2 id="b5a8" class="ne ln hi bd jo nf ng nh lr ni nj nk lv jy nl nm lx kc nn no lz kg np nq mb nr bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>