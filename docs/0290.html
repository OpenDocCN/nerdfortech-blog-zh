<html>
<head>
<title>Hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/hyperparameters-c0428b4fe46b?source=collection_archive---------1-----------------------#2020-09-11">https://medium.com/nerd-for-tech/hyperparameters-c0428b4fe46b?source=collection_archive---------1-----------------------#2020-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d351336d439457398df34047a8158170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UlRC7tEvxiPU2CNW6E5Kig.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片来自 Unsplash 上的 Feelfarbig 杂志</figcaption></figure><div class=""/><blockquote class="iu iv iw"><p id="6d08" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated">超参数是上下文配置值，用于控制模型从数据中学习并得出根据模型描述数据的参数的偏好或能力。</p></blockquote><p id="2685" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">算法从数据中自动学习(调整)它们的内部参数(如系数)。但是他们按照自己的步调做，完全是机械的感觉。为了在模型的学习过程和商业价值之间建立平衡，我们试图设定这些模型学习的速度、复杂度或抽象级别。历元的数量、树的深度、正则项所有这些都控制着模型在数据中发现模式的速度。这些参数帮助我们优化算法到模型的转换过程。这些超参数的选择是上下文相关的。没有什么灵丹妙药。寻找模型超参数的过程称为——超参数调整，该模型超参数在验证指标上产生最佳/可接受的分数。</p><p id="69d5" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hy">该过程大致有以下步骤:</strong></p><ol class=""><li id="c89d" class="jz ka hx ja b jb jc jf jg jw kb jx kc jy kd jv ke kf kg kh bi translated">估计器(线性回归或决策树)</li><li id="d24f" class="jz ka hx ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">得分函数(量化模型在迭代之间的学习)</li><li id="e101" class="jz ka hx ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">参数空间(超参数值边界)</li><li id="e238" class="jz ka hx ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">遍历参数空间的方法。</li><li id="dfa3" class="jz ka hx ja b jb ki jf kj jw kk jx kl jy km jv ke kf kg kh bi translated">交叉验证方案(K 倍)</li></ol><p id="8224" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">我们将集中讨论第 4 点。</p><p id="ef5e" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">我们可以将方法分为两部分:</p><h1 id="ad8f" class="kn ko hx bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">蛮力</h1><blockquote class="iu iv iw"><p id="594f" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated"><strong class="ja hy">grid search</strong>——在超参数空间中尝试所有可能的组合，然后决定最佳的、资源密集且耗时的组合</p><p id="45ee" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated"><strong class="ja hy"> RandomSearch </strong> —当我们知道某些参数比其他参数更有效时使用，从参数空间中随机选择值，改进网格搜索</p></blockquote><h1 id="b754" class="kn ko hx bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">启发式搜索</h1><blockquote class="iu iv iw"><p id="8165" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated"><strong class="ja hy">爬山</strong> —在超参数空间中寻找误差下降的最佳方向，然后停止，会陷入局部最大值</p><p id="0512" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated"><strong class="ja hy">贝叶斯搜索</strong> —考虑过去的所有迭代，并基于概率模型进行全局搜索。没有局部最大值问题。通常比网格搜索快，比随机搜索慢</p></blockquote><p id="4b01" class="pw-post-body-paragraph ix iy hx ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">参数调整愉快。</p></div></div>    
</body>
</html>