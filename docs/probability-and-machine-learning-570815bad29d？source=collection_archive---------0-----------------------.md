# 概率与机器学习？—第 1 部分—概率与非概率机器学习模型

> 原文：<https://medium.com/nerd-for-tech/probability-and-machine-learning-570815bad29d?source=collection_archive---------0----------------------->

数学是机器学习的基础，它的分支如线性代数、概率、统计都可以认为是 ML 的组成部分。作为一名计算机科学与工程专业的学生，我在本科期间遇到的一个问题是，通过数学课程获得的知识可以以何种方式应用于 ML，以及在 ML 中发挥基本作用的数学领域是什么。相信这是大部分对机器学习感兴趣的人的共同疑问。因此，我决定就“机器学习的数学”相关的一些基本概念写一个博客系列。在这一系列文章中，我的意图是提供一些研究领域的方向，并解释这些概念如何与 ML 相关联。我不打算深入研究这些概念，我相信有很多参考资料都有很好的例子来详细解释这些概念。

作为第一步，我想写一下概率和机器学习之间的关系。在机器学习中，有概率模型，也有非概率模型。为了更好地理解概率模型，关于随机变量和概率分布等概率的基本概念的知识将是有益的。我将在下一篇博客中写下这样的概念。然而，在这篇博客中，重点将是提供一些关于什么是概率模型以及如何区分一个模型是否是概率模型的想法。

![](img/aacfb93b8b4c00513f7aafdbfea12c95.png)

# 什么是概率机器学习模型？

为了理解什么是概率机器学习模型，我们来考虑一个有 N 个类的分类问题。如果分类模型(分类器)是概率性的，对于给定的输入，它将提供(N 个类别中的)每个类别的概率作为输出。换句话说，概率分类器将在 N 个类别上提供概率分布。通常，具有最高概率的类被选为输入数据实例所属的类。

然而，逻辑回归(这是一种基于 Sigmoid 函数的概率二元分类技术)可以被认为是一个例外，因为它仅提供与一个类别相关的概率(通常是类别 1，并且不必具有"*1-类别 1 的概率=类别 0 的概率"*关系)。由于这些性质，逻辑回归在[多标签分类](https://en.wikipedia.org/wiki/Multi-label_classification)问题中也是有用的，其中单个数据点可以有多个类别标签。

概率模型的一些例子是逻辑回归、贝叶斯分类器、隐马尔可夫模型和神经网络(具有 Softmax 输出层)。

如果模型是非概率性的(确定性的)，它通常只输出输入数据实例所属的最可能的类。传统的“支持向量机”是一种流行的非概率分类器。

让我们讨论一个例子来更好地理解概率分类器。以将动物图像分为五类的任务为例——{狗、猫、鹿、狮子、兔子}。作为输入，我们有一个(狗的)图像。对于这个例子，让我们考虑分类器工作良好，并为我们正在讨论的特定输入提供正确/可接受的结果。当图像作为输入被提供给概率分类器时，它将提供诸如(狗(0.6)，猫(0.2)，鹿(0.1)，狮子(0.04)，兔子(0.06)的输出。但是，如果分类器是非概率的，它将只输出“狗”。

# 为什么是概率 ML 模型？

概率模型的一个主要优点是，它们提供了一种关于预测的不确定性的想法。换句话说，我们可以了解机器学习模型对其预测的信心程度。如果我们考虑上面的例子，如果概率分类器为“狗”类分配概率 0.9 而不是 0.6，这意味着分类器更确信图像中的动物是狗。当涉及到疾病诊断和自动驾驶等关键的机器学习应用时，这些与不确定性和信心相关的概念极其有用。此外，概率结果将对许多与机器学习相关的技术有用，如[主动学习](https://en.wikipedia.org/wiki/Active_learning_(machine_learning))。

# 目标函数

为了识别一个特定的模型是否是概率性的，我们可以看看它的目标函数。在机器学习中，我们的目标是优化模型，以便在特定任务中表现出色。拥有目标函数的目的是基于模型的输出提供一个值，因此可以通过最大化或最小化特定值来实现优化。在机器学习中，目标通常是最小化预测误差。因此，我们将所谓的损失函数定义为目标函数，并试图在 ML 模型的训练阶段最小化损失函数。

如果我们采用线性回归等基本的机器学习模型，目标函数是基于误差平方的。训练的目标是最小化均方误差/均方根误差(RMSE)(等式。1).计算均方误差背后的直觉是，对特定数据点的预测所产生的损失/误差是基于实际值和预测值之间的差异(注意，当涉及到线性回归时，我们谈论的是回归问题，而不是分类问题)。

如果模型给出的预测明显高于或低于实际值，则特定数据点造成的损失会更高。当预测值与实际值非常接近时，损失会更小。如你所见，这里的目标函数不是基于概率，而是基于实际值和预测值之间的差异(绝对差异)。

![](img/230f0eea6fe94bc6f980da041c2f8390.png)

等式 1

这里，n 表示数据集中数据实例的数量，y_true 是正确/真值，y_predict 是预测值(通过线性回归模型)。

对于支持向量机，目标是最大化支持向量之间的间隔或距离。这个概念也被称为“大幅度直觉”。如您所见，在线性回归和支持向量机中，目标函数都不是基于概率的。因此，它们可以被认为是非概率模型。

另一方面，如果我们考虑具有 softmax 输出层的神经网络，损失函数通常使用交叉熵损失(CE 损失)来定义(等式。2).请注意，我们考虑的是具有“n”个数据点的训练数据集，因此最终将每个数据点的平均损失作为数据集的 CE 损失。这里，y_i 表示数据点 I 的真实标签，p(y_i)表示类别 y_i 的预测概率(该数据点属于由模型分配的类别 y_i 的概率为**)。**

交叉熵损失背后的直觉是:如果概率模型能够以高置信度预测数据点的正确类别，损失将会更小。在我们讨论的关于图像分类的示例中，如果模型为“狗”类(这是正确的类)提供了 1.0 的概率，则由于该预测导致的损失=-log(P(‘狗’))=-log(1.0)= 0。相反，如果“狗”类的预测概率为 0.8，则损失= -log(0.8)= 0.097。然而，如果模型为正确的类别提供了低概率，如 0.3，则损失= -log(0.3) = 0.523，这可以被认为是重大损失。

![](img/89f5da261d31baf8c75a13bd3ae83e72.png)

情商。2

在基于逻辑回归的二元分类模型中，损失函数通常使用二元交叉熵损失(BCE 损失)来定义。

![](img/c637a7af6e489b069999d9871afd43b1.png)

情商。3

这里，y_i 是类标签(如果相似，则为 1，否则为 0 ), p(s _ I)是数据集中每个点‘I’成为类 1 的预测概率。n 是数据点的数量。注意，由于这是一个二元分类问题，所以只有两个类，类 1 和类 0。

正如你所观察到的，这些损失函数是基于概率的，因此它们可以被认为是概率模型。所以，如果想快速识别一个模型是否是概率性的，最简单的方法之一就是分析模型的损失函数。

所以，这篇文章到此为止。我希望你能够清楚地理解概率模型的含义。在下一篇博客中，我将解释一些概率概念，如概率分布和随机变量，这将有助于理解概率模型。如果你发现这里写的任何东西你认为是错误的，请随时评论。这不仅会使这篇文章更可靠，而且还会给我提供扩展知识的机会。谢谢，祝阅读愉快。