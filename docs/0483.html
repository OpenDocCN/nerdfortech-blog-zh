<html>
<head>
<title>Natural Language Processing: Neural Networks and Matrix Calculus</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理:神经网络和矩阵演算</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/natural-language-processing-neural-networks-and-matrix-calculus-eafaa6154c33?source=collection_archive---------1-----------------------#2020-12-28">https://medium.com/nerd-for-tech/natural-language-processing-neural-networks-and-matrix-calculus-eafaa6154c33?source=collection_archive---------1-----------------------#2020-12-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/0119da0f9a89e0abd0153a776b1bf7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/0*ri3SyAHgBARCgIYS.jpg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图片来自<a class="ae iq" href="https://earth-chronicles.com/science/a-neural-network-simulating-the-structure-of-the-brain-is-created.html" rel="noopener ugc nofollow" target="_blank">https://earth-chronicles . com/science/a-neural-network-simulating-the-structure of-the-brain-is-created . html</a></figcaption></figure><p id="6df8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在我的<a class="ae iq" href="https://chozintun.medium.com/natural-language-processing-myanmar-study-group-word-vectors-evaluations-5b62d33b5e7c" rel="noopener">上一篇</a>中，我已经讨论了关于各种生成词向量的方法的进化的概念，这是现代自然语言处理(NLP)的基础。在此基础上，结合神经网络等深度学习算法，发展了许多先进的NLP模型。</p><p id="f19f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在本文中，我们将首先介绍神经网络及其如何用于分类，然后介绍如何使用反向传播算法对其进行训练。我们将看到如何使用链规则和雅可比矩阵等不同的矩阵演算来更新参数和优化模型。最后，我们将讨论训练神经网络的一些有用的提示和技巧，从手动梯度检查到参数初始化、正则化、非线性和学习速率。</p><p id="a851" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在进入神经网络之前，让我们后退一步，讨论一下简单的分类器。</p><h1 id="fa06" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">传统的机器学习方法</strong></h1><p id="5453" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">分类是监督机器学习算法的一个类别，是预测给定数据点的类别的过程。在传统的机器学习方法中，通过使用softmax/logistic回归来训练两类分类器，以找到最佳分离类的决策边界(超平面)。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/3e5224a877c4f64d9e92332756b1e428.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*5dm4ULjEO5oruYVFKouyig.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">Karpathy的ConvNetJS可视化</figcaption></figure><p id="f110" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">机器学习模型的第一步是识别目标函数:它可以是最大化概率或最小化损失。</p><h2 id="8d2a" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated">Softmax回归</h2><p id="0dd1" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">预测函数可以是如下逻辑回归的简单softmax函数，并且其目标是最大化每个训练示例(x，y)的正确类别y的概率。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/06fed8e9a06a4a46c9beb5cd08fd8645.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*I9WMgs8fwhSWaA7rGtZN8Q.png"/></div></figure><p id="380c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果我们考虑最小化损失函数，目标函数可以被改写成负对数概率的形式。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lm"><img src="../Images/a024f979221953c055053e98ba0064f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*JUffRRXxp9mSpvYDJYsStg.png"/></div></div></figure><p id="b603" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这是推导目标函数的一种方法。接下来，我们将了解如何使用softmax回归和交叉熵损失函数作为目标函数来训练模型。</p><p id="fb90" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">交叉熵</strong></p><p id="7f27" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">交叉熵的概念来源于信息论。交叉熵是给定随机变量或事件集的两个概率分布之间差异的度量，公式如下，其中p是真实的概率分布，q是我们计算的模型概率(可以是softmax回归函数)</p><p id="abcc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">由于P是一个独一无二的向量，对于右类为1，其他地方为0，剩下的唯一项是真实类的负对数概率(来自softmax的公式)。全数据集{xi，yi}Ni=1上的交叉熵损失函数可以写成:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/55403ce9fe93e42506f4d54217ecce0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*sxs_Sgx8_ANpD7AQ34pu6Q.png"/></div></figure><p id="4336" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">然而，逻辑回归不是很强大，因为它只给出线性决策边界。我们将无法解决更复杂的非线性问题，如下所示。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/3591e9602e74e0de3a08654d691774c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*G4LGI24H5yZkMeSwbxqxrA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">Karpathy的ConvNetJS可视化</figcaption></figure><p id="7f2f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这就是神经网络分类器发挥作用的时候。</p><h1 id="537b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">神经元</h1><p id="7d4e" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">人工神经元有输入信号(输入)，它们被传递到细胞体，在那里它们被加权和求和。之后，它们通过激活函数转换成输出信号。信号的传播由神经元之间的连接和它们相关的权重决定。下面是人工神经元到生物神经元的映射。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/34510b862cd8e350250851baf818cc0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*5EJTyOurHLbYuN8y0pmmtw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">神经元</figcaption></figure><p id="00d3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">f =激活函数，w =权重，b =偏差，h =隐藏层，x =输入</p><p id="a7fd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">多层神经网络</strong></p><p id="98d2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果我们通过一组逻辑回归函数输入一个输入向量，那么我们会得到一个输出向量，然后我们可以将其输入另一个逻辑回归函数，逐渐地，我们将拥有一个多重神经网络。</p><p id="d06c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在多层神经网络中，第一层通过权衡输入做出四个非常简单的决定。第二层中的每个神经元通过权衡来自第一层决策的结果来做出决策，从而在比第一层中的神经元更复杂和更抽象的水平上做出决策。甚至更复杂的决定可以在第三层的神经元中做出，等等。以这种方式，多层神经网络可以参与复杂的决策制定。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/37d49c33ba3704f5bfd79266f77a7c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*sDloE0CtkSof1CdwtsE96A.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">多层神经网络</figcaption></figure><blockquote class="lv lw lx"><p id="6f3e" class="ir is ly it b iu iv iw ix iy iz ja jb lz jd je jf ma jh ji jj mb jl jm jn jo hb bi translated">然而，在本例中，我们仅堆叠线性回归函数，额外的层可以仅编译成单个线性变换。没有非线性，深度神经网络只能做线性变换。</p><p id="ab2c" class="ir is ly it b iu iv iw ix iy iz ja jb lz jd je jf ma jh ji jj mb jl jm jn jo hb bi translated">因此，激活函数起着重要的作用，我们将非线性嵌入其中来处理复杂的问题。它将指导中间隐藏变量应该是什么，以便很好地预测下一层的目标。非线性激活函数的一些例子是“Tanh”和“ReLu”。</p></blockquote><p id="725d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们可以尝试tensorflow在<a class="ae iq" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">游乐场</a>的现场演示，这将有助于我们了解训练神经网络的技巧和诀窍。我们可以改变不同的选项，如学习速率、激活、隐藏层数和每层神经元的数量，以观察它们对模型的影响。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/e79a176f1208e0700b4dd8d836e0293a.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*aCVmbgSot5jCow_4kdOAqA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">截图来自https://playground.tensorflow.org/<a class="ae iq" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><h2 id="a82f" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated">正向传播</h2><p id="d366" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">我们可以将神经网络方程表示为图形:源节点是输入，内部节点是操作，边传递操作的结果。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es md"><img src="../Images/2e0e5ed3cfda0ce429857fdeaec89b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*u93e3Y7RTMYQFqayLHlNQA.png"/></div></figure><p id="131c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">计算正向传播的公式如下:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es me"><img src="../Images/c16846d35237c6860b06a3e7efcda38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*5I4Zzxul6Ea63kWD72w2kw.png"/></div></figure><h2 id="6777" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated">反向传播</h2><p id="b843" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">反向传播算法是一种训练前向神经网络权重的方法。其原理是通过修改输入信号的内部权重来模拟给定的目标函数，以产生期望的输出信号。</p><p id="638d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">雅可比矩阵和链式法则在反向传播算法中起着至关重要的作用。</p><p id="5169" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="ly">雅可比</em> </strong></p><p id="11b7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">雅可比矩阵是一个向量值函数的一阶偏导数的矩阵，它只是梯度在有向量输出的情况下的推广。</p><p id="3edb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">给定一个具有m个输出和n个输入的函数，</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/3d7997a00882287ac9016c1b21199d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*89luy1nvlUoh_rJT40Cg2Q.png"/></div></figure><p id="23d0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">它的雅可比矩阵是一个m×n的偏导数矩阵。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/2acab65f6bd69bfe1ffa289ab6690986.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*HBRrK5XNiH3F9PMUOlr0Qg.png"/></div></figure><p id="9180" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="ly">链式法则和反向传播</em> </strong></p><p id="f513" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">当我们观察每个节点时，它具有“局部梯度”,即其输出相对于其输入的梯度。它接收“上游梯度”，它的任务是传递正确的“下游梯度”。这是通过将“上游梯度”乘以“局部梯度”实现的。这基本上是链式法则。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/61fb0d5ce26255ec5cde8e5001146159.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*6JSZ-arTQQzZLFiPptXP6Q.png"/></div></figure><p id="2e21" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">借助雅可比矩阵和链式法则，通过反向传播计算梯度并调整权重。一种直观的方法是分别求出W和b的梯度。但是，有些计算可以重复使用。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/8549113b112fbb05ceee68d984f355f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*NaSabB63mqdSCAoSpHXxaw.png"/></div></figure><p id="55c7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">因此，我们同时计算所有的梯度。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/d2a4718ca32a822b44caa39c1e91d7b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*ufxqLurB4AXIpn2KsYDi-Q.png"/></div></figure><p id="c10f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="ly">计算</em> </strong></p><p id="45d4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们可以通过沿着边缘向后计算W和b的(雅可比)梯度。下面是b的例子。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/f3a36555037c904b0085f7c18bb50d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*xZsBV6vkjnvH_IOEliYA8Q.png"/></div></figure><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/438f621fb40826f4858e1d2fd28f3ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*bYZzJp4aJCTdO-j_VMErfQ.png"/></div></figure><h1 id="5460" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">神经网络:技巧和提示</h1><p id="ed40" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">讨论了神经网络的数学基础后，下面是一些使用神经网络进行NLP的有用提示。</p><p id="4553" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">正则化— </strong>当我们拥有大量特征时，神经网络容易过拟合，正则化是用于解决该问题的技术之一。正则化方法之一是通过简单地在损失函数上附加一个额外的项来引入L2正则化罚函数。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/50a53cbac65ed1d68077d496f74aaa1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*VJI5ugO2ykGHSvllgavYiw.png"/></div></figure><p id="e0bb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">神经元单元— </strong>神经网络包含s形神经元以引入非线性。我们根据需要用不同的非线性激活函数构造更好的网络。(乙状结肠、双曲结肠、硬双曲结肠、ReLU、渗漏ReLU)</p><p id="ddf7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">参数初始化</strong> —在构建神经网络时，以合理的方式初始化参数是实现卓越性能的重要步骤。</p><p id="b8ef" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果权重为0，则将隐藏层偏差初始化为0，并将偏差输出为最佳值(最佳值可通过<em class="ly">平均目标</em>或<em class="ly">平均目标</em>的反sigmoid计算)</p><p id="ec19" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">对于权重，初始化为小的随机值，以避免妨碍学习的对称性。</p><p id="5f64" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">学习策略— </strong>可以使用学习速率来控制训练期间模型参数更新的速率/幅度。</p><p id="f595" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们可以将学习速率设置为恒定速率(通常是10的幂),或者通过在训练模型时允许学习速率降低来调整学习速率。</p><p id="ac80" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优化器</strong> —随机梯度下降(SGD)被广泛用作神经网络中的优化器。有自适应优化方法，如AdaGrad和Adam，它们允许学习率因每个参数而异。在许多情况下，Adam方法是一个相当好且安全的起点。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><p id="d6cd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我会定期发布<a class="ae iq" href="https://myanmarnlp.github.io/reading-group/" rel="noopener ugc nofollow" target="_blank">我们学习小组</a>的旅程和讨论的话题。</p><p id="257e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">免责声明</strong>:照片和方程式引用自<a class="ae iq" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank"> <strong class="it hj">斯坦福CS 224N </strong> </a> <strong class="it hj"> </strong>讲义。</p><h1 id="3ff3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">参考资料和进一步阅读</h1><div class="mt mu ez fb mv mw"><a href="http://web.stanford.edu/class/cs224n/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">斯坦福CS 224N |深度学习的自然语言处理</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">自然语言处理(NLP)是人工智能(AI)的重要组成部分，模拟人们如何分享…</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">web.stanford.edu</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk ik mw"/></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://playground.tensorflow.org/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">张量流-神经网络游乐场</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">这是一种构建从数据中学习的计算机程序的技术。它非常松散地基于我们如何思考…</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">playground.tensorflow.org</p></div></div><div class="nf l"><div class="nl l nh ni nj nf nk ik mw"/></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://suzyahyah.github.io/calculus/machine%20learning/2018/04/04/Jacobian-and-Backpropagation.html" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">雅可比矩阵、链式法则和反向传播</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">模型预备雅可比矩阵和反向传播</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">suzayah . github . io</p></div></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://cs231n.github.io/optimization-2/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">用于视觉识别的CS231n卷积神经网络</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">目录:动机。在本节中，我们将通过对以下方面的直观理解来培养专业技能</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">cs231n.github.io</p></div></div></div></a></div></div></div>    
</body>
</html>