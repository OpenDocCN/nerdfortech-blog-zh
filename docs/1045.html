<html>
<head>
<title>NLP Zero to One: Deep Learning Training Procedure (Part 4/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 零到一:深度学习训练程序(第 4/30 部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-zero-to-one-deep-learning-training-procedure-part-4-30-c8d1e3ba0db6?source=collection_archive---------18-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-zero-to-one-deep-learning-training-procedure-part-4-30-c8d1e3ba0db6?source=collection_archive---------18-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a493" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">反向传播、损失函数和批处理</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b5c0e48ee10c28b141bd4a15d0790571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1tSM6VKXAhXYTZrnWwUrQQ.png"/></div></div></figure><h1 id="726a" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">介绍..</h1><p id="44a9" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">在上一篇<a class="ae kx" href="https://medium.com/p/68a5bca3bcf4/edit" rel="noopener">博客</a>中，我们讨论了感知器算法如何在称为节点/神经元的基本计算模块中工作，并了解了这些神经元如何聚集在一起形成深度完全连接的分层神经网络。我们还讨论了不同的激活函数。在这个博客中，我们将专注于神经网络的学习/训练过程。</p><h1 id="6d70" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">训练深度神经网络..</h1><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="b0d4" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">一个神经网络的工作是正确地/ <strong class="kd hj"> <em class="lf">准确地</em> </strong>预测给定特征输入“X”的“y”。训练的目标是学习所有层中所有节点的参数<em class="lf">“权重”</em>“wᵢ”<em class="lf">“偏差”</em>“bᵢ”<em class="lf">“</em>，以使每个训练观测的预测输出尽可能接近真实值。让我们将所有网络可学习参数(所有节点的 Wᵢ,bᵢ)表示为θ</p><p id="330f" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">一般来说，在机器学习中，我们遵循两个重要步骤来促进这个训练过程。我们将定义<strong class="kd hj">损失函数</strong>来计算预测输出和实际输出之间的距离<br/> 2。找出最小化该损失函数的参数，即最小化实际和预测之间的距离。我们将使用梯度下降算法来优化该参数(<em class="lf">在我们的例子中，参数是每个节点</em>的“权重”和“偏差”)</p><p id="76eb" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">在像逻辑回归这样的传统机器学习模型中，计算这个梯度是非常简单的。但是在神经网络中，当一个神经元的权重不直接影响损失时，当损失被附加到更后面的层时，很难计算第一层中某个权重的偏导数。<br/>为了找出给定权重的梯度，我们需要<strong class="kd hj">将误差</strong>从输出节点反向传播到感兴趣的节点。一旦获得了网络中所有可学习参数的梯度，我们就可以通过优化来更新参数。让我们详细看看每个步骤:</p><h1 id="f5f6" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">损失计算</h1><p id="5e55" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">损失计算步骤给出了我们的网络对特征输入 X 的预测程度。因此，对于给定的输入 X，我们必须设计一个损失函数<strong class="kd hj"> L(预测，实际)。</strong>取决于它是分类还是回归，损失函数被定义。让我们看看一些流行的损失函数:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lg"><img src="../Images/f0a246350c31bb19bc9561b568a1f560.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DcKIo2IgvsfsvKRJWVEF5A.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">不同的损失函数，由作者生成</figcaption></figure><p id="f7cb" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">一旦为输入 x 和网络参数θ计算出输出预测 y^。我们可以计算<strong class="kd hj">损失/误差 L(ŷ,y) </strong>，其中“y”是输入 x 的真值</p><h1 id="5a5f" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">误差反向传播</h1><p id="6d16" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">为了改进我们的预测，我们可以使用 SGD 来减少整个网络的误差。为了计算每个参数θᵢ的梯度(导数:<em class="lf">d</em>T2】l(ŷ,y)/<em class="lf">d</em>θᵢ)，我们可以使用微积分的链式法则。<br/>在查看如何计算梯度的示例之前，让我们简要了解一下链式法则的概念:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ll"><img src="../Images/81c8ac5ba1c6eda905c8f80380441d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*uOTT6-Ov-pucHaPWKBJ_ow.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">图形链规则</figcaption></figure><p id="4409" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">对于 f (x) = u(v(w(x)))，<br/><strong class="kd hj"><em class="lf">d</em>f(x)/<em class="lf">d</em>x</strong>可以使用图中所示的链规则来计算。</p><p id="3627" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">让我们来看一个如何使用链式法则计算参数θᵢ:梯度的示例</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lm"><img src="../Images/5a6f00f09b9a7406fc11bd7265d48965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKbRSkLmaB5jtQZ6GJChdA.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">作者生成的示例</figcaption></figure><h1 id="e15c" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">参数更新..</h1><p id="02b3" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">在获得网络中所有可学习参数θᵢ的梯度后，我们可以根据学习速率α更新每层的参数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/9421ef1465a93b23b70079f5602e4088.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*BUKpaL1HXM9BhzbWFkRIpg.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">来源[1]</figcaption></figure><p id="a6f3" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">在<strong class="kd hj">训练</strong>过程中，权重的更新量被称为步长或<strong class="kd hj">“学习率”</strong>如果学习率很高，则参数更新会更大。我们到目前为止讨论的训练神经网络是基于<strong class="kd hj">梯度下降法，特别是随机梯度下降法。</strong>随机梯度下降的思想很简单，它更新为梯度方向的一组权重θ:<em class="lf">d</em>t22】l(ŷ,y)/<em class="lf">d</em>θ)以减少误差<strong class="kd hj"> L(ŷ,y).</strong></p><p id="1a17" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">还有其他优化方法，如随机梯度下降法，最流行的方法是<em class="lf"> Adagrad、RMS-Prop 和 ADAM。让我们简单讨论一下:</em></p><p id="705d" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated"><strong class="kd hj"> Adagrad: </strong>它是<strong class="kd hj"> <em class="lf"> </em> </strong>基于自适应梯度的优化方法。它使学习速率适应网络中的每个参数。它对不频繁的参数做更多的更新，对频繁的参数做更少的更新。<br/><strong class="kd hj">RMS-Prop</strong><em class="lf"/>:它将学习率除以梯度平方的平均值，并按指数规律衰减。<br/> <strong class="kd hj">亚当:</strong> <em class="lf"> </em>自适应矩估计像 Adagrad 就是一种自适应优化方法。它还计算每个参数的学习率，此外，它还结合了过去梯度的平均值[1]</p><h1 id="536c" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">注意..</h1><p id="0754" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated"><strong class="kd hj">小批量梯度下降:</strong>既不使用整个数据更新模型，也不使用数据中的单个数据点更新模型。小批量梯度下降将数据集分成多个批次，并使用小批量计算单次更新的误差。它具有更快的训练和收敛速度。<br/> <strong class="kd hj">更多主题:</strong>消失梯度、正则化(L1、L2)、辍学和批量归一化是深度学习中的其他重要概念<strong class="kd hj">。我们将在接下来的博客中详细讨论这些话题。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lo"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">由作者生成</figcaption></figure><p id="c12f" class="pw-post-body-paragraph kb kc hi kd b ke la ij kg kh lb im kj kk lc km kn ko ld kq kr ks le ku kv kw hb bi translated">上一篇:<a class="ae kx" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-deep-learning-theory-basics-part-3-30-baa8cbbe271d?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kd hj"> NLP 零对一:深度学习理论基础知识(Part 3/30) </strong> </a> <br/>下一篇:<a class="ae kx" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-dense-representations-word2vec-part-5-30-9b38c5ccfbfc?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kd hj"> NLP 零对一:密集表示法、Word2Vec (Part 5/30) </strong> </a></p></div></div>    
</body>
</html>