<html>
<head>
<title>Optimizers in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的优化器</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/optimizers-in-machine-learning-f1a9c549f8b4?source=collection_archive---------2-----------------------#2022-03-26">https://medium.com/nerd-for-tech/optimizers-in-machine-learning-f1a9c549f8b4?source=collection_archive---------2-----------------------#2022-03-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8cb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化器是 ML 模型学习过程中的一个关键元素。PyTorch 本身有 13 个优化器，为问题选择一个合适的优化器很有挑战性。</p><p id="ea94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本教程中，我将介绍五个最流行的优化器，解释它们的优点和局限性，以及它们背后的数学原理。那么，让我们开始吧！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/4ee94d6e0210ba84f3c4a25449ea8db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kzOebepMEFhXvxSs"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jt" href="https://unsplash.com/@acerthings?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Einar Ingi Sigmundsson </a>拍摄</figcaption></figure><h1 id="1fdb" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">什么是优化？</h1><p id="3290" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">ML 模型的最终目标是达到损失函数的最小值。在我们传递输入之后，我们计算误差并相应地更新权重。这就是优化器发挥作用的地方。它定义了如何调整参数以接近最小值。</p><p id="1654" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kx">所以从本质上来说，优化就是为模型寻找最优参数的过程，从而显著降低误差函数。</em></p><h1 id="d059" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">香草渐变下降</strong></h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/8366ab946e7b35775b54cfb742cc38d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0jdmcYVXauPxteDEJ1v_bA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">α——学习率</figcaption></figure><p id="6efd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在机器学习中，梯度下降有三种不同的变体:</p><ol class=""><li id="f85b" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated"><strong class="ih hj">随机梯度下降(SGD) </strong> —计算每个随机样本的梯度</li><li id="d87a" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj">小批量梯度下降</strong> —计算随机取样批量的梯度</li><li id="a8e3" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj">批量梯度下降</strong>-计算整个数据集的梯度</li></ol><p id="b3a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你可能想到的，更新每个样本的<strong class="ih hj">的权重会使训练变得不稳定和混乱。另一方面，对于像 ImageNet 这样的大型数据集，一次传递整个数据集很慢，甚至是不可能的。</strong></p><p id="2baa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Mini-Batch GD </strong>二者兼而有之，目前是训练深度学习模型的首选算法。主要是因为它利用了 GPU 的能力，使训练更加稳定。</p><blockquote class="ln lo lp"><p id="2e00" class="if ig kx ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">如今，<strong class="ih hj">新币</strong>主要指的是小批量梯度下降，所以我们将在博客的其余部分坚持这一约定。</p></blockquote><h2 id="6c70" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="4e11" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">总是收敛</li><li id="4442" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">易于计算</li></ul><h2 id="f98d" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated"><strong class="ak">缺点:</strong></h2><ul class=""><li id="1564" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">慢的</li><li id="66e7" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">容易陷入局部最小值或鞍点</li><li id="9d68" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">对学习速度敏感</li></ul><p id="0bf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SGD 是 50 年代的基础优化算法。它简单明了，易于计算，但是它面临着巨大的挑战，尤其是对于更复杂的模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ml"><img src="../Images/c361b1d5afd16f7f15b42c0b6d7c0b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-vpdZ8mRXMXqCUdr1plTRw.png"/></div></div></figure><p id="0574" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道如果斜率为 0，那么模型<strong class="ih hj">收敛</strong>。虽然在<strong class="ih hj">凸</strong>函数(一个最小值)中是这种情况，但大多数深度学习模型都是<strong class="ih hj">非凸</strong>(多个局部最小值)。在这种情况下，我们可能会堆积在这些点中的一个点上，并且可能永远不会达到全局最小值。</p><blockquote class="ln lo lp"><p id="2217" class="if ig kx ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">令人惊讶的是，在更复杂的深度学习模型中，局部最小值不像鞍点那样经常出现。</p></blockquote><p id="b73a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们使用更好的技术来解决这个问题之前，我将介绍一个概念，我认为这个概念是理解所有其他优化器的关键。</p><h1 id="e56c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">指数移动平均线</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/d6a033a83e76a1a0268e0070a4df6b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JUvvfd2oxL2x0R62FzZ4Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">e —指数平均值，x —新点</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/0e09a7014667e497dc37d5e289012c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ryqxh4mKXzFJp1ZlJy8Mew.png"/></div></div></figure><p id="383b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，EMA 平滑了图形，减少了振荡。参数<strong class="ih hj"> β </strong>定义了新点和加权平均值的重要性。</p><p id="6acd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kx">为什么是指数级？</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ml"><img src="../Images/ebcd0ddb247536b7b20dbda1a8bcec67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*swIK_hNnkhjGL7yGDYVZNQ.png"/></div></div></figure><p id="ec72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你在上面的例子中看到的，权重<strong class="ih hj"> β </strong>呈指数增长。由于<strong class="ih hj"> β &lt; 1，</strong>旧术语的重要性降低，我们考虑更近的点。</p><p id="6dc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上<strong class="ih hj"> EMA </strong>减少了摆动，创造了平均轨迹。这就是我们想要的优化器！</p><h1 id="44a2" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">动力</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/712e367d53e2a2b98e439aa5c0646b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmKZ_dhUvunLxWKamoqibA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">α——学习率</figcaption></figure><h2 id="739c" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="1d80" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">有助于避免鞍点和最小值</li><li id="f6ea" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">收敛更快</li><li id="95bb" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">减少振荡</li></ul><h2 id="6545" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated">缺点:</h2><ul class=""><li id="29e1" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">所有参数的学习率相同</li></ul><p id="0b4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kx">关于动量的流行故事说:</em></p><blockquote class="mn"><p id="8fae" class="mo mp hi bd mq mr ms mt mu mv mw jc dx translated">SGD 是一个走下坡路的人，缓慢但稳定。动量是一个沉重的下坡球，平稳而快速。</p></blockquote><p id="2045" class="pw-post-body-paragraph if ig hi ih b ii mx ik il im my io ip iq mz is it iu na iw ix iy nb ja jb jc hb bi translated">动量利用<strong class="ih hj"> EMA </strong>的能力来减少改变<strong class="ih hj">方向</strong>的梯度振荡，并在梯度稳定地指向<strong class="ih hj">的地方建立<strong class="ih hj">动量</strong>。它帮助我们“滚动”局部最小值和平台，并继续向全局最小值前进。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nc"><img src="../Images/ba533f0f3bea2a039bd5cb4eaffbe024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSYZkZBGz4XfsQ4MzPca3w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图形灵感来自塞巴斯蒂安·路德·波斯特[2]</figcaption></figure><p id="cc3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着动力的增加，优化器似乎越来越稳定地向最小值前进。尽管如此，它仍然<strong class="ih hj">在其移动的方向上超过</strong>。这个挑战是由<strong class="ih hj"> AdaGrad </strong>优化器解决的。</p><h1 id="d328" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">阿达格拉德</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/da24a15bb118997aee77313f7fc116b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UH85obkLdTrQfAarOjmSXA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">g-梯度的平方和，ɛ-避免被 0 除的小常数</figcaption></figure><h2 id="ed03" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="2a0c" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">参数的自适应学习率</li><li id="b078" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">没有学习率的手动调整</li></ul><h2 id="759a" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated"><strong class="ak">缺点:</strong></h2><ul class=""><li id="1b58" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">学习率消失</li></ul><p id="2518" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AdaGrad 是第一个为不同的模型参数引入自适应学习率的算法。让我告诉你为什么它很重要:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nd"><img src="../Images/06f772fa63cefa094b457eeb0d3f8160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-YMBS3u2Pu6TXwC7TmRR9w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">自适应学习率对参数的影响[1]</figcaption></figure><p id="fe20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果学习率对于一个大的梯度来说太高了，我们就会过冲并反弹。如果学习速率太低<strong class="ih hj"/>，学习就很慢并且可能<strong class="ih hj">永远不会收敛</strong>。</p><p id="bb3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AdaGrad 使用<strong class="ih hj">的平方和先前的梯度</strong>来解决这个问题。如果坡度<strong class="ih hj">高</strong>，则学习率<strong class="ih hj">降低</strong>。如果坡度<strong class="ih hj">低</strong>，则<strong class="ih hj">增加</strong>。以这种方式，算法沿着所有的<strong class="ih hj">尺寸</strong>平滑地调整步长的大小。</p><blockquote class="ln lo lp"><p id="ad00" class="if ig kx ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">AdaGrad 主要与<strong class="ih hj">稀疏数据</strong>一起使用，其中不频繁的要素比频繁的要素获得更大的更新。</p></blockquote><p id="f88f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个绝妙的解决方案，但是由于我们<strong class="ih hj">累积</strong>平方梯度，学习率将随着每次迭代<strong class="ih hj">降低</strong>，最终<strong class="ih hj">收缩</strong>。那么学习过程可能会在我们达到收敛之前停止。</p><h1 id="b8a6" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak"> RMSProp </strong></h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/d9240ca3bcaeea472e6759672fbe63f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5xVOfQsDXM1f_JhWX0aHGQ.png"/></div></div></figure><h2 id="17b7" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="9f3a" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">学习率不会消失</li><li id="9ac1" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">每个参数的自适应学习率</li></ul><p id="5fe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RMSProp 是 AdaGrad 的升级版，它利用了 mighty <strong class="ih hj"> EMA </strong>(再次)。我们<strong class="ih hj">控制</strong>先前信息的数量，而不是仅仅<strong class="ih hj">累积</strong>平方梯度。这样分母就不会变大，学习率也不会消失！</p><blockquote class="ln lo lp"><p id="5444" class="if ig kx ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">RMSProp 仍然用于强化学习，在某些情况下，它实际上比 Adam 更稳定。[3]</p></blockquote><h1 id="056d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/8a02e50f0be5c66ea0ef5401c69a7593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ag0KeoIawN2jBnkQZBz2cA.png"/></div></div></figure><h2 id="9641" class="lt jv hi bd jw lu lv lw ka lx ly lz ke iq ma mb ki iu mc md km iy me mf kq mg bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="8c63" class="kz la hi ih b ii ks im kt iq mh iu mi iy mj jc mk lf lg lh bi translated">适应性学习率</li><li id="f31f" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc mk lf lg lh bi translated">动力</li></ul><p id="e799" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本质上，Adam 是动量和 RMSProp 的组合。它具有减少振荡、更平滑的路径和自适应学习速率的能力。结合这些能力使其成为最强大的和最适合不同问题的。</p><blockquote class="ln lo lp"><p id="42bf" class="if ig kx ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">好的开始配置是学习率 0.0001，动量 0.9，平方梯度 0.999。</p></blockquote><h1 id="b157" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">比较</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ne"><img src="../Images/9f186b1f20f2d4fb151ba7eb0589554c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWjfem1Xv1aVkAsA0QxmoQ.png"/></div></div></figure><p id="5125" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这张图完美地总结了每种算法的优缺点。</p><p id="eefd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">纯<strong class="ih hj"> SGD </strong>在局部最小值中获得股票。</p><p id="9cab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动量</strong>向两个方向溢出，但找到了全局最小值的方法。</p><p id="6084" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RMSProp </strong>沿所有维度平滑移动。</p><p id="8f07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，<strong class="ih hj"> Adam </strong>跑得有点快，但动作稳定，是所有选手中最快的(这一点你必须相信我)。<br/>如果你想玩这些可视化，尝试不同的功能，请访问这个<a class="ae jt" href="https://emiliendupont.github.io/2018/01/24/optimization-visualization/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">网站</strong> </a>。</p><h1 id="3ed5" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结论</h1><p id="ba65" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在这篇博客中，我们浏览了深度学习中最受欢迎的五个优化器。即使现在大部分都不用了，但分析他们所面临的挑战对于深入理解和欣赏亚当是至关重要的。</p><p id="4fa0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">读完这篇博客后，我希望你能获得不同优化算法背后的直觉，这有助于你进一步探索这个话题。</p><p id="79e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看我的<a class="ae jt" href="https://maciejbalawejder.medium.com/" rel="noopener"> <strong class="ih hj">中</strong> </a>和<a class="ae jt" href="https://github.com/maciejbalawejder" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">Github</strong></a><strong class="ih hj"/>简介如果你想看我的其他项目。</p><h1 id="0f4e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考</h1><p id="d01c" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated"><a class="ae jt" href="https://datascience.stackexchange.com/questions/82240/why-sparse-features-should-have-bigger-learning-rates-associated-and-how-adagra" rel="noopener ugc nofollow" target="_blank">【1】</a>为什么稀疏特征要有较大的关联学习率？阿达格拉德是如何做到的？</p><p id="487d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://ruder.io/optimizing-gradient-descent/index.html#momentum" rel="noopener ugc nofollow" target="_blank">【2】</a>梯度下降优化算法概述</p></div></div>    
</body>
</html>