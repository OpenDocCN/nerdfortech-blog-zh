<html>
<head>
<title>Apache Spark’s Logical and Physical Plans Using Explain() Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Explain()方法的 Apache Spark 的逻辑和物理计划</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/apache-sparks-logical-and-physical-plans-using-explain-method-2487b9be537?source=collection_archive---------8-----------------------#2021-06-28">https://medium.com/nerd-for-tech/apache-sparks-logical-and-physical-plans-using-explain-method-2487b9be537?source=collection_archive---------8-----------------------#2021-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8974" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">park 建议使用结构化 API(data frame、DataSet、SQL ),而不是低级 rdd，以利用 Catalyst 和钨引擎的强大功能来优化整体工作负载。</p><p id="0686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在处理 rdd 时，我们通知 Spark 关于<strong class="ih hj">【如何】</strong>我们希望数据被处理。但是在使用结构化 API 时，我们会通知 Spark 我们想要处理的<strong class="ih hj">【什么】</strong>，并让 Spark 找出如何处理的最佳方法。Spark 创建逻辑和物理计划，并确定要实施的最佳计划。</p><p id="04a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用结构化 API 编写的代码如果有效，则被转换成逻辑计划，即一组抽象转换，而没有对代码将运行的驱动程序或执行器的任何引用。因为这仅包括语法上正确的代码，并且对各种列/表的引用还没有被验证，所以它被称为未解析的逻辑计划。<br/>这个未解析的逻辑计划通过一个分析器，该分析器再次验证代码 Spark 目录(数据帧信息的存储库)中使用的列和表，最终“解析”它。如果目录中不存在被引用的列/表，那么分析器可以拒绝未解析的逻辑计划。如果一切顺利，那么分析器的输出就是一个已解决的逻辑计划。<br/>这个解析的逻辑计划现在被提供给 Catalyst Optimizer，它根据一组内部规则产生一个优化的逻辑计划。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/0eca6ff5a3a3c84d6967a5152cadbd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zi3HueJBqzFY7eOeXCOw_w.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">从逻辑到物理计划创建的各个阶段</figcaption></figure><p id="edf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用优化的逻辑规划，产生物理规划的许多不同版本，并通过基于成本的选择模型。然后选择最佳的物理计划并在集群上执行。</p><p id="d1e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">explain() 方法帮助我们查看这些计划。它在 Spark 版本 1.3+中可用，并接受一个名为“<strong class="ih hj">扩展</strong>”的<strong class="ih hj">可选布尔参数</strong>。默认情况下，该参数设置为 False，因此只显示物理平面图。要查看各种逻辑计划以及物理计划，您需要将扩展参数作为 True <em class="kc">传递，例如 df.explain(extended = True)或 df.explain(True) </em></p><p id="96cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用一个例子来回顾一下。下面的代码片段从示例数据创建一个 dataframe，然后调用 explain()方法，不使用扩展参数，将扩展参数设置为 True。</p><figure class="jn jo jp jq fd jr"><div class="bz dy l di"><div class="kd ke l"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">展示解释计划的 PySpark 代码</figcaption></figure><p id="97ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们在下面看到的，只有在调用 explain()方法时没有将扩展参数设置为 True，我们才能获得物理平面图信息。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kf"><img src="../Images/bf479092ec25eb86a1c33b59b3431221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlE-RTlzEFjqrxhQnP3phA.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">解释未将扩展参数设置为 True 的方法调用</figcaption></figure><p id="222b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们将扩展参数设置为 True 时，我们得到所有的计划(3 个逻辑和 1 个物理)</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kf"><img src="../Images/2b139da8119b3e6cd5a0cad3b9ea17ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Me-V9vpE-ZJkYA8NCUlzpw.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">解释扩展参数设置为 True 的方法调用</figcaption></figure><blockquote class="kg kh ki"><p id="c4ee" class="if ig kc ih b ii ij ik il im in io ip kj ir is it kk iv iw ix kl iz ja jb jc hb bi translated">如果我们仔细观察计划，我们可以看到 Catalyst Optimizer 在这里工作。我在代码中应用了两个独立的过滤器，它们在已解析或未解析的逻辑计划和已分析或已解析的逻辑计划中显示为独立的步骤。一旦计划被提供给 Catalyst 优化器，过滤器就被组合并优化成一个调用，如优化的逻辑计划所示。</p></blockquote><p id="a850" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在 Spark 3.0 中，一个新的<strong class="ih hj">可选字符串参数</strong><strong class="ih hj">模式</strong>被添加到 explain()方法中，该方法可用于指定显示计划的格式。<em class="kc">注意:“扩展”和“模式”不能同时使用。<br/> </em>模式参数可以取值，<br/> 1。<strong class="ih hj">简单</strong> - &gt;仅打印物理平面图<br/> 2。<strong class="ih hj">扩展</strong> - &gt;打印逻辑和物理平面图<br/> 3。<strong class="ih hj"> codegen </strong> - &gt;打印一份物理平面图和生成的代码(如有)<br/> 4。<strong class="ih hj">成本</strong> - &gt;打印逻辑计划和统计数据(如果有的话)<br/> 5。<strong class="ih hj">格式化</strong> - &gt;打印物理平面图和节点细节</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es km"><img src="../Images/a026ce45ce6fd3de3a96f81a7c9a5e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OPWvVxdpY8xkFFlmhT_NUA.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">将扩展和模式一起传递会导致错误</figcaption></figure><p id="bf59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kc">您可以使用不同的模式值运行代码，并查看计划的预期输出，以便进行更深入的分析。</em></p><h1 id="fbe8" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="b9e2" class="ll lm hi ih b ii ln im lo iq lp iu lq iy lr jc ls lt lu lv bi translated"><a class="ae lw" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.explain.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark 文档</a></li><li id="b394" class="ll lm hi ih b ii lx im ly iq lz iu ma iy mb jc ls lt lu lv bi translated">比尔·钱伯斯和马泰·扎哈里亚的《火花:权威指南》</li><li id="e7ae" class="ll lm hi ih b ii lx im ly iq lz iu ma iy mb jc ls lt lu lv bi translated"><a class="ae lw" href="https://www.youtube.com/watch?v=Ofk7G3GD9jk" rel="noopener ugc nofollow" target="_blank">三个 Apache Spark APIs 的故事:rdd、数据帧和数据集——Jules Damji</a></li></ul></div></div>    
</body>
</html>