<html>
<head>
<title>Review — EncNet: Context Encoding for Semantic Segmentation (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习— EncNet:语义分割的上下文编码(语义分割)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-encnet-context-encoding-for-semantic-segmentation-semantic-segmentation-9f6f1a4d5010?source=collection_archive---------9-----------------------#2021-05-02">https://medium.com/nerd-for-tech/review-encnet-context-encoding-for-semantic-segmentation-semantic-segmentation-9f6f1a4d5010?source=collection_archive---------9-----------------------#2021-05-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="549a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><span class="l ix iy iz bm ja jb jc jd je di"> U </span> sing <strong class="ak"> Context 编码模块，优于</strong> <a class="ae jf" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>和<a class="ae jf" href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv3 </a>，<a class="ae jf" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> FCN </a>，<a class="ae jf" href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------" rel="noopener" target="_blank"> DilatedNet </a>，<a class="ae jf" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv2 </a>，<a class="ae jf" href="https://towardsdatascience.com/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------" rel="noopener" target="_blank"> CRF-RNN </a>，<a class="ae jf" href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------" rel="noopener" target="_blank"> DeconvNet </a>，<a class="ae jf" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc"> DPN </a>，<a class="ae jf" href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------" rel="noopener" target="_blank">RefineNet</a>&amp;<a class="ae jf" rel="noopener" href="/@sh.tsang/resnet-38-wider-or-deeper-resnet-image-classification-semantic-segmentation-f297f2f73437">ResNet</a></h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/a4bcca0aa5967eb718587e05e2003933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*dOxz0DhdxoCBubF3e31AqA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">基于场景上下文缩小可能类别的列表使得标记更加容易。</figcaption></figure><p id="60ce" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi kp translated"><span class="l ix iy iz bm ja jb jc jd je di">在</span>这个故事中，对 Rutgers 大学、亚马逊公司、SenseTime 和香港中文大学的(EncNet)的<strong class="jv hj">语义分割的上下文编码进行了评述。在本文中:</strong></p><ul class=""><li id="cf33" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated"><strong class="jv hj">引入了上下文编码模块</strong>，该模块<strong class="jv hj">捕获场景</strong>的语义上下文，并选择性地突出显示依赖于类别的特征图。</li><li id="eb20" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">比如上图中的<strong class="jv hj">套房场景</strong>很少会有马，但更有可能会有椅子、床、窗帘等。在这种情况下，这个模块有助于<strong class="jv hj">突出椅子、床和窗帘。</strong></li></ul><p id="3631" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">这是一篇发表在<strong class="jv hj"> 2018 CVPR </strong>的论文，引用超过<strong class="jv hj"> 500 次</strong>。(<a class="le lf ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----9f6f1a4d5010--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="4aa1" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">概述</h1><ol class=""><li id="c32c" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko mj kw kx ky bi translated"><strong class="jv hj">上下文编码模块</strong></li><li id="fde9" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">语义编码丢失</strong></li><li id="f853" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">语义分割结果</strong></li><li id="f3db" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">图像分类结果</strong></li></ol></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="7827" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 1。上下文编码模块</strong></h1><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mk"><img src="../Images/7c317620db6c7483129b1188ef61c6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eu3ntqcGxsBPIZ3E3Y47Lw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">上下文编码模块&amp;语义编码损失(SE-loss) </strong></figcaption></figure><h2 id="c0f8" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">1.1.整体架构</h2><ul class=""><li id="2154" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko kv kw kx ky bi translated">给定一幅输入图像，用<strong class="jv hj">一个预先训练好的</strong><a class="ae jf" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jv hj">ResNet</strong></a><strong class="jv hj"/>提取出大小为<em class="nd">C</em>×<em class="nd">H</em>×<em class="nd">W</em>的密集卷积特征图。</li><li id="d8df" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">建议的<strong class="jv hj">上下文编码模块</strong>在顶部，包括<strong class="jv hj">编码层</strong>被设计为捕获编码语义并预测以这些编码语义为条件的缩放因子。</li><li id="2767" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">这些学习到的因素选择性地突出显示依赖于类别的特征图(用颜色可视化)。</li><li id="8a22" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">在另一个分支中，使用<strong class="jv hj">语义编码损失(SE-loss) </strong>来调整训练，这使得上下文编码模块预测场景中类别的存在。</li><li id="9f6d" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">最后，上下文编码模块的表示被馈送到最后的卷积层，以进行逐像素预测。</li></ul><h2 id="e0cb" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">1.2.编码层(由 Deep TEN 提出)</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ne"><img src="../Images/7e0a9437917235df0f003fbc8dc85b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*z9iQCphi0jxTQFwYFFlKbQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">编码层(深十层)。在本文中，描述符是输入特征图</strong></figcaption></figure><ul class=""><li id="4ea2" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">编码层将形状为<em class="nd"> C </em> × <em class="nd"> H </em> × <em class="nd"> W </em>的输入特征图视为一组<em class="nd">C</em>-维输入特征<em class="nd"> X </em> = { <em class="nd"> x </em> 1，…，<em class="nd"> xN </em> }，其中<em class="nd"> N </em>是由<em class="nd"> H </em> × <em class="nd"> W </em>给出的特征总数， 其中<strong class="jv hj">学习</strong>一个固有的<strong class="jv hj">码本<em class="nd"> D </em> </strong> = { <em class="nd"> d </em> 1、…、<em class="nd"> dK </em> }包含<em class="nd"> K </em>个码字(视觉中心)和<strong class="jv hj">一组视觉中心<strong class="jv hj"><em class="nd">S</em></strong>= {<em class="nd">S</em>1、…、<em class="nd">sK<em class="nd"/></em></strong></li><li id="b01c" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">在本文中，编码层中的码字数量<em class="nd"> K </em>为 32。</li><li id="81ec" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">首先，通过从每个<em class="nd"> dk </em>中减去每个<em class="nd"> xi </em>得到剩余<em class="nd"> rik </em>:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nf"><img src="../Images/409a2eefb131283c6e79677bf2f70588.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*D0SffDnQWUgA-S75cdMoqg.png"/></div></figure><ul class=""><li id="33ad" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">考虑用于将描述符分配给码字的分配权重。硬分配为每个描述符<em class="nd"> xi </em>提供单个非零分配权重，其对应于最近的码字。</li><li id="352e" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">硬分配没有考虑码字的模糊性，也使得模型不可微。</li><li id="74cf" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><strong class="jv hj">软权重分配</strong>通过为每个码字分配一个描述符来解决这个问题。</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ng"><img src="../Images/0a88e9f0547e99a2f7347280cce3f661.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*2Zwbl95vtfKP_ka-Sp0qMw.png"/></div></figure><ul class=""><li id="7589" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">其中<em class="nd"> β </em>为平滑因子。</li><li id="c8cd" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">实际上，平滑因子是可以学习的，即<em class="nd"> sk </em>:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nh"><img src="../Images/fd19764e8bce5ad1ae94d3f1a46eb25f.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*ynIia16CdT2ycXAvaOGokg.png"/></div></figure><ul class=""><li id="c6aa" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">通过如下汇总，得到<em class="nd"> ek </em>:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ni"><img src="../Images/6905e1cafdc8110dc49331b2e9ce250b.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*VzSEt8tJocp1FSy7TX1vRg.png"/></div></figure><ul class=""><li id="bed7" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">其中<em class="nd"> N </em>是由<em class="nd"> H </em> × <em class="nd"> W </em>给出的特征总数，如上所述。</li><li id="4012" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">然后，应用聚合:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nj"><img src="../Images/ae97255f7af9f7ace758eea0a0b91b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*ZjkS5-6RBI_vu1jAXwu2Jg.png"/></div></figure><ul class=""><li id="0f9c" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">其中<em class="nd"> ϕ </em>为批量规格化和 ReLU。</li></ul><h2 id="8d6a" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">1.3.特征图注意</h2><ul class=""><li id="863b" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko kv kw kx ky bi translated"><strong class="jv hj">使用编码层之上的全连接层</strong>和作为激活函数的<strong class="jv hj"> sigmoid </strong>，其输出预测的特征图缩放因子:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nk"><img src="../Images/65ef649245fc4e65909d6634ec605398.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*endjZ8jDj-Tr6bFHeKpQpw.png"/></div></figure><ul class=""><li id="e41d" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">其中<em class="nd"> W </em>表示层权重，𝛿是 sigmoid 函数。</li><li id="49eb" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><strong class="jv hj">在输入特征图<em class="nd"> X </em>和缩放因子γ之间应用通道式乘法⊗ </strong>，以获得模块输出:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nl"><img src="../Images/3e9f1c3a0bcd6cdb9f6d76716aa6d16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*C2u0t7a2TfoQvGXqKoDupg.png"/></div></figure><ul class=""><li id="914c" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">使用双线性插值对输出预测进行 8 次上采样，以计算损耗。</li></ul><blockquote class="nm nn no"><p id="e21c" class="jt ju nd jv b jw jx ij jy jz ka im kb np kd ke kf nq kh ki kj nr kl km kn ko hb bi translated">作为所提出的方法的效用的一个直观示例，<strong class="jv hj">考虑强调天空场景中飞机的概率，但不强调车辆的概率。</strong></p></blockquote></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="e4d1" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 2。语义编码损失</strong></h1><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es jg"><img src="../Images/225b157cc4a1ca5e09e5bdc47cf6dd5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*hujaAy1PNgw_MeFNRPVcCw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">扩张策略和损失</strong></figcaption></figure><ul class=""><li id="dbd4" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">没有全局信息，网络可能难以理解上下文。</li></ul><blockquote class="nm nn no"><p id="c6d1" class="jt ju nd jv b jw jx ij jy jz ka im kb np kd ke kf nq kh ki kj nr kl km kn ko hb bi translated"><strong class="jv hj">语义编码损失(SE-loss) </strong>迫使网络以非常小的额外计算成本<strong class="jv hj">理解全局语义信息。</strong></p></blockquote><ul class=""><li id="fc33" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">具有 sigmoid 激活函数的附加全连接层建立在编码层之上，以对场景中对象类别的存在进行单独预测，并利用<strong class="jv hj">二进制交叉熵损失进行学习。</strong></li><li id="7f5b" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">与每像素损失不同，<strong class="jv hj"> SE-loss 同等考虑大小物体。</strong>因此，小物体的分割常常得到改进。</li><li id="eca3" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">SE 损耗被添加到基本网络的第 3 级和第 4 级<strong class="jv hj">。</strong></li><li id="7bee" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">SE-loss 的基本事实是从基本事实分割掩模直接生成的，没有任何附加注释。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="a318" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 3。语义分割结果</strong></h1><h2 id="1acb" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">3.1.PASCAL 环境下的消融研究</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ns"><img src="../Images/305ba4678671c1e1ecb0ce245b6d1b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*61did9MG7H5O05wtbUPaPg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">PASCAL 环境数据集的消融研究</strong></figcaption></figure><ul class=""><li id="88c6" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">与基线 FCN 相比，<strong class="jv hj">简单地在顶部添加上下文编码模块</strong>产生结果<strong class="jv hj"> 78.1/47.6 (pixAcc 和 mIoU) </strong>，</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nt"><img src="../Images/39f40a66945729b448597a21ed9d6720.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*Rt6nc52sRquuql5K-0Ijew.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">权重对 SE-loss 的影响<em class="nu">α</em>T21】</strong></figcaption></figure><ul class=""><li id="787a" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">为了研究<strong class="jv hj"> SE 损失</strong>的影响，我们测试了 SE 损失<em class="nd"> α </em> ={0.0，0.1，0.2，0.4，0.8}的不同<strong class="jv hj">权重</strong>，我们发现<strong class="jv hj"> <em class="nd"> α </em> = 0.2 产生最佳性能。</strong></li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nv"><img src="../Images/e4bd0d440e25d5ed7c2a1e71f83496f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*kWQQfdX7MtjGEuKxi2aAkg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">码字数量的影响<em class="nu">K</em>T35】</strong></figcaption></figure><ul class=""><li id="798d" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">此外，<strong class="jv hj">超过码字数量<em class="nd"> K </em> = 32，改进达到饱和</strong> ( <em class="nd"> K </em> = 0 表示使用全局平均池代替)。</li><li id="f6f4" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><strong class="jv hj">更深的预训练网络</strong>提供了更好的特征表示，EncNet 在 mIoU 中使用<a class="ae jf" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jv hj">ResNet</strong></a><strong class="jv hj">-101</strong>获得了<strong class="jv hj">额外 2.5%的改进。</strong></li><li id="e445" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">最后，<strong class="jv hj">多尺度评测</strong>得出我们的最终分数<strong class="jv hj"> 81.2% pixAcc 和 52.6% mIoU </strong>，包括背景在内是 51.7%。</li></ul><h2 id="8a40" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">3.2.关于 PASCAL-Context 的结果</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nw"><img src="../Images/32122952f65cd8c79d8ded15bae358ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*oTlJQlUSC6i8zwas5lOa3Q.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">PASCAL 上下文数据集上的分割结果</strong></figcaption></figure><ul class=""><li id="151f" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">所提出的 EncNet 在不使用 COCO 预训练或更深的模型(<a class="ae jf" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> -152)的情况下优于先前最先进的方法。</li></ul><h2 id="2d43" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">3.3.PASCAL VOC 2012 的结果</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nx"><img src="../Images/bb8f08f2ac24340468fd9a1ef77b8eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VvabuF-b0cIStN5mLByU5g.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">PASCAL VOC 2012 测试集的结果</strong></figcaption></figure><ul class=""><li id="19c1" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">如上图所示，EncNet 获得了 85.9% mIoU 的最佳结果。</li><li id="61e3" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">与最先进的<a class="ae jf" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>和<a class="ae jf" href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv3 </a>方法相比，EncNet 具有更低的计算复杂度。</li><li id="6465" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">也优于<a class="ae jf" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> FCN </a>、<a class="ae jf" href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------" rel="noopener" target="_blank">扩容网</a>、<a class="ae jf" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank">深拉布 v2 </a>、<a class="ae jf" href="https://towardsdatascience.com/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------" rel="noopener" target="_blank"> CRF-RNN </a>、<a class="ae jf" href="https://towardsdatascience.com/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------" rel="noopener" target="_blank">解扩网</a>、<a class="ae jf" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc"> DPN </a>、<a class="ae jf" href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------" rel="noopener" target="_blank"> RefineNet </a>、<a class="ae jf" rel="noopener" href="/@sh.tsang/resnet-38-wider-or-deeper-resnet-image-classification-semantic-segmentation-f297f2f73437"> ResNet-38 </a>。</li></ul><h2 id="142e" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">3.4.ADE20K 上的结果</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ny"><img src="../Images/b8b4da292562abee099c3899c1a22250.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*dEJl7wh5HLQfrwmIW1lunw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">ade 20k 验证集上的分割结果</strong></figcaption></figure><ul class=""><li id="e109" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">EncNet-101 使用浅得多的基础网络实现了与最先进的<a class="ae jf" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a> -269 相当的结果。</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nz"><img src="../Images/f036559cd29e015d8ecb4bc321a8e721.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*u2D21pXUYQirNFEuo6EX-w.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">ade 20k 测试集的结果</strong></figcaption></figure><ul class=""><li id="2f04" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">EncNet 最终得分为 0.55675，超过 PSP-Net-269(2016 年第一名)和 COCO Place Challenge 2017 的所有参赛作品。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="3672" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">4.图像分类结果</h1><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es oa"><img src="../Images/2fbf4851dbda288da7b289502de9b5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*YhKVwED2typILRt69NxQVQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">比较 CIFAR-10 </strong>上的模型深度、参数数量(M)、测试误差(%)</figcaption></figure><ul class=""><li id="a68f" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">上下文编码模块也可以插入图像分类网络。</li><li id="d4e2" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">使用一个浅 14 层<a class="ae jf" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>作为基线。</li><li id="6b26" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">在<a class="ae jf" href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=post_page---------------------------" rel="noopener" target="_blank"> SENet </a>中，SE 模块被添加到每个 Resblock 的顶部。</li><li id="fda1" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">类似地，<strong class="jv hj">提出的上下文编码模块也可以添加到每个 Resblock 之上。</strong></li><li id="db90" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">具有上下文编码模块的 14 层浅网络在 CIFAR10 数据集上实现了<strong class="jv hj"> 3.45%的错误率</strong>，如上表所示，其性能可与最先进的方法相媲美，如<a class="ae jf" href="https://towardsdatascience.com/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004?source=post_page---------------------------" rel="noopener" target="_blank"> WRN </a>、<a class="ae jf" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------" rel="noopener" target="_blank"> ResNeXt </a>和<a class="ae jf" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="f503" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">参考</h2><p id="3bf5" class="pw-post-body-paragraph jt ju hi jv b jw me ij jy jz mf im kb kc ob ke kf kg oc ki kj kk od km kn ko hb bi translated">【2018 CVPR】【EncNet】<br/><a class="ae jf" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">语义切分的上下文编码</a></p><h2 id="2b47" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">语义分割</h2><p id="a65f" class="pw-post-body-paragraph jt ju hi jv b jw me ij jy jz mf im kb kc ob ke kf kg oc ki kj kk od km kn ko hb bi translated">)(我)(们)(都)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(就)(是)(这)(些)(事)(,)(我)(们)(还)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(就)(没)(想)(到)(这)(些)(事)(了)(。 )(我)(们)(都)(不)(在)(这)(些)(事)(上)(了)(,)(我)(们)(都)(不)(在)(这)(些)(事)(上)(了)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(。</p><h2 id="eee8" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated"><a class="ae jf" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>