<html>
<head>
<title>Reinforcement Learning: Introduction to Policy Gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:政策梯度介绍</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b?source=collection_archive---------2-----------------------#2021-07-14">https://medium.com/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b?source=collection_archive---------2-----------------------#2021-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="879c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在以前的帖子中，我一直在研究一种形式的强化学习，Q 学习，在这种学习中，代理人找到一种最优策略，使状态轨迹上的总回报最大化。后来，我用深度神经网络扩展了 Q 学习，并探索了 DQN 的实现。在这篇文章中，我想探索一种不同的强化学习方法，称为策略梯度，旨在直接优化策略。与 Q 学习不同，代理不是基于状态-动作值选择最佳动作，而是从动作的概率分布中选择。我将展示政策梯度定理的证明和一个朴素的算法，加强(威廉姆斯 1992)，使用这种推导。令人惊讶的是，威廉姆斯是我现在就读的东北大学的教授！</p><h1 id="6b68" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">介绍</h1><p id="7b14" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">首先，我们来定义一些术语。与选择带有一些噪声的最大值操作的 epsilon greedy 算法不同，我们基于当前策略选择操作。π(a | s，θ) = Pr{Aₜ = a | Sₜ = s，θₜ = θ}，这是给定时间步长 t 的状态 s 和时间步长 t 的策略参数θ时，时间步长 t 的动作 a 的概率</p><p id="e15b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，代理将基于性能测量函数 J(θ)相对于θ的梯度来学习策略。我们将使用梯度上升来调整政策参数，以找到最优政策:θₜ₊₁ = θₜ + α∇J(θₜ).</p><p id="c7b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在处理策略函数π时，我们可以通过确保π(a | s，θ) ∈ (0，1)来保留探索的概念，因此策略不会变得确定。对于参数化的数字偏好，我们为状态-动作对得出一个值 h(s，a，θ)。我们可以使用 softmax 将它转化为概率分布:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/af82c15660d6122a5588a62f44373480.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*q0bMxaQ_tST4RdDgLhHfhQ.png"/></div></figure><p id="fbbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与行动偏好相比，根据 softmax 而不是 epsilon 贪婪策略选择行动有一些优势。首先，行动偏好允许代理接近确定性策略，而 epsilon greedy 将需要保持一定程度的噪音以鼓励探索。行动偏好也集中于建立一个最优的随机政策，而不是集中于一个值。这意味着，如果策略是确定性的，那么最佳行动的概率将趋近于 1。如果最佳策略是随机的，那么行动偏好将形成概率分布。对于ε贪婪策略，没有形成随机策略的自然方法。</p><p id="cf99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个优点是在策略参数的函数上平滑地调整动作概率。对于ε贪婪策略，如果我们基于最大值选择一个动作，Q 值的微小变化会导致不同的动作。这可能极大地高估了所选行动的重要性，并汇聚成一个次优的政策。</p><p id="61d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看代理如何学习最优策略的参数。请记住，我们将根据性能测量函数 J(θ)来调整参数。这就把我们带到了策略梯度定理，它让我们在不推导状态分布的情况下近似 J(θ)相对于θ的梯度。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ko"><img src="../Images/e83fee649a2bfa64489022d115ac14b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*cBTi8zEzgg-hTQDMDPBbvA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">摘自萨顿巴尔托律师事务所，2017 年</figcaption></figure><p id="1baf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">μ(s)这里是我们的随机策略π的策略分布。q 是遵循策略π的动作值函数，而π(a|s，θ)是给定参数θ下的状态的动作分布。</p><p id="db74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就解决了未知环境下需要知道状态分布的问题。以下证明(<a class="ae kt" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" rel="noopener ugc nofollow" target="_blank">萨顿&amp;巴尔托，2017</a>；秘书 13.2)将借助<a class="ae kt" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" rel="noopener ugc nofollow" target="_blank">莉莲翁的文章</a>进行详细演练。</p><h1 id="c9a5" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">政策梯度定理的推导</h1><p id="d6b9" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">定义 J(θ) = V_πθ(s)，其中 V 是带参数θ的策略π的值函数。然后我们有:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/418ce3837d49cae9c5deb8e0890c6258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bv_VVE1r7E4HNqOnFbyF_g.png"/></div></div></figure><p id="820e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以将值函数展开为类似于期望方程的值函数，其中，对于动作空间中的所有动作，我们找到遵循策略π的动作的概率乘以遵循策略π的状态-动作对的值的总和。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es kz"><img src="../Images/847537f7e1165b5f222db23a788d5f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJ1fMxo6I3IfjUVQNDGXAQ.png"/></div></div></figure><p id="c68f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后在(3)中，我们应用乘积法则:d/dx[f(x)g(x)]= f '(x)g(x)+f(x)g '(x)。在(4)中，我们扩展了 q 函数。如前所述，给定一个状态-动作对，q 函数给出一个值。如果我们假设环境是非确定性的，那么下一个状态也将是非确定性的。给定状态-动作对和下一个状态 s '的奖励+值函数，我们乘以新状态 s '和奖励 r 的概率。这给了我们所得到的新状态 s '的期望值。然后我们对所有可能的新状态求和。请注意，奖励使用了强化学习中的一个基本概念，我们必须考虑当前的奖励和未来的效用。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es la"><img src="../Images/a9de21219c4c74edd60cdc6c4ce1d485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjU3Op8hexR0RADxsr3ZcA.png"/></div></div></figure><p id="97a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以将 p(s '，r | s，a)简化为 p(s' | s，a)因为 p(s '，r | s，a)中所有奖励的总和与 p(s' | s，a)相同。然后，我们可以跳过 p(s '，r| s，a ),因为 r 不是θ的函数(梯度是关于θ的)。奖励消失了，因为它是一个常数。</p><p id="869a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回忆一下我们推导的那一行①，∇J(θ) = ∇v(s).请注意，在我们展开的等式的末尾，有一个类似的 v(s’)项。我们可以观察到∇v(s)是递归的。让我们试着扩展一下递归。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lb"><img src="../Images/f1c9b48b49d1d880b31194ba9c11cb05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FL1x3xPpS8FXCiYSfynjA.png"/></div></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lc"><img src="../Images/ad3e08ab22373773834cc93f5129c786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHr-N82hnQ_xs1rqumHh7A.png"/></div></div></figure><p id="b275" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，在第(8)行中，除了从 s '到 s ' '的转换之外，我们用我们到目前为止得到的结果替换了术语∇v(s'。我们可以定义这个转换来稍微简化这个等式。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ld"><img src="../Images/d82e721aa4c5938cab1c79821b8b2962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*lRP9nJGPQWUsAUQbji0Yew.png"/></div></figure><p id="58fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的函数是在遵循策略π的 k 个步骤中从状态 s 到状态 s’的转移的概率。然后我们可以修改步骤(6)和(7)来使用它，但也要记住对所有可能的下一个状态 s’求和。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es le"><img src="../Images/73cff0596d1820fb33826b225d0a7a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_F2LfIvYBNvtZMY_F0ILg.png"/></div></div></figure><p id="9841" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以进一步简化。如果把相乘的项进行分布，就可以把 Pr(s → s '，k=1)和 Pr(s' → s ' '，k=1)组合起来得到 Pr(s → s ' '，k=2)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lf"><img src="../Images/f0bbeb038bcc3fcbf6d9fd6ed2f15036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*-uRaL4lxCF-9-TY-93hVsQ.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lg"><img src="../Images/f680830c0a4966e79a7f69449981e2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TShIpODDu5PNhcERAVYlwA.png"/></div></div></figure><p id="e49a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在反复展开递归位之后，我们可以把它写成求和。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lh"><img src="../Images/88023f93095b84504e7fa5208a25c64a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJPqXIBPiEQtVTfV5OxfHg.png"/></div></div></figure><p id="7467" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们对从起始状态 s 到状态 x 的跃迁的所有可能长度为 k 步的所有目标状态 x 求和。注意，在 k=0 时，Pr(s → s，k = 0，π)通常为 1，因此我们没有忽略第一个ϕ(s 项。</p><p id="c105" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以走向证明的结论。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es li"><img src="../Images/c9a3f81d9a8ae86d92989e370e525cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69sXgsFgXNQot-KUZbrsRA.png"/></div></div></figure><p id="a240" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第(12)行，我们有从起始状态 s0 到结束状态 s 的任何 k 长度步长的跃迁分布，我们将其简化为η(s)。然后，在第(14)行，我们将η(s)归一化为每个最终状态 s 的概率分布∈ (0，1)。然后，我们构建了μ(s ),这是遵循我们的随机策略的动作的概率分布。当我们展开ϕ(s)函数时，我们已经证明了关系式:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ko"><img src="../Images/e83fee649a2bfa64489022d115ac14b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*cBTi8zEzgg-hTQDMDPBbvA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">摘自萨顿巴尔托律师事务所，2017 年</figcaption></figure><h1 id="ccba" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">强化算法</h1><p id="b138" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">现在有了策略梯度定理，我们可以提出一个简单的算法，利用梯度上升来更新我们的策略参数。该定理给出了所有状态和动作的总和，但当我们更新参数时，我们将只使用一个样本梯度，因为我们无法获得所有可能的动作和状态的梯度。我们可以把它写成期望值，因为样本梯度的期望值和实际梯度是一样的。因此，我们有以下内容:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lj"><img src="../Images/616dc7e3d54706133eae0462986e3ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-uyOz7SKdqZVU9ns7Wj_g.png"/></div></div></figure><p id="b548" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我们在(2)中用 Sₜ替换了 s。这是因为我们正在公式化方程以采用样本梯度，其中我们使用样本状态 Sₜ和样本动作 Aₜ，而不是所有状态和动作的总和。现在，让我们用一个简单的动作 Aₜ.代替所有动作的总和</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lk"><img src="../Images/1507695d58a06fdb26072ff2948f99b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jc0TGpqb-jSVu0s-JJPCJA.png"/></div></div></figure><p id="b045" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在(3)中，我们将该值乘以末尾的项，这样样本动作的值就由选择该动作的概率来加权。然后在(4)中，我们可以去掉所有动作的总和，用一个样本动作 Aₜ.代替 a 在(5)中，我们可以用时间步长 t 的累积贴现回报 Gₜ来代替状态-行动价值函数。然后，我们可以使用事实 d/dx[ln(x)] = 1/x 来简化方程</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ll"><img src="../Images/869c6d417a02e269144dbe74c2e30e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*XpqigJ970nKU2P6QsGa4RA.png"/></div></figure><p id="34f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以在期望值中使用这一项来执行梯度上升，以更新我们的策略参数。我们可以在朴素强化算法中使用这个更新。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lm"><img src="../Images/d33dfeb26b1d12690b9ea6331538dc1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1I6E6g7H62ICQ-g22EmOA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">摘自萨顿巴尔托律师事务所，2017 年</figcaption></figure><p id="316b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，该算法是一种蒙特卡罗方法，因为我们不是在每个时间步长后更新参数，而是使用静态参数θ对整个轨迹进行间歇采样，然后更新参数。在上面取自教科书(萨顿和巴尔托，2017 年)的伪代码中，更新行略有错误，因为我们不应该更新我们用来获得策略日志概率的相同参数。在我们的实现中，我们可以简单地累加梯度，并在一集结束时一次性更新参数。现在让我们开始实现吧！</p><h1 id="4536" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">实施加固</h1><p id="cceb" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">对于我的实现，我决定使用 Pytorch 而不是 Keras，因为我想探索其他机器学习库。对于策略参数，我使用了神经网络，因为它可以为我做梯度上升。请注意，我将在梯度前面添加一个负号来执行梯度上升，而不是梯度下降。这是我的关系网:</p><pre class="kh ki kj kk fd ln lo lp lq aw lr bi"><span id="e0fb" class="ls je hi lo b fi lt lu l lv lw">#Using a neural network to learn our policy parameters<br/>class PolicyNetwork(nn.Module):<br/>    <br/>    #Takes in observations and outputs actions<br/>    def __init__(self, observation_space, action_space):<br/>        super(PolicyNetwork, self).__init__()<br/>        self.input_layer = nn.Linear(observation_space, 128)<br/>        self.output_layer = nn.Linear(128, action_space)<br/>    <br/>    #forward pass<br/>    def forward(self, x):<br/>        #input states<br/>        x = self.input_layer(x)<br/>        <br/>        #relu activation<br/>        x = F.relu(x)<br/>        <br/>        #actions<br/>        actions = self.output_layer(x)<br/>        <br/>        #get softmax for a probability distribution<br/>        action_probs = F.softmax(actions, dim=1)<br/>        <br/>        return action_probs</span></pre><p id="f7dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实现算法后，我用以下参数测试了我的结果:</p><ul class=""><li id="cd8e" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">γ(折扣系数):0.99</li><li id="1ae0" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">剧集数量:1000</li><li id="2e2f" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">最大步数:10000</li></ul><p id="c57c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们看看我们的代理如何处理 150 集的随机策略:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ml"><img src="../Images/0c810b2e68c08a77f2f24340f9359844.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/0*jfXeE9Vpae7btuFW.png"/></div></figure><p id="08eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不出所料，这些都是一些低于标准的结果，平均得分为 21.48。现在，让我们看看我们的算法超过 1000 集的训练历史。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mm"><img src="../Images/8774ab0ca51c58d9858cfeb1cbfcf492.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*JPYvWUlQMQa1sUcvrBXdcg.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">每集累积奖励</figcaption></figure><p id="5d1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看我们的训练历史，我们可以看到我们的代理人随着时间的推移慢慢学习。然而，我们注意到，与其他学习方法相比，比如我在<a class="ae kt" rel="noopener" href="/nerd-for-tech/first-look-at-reinforcement-learning-67688f36413d">上一篇</a>文章中提到的 DQN，这种学习方法很慢，而且变化很大。这是意料之中的，因为与使用深度神经网络的算法相比，增强算法不太复杂。尽管如此，我们仍然能够使用学习的策略在 50 集上获得 79.6 的平均分数，这优于人类基准(me)30.8。</p><p id="79b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，这是对政策梯度方法的一个很好的介绍，这比我从 CS4100 讲座中学到的 Q-learning 更难理解。我认为试图理解萨顿&amp;巴尔托教科书中每个方程背后的所有推导和推理是很有趣的，这让我对这个话题有了更深的理解。展望未来，我肯定想学习更多关于策略梯度方法和 Pytorch 的知识，我在实现 REINFORCE 时曾使用过 py torch。</p><p id="f181" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的代码:<a class="ae kt" href="https://github.com/chengxi600/RLStuff/blob/master/Policy%20Gradients/REINFORCE.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Policy % 20 gradients/reinforce . ipynb</a></p><p id="36a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ul class=""><li id="6ea9" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated"><a class="ae kt" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" rel="noopener ugc nofollow" target="_blank">强化学习:导论(萨顿&amp;巴尔托 2017) </a></li><li id="0e96" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated"><a class="ae kt" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" rel="noopener ugc nofollow" target="_blank">政策梯度算法(Lilian Weng) </a></li></ul></div></div>    
</body>
</html>