<html>
<head>
<title>Recurrent Neural Networks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络。</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/recurrent-neural-networks-8186403e7e0d?source=collection_archive---------15-----------------------#2021-07-14">https://medium.com/nerd-for-tech/recurrent-neural-networks-8186403e7e0d?source=collection_archive---------15-----------------------#2021-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9bb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">递归神经网络模仿大脑的工作。就像我们根据上下文、主题、性别和情境来选择单词一样，RNNs 也根据之前的输入/上下文来分类和预测单词。学习 rnn 很有趣。他们也有相当惊人的应用。</p><p id="a852" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将把它分成三个部分:</p><ol class=""><li id="8d12" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">什么是 RNN，我们为什么需要它？</li><li id="f557" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">什么是长短期记忆，它是如何工作的？</li><li id="034a" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">RNN 的应用。</li></ol><p id="30a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将首先讨论应用程序，以了解下一部分的背景。一旦你理解了 rnn 在哪里使用，学习这个概念就很容易了。</p><h2 id="4567" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated"><strong class="ak">无线网络的应用:</strong></h2><p id="aa69" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">rnn 主要用于<em class="kr">文本预测</em>。RNNs 理解文本的<em class="kr">上下文。他们甚至可以学习拼写！！RNNs 还可用于<em class="kr">语音识别、语音到文本的转换、</em>反之亦然。其他应用包括<em class="kr">文本摘要、视频标记、翻译、语音搜索、</em>等等。基本上，它也可以用于文本、音频、视频、语音和图像。</em></p><h2 id="ed56" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">递归神经网络；</h2><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/78e6d531977c350fa39b9db59b881649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jqfn9m-2wC5pfLpcwzk2Ug.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">致谢:维基百科</figcaption></figure><p id="d9e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">引入 RNNs 是为了克服前馈网络的局限性。前馈结构不能基于以前的数据进行预测，因为它是单向的，而且它只能接受固定大小的数据并给出固定大小的输出。RNNs 没有这个问题。RNN 是一个具有反馈架构的<em class="kr">人工神经网络</em>，这意味着网络的输出被反馈到网络的<em class="kr">隐藏层</em>。rnn 使用反向传播算法。反向传播使用梯度方法来更新输入的权重。他们记得以前的预测，并使用这些预测来改变输入的权重，以更好地预测下一个单词或结果。rnn 以矩阵的形式考虑所有数据，因为这是让它们理解的语言。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es li"><img src="../Images/4afe8bdde1a4c600f25fa98da83812b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eO6rgbbtpL0vjT2No5VeiQ.png"/></div></div></figure><p id="9a99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举个例子来更好地理解。假设你计划学习 3 门课程(每天一门)。你已经制定了你想学习的顺序。这些科目是数学、科学和英国文学。顺序是<em class="kr">数学- &gt;理科- &gt;英国文学- &gt;数学- &gt;理科..如此等等。</em>所以，预测取决于你前一天学了什么。通过前馈网络这是不可能的，因为它不记得任何以前的预测。这就是 RNN 出现的原因。RNN 有一个存储先前预测的记忆。因此，当你发送<em class="kr">数学</em>作为输入时，你得到<em class="kr">科学</em>作为输出。科学和英国文学也是如此。某天你错过了学习，所以你将会有一个昨天<em class="kr">预测的输入</em>和另一个你<em class="kr">上次实际学习过的</em>(因为你错过了一天)。因此，RNN 必须记住前一天的预测，然后再预测今天的情况。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lj"><img src="../Images/46d307055c36d64df86fea732c8b9316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*nKFSOFF2VOPLmEQ_oz8WjA.png"/></div></figure><p id="1feb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们再举一个例子。这些句子是关于海洋食物链的，“鲨鱼以金枪鱼为食。”、“金枪鱼以小鱼为食。”，“小鱼以海洋植物为食。”，“海洋植物以微生物为食。”和“微生物以鲨鱼为食”。现在，假设您将这些数据输入到 RNNs。然后你输入<em class="kr">‘sharks’</em>，模型应该根据给定的数据造出一个有意义的句子。RNN 会明白，因为有一个名词和可能性会'<em class="kr">饲料'</em>或一个<em class="kr">时期(。).</em>但是如果它是一个<em class="kr">’那么这个句子就是“鲨鱼”这没有任何意义。而其他可以形成的不想要的句子有<em class="kr">“鲨鱼以鲨鱼为食。”，“海洋植物。”，“海洋植物以鲨鱼为食。”等</em>。发生这种情况是因为反向传播只关心名词或动词，但缺乏对有关输入的每种可能性的意义的理解。由于反向传播，还有一些其他问题:</em></p><ol class=""><li id="a8d1" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><em class="kr">消失梯度:</em>反向传播使用梯度方法来更新权重。但有时梯度很小，以至于随着梯度的更新，预测不会改变。就像消失了一样。随着更多的层被添加到网络，损失函数几乎变为零。因此使得网络很难训练。</li><li id="101d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><em class="kr">爆炸渐变:</em>顾名思义是消失渐变的反义词。在这里，梯度是如此之大，以至于在模型中进行了大的改变。这使得模型不稳定，并且变得无法训练模型。因此，重要的是用正确的量更新梯度，以使其在正确的方向上训练。</li></ol><p id="719e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些问题可以用长短期记忆来解决。</p><h2 id="6506" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated"><strong class="ak">长短期记忆(LSTM): </strong></h2><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lk"><img src="../Images/2dc9b13f0e62f7ccf9a589e59d941d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8nFrwolzTYtUWSaziiJGkg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">演职员表:<a class="ae ll" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> LSTM，科拉的博客</a></figcaption></figure><p id="349b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与具有短期依赖性的基本 rnn 相比，LSTMs 用于长期依赖性。标准的 rnn 结构简单，只有一个不断重复的<em class="kr"> tanh </em>层。但是 LSTMs 有四个循环的<em class="kr"> sigmoid </em>层(<em class="kr"> tanh </em>和<em class="kr"> sigmoid </em>是激活函数)。LSTMs 有一个<em class="kr">单元状态</em>(如上图所示)，它从每一层收集信息以获得输出，并由门调节。门是<em class="kr"> sigmoid 函数</em>和<em class="kr">逐点乘法运算</em>的组合。<em class="kr"> sigmoid 函数</em>的输出在 0 和 1 之间。LSTMs 有 3 个这样的门。LSTMs 有四个主要步骤，我们将详细讨论每个步骤:</p><p id="61f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下步骤以文本预测为例进行解释，以使其更容易理解。</p><p id="7cec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kr">第一步:(遗忘步骤)</em> </strong></p><p id="7b72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个步骤被称为<em class="kr">遗忘步骤</em>，因为它的主要工作是决定是否需要丢弃特定信息。这是通过一个<em class="kr">s 形</em>层完成的，这个门被称为<em class="kr">忘记门</em>。这个门的输入是<em class="kr"> h(t-1) </em>和<em class="kr"> x(t) </em>。这个门的输出<em class="kr">【f(t)】</em>不是 1 就是 0。1 表示不丢弃信息，0 表示丢弃信息。这个门有助于记住当前主题的性别，因此可以使用正确的代词。而当接收到新的主题时，旧的主题就被扔掉了。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lm"><img src="../Images/b257109f6234a40569e719b888893edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*VwANgwwWrh31jDAoPzi7og.png"/></div></figure><p id="16bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kr">第二步:</em> </strong></p><p id="be22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一步决定在单元状态中存储哪些信息。这涉及到两个门。一个是称为<em class="kr">输入门层</em>的<em class="kr">s 形门</em>。这个门决定需要更新的值。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es ln"><img src="../Images/3f83cfbe84d5d4aa99bc7f3dd9acc89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*piQ1cDflLre5EkiVBlaWEA.png"/></div></figure><p id="8a9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二个门是 tanh，它为新的候选键创建一个向量。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lo"><img src="../Images/73ecd63fdf0746d7cd0698390bd46ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*e0ZT6bp6nMBEEWHnruZZGA.png"/></div></figure><p id="d3f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后这两个门被组合以创建状态的更新。</p><p id="0062" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kr">第三步:</em> </strong></p><p id="b3ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，单元状态已经接收到来自前面步骤的信息。它会忘记要忘记的状态，更新要更新的信息，把这两个结合起来得到 c(t)。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lp"><img src="../Images/bb0010be016ff37de37331ea054d4130.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*NaRcvjAIXUCpPJinwq1VBA.png"/></div></figure><p id="27cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kr">第四步:</em> </strong></p><p id="1697" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是最后一步。它有两层 tanh 和乙状结肠。这里决定了产量。Sigmoid 用于选择在输出中提供单元状态的哪些部分。然后通过 tanh，这两个门的输出相乘。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lq"><img src="../Images/af3253b378fa713606bdd38f23f36aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*oa7lU1LpzXnmskegWKW-Yg.png"/></div></figure><p id="9868" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，获得了输出。</p><p id="9763" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><ol class=""><li id="6217" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><a class="ae ll" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解 LSTM 网络</a>(强烈推荐)</li></ol><p id="ccbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<a class="ae ll" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理有效性</a></p><p id="8afa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.<a class="ae ll" href="https://www.youtube.com/watch?v=WCUNPb-5EYI&amp;t=1411s" rel="noopener ugc nofollow" target="_blank">循环神经网络(RNN)和长短期记忆(LSTM) </a>作者布兰登·罗勒</p><p id="1667" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作者:SMRUTHI R PALADHI</p></div></div>    
</body>
</html>