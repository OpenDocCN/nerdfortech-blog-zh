<html>
<head>
<title>Containerized Ceph OSD Replacement</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集装箱化Ceph OSD更换</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/containerized-ceph-osd-replacement-744918b7417a?source=collection_archive---------7-----------------------#2021-07-17">https://medium.com/nerd-for-tech/containerized-ceph-osd-replacement-744918b7417a?source=collection_archive---------7-----------------------#2021-07-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d404" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如今，随着每项服务都迁移到微服务，我们认为是时候在我们的Ceph集群上以SDS的形式进行迁移了。当从恶魔化的Ceph集群迁移到容器化的存储守护进程(CSD)时，我确保我们在容器化的方法中为每种情况设置了一个过程。</p><p id="eaae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个过程中，我将演示如何删除和添加一个新的OSD磁盘，同时模拟一个失败的磁盘OSD。这个过程包括诊断集群的状态、故障磁盘&amp;最终替换磁盘模拟。</p><p id="439e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在投入时间和精力后，我认为与社区分享这个过程会很棒。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h2 id="dbd0" class="jk jl hi bd jm jn jo jp jq jr js jt ju iq jv jw jx iu jy jz ka iy kb kc kd ke bi translated"><strong class="ak">要求</strong></h2><ul class=""><li id="6c0c" class="kf kg hi ih b ii kh im ki iq kj iu kk iy kl jc km kn ko kp bi translated">Ceph星团</li><li id="7a2a" class="kf kg hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">已安装“ceph-common”软件包</li><li id="6c10" class="kf kg hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">已安装“ceph-osd”软件包</li></ul><h2 id="e66f" class="jk jl hi bd jm jn jo jp jq jr js jt ju iq jv jw jx iu jy jz ka iy kb kc kd ke bi translated"><strong class="ak">程序</strong></h2><p id="2876" class="pw-post-body-paragraph if ig hi ih b ii kh ik il im ki io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">首先，我们安装了一个“健康的”容器化Ceph集群。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="f7d0" class="jk jl hi ld b fi lh li l lj lk">[root@mon0 vagrant]# ceph -s<br/> cluster:<br/> id: 38456c75–20b5–4fac-81d7–228f89eb3255<br/> health: HEALTH_OK<br/> <br/> services:<br/> mon: 3 daemons, quorum mon0,mon1,mon2 (age 112m)<br/> mgr: mon1(active, since 2d), standbys: mon2, mon0<br/> osd: 8 osds: 8 up (since 3m), 8 in (since 3m)<br/> rgw: 6 daemons active (mon0.rgw0, mon0.rgw1, mon1.rgw0, mon1.rgw1, mon2.rgw0, mon2.rgw1)<br/> <br/> task status:<br/> <br/> data:<br/> pools: 11 pools, 256 pgs<br/> objects: 407 objects, 16 KiB<br/> usage: 8.3 GiB used, 78 GiB / 86 GiB avail<br/> pgs: 256 active+clean</span></pre><p id="a670" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们在主机“OSD0”上有一个有故障的OSD磁盘，我们想要替换该磁盘。</p><p id="2354" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，通过运行“ceph osd树”来确定发出的磁盘主机位置</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="3177" class="jk jl hi ld b fi lh li l lj lk">[root@mon0 vagrant]# ceph osd tree<br/>ID CLASS WEIGHT  TYPE NAME     STATUS REWEIGHT PRI-AFF <br/>-1       0.08398 root default                          <br/>-3       0.02100     host osd0                         <br/> 0   hdd 0.01050         osd.0   down  1.00000 1.00000 <br/> 6   hdd 0.01050         osd.6     up  1.00000 1.00000 <br/>-5       0.03149     host osd1                         <br/> 2   hdd 0.01050         osd.2     up  1.00000 1.00000 <br/> 4   hdd 0.01050         osd.4     up  1.00000 1.00000 <br/> 7   hdd 0.01050         osd.7     up  1.00000 1.00000 <br/>-7       0.03149     host osd2                         <br/> 1   hdd 0.01050         osd.1     up  1.00000 1.00000 <br/> 5   hdd 0.01050         osd.5     up  1.00000 1.00000 <br/> 8   hdd 0.01050         osd.8     up  1.00000 1.00000</span></pre><blockquote class="ll lm ln"><p id="3a07" class="if ig lo ih b ii ij ik il im in io ip lp ir is it lq iv iw ix lr iz ja jb jc hb bi translated">ceph osd tree cmd将显示所有参与的集群osd以及它们在一个漂亮的树形结构中的挤压位置</p></blockquote><p id="764f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您还可以通过使用grep“down”来缩小搜索范围，如下所示。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="9849" class="jk jl hi ld b fi lh li l lj lk">[root@mon0 vagrant]# ceph osd tree | grep down<br/> 0 hdd 0.01050 osd.0 down 1.00000 1.00000</span></pre><p id="6ed9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很好，我们发现磁盘“osd.0”有问题，现在我们可以使用“ceph OSD find”cmd后跟osd id来搜索问题磁盘的主机。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="f782" class="jk jl hi ld b fi lh li l lj lk">[root@mon0 vagrant]# ceph osd find osd.0<br/>{<br/>    "osd": 0,<br/>    "addrs": {<br/>        "addrvec": [<br/>            {<br/>                "type": "v2",<br/>                "addr": "192.168.42.13:6800",<br/>                "nonce": 49383<br/>            },<br/>            {<br/>                "type": "v1",<br/>                "addr": "192.168.42.13:6801",<br/>                "nonce": 49383<br/>            }<br/>        ]<br/>    },<br/>    "osd_fsid": "8b8cf769-bc84-4b11-a68f-8c553126dc30",<br/>    "host": "osd0",<br/>    "crush_location": {<br/>        "host": "osd0",<br/>        "root": "default"<br/>    }<br/>}</span></pre><p id="82f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以继续这个过程，并访问发出的磁盘主机，在本例中是“osd0”。</p><p id="fed1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最棘手的是，在访问osd磁盘主机后，通过执行以下命令删除发出的osd:</p><blockquote class="ll lm ln"><p id="f7ed" class="if ig lo ih b ii ij ik il im in io ip lp ir is it lq iv iw ix lr iz ja jb jc hb bi translated">请记住，在运行一个命令到另一个命令时，执行“ceph -w”来观察Ceph的健康状况，以实现数据完整性。</p></blockquote><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="a53f" class="jk jl hi ld b fi lh li l lj lk">[root@osd0 vagrant]# ceph osd out osd.0<br/>[root@osd0 vagrant]# ceph osd down osd.0<br/>[root@osd0 vagrant]# systemctl stop ceph-osd@0.service<br/>[root@osd0 vagrant]# ceph osd rm osd.0<br/>[root@osd0 vagrant]# ceph auth del osd.0<br/>[root@osd0 vagrant]# ceph osd crush rm osd.0</span></pre><p id="e1e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更换物理磁盘之前，执行下一个命令以防止回填:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="650e" class="jk jl hi ld b fi lh li l lj lk">[root@osd0 vagrant]# ceph osd set noout</span></pre><p id="fbd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在从发布的磁盘中删除我们的集群的任何踪迹之后，使用“lslbk”cmd来标识托管OSD的磁盘，以便在磁盘上删除卷组和逻辑卷。</p><figure class="ky kz la lb fd lt er es paragraph-image"><div class="er es ls"><img src="../Images/b68cded1c631f9dd70343de2982c6c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*P6D_UABzZ1xhItfYXym45w.png"/></div></figure><p id="96d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用“ceph-volume”实用程序删除磁盘的VG和LV，同时指定发布的磁盘路径，如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="29fd" class="jk jl hi ld b fi lh li l lj lk">[root@osd0 vagrant]# ceph-volume lvm zap /dev/sdd</span></pre><p id="60b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在可以在激励新磁盘进入集群后取消设置“noout”选项。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="70eb" class="jk jl hi ld b fi lh li l lj lk">[root@osd0 vagrant]# ceph osd unset noout</span></pre><p id="c7f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，磁盘应该是没有LVM的裸磁盘，这样我们就可以模拟一个新的磁盘附加场景。</p><p id="b448" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们按照我们的命名约定，在新设备上配置一个新的PV、VG和LV，为新磁盘进入我们的集群做准备。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="7321" class="jk jl hi ld b fi lh li l lj lk">[root@osd0 vagrant]# pvcreate /dev/sdc<br/>[root@osd0 vagrant]# vgcreate ceph-block-osd.0 /dev/sdc<br/>[root@osd0 vagrant]# lvcreate -L 10.70g -n ceph-block-osd.0 ceph-block-osd.0 /dev/sdc</span></pre><p id="8c98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们准备使用“ceph-volume”实用程序将这个新磁盘添加到我们的集群中，如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="7b2e" class="jk jl hi ld b fi lh li l lj lk">[root@osd0 vagrant]# ceph-volume lvm create --bluestore --data ceph-block-osd.0/ceph-block-osd.0</span></pre><p id="1e98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该命令将以容器化的方式创建一个新的OSD层。该命令还打包了多个操作来配置新的OSD，例如将磁盘配置为其存储后端的“bluestore ”,以处理其所有数据。</p><figure class="ky kz la lb fd lt er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es lw"><img src="../Images/cd9a09a8321df64706ff0d269c7da0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q2YpxMu0gac8HTyTncdMhQ.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated">由“ceph-volume”执行的命令</figcaption></figure><p id="c844" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您所看到的，在运行这个命令之后，我们为新磁盘创建了一个新的osd容器，这个容器安装在它的上面。</p><figure class="ky kz la lb fd lt er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mf"><img src="../Images/521a79297de1ba7e344dcff4d26e0b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OwKIoImTRrJk9yhO4E4pnw.png"/></div></div></figure><p id="935f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以运行“ceph -s”和“ceph osd tree”命令来检查配置新磁盘后集群的状态。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="76fe" class="jk jl hi ld b fi lh li l lj lk">[root@osd0 vagrant]# ceph -s<br/>  cluster:<br/>    id:     38456c75-20b5-4fac-81d7-228f89eb3255<br/>    health: HEALTH_OK<br/> <br/>  services:<br/>    mon: 3 daemons, quorum mon0,mon1,mon2 (age 5h)<br/>    mgr: mon1(active, since 2d), standbys: mon2, mon0<br/>    osd: 8 osds: 8 up (since 88m), 8 in (since 88m)<br/>    rgw: 6 daemons active (mon0.rgw0, mon0.rgw1, mon1.rgw0, mon1.rgw1, mon2.rgw0, mon2.rgw1)<br/> <br/>  task status:<br/> <br/>  data:<br/>    pools:   11 pools, 256 pgs<br/>    objects: 407 objects, 16 KiB<br/>    usage:   8.5 GiB used, 77 GiB / 86 GiB avail<br/>    pgs:     256 active+clean<br/> <br/>  io:<br/>    client:   4.0 KiB/s rd, 0 B/s wr, 3 op/s rd, 2 op/s wr</span></pre></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><pre class="lc ld le lf aw lg bi"><span id="f4d6" class="jk jl hi ld b fi mg mh mi mj mk li l lj lk">[root@osd0 vagrant]# ceph osd tree<br/>ID CLASS WEIGHT  TYPE NAME     STATUS REWEIGHT PRI-AFF <br/>-1       0.08398 root default                          <br/>-3       0.02100     host osd0                         <br/> 0   hdd 0.01050         osd.0     up  1.00000 1.00000 <br/> 6   hdd 0.01050         osd.6     up  1.00000 1.00000 <br/>-5       0.03149     host osd1                         <br/> 2   hdd 0.01050         osd.2     up  1.00000 1.00000 <br/> 4   hdd 0.01050         osd.4     up  1.00000 1.00000 <br/> 7   hdd 0.01050         osd.7     up  1.00000 1.00000 <br/>-7       0.03149     host osd2                         <br/> 1   hdd 0.01050         osd.1     up  1.00000 1.00000 <br/> 5   hdd 0.01050         osd.5     up  1.00000 1.00000 <br/> 8   hdd 0.01050         osd.8     up  1.00000 1.00000</span></pre></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><pre class="lc ld le lf aw lg bi"><span id="9a46" class="jk jl hi ld b fi mg mh mi mj mk li l lj lk">[root@osd0 vagrant]# ceph auth list<br/>installed auth entries:</span><span id="4a9d" class="jk jl hi ld b fi ml li l lj lk">osd.0<br/> key: AQDo1/JggRh6IhAAVx/3rMNW7DlLdKA/vj693A==<br/> caps: [mgr] allow profile osd<br/> caps: [mon] allow profile osd<br/> caps: [osd] allow *</span></pre><p id="4d11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们成功地用“HEALTH_OK”作为指标替换了OSD。此外，我们确保该磁盘是crush层次结构的一部分，并且列在Ceph的身份验证列表中。</p><p id="9ccb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这个过程对您有所帮助，并使迁移到容器环境变得更加容易。感谢观看，我说你在下一个。</p></div></div>    
</body>
</html>