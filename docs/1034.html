<html>
<head>
<title>NLP Theory and Code: Encoder-Decoder Models (Part 11/30)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP理论和代码:编码器-解码器模型(第11/30部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7?source=collection_archive---------7-----------------------#2021-03-02">https://medium.com/nerd-for-tech/nlp-theory-and-code-encoder-decoder-models-part-11-30-e686bcb61dc7?source=collection_archive---------7-----------------------#2021-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="91af" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">序列到序列网络，上下文表示</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/7915948cd0b05735b68b8a529eb922a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t88tLfexKAoUgqIF3PZ85A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><h1 id="b48f" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍..</h1><p id="41d9" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">在像机器翻译这样的任务中，我们必须从输入单词序列映射到输出单词序列。读者必须注意，这与“序列标记”不同，在“序列标记”中，任务是将序列中的每个单词映射到预定义的类别，如词性或命名实体任务。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lb"><img src="../Images/dec386ba2172958f9734d2b27b3238a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hw2j8vng7Dy-c0-V3UOn5A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者生成</figcaption></figure><p id="4931" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">在上面的两个例子中，模型的任务是将序列中的每个单词映射到标签/类别。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lh"><img src="../Images/d0fe30ff7cb9883536c188e141205352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*58Oy6EOoKjxyjQ9PoOUuEQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">谷歌翻译</figcaption></figure><p id="fec1" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">但是在机器翻译这样的任务中:输入序列的长度不一定是输出序列的长度。正如您在google翻译示例中看到的，输入长度是“5”，输出长度是“4”。由于我们将一个输入序列映射到一个输出序列，因此序列模型被命名为<strong class="kh hj">。</strong>不仅输入和输出序列的长度不同，而且单词的顺序也不同。这在NLP中是非常复杂的任务，并且编码器-解码器网络在处理这种复杂的序列到序列映射任务方面非常成功。</p><p id="6bef" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">编码器-解码器网络可以解决的一个更重要的任务是文本摘要，其中我们将长文本映射到短摘要/摘要。在这篇博客中，我们将试图理解编码器-解码器网络的架构及其工作原理。</p><h1 id="e48f" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">编码器-解码器网络..</h1><p id="3845" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">这个网络已经被应用到非常广泛的应用中，包括机器翻译、文本摘要、问答和对话。让我们试着理解编码器-解码器网络的基本思想。编码器获取输入序列并创建其上下文表示(也称为上下文),解码器将该上下文表示作为输入并生成输出序列。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/c7e3e6ff878efb092c954e11648ab60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtBwvO4zRhapgKfKMY0znw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成的编码器-解码器核心思想的图示</figcaption></figure><h1 id="2eb9" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">RNN的编码器和解码器…</h1><p id="e438" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">RNN的所有变体都可以用作编码器和解码器。在RNN的中，我们有隐藏状态的概念“<strong class="kh hj"> ht </strong>”，这可以被看作是在序列链中直到时间步“<strong class="kh hj">t”</strong>它已经看到的单词/记号的总结。</p><h2 id="f38c" class="lj jo hi bd jp lk ll lm jt ln lo lp jx ko lq lr jz ks ls lt kb kw lu lv kd lw bi translated">编码器:</h2><p id="ca12" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">编码器接收输入序列并生成一个上下文，该上下文是解码器输入的本质。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/1834275fb9e2f193e163d31dceac740e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8-0bMgnmVJ5Eb0nOWedIg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">使用RNN作为编码器，由作者生成</figcaption></figure><p id="24c7" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">编码器的全部目的是为输入序列生成上下文表示/上下文。使用RNN作为编码器，RNN序列链的最终隐藏状态可以用作上下文的代理。这是构成编码器-解码器模型基础的最关键的概念。我们将使用下标e和d来表示编码器和解码器的隐藏状态。忽略编码器的输出，因为目标是为解码器生成最终的隐藏状态或上下文。</p><h2 id="8152" class="lj jo hi bd jp lk ll lm jt ln lo lp jx ko lq lr jz ks ls lt kb kw lu lv kd lw bi translated">解码器:</h2><p id="952a" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">解码器将上下文作为输入，并生成一系列输出。当我们使用RNN作为解码器时，上下文是RNN编码器的最终隐藏状态。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/84637cfedf9f12bbbacb0854ac431404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvFuthB4rozwcuGqb1YZvg.png"/></div></div></figure><p id="39a7" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">第一解码器RNN单元将“上下文”作为其先前的隐藏状态。然后，解码器生成输出，直到生成序列结束标记。</p><p id="9e8d" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">RNN解码器中的每个单元自动回归地接受输入，即解码器使用它自己在时间t的估计输出作为下一个时间步长xt+1的输入。如果上下文仅对第一解码器RNN单元可用，则一个重要的缺点是，随着越来越多的输出序列产生，上下文变弱。为了克服这个缺点，可以在每个解码RNN时间步骤中使用“上下文”。与香草RNN略有不同。让我们来看看更新后的解码器RNN方程。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/b20b1e41608aef09b297ec61dbb4ed65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3S4EBOyErTo7t_3GYmkzxw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">RNN解码器的更新方程式，由作者生成</figcaption></figure><h1 id="3812" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">训练编码器-解码器模型..</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/f328fef40de045e67874290e4477539e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4VanjWwBjLhEAUIUcNsUA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">完整的编码器和解码器网络</figcaption></figure><p id="6cd9" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">训练数据由多组输入句子和它们各自的输出序列组成。我们在解码器中使用交叉熵损失。编码器-解码器架构是端到端训练的，就像RNN语言模型一样。计算损失，然后使用梯度下降优化反向传播以更新权重。通过平均每个目标词的交叉熵损失来计算总损失。</p><h1 id="7df3" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">注意..</h1><p id="42ee" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">语言类型学:这是一个与机器翻译相关的概念。这些语言在许多方面都不同。对这些系统性差异和跨语言相似性的研究被称为语言类型学。</p><p id="af3e" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated"><strong class="kh hj">词汇空缺:</strong>这是一个与机器翻译相关的概念。在一种给定的语言中，可能没有一个词或短语可以表达另一种语言中一个词的确切含义。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/1f25b94e9e735d81faaa5391824dd85a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Beuz1nMOAgZRT_x8Kslhw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由作者生成</figcaption></figure><p id="4d96" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">上一篇:<a class="ae ma" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-bi-directional-lstm-part-10-30-cab0eab65533?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP零比一:双向LSTM部分(10/30) </strong> </a> <br/>下一篇:<a class="ae ma" href="https://kowshikchilamkurthy.medium.com/nlp-zero-to-one-attention-mechanism-part-12-30-c5c36670c81f?source=your_stories_page-------------------------------------" rel="noopener"> <strong class="kh hj"> NLP零比一:注意机制(12/30) </strong> </a></p></div></div>    
</body>
</html>