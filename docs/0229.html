<html>
<head>
<title>NLP with Hugging Face Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有拥抱脸变形金刚的 NLP</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/nlp-with-hugging-face-transformers-a41caadf6f2?source=collection_archive---------0-----------------------#2020-07-27">https://medium.com/nerd-for-tech/nlp-with-hugging-face-transformers-a41caadf6f2?source=collection_archive---------0-----------------------#2020-07-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2bf5c673110c775157fe808f8f364348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfDk5y2DORltfeIVEAEdNg.png"/></div></div></figure><p id="8082" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您曾经经历过任何自然语言处理项目或教程，您一定已经学会了这些东西:</p><ol class=""><li id="535e" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">分类者</li><li id="e15d" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">编码器-解码器</li><li id="1655" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">LSTM</li><li id="8fa4" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">手套或 Word2Vec</li><li id="f588" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">嵌入层</li></ol><p id="0799" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但就这样吗？虽然我们大多数人都使用 rnn 处理文本，但我们很少注意到它们无法在 GPU 上训练得更快。这是由于大多数顺序处理文本的 NLP 模型的架构。我们能即兴创作这个模型吗？</p><p id="47aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在过去的几年里，自然语言处理模型有所增加。从 2015 年<a class="ae kc" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">关注</strong> </a>机制、<a class="ae kc" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">变形金刚</strong> </a>的发布，再加上即将发布的<a class="ae kc" href="https://github.com/openai/gpt-3" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">【GPT-3】</strong></a>，相信计算机对人类语音的处理能力将远远好于人类自己。</p><p id="a5f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从 LSTMs 中的<strong class="is hj">监督序列</strong>学习到拥有 3.4 亿参数的<a class="ae kc" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank"> Bert </a>中的<strong class="is hj">半监督</strong> <strong class="is hj">序列</strong>学习，NLP 的需求和应用暴涨。事实上，GPT 3 号有 1750 亿个参数！作为一个初学者，我所学的只有 LSTMs。</p><p id="a125" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了在研究热潮中保持优势，你需要保持更新。</p><p id="10a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">拥抱脸的变形金刚简化了这些巨大模型的应用。而这种独特的能力并不是以学习任何新的深度学习库为代价的。它与 TensorFlow 和 PyTorch 都集成得很好。</p><p id="6e07" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们先来看看这个包的一些直接应用。所有的代码也可以在这里正式获得<a class="ae kc" href="https://huggingface.co/transformers/task_summary.html" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="7f07" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak"> 1。情绪分析</strong></h1><p id="0d2b" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">情感分析是对情感(积极、消极和中性)的解释和分类。对于这个任务，大多数人会创建一个单词包，并将其传递给逻辑回归(可选的降维)。但是为什么要训练，当我们有了允许我们直接应用它的功能时。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="0549" class="lp ke hi ll b fi lq lr l ls lt">from transformers import *</span><span id="4bd9" class="lp ke hi ll b fi lu lr l ls lt">nlp = pipeline('sentiment-analysis')</span><span id="4b8d" class="lp ke hi ll b fi lu lr l ls lt">print(nlp('I love my country.'))</span></pre><h1 id="d873" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak"> 2。</strong>命名实体识别</h1><p id="c8cd" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">NER 方法也在 NLTK 这样的包中广泛使用。这有助于我们将单词分为不同的类别，如组织名称、人名等</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="4dbb" class="lp ke hi ll b fi lq lr l ls lt">nlp_token_class = pipeline('ner')</span><span id="ecb3" class="lp ke hi ll b fi lu lr l ls lt">nlp_token_class('FAANG  stands for Facebook, Apple, Amazon, Netflix and Google.')</span></pre><h1 id="80c4" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak"> 3。文本总结</strong></h1><p id="d887" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">文本摘要有两种类型:抽取型和抽象型。</p><p id="1b5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">提取摘要从文本中选择具有最有价值的上下文的句子，而抽象被明确地训练以从文本中创建摘要。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f799" class="lp ke hi ll b fi lq lr l ls lt">TEXT = """</span><span id="5286" class="lp ke hi ll b fi lu lr l ls lt">100+ Days of ML Code is a commitment to better your understanding of this powerful tool by dedicating at least 1 hour of your time every day to studying and/or coding machine learning for at-least 100 days.</span><span id="8fde" class="lp ke hi ll b fi lu lr l ls lt">Everyone, beginners as well as professionals are welcome to take up the challenge, join, contribute and collaborate.</span><span id="8d2c" class="lp ke hi ll b fi lu lr l ls lt">At the end of this 100 Days Of ML journey, all the members will be able to showcase a rich portfolio of code, analysis and narrative, treating all the above topics and models plus all the additional content, that as a member you will invariably experience and explore yourself, throughout this learning journey.</span><span id="d698" class="lp ke hi ll b fi lu lr l ls lt">"""</span><span id="31e4" class="lp ke hi ll b fi lu lr l ls lt">summarizer = pipeline('summarization')</span><span id="6bfb" class="lp ke hi ll b fi lu lr l ls lt">print(summarizer(TEXT))</span></pre><h1 id="1970" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">4.问题回答</h1><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="0d85" class="lp ke hi ll b fi lq lr l ls lt">qa = pipeline('question-answering')</span><span id="7d4d" class="lp ke hi ll b fi lu lr l ls lt">print(qa(context=TEXT, question='How many days is the challenge for? '))</span><span id="fb25" class="lp ke hi ll b fi lu lr l ls lt">print(qa(context=TEXT, question='What will I get at the end?'))</span></pre><h1 id="9e25" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">5.翻译</h1><p id="5b34" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">如果你知道的话，翻译是顺序学习最流行的例子之一。但是有了拥抱脸，你就不需要训练自己的脸了。(对于大部分的流行语言。有了迁移学习，你当然也可以建立你的客户模型。)</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="0209" class="lp ke hi ll b fi lq lr l ls lt">translator = pipeline('translation_en_to_fr')</span><span id="c74c" class="lp ke hi ll b fi lu lr l ls lt">translator("100 Days of ML Code is a commitment to better your understanding of this powerful tool by dedicating at least 1 hour of your time every day to studying and/or coding machine learning for at-least 100 days.")</span></pre></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="30ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些只是一些主要的应用，但还有更多像文本生成，寻找丢失的单词等。但是像我们之前讨论的伯特和 GPT 这样的模型呢？</p><p id="0bb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">好吧，拥抱脸为我们提供了使用那些预先训练好的模型以及 PyTorch 和 Tensorflow 的灵活性。</p><p id="1b26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你是这个领域的新手，请记住几个关键词:</p><ol class=""><li id="377e" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><strong class="is hj">标记化</strong>:任何 ML/神经模型都只是一堆公式，不能识别字符串作为输入。所以我们需要将每个单词/字符转换成一些唯一的数字。</li><li id="f072" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">填充和截断</strong>:模型需要有一些固定大小的输入或批量输入大小，因此，小句必须用一些独特的字符连接起来，而长句必须在一定长度上截断。</li><li id="1765" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">型号</strong>:只是参考我们将要尝试的不同型号。</li><li id="b9a6" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">解码:如果一个模型只理解数字，它也会生成数字。这个函数帮助我们解码数字。</li></ol><p id="d21e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们来看看同样的应用程序，但是有更好的模型。此处提供了所有集成型号的列表<a class="ae kc" href="https://huggingface.co/transformers/model_summary.html" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="9629" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">6.掩蔽语言建模</h1><p id="d25f" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">这种模式有助于填补句子中的某些空白。请注意，在预测之前，您必须在句子中添加一个特殊的屏蔽字符，如下所示。</p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="mc md l"/></div></figure><h1 id="5883" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">7.文本生成</h1><p id="a531" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">文本生成是我们学习 LSTMs 的一个很好的例子。但是这种方法(以及许多其他模型)的缺点是，在几次预测之后，预测开始重复自己。因此，为了阻止这一点，我们需要添加一些特殊的东西。类似于<sep>、<end>等特殊标记通常用于此目的。</end></sep></p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="mc md l"/></div></figure><p id="e0ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更多型号可在官方文档中查看，并在 Pipeline 和 Pretrained 模块中提供。</p><p id="e020" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">既然您现在已经熟悉了这些转换器的应用和灵活性，我希望您能很快熟悉 NLP。</p><p id="7c66" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学习新事物的荣誉。如果你愿意，请留下一些掌声和评论。</p></div></div>    
</body>
</html>