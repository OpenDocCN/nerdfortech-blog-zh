<html>
<head>
<title>Review — Zhu TMM’20: Generative Adversarial Network-Based Intra Prediction for Video Coding (HEVC/VVC Intra)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朱 20:基于生成竞争网络的视频帧内预测编码(帧内编码)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-zhu-tmm20-generative-adversarial-network-based-intra-prediction-for-video-coding-c8a217c564ea?source=collection_archive---------8-----------------------#2021-03-06">https://medium.com/nerd-for-tech/review-zhu-tmm20-generative-adversarial-network-based-intra-prediction-for-video-coding-c8a217c564ea?source=collection_archive---------8-----------------------#2021-03-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="04c8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>进行 HEVC 帧内编码修复，性能优于<a class="ae ix" href="https://sh-tsang.medium.com/review-ipfcn-intra-prediction-using-fully-connected-network-hevc-intra-prediction-28de33dff3a5" rel="noopener"> <strong class="ak"> IPFCN </strong> </a> <strong class="ak">。</strong></h2></div><p id="4e75" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">在</span>这个故事里，<strong class="ja hj">中国科学院、香港城市大学、香港城市大学深圳研究所、深圳大学合作的</strong>、<strong class="ja hj">朱 TMM'20 </strong>)基于生成对抗网络的视频编码帧内预测。在本文中:</p><ul class=""><li id="a268" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a>用于通过对和帧内预测的可用重构像素进行调节来填充缺失部分。</li><li id="b5de" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">该<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>修补被用作与传统帧内预测竞争的帧内编码预测。</li></ul><p id="65c2" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这是在<strong class="ja hj"> 2020 </strong> <strong class="ja hj"> TMM </strong>的一篇论文，其中 TMM 的<strong class="ja hj">高影响因子为 6.051 </strong>。(<a class="kr ks ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----c8a217c564ea--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="6d0d" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">概述</h1><ol class=""><li id="a79c" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt lx kj kk kl bi translated"><strong class="ja hj">传统帧内预测的简要回顾</strong></li><li id="cb0d" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="ja hj">甘</strong> </a> <strong class="ja hj">基于帧内预测的修复</strong></li><li id="e91b" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">将</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="ja hj">甘</strong> </a> <strong class="ja hj">应用于视频编码的两种方案</strong></li><li id="7abb" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">一些训练细节</strong></li><li id="44d9" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt lx kj kk kl bi translated"><strong class="ja hj">实验结果</strong></li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="0257" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated"><strong class="ak"> 1。传统帧内预测的简要回顾</strong></h1><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ly"><img src="../Images/af63a5d412fffb17d2465c9af44e54b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*sTxiKxbN8SV92pQxJPYOPQ.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">(一)HEVC 的 35 种帧内模式。(b)角度模式 29 的例子。</strong></figcaption></figure><ul class=""><li id="c4b1" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在 HEVC 帧内模式中，有 35 个帧内预测。</li><li id="b295" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">模式 0 和 1 </strong>:平面和 DC 模式，预测平滑区域。</li><li id="d4c0" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">模式 2 至 34 </strong>:它们是外推边界参考像素的角度预测。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mk"><img src="../Images/255372b6f0a8ac3019bf41cc7c15a2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*2lp3yHw2oumt-INNypAt5w.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc"> (a)待预测 PU(64×64)。(b)预测结果(模式 0-34)。</strong></figcaption></figure><ul class=""><li id="113b" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj"> (a) </strong>:待预测的预测单元(PU)。</li><li id="97a7" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj"> (b) </strong>:如所见，所有 35 个传统帧内预测都不能很好地预测复杂结构。</li></ul><blockquote class="mp mq mr"><p id="40db" class="iy iz ms ja b jb jc ij jd je jf im jg mt ji jj jk mu jm jn jo mv jq jr js jt hb bi translated">因此，本文提出采用<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a>进行帧内编码。</p></blockquote></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="af03" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">2.<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a>基于帧内预测的修复技术</h1><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mw"><img src="../Images/3b208d70ed9557b348f08113d5d34e80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWatDV2C77u8JWi1ZMXocg.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">基于</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="bd lc"> GAN </strong> </a> <strong class="bd lc">的帧内预测修复架构</strong></figcaption></figure><ul class=""><li id="e658" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在本文中，架构不是重点。主要新颖之处在于作者如何应用<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>作为帧内预测来提高编码效率。</li><li id="f183" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">该架构主要遵循“全局和局部一致的图像完成”中的架构。</li><li id="a466" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">将 128 × 128(缺失部分在右下方)以及左侧、左上和顶部的参考像素输入到网络中。</li></ul><h2 id="b737" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">2.1.面具</h2><ul class=""><li id="c7c0" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated"><strong class="ja hj">有两个尺寸为 128 × 128 的掩模</strong>用于<strong class="ja hj">指示缺失部分。</strong> '⊗'表示逐像素乘法。</li><li id="5caa" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">在掩码 1 中，左上、上和左的块中的值是 1，而右下的块中的值是 0。</li><li id="efef" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">在掩码 2 中，值与掩码 1 中的值相反。</li></ul><h2 id="083d" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">2.2.发电机</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nl"><img src="../Images/44eb7aa005c316ab89921daf2d668fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*BNfFBSlGqD3qegVehKVRwA.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">发电机</strong></figcaption></figure><ul class=""><li id="ec43" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">生成器<em class="ms">G</em>T25】具有<strong class="ja hj"> 17 个卷积层</strong>，用于预测缺失部分。</strong></li></ul><h2 id="fe1b" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">2.3.鉴别器</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nm"><img src="../Images/c776ca5c2d2cf5152c2afc4fa80d5a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*2edohOgN2rqkq56B-eseRQ.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">真假决定层</strong></figcaption></figure><ul class=""><li id="1a6b" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">另一个是<strong class="ja hj">鉴别器<em class="ms"> D </em> </strong>，可以看作是鉴别预测缺失部分是真是假的二元分类器。</li><li id="78c8" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">为了提高性能，鉴别器分为两部分，即局部鉴别器和全局鉴别器。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nm"><img src="../Images/ed7f317eca394869d3f26c39633dbd75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*8kGbrfBEtp9Hj0VW3VAkgg.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">本地鉴别器</strong></figcaption></figure><ul class=""><li id="0731" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">对于<strong class="ja hj">本地鉴别器</strong>，有<strong class="ja hj"> 5 个卷积层</strong>和<strong class="ja hj"> 1 个全连接层</strong>，<strong class="ja hj">输入</strong>为<strong class="ja hj">预测缺失部分</strong>。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nn"><img src="../Images/fc760948fd52f2cc267796790ae747b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*bE7qJ-Yz0KgLG5Bfi8kr-w.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">全局鉴别器</strong></figcaption></figure><ul class=""><li id="e15e" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">对于<strong class="ja hj">全局鉴别器</strong>，有<strong class="ja hj"> 6 个卷积层</strong>和<strong class="ja hj"> 1 个全连接层</strong>，<strong class="ja hj">输入</strong>为<strong class="ja hj">整体 128 × 128 </strong> <strong class="ja hj">图像</strong>，其中缺失部分为预测，其他块来自原始输入，如上图所示。</li></ul><h2 id="ba24" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">2.4.来自<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a>的 35 个版本的预测</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es no"><img src="../Images/07647301cf532b028797c5cf39aff0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*6zjx1V_SZwvSLD4gUiOorw.png"/></div></figure><ul class=""><li id="c944" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">通过在缺失部分填充 35 种不同的颜色，生成来自</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="ja hj">甘</strong> </a> <strong class="ja hj">的 35 个预测。</strong></li><li id="8a01" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">最好的是具有最低率失真成本的那个。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="89a7" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">3.将<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>应用于视频编码的两种方案</h1><ul class=""><li id="52dc" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">基于<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>的帧内预测修复有两种方案。</li><li id="ec16" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">方案一</strong>:一个是<strong class="ja hj"> 1 </strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="ja hj">甘</strong> </a> <strong class="ja hj">型号</strong>为 64 × 64 块。对于 32 × 32、16 × 16 和 8 × 8 块，块预测由<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>根据块大小和位置从 64 × 64 块预测中复制。</li><li id="77be" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">方案二</strong>:另一个是<strong class="ja hj"> 4 </strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="ja hj">甘</strong> </a> <strong class="ja hj">型号</strong>用于 64 × 64、32 × 32、16 × 16、8 × 8 块。</li><li id="d6fc" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">下面显示了这两种方案的优缺点:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es np"><img src="../Images/91c65fdbcc86b40f70df7d0e69db4462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*TWBJ2WqjEVD4bpPZwYdGUQ.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">两种方案对比</strong></figcaption></figure><blockquote class="mp mq mr"><p id="d94b" class="iy iz ms ja b jb jc ij jd je jf im jg mt ji jj jk mu jm jn jo mv jq jr js jt hb bi translated">最后，<strong class="ja hj">方案一采用</strong>，因为其实现简单，<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>模型少，编码器端运算量少，易于适应 VVC。</p></blockquote></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="d0c3" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">4.一些训练细节</h1><h2 id="2608" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">4.1.训练数据集和输入</h2><ul class=""><li id="3615" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated"><strong class="ja hj">训练数据集</strong>由来自<strong class="ja hj">未压缩彩色图像数据库</strong>的分辨率为<strong class="ja hj"> 512 × 384 </strong>的<strong class="ja hj"> 800 幅图像</strong>组成。</li><li id="32d3" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">它们使用 QP 22 编码。</li><li id="3e4f" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">这个样本和它对应的没有任何编码失真的基础事实形成一个训练对。</li><li id="af3f" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">只提取亮度分量用于训练。</li><li id="923d" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">在训练阶段，为每个样本随机设置缺失部分的初始像素值:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nq"><img src="../Images/187604f8b7f2ebcff6ff4b71db07dc49.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*wJUqDDmp7r9QoHFdEhdNug.png"/></div></figure><ul class=""><li id="0846" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">其中⌊ ⋅ ⌋是地板舍入运算的函数，<em class="ms"> X </em>从{0，1，2，…，34}中随机选择，<em class="ms"> k </em>表示位深度。</li></ul><h2 id="8f26" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">4.2.损失函数</h2><ul class=""><li id="d90f" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated"><strong class="ja hj">通过均方误差损失函数(<strong class="ja hj"> MSE </strong>)对发生器的前几个时期</strong>进行训练:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es nr"><img src="../Images/81a1df583746dd3997b1eff4998caf4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*2iwe7pBx2Blx_pl8UxqxiQ.png"/></div></figure><ul class=""><li id="f1f0" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">其中<em class="ms"> A </em> 1 和<em class="ms"> A </em> 2 为局部信息，即地面真实和预测块的缺失部分。</li><li id="fb02" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">几个纪元后，整个<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>网络就可以训练好了。对于每次训练迭代，生成器和鉴别器将被逐一重复更新。这是一个极小极大优化问题:</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ns"><img src="../Images/2fab313724c3b18373a9d44a94927a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*7XBzHHehZteiGn4jqq6FUA.png"/></div></figure><ul class=""><li id="7e96" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">其中<em class="ms"> B </em> 1 和<em class="ms"> B </em> 2 为全局信息，即整个 128 × 128 图像。</li><li id="f4e1" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><em class="ms"> α </em> = 2500 以平衡 MSE 损失和二值交叉熵损失。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="7fc2" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">5.实验结果</h1><ul class=""><li id="aaeb" class="kd ke hi ja b jb ls je lt jh lu jl lv jp lw jt ki kj kk kl bi translated">用的是 HM-16.17。使用所有帧内配置。</li><li id="8a73" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">不同于训练数据的测试序列在普通测试条件(CTC)下用四个 qp 编码，包括{22，27，32，37}。</li></ul><h2 id="fd70" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">5.1.帧内预测比较</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nt"><img src="../Images/055cf9b2fa739fae64ef736b17f69601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-6CbyRtC1Pd4RYGt0DfSA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">帧内预测比较。</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="bd lc">【甘】</strong> </a> <strong class="bd lc">基于帧内预测。(b)基于角度的帧内预测。</strong></figcaption></figure><ul class=""><li id="0a3a" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">来自基于<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>的帧内预测的结果更加一致并且具有更低的 sad。</li><li id="4c0a" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">最好的是模式 31，最小 SAD 值为 24578。</li></ul><h2 id="7629" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">5.2.敌对术语的影响</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nu"><img src="../Images/3bd0c775e0d13016acfc396818a42805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0GQ7B2toPkn1mcZR0XRKQ.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc"> (a) &amp; (c):无对抗性项，(b) &amp; (d):有对抗性项</strong></figcaption></figure><ul class=""><li id="5cb7" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">有对抗项的结果比没有对抗项的结果更清晰</strong>，并且<strong class="ja hj"> MSE 值也小得多。</strong></li><li id="df6b" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">原因是局部和全局鉴别器使得预测的像素信息在局部和全局中一致。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nv"><img src="../Images/c1fe28e7c5c8ec878296efb1704429c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNrj1yMJ8tojYUnUoJDLag.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">BD-率(%) </strong></figcaption></figure><ul class=""><li id="7fe5" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">针对具有对立项的帧内预测的修补可以<strong class="ja hj">实现更多编码增益。</strong></li></ul><h2 id="319a" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">5.3.与 SOTA 方法的比较</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nw"><img src="../Images/e40f2a9e0f1b1ec304e833858d822ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9e0Cs5ib4XCG1dH_GuangA.png"/></div></div></figure><ul class=""><li id="92dc" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">对于具有 35 种模式和 1 种模式的<strong class="ja hj">，对于亮度</strong>和两种色度分量，它们分别可以实现平均<strong class="ja hj"> 6.6% </strong>，7.5%，7.5%和<strong class="ja hj"> 6.2% </strong>，7.2%，7.5%的比特率降低，优于<a class="ae ix" href="https://sh-tsang.medium.com/review-ipfcn-intra-prediction-using-fully-connected-network-hevc-intra-prediction-28de33dff3a5" rel="noopener"><strong class="ja hj">【IPF cn</strong></a><strong class="ja hj">【14】<strong class="ja hj">。</strong></strong></li><li id="82d1" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated">这是因为基于<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>的修复依赖于相邻可用块的结构信息，如人脸、桌子等物体。</li></ul><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nx"><img src="../Images/671da100b1d7142785aeaee98871ce40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uE8ZNldEUimlD_zgDBjybw.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">蓝色:常规帧内预测，红色:</strong> <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> <strong class="bd lc">甘</strong> </a> <strong class="bd lc">基于帧内预测</strong></figcaption></figure><ul class=""><li id="09aa" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">基于<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>帧内预测的块主要位于纹理区域。</li></ul><blockquote class="mp mq mr"><p id="a2df" class="iy iz ms ja b jb jc ij jd je jf im jg mt ji jj jk mu jm jn jo mv jq jr js jt hb bi translated">相邻块中的结构信息越多，选择基于<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>的帧内预测的概率就越高。</p></blockquote><figure class="lz ma mb mc fd md er es paragraph-image"><div class="er es ny"><img src="../Images/28ab277bca0ea98404fd24fb61a0211a.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*6aMhVho1lx4jJ3nh7t7urQ.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">失败案例</strong></figcaption></figure><ul class=""><li id="5f61" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">高速的动作，还有人物，是<a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a>无法预测的。</li></ul><h2 id="e95e" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">5.4.小型 QP 集和大型 QP 集</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nz"><img src="../Images/08dc5117e7bd21e7c7c2c345e21ca988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ewcBnCjBoZgsJfFUxwJfoA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">使用不同 QP 范围的 BD-Rate(%)</strong></figcaption></figure><ul class=""><li id="243d" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">使用相同的模型，但将其应用于不同的 QP 范围，也有 BD-rate 减少。</li></ul><h2 id="e79f" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">5.5.适应 VVC</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es oa"><img src="../Images/d5c8112ef96d1da3a495b5002e2305e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UfBmBy8M1HdXwSVMC0DDA.png"/></div></div></figure><ul class=""><li id="2031" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated">在小、正常和大 QP 设置下，对于亮度分量，所提出的方法分别实现了 3.10%、6.75%和 6.83%的比特率降低。</li></ul><h2 id="6aee" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">5.6.计算复杂性分析</h2><figure class="lz ma mb mc fd md er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es ob"><img src="../Images/4e4584d782044709ff97f710885ac0af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*zcb0wF5PInFOt74tq70mNA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated"><strong class="bd lc">计算复杂度分析</strong></figcaption></figure><ul class=""><li id="14e2" class="kd ke hi ja b jb jc je jf jh kf jl kg jp kh jt ki kj kk kl bi translated"><strong class="ja hj">使用 CPU+GPU </strong>，在<strong class="ja hj"> HM(VTM) </strong>下，对于<strong class="ja hj">编码</strong>和<strong class="ja hj">解码</strong>，该方法的计算复杂度与原 HM(VTM)相比，平均为<strong class="ja hj"> 7(2.5) </strong>和<strong class="ja hj"> 160(257) </strong>倍。</li><li id="61de" class="kd ke hi ja b jb km je kn jh ko jl kp jp kq jt ki kj kk kl bi translated"><strong class="ja hj">使用 CPU </strong>，<a class="ae ix" href="https://sh-tsang.medium.com/review-ipfcn-intra-prediction-using-fully-connected-network-hevc-intra-prediction-28de33dff3a5" rel="noopener"> <strong class="ja hj"> IPFCN </strong> </a>和该方法的计算复杂度与原 HM 相比，对于<strong class="ja hj">编码</strong>平均为<strong class="ja hj"> 86 </strong>和<strong class="ja hj"> 149 </strong>次，对于<strong class="ja hj">解码</strong>平均为<strong class="ja hj"> 201 </strong>和<strong class="ja hj"> 5264 </strong>次。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="8dfa" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">参考</h2><p id="b13e" class="pw-post-body-paragraph iy iz hi ja b jb ls ij jd je lt im jg jh oc jj jk jl od jn jo jp oe jr js jt hb bi translated">【2020 TMM】【朱 TMM ' 20】<br/><a class="ae ix" href="https://ieeexplore.ieee.org/abstract/document/8744274" rel="noopener ugc nofollow" target="_blank">基于生成对抗性网络的视频编码帧内预测</a></p><h2 id="6150" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">生成对抗网络</h2><p id="3811" class="pw-post-body-paragraph iy iz hi ja b jb ls ij jd je lt im jg jh oc jj jk jl od jn jo jp oe jr js jt hb bi translated"><strong class="ja hj">图像合成</strong> [ <a class="ae ix" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a> ] [ <a class="ae ix" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a> ] [ <a class="ae ix" rel="noopener" href="/@sh.tsang/review-lapgan-laplacian-generative-adversarial-network-gan-e87200bbd827">拉普甘</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/review-dcgan-deep-convolutional-generative-adversarial-network-gan-ec390cded63c">DCGAN</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-pix2pix-image-to-image-translation-with-conditional-adversarial-networks-gan-ac85d8ecead2" rel="noopener">pix 2 pix</a>]<br/><strong class="ja hj">超分辨率</strong>[<a class="ae ix" rel="noopener" href="/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490">SRGAN&amp;SRResNet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-enhancenet-automated-texture-synthesis-super-resolution-8429635aa75e">EnhanceNet</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-esrgan-enhanced-super-resolution-generative-adversarial-networks-super-resolution-e8533ad006b5">ESRGAN</a><br/><strong class="ja hj">模糊</strong></p><h2 id="7bd7" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated">编解码器帧内预测</h2><p id="4b1f" class="pw-post-body-paragraph iy iz hi ja b jb ls ij jd je lt im jg jh oc jj jk jl od jn jo jp oe jr js jt hb bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(情)(情)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(情)(况)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(有)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(还)(有)(什)(么)(好)(的)(情)(情)(意)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(不)(知)(道)(,)(我)(们)(还)(有)(些)(不)(知)(道)(的)(情)(况)(。</p><h2 id="021c" class="mx lb hi bd lc my mz na lg nb nc nd lk jh ne nf lm jl ng nh lo jp ni nj lq nk bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>