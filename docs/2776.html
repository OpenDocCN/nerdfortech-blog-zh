<html>
<head>
<title>Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/machine-learning-c1a09efe6833?source=collection_archive---------17-----------------------#2021-05-19">https://medium.com/nerd-for-tech/machine-learning-c1a09efe6833?source=collection_archive---------17-----------------------#2021-05-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b3ac" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">5 分钟内学会</h2></div><h2 id="ec87" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">什么是机器学习？</h2><p id="d7cc" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">教导或训练机器执行和预测结果。</p><p id="0469" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated"><strong class="jx hj">机器学习的类型</strong></p><ul class=""><li id="2bbc" class="kt ku hi jx b jy ko kb kp ji kv jm kw jq kx kn ky kz la lb bi translated">监督学习:由特征和标签组成</li><li id="a2b8" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ky kz la lb bi translated">无监督学习:仅由特征组成</li></ul><h2 id="59f9" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">监督学习</h2><p id="1dd9" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">采用训练数据集中的特征和标签来学习和预测结果。</p><ul class=""><li id="d05a" class="kt ku hi jx b jy ko kb kp ji kv jm kw jq kx kn ky kz la lb bi translated">分类</li><li id="d0ad" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ky kz la lb bi translated">回归</li></ul><h2 id="10b4" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">无监督学习</h2><p id="18a2" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">仅包含特征，并使用特征和图案提取标签。</p><ul class=""><li id="6cb9" class="kt ku hi jx b jy ko kb kp ji kv jm kw jq kx kn ky kz la lb bi translated">减少</li><li id="5d53" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ky kz la lb bi translated">使聚集</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lh"><img src="../Images/1ce135bfcdca78ccf8c19787e127077a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONBJYeuYAs_wTcn4EVH7iw.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated"><strong class="bd iz">机器学习的类型</strong></figcaption></figure><h1 id="e23b" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">机器学习算法</h1><ol class=""><li id="ac9c" class="kt ku hi jx b jy jz kb kc ji mi jm mj jq mk kn ml kz la lb bi translated">线性回归</li><li id="7b2e" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">逻辑回归</li><li id="ef39" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">朴素贝叶斯</li><li id="6886" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">决策图表</li><li id="0b6d" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">支持向量机</li><li id="f2fa" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">k 倍交叉验证</li><li id="1cc7" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">随机森林</li><li id="26c6" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">k 均值聚类</li><li id="5bb4" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">k-最近邻</li><li id="2e9e" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">分层聚类</li><li id="b69e" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">主成分分析</li></ol><h1 id="ae0e" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">线性回归</h1><pre class="li lj lk ll fd mm mn mo mp aw mq bi"><span id="6f35" class="ix iy hi mn b fi mr ms l mt mu">from sklearn.linear_model import LinearRegression<br/>model = Linear_Regression()</span></pre><h1 id="72a6" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">逻辑回归</h1><pre class="li lj lk ll fd mm mn mo mp aw mq bi"><span id="706c" class="ix iy hi mn b fi mr ms l mt mu">from sklearn.linear_model import LogisticRegression<br/>model = LogisticRegression()</span></pre><h1 id="01a6" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">朴素贝叶斯</h1><pre class="li lj lk ll fd mm mn mo mp aw mq bi"><span id="602a" class="ix iy hi mn b fi mr ms l mt mu">from sklearn.naive_bayes import GaussianNB,MultinomialNB<br/>model = GaussianNB()<br/>model = MultinomialNB()</span></pre><h1 id="303a" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">决策图表</h1><pre class="li lj lk ll fd mm mn mo mp aw mq bi"><span id="afba" class="ix iy hi mn b fi mr ms l mt mu">from sklearn.tree import DecisionTreeclassifier<br/>model = DecisionTreeClassifier()</span></pre><h1 id="db10" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">支持向量机</h1><pre class="li lj lk ll fd mm mn mo mp aw mq bi"><span id="3c35" class="ix iy hi mn b fi mr ms l mt mu">from sklearn.svm import SVC<br/>model = SVC(gamma=10)</span></pre><h1 id="f4eb" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">k 倍交叉验证</h1><pre class="li lj lk ll fd mm mn mo mp aw mq bi"><span id="6e5a" class="ix iy hi mn b fi mr ms l mt mu">from sklearn.model_selection import cross_val_score</span></pre><h1 id="3628" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">随机森林</h1><pre class="li lj lk ll fd mm mn mo mp aw mq bi"><span id="4162" class="ix iy hi mn b fi mr ms l mt mu">from sklearn.ensemble import RandomForestClassifier<br/>model = RandomForestClassifier(n_estimators=10)</span></pre><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mv"><img src="../Images/aca5d5d84f809d8d1dc36f96c1992789.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7RdxpiYRVAMu8rSi"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">你准备好了吗？</figcaption></figure><p id="7744" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">机器已经准备好预测数据，现在轮到你开始研究上面和下面提供的这些算法了。</p><p id="90e1" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">让我们深入研究人工智能，让机器变得更有趣。</p><p id="17f4" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">认识一下我的机器人:)狮子座！！</p><p id="d57a" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">每种算法在预测结果方面都有其独特的作用。大多数机器学习算法需要不同的方法和策略来处理问题。以我的情况来说，我曾经用不合适的算法玩不一致的问题来理解深度。</p><h1 id="bed9" class="lx iy hi bd iz ly lz ma jd mb mc md jh io me ip jl ir mf is jp iu mg iv jt mh bi translated">详细算法:</h1><h2 id="4eb1" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">线性回归</h2><p id="f537" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">这是一种模型的线性方法，通过将数据分为两类，在数据中形成一条线性线。最有效的是，线性模型很好地用于基于分类的问题。它用数据中的输入和目标来陈述模型。</p><h2 id="65ec" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">逻辑回归</h2><p id="2f41" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">它是建立在二元因变量逻辑函数基础上的统计模型。它非常类似于线性回归，用于预测分类相关变量。</p><h2 id="ab7e" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">朴素贝叶斯</h2><p id="2507" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">这是一种监督学习算法，可以更准确、更快速地做出最佳预测。除了(我的 fav 算法)它有两种类型的模型高斯 NB，多项式 NB。</p><h2 id="e749" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">决策图表</h2><p id="bf85" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">在决策树中，数据按照一定的参数被连续分割。它有决策节点和树叶等实体。基于这些实体，算法将做出决定。主要用于分类和回归问题。</p><h2 id="eadd" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">支持向量机</h2><p id="5e80" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">支持向量机(SVM)处理特征和标签的两个数据点之间的超平面。超平面状态作为两个不同数据点的决策边界，用红色和蓝色开始它们。</p><h2 id="a32e" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">k 倍交叉验证</h2><p id="2823" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">交叉验证评估机器学习算法的性能。它将数据分成“K”个点，例如 k=2，k=4。这些分裂点在验证算法中称为折叠。</p><h2 id="1334" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">随机森林</h2><p id="a061" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">随机森林更简单，类似于决策树，而在随机森林中，它是决策树的组合，以获得更高的准确性和工作速度。它在决策树的数量上有估计器。它比单棵树预测的结果更好。</p><h2 id="4e67" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">k 均值聚类</h2><p id="584d" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">这是一种无监督的学习，它对数据进行分组而没有标签。使用聚类，该算法在数据点中的“K”的帮助下基于三个特征划分成组。</p><h2 id="d6b2" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">k-最近邻</h2><p id="e0e8" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">它是 ML 中处理分类和回归问题的最好算法。KNN 根据数据中的相似性特征，使用数据对最新数据进行分类。简而言之，它将以前的数据存储为一条记录，每当新数据出现时，它就开始搜索其数据的旧记录。</p><h2 id="f71d" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak">层次聚类</strong></h2><p id="b479" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">这是一种带有聚类分析的无监督学习，它将未标记的数据分成聚类。从某种意义上来说，聚类将相关数据点及其属性挑选成组。</p><h2 id="6078" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">主成分分析</h2><p id="71cb" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">PCA 是一种无监督学习，用于对大型数据集进行降维，以将它们缩减为较小的数据集，但仍然由大型数据集中的相似特征组成。最常见的是，它用于过滤图像的噪声数据集，如分辨率、压缩。</p></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><h1 id="ced6" class="lx iy hi bd iz ly nd ma jd mb ne md jh io nf ip jl ir ng is jp iu nh iv jt mh bi translated">梯度下降</h1><blockquote class="ni nj nk"><p id="bbdc" class="jv jw nl jx b jy ko ij ka kb kp im kd nm kq kf kg nn kr ki kj no ks kl km kn hb bi translated"><em class="hi">它是以最速下降的形式通过反方向移动使一个函数的代价最小化的优化算法</em> <strong class="jx hj"> <em class="hi">全局损失最小。</em> </strong></p></blockquote><ul class=""><li id="1a3d" class="kt ku hi jx b jy ko kb kp ji kv jm kw jq kx kn ky kz la lb bi translated">它是机器学习的核心&amp;深度学习。</li><li id="01aa" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ky kz la lb bi translated">w = w-alpha *梯度</li><li id="a318" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ky kz la lb bi translated">梯度=σ损失/σw</li><li id="d89f" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ky kz la lb bi translated">阿尔法=学习率</li></ul><p id="422c" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">在梯度下降中，还有其他三种类型:</p><ol class=""><li id="b232" class="kt ku hi jx b jy ko kb kp ji kv jm kw jq kx kn ml kz la lb bi translated">随机梯度下降(SGD):这是一种简单但非常有效的方法，通过随机向前拾取来拟合线性分类器和回归器。它用于大型训练数据集。</li><li id="2cae" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">批量梯度下降:它采用所有的训练样本，然后调整权重。它用于小型训练数据集。</li><li id="f146" class="kt ku hi jx b jy lc kb ld ji le jm lf jq lg kn ml kz la lb bi translated">小批量梯度下降:类似于 SGD 使用的批量。</li></ol></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><div class="li lj lk ll fd np"><a href="https://www.linkedin.com/in/akshithkumar-05/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab dw"><div class="nr ab ns cl cj nt"><h2 class="bd hj fi z dy nu ea eb nv ed ef hh bi translated">Akshith Kumar - Andhra Pradesh，印度|职业简介| LinkedIn</h2><div class="nw l"><h3 class="bd b fi z dy nu ea eb nv ed ef dx translated">我从事过许多与机器学习中的数据科学相关的不同项目。我的主要兴趣领域是…</h3></div><div class="nx l"><p class="bd b fp z dy nu ea eb nv ed ef dx translated">www.linkedin.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od lr np"/></div></div></a></div></div></div>    
</body>
</html>