<html>
<head>
<title>Introduction to Convolutional Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积网络简介</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/convolutional-networks-b54335f4e21f?source=collection_archive---------14-----------------------#2021-05-02">https://medium.com/nerd-for-tech/convolutional-networks-b54335f4e21f?source=collection_archive---------14-----------------------#2021-05-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="e328" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">卷积神经网络或CNN</h2></div><p id="0e04" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">卷积网络或卷积神经网络也称为CNN，是一种特殊类型的神经网络，用于处理具有网格状拓扑结构的数据。例如，图像数据可以表示为2d网格像素。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es jt"><img src="../Images/7dae0315ebd9336f74223431e8665181.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*z-ER6qO1CLWm_T6O_tr-2w.jpeg"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated"><strong class="bd kf">图像的二维像素表示</strong></figcaption></figure><p id="792c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">CNN采用了一种叫做卷积<strong class="iz hj">的数学运算。卷积运算是一种特殊类型的线性运算，卷积网络是简单的神经网络，它在至少一层中使用卷积运算而不是简单的矩阵乘法。</strong></p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="87d2" class="kn ko hi bd kf kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">1.卷积运算</h1><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/f66d173d01269184c895b3c11375ee30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_GfG3qc4C93q1BOmLBJqTA.png"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx translated"><strong class="bd kf">卷积运算</strong>用一个<strong class="bd kf">星号(*) </strong>表示</figcaption></figure><p id="6c86" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在卷积网络术语中，第一个自变量(在本例中为I)通常称为输入，第二个自变量(在本例中为K)称为内核，然后我们执行卷积运算(*)，得到的输出由I *K给出，有时该输出也称为特征映射。</p><p id="3add" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在机器学习或深度学习应用中，这些输入I通常是多维数据数组，而核心K通常是多维参数数组。</p><p id="081a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们经常一次在一个以上的轴上使用卷积运算，例如，如果我们想使用二维图像I作为输入，那么我们可能需要使用二维核k。</p><p id="da12" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">卷积通常对应于一个非常稀疏的矩阵(其元素大部分等于零的矩阵)。这是因为内核通常比输入图像小得多。任何与矩阵乘法一起工作并且不依赖于矩阵结构的特定属性的神经网络算法都应该与卷积一起工作，而不需要对神经网络进行任何进一步的改变。</p><p id="7582" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">稀疏矩阵:</strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lj"><img src="../Images/3186a230582c8436a4a922db191d0d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pDp9uYHKzfElB_qUBqElw.jpeg"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lk"><img src="../Images/f829dde1cc0103d7b7815b6506ea14b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*WuFXfe0iO56inB4uQ__inQ.png"/></div></figure><h1 id="8c6e" class="kn ko hi bd kf kp ll kr ks kt lm kv kw io ln ip ky ir lo is la iu lp iv lc ld bi translated"><strong class="ak"> 2。CNN与传统的神经网络有何不同？</strong></h1><p id="a505" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">传统的神经网络层使用参数矩阵的矩阵乘法，其中单独的参数描述每个输入和每个输出之间的相互作用，即每个输出单元与每个输入单元相互作用。</p><p id="9b65" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个简单的神经网络:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lv"><img src="../Images/a0749fdb425df76d686b7a90adddf278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YkQGiXpRDKmbo_hspVjPzA.png"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">每个输出都与每个输入相互作用。</figcaption></figure><p id="2f7b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，卷积网络具有稀疏的相互作用，有时称为稀疏连通性或稀疏权重。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lw"><img src="../Images/a9a3cfd5bae02694775f306eb553c812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*_hUHcrtgEUxXTqrTZD5JyQ.jpeg"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">隐藏层中的神经元与更少的输入交互。</figcaption></figure><h2 id="1632" class="lx ko hi bd kf ly lz ma ks mb mc md kw jg me mf ky jk mg mh la jo mi mj lc mk bi translated"><strong class="ak"> 2.1使用稀疏连接的好处是什么，或者我们为什么在任何简单的神经网络上使用稀疏连接？</strong></h2><p id="aac1" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">在处理图像时，输入图像可能有数千或数百万个像素，但我们可以检测到小的、有意义的特征，如核仅占用数十或数百个像素的边缘。这意味着我们需要存储更少的参数，这既降低了模型的内存需求，又提高了其统计效率。这也意味着计算输出需要更少的操作，因此更便宜。这些效率的提高通常是相当大的。如果有m个输入和n个输出，那么矩阵乘法需要m×n个参数，并且实际中使用的算法具有O(m × n)运行时间(每个例子)。如果我们将每个输出的连接数限制为k，那么稀疏连接方法只需要k × n个参数和O(k × n)个运行时间。对于许多实际应用，在保持k比m小几个数量级的同时，有可能在机器学习任务上获得良好的性能。</p><h2 id="0178" class="lx ko hi bd kf ly lz ma ks mb mc md kw jg me mf ky jk mg mh la jo mi mj lc mk bi translated">2.2 CNN中的参数共享</h2><p id="dedf" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">在传统的神经网络中，当计算该层的输出时，权重矩阵的每个元素仅使用一个参数(权重),然而在CNN中具有参数共享。参数共享是指在计算图层输出时多次使用相同的参数。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lw"><img src="../Images/efe644816d67aa002ad7c83094cbd719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*kMsa8H6I4r2ebP5DS14c0g.jpeg"/></div></figure><h1 id="6043" class="kn ko hi bd kf kp ll kr ks kt lm kv kw io ln ip ky ir lo is la iu lp iv lc ld bi translated">3 .共享:</h1><p id="5fbc" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">典型的CNN由三个阶段组成，在第一阶段，该层并行执行几个卷积以产生一组线性函数。在第二阶段，每个线性激活通过一个非线性函数运行，在第三阶段，我们使用一个汇集<strong class="iz hj">函数。</strong></p><p id="79d0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">汇集函数用附近输出的汇总统计来代替某一位置的网络输出。例如<strong class="iz hj"> maxpooling </strong>操作报告邻域中的最大数量。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/c33fc788f8846061e57f24bf284aabcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gaD6SJ6kQNVOclE_WkwLNQ.png"/></div></div></figure></div></div>    
</body>
</html>