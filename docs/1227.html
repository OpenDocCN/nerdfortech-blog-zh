<html>
<head>
<title>Types of Optimization Algorithms used in Neural Networks and Ways to Optimize Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中使用的优化算法类型和优化梯度下降的方法</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-descent-1e32cdcbcf6c?source=collection_archive---------1-----------------------#2021-03-10">https://medium.com/nerd-for-tech/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-descent-1e32cdcbcf6c?source=collection_archive---------1-----------------------#2021-03-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ff66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您是否曾经想过，通过更新模型参数，如<strong class="ih hj">权重</strong>和<strong class="ih hj">偏差</strong>值，为您的神经网络模型使用哪种优化算法来产生稍好且更快的结果？我们应该使用<strong class="ih hj">梯度下降</strong>还是<strong class="ih hj">随机</strong>梯度下降还是<strong class="ih hj"> Adam？</strong></p><p id="17f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在写这篇文章之前，我也不知道这些不同类型的优化策略之间的主要区别，以及哪一种优于另一种。</p><p id="9995" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:</strong></p><p id="11a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">拥有良好的理论知识是惊人的，但在实时深度学习项目中用代码实现它们是完全不同的事情。基于不同的问题和数据集，您可能会得到不同的意外结果。因此，作为奖励，我还添加了各种课程的链接，这些课程在我学习数据科学和 ML 的过程中帮助了我很多，实验并比较了不同的优化策略，这促使我写了这篇关于在实施深度学习和比较不同优化策略时不同优化器之间的比较的文章。以下是一些帮助我成为今天的我的资源。我个人是</em><a class="ae je" href="https://datacamp.pxf.io/x9NMDy" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">data camp</strong></a><em class="jd">的粉丝，我就是从它开始的，现在还在通过</em><a class="ae je" href="https://datacamp.pxf.io/x9NMDy" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">data camp</strong></a><em class="jd">学习，不断做新的课程。他们真的有一些令人兴奋的课程。一定要去看看。</em><a class="ae je" href="https://datacamp.pxf.io/x9NMDy" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">data camp</strong></a><em class="jd">课程库中有一些令人惊艳的新课程，一定要去看看。</em></p><p id="5b15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jf translated">从今天起至 12 月 3 日美国东部时间晚上 8 点，ataCamp 将提供首月订阅，仅售 1 美元。此外，他们有一个黑色星期五销售，年度订阅有 75 %的折扣。这是他们第一次进行这样的销售。所以一定要去看看 <a class="ae je" href="https://datacamp.pxf.io/x9NMDy" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">数据营</strong> </a> <strong class="ih hj">。另外，</strong><a class="ae je" href="https://datacamp.pxf.io/x9N7JA" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="jd">dat camp cheat sheets</em></strong></a><strong class="ih hj">是我经常使用的一个非常方便、非常有用的工具。</strong></p><p id="eb3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，这将是获取一些年度订阅(我有)的最佳时间，基本上可以无限制地访问  <a class="ae je" href="https://datacamp.pxf.io/x9NMDy" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">数据营</strong> </a> <strong class="ih hj"> <em class="jd">上的所有课程和其他内容，并在这个疫情期间卓有成效地利用你坐在家里的时间。</em>所以，加油吧，伙计们，快乐学习，充分利用这段隔离时间，让疫情变得更强大、更熟练。</strong></p><p id="563e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> 1)这是 DataCamp 上的课程链接</em> <a class="ae je" href="https://datacamp.pxf.io/3PdGkK" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">深度学习在 python 中使用</em></strong><strong class="ih hj"><em class="jd">Keras</em></strong></a><em class="jd">包</em> <strong class="ih hj"> <em class="jd"> </em> </strong> <em class="jd">或者绝对可以从</em><strong class="ih hj"><em class="jd"/></strong><a class="ae je" href="https://datacamp.pxf.io/YgyQVq" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="jd">开始使用 Keras </em>构建 CNN 进行图像处理 </strong> <em class="jd">如果理解</em> <strong class="ih hj"> <em class="jd">深度学习和 AI </em> </strong> <em class="jd">基础知识是你现在想要的，那么</em> <strong class="ih hj"> <em class="jd"> </em> </strong> <em class="jd">以上两门课程是最好的深度学习课程，你可以在那里找到，学习深度学习的基础知识，并在 python </em> <strong class="ih hj"> <em class="jd">中实现。这些是我的第一个深度学习课程，对我正确理解基础知识帮助很大。</em> </strong></a></p><p id="beef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> 2)这些关于在 Python 中</em> <a class="ae je" href="https://datacamp.pxf.io/do7myM" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">构建聊天机器人的课程</strong> </a> <strong class="ih hj"> </strong>和<strong class="ih hj"> </strong> <a class="ae je" href="https://datacamp.pxf.io/NK6rG7" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> NLP 基础在 Python 中使用 NLTK</strong></a><strong class="ih hj"/><strong class="ih hj"/>也推荐给对学习 AI 和深度学习更感兴趣的人。所以基于你的兴趣去尝试一下吧。</p><p id="fa99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">3)</em><a class="ae je" href="https://datacamp.pxf.io/5b79kD" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="jd">Python 中的机器学习使用 Scikit-learn</em></strong></a><strong class="ih hj">-</strong><em class="jd">本课程将教你如何用 Python 实现不同数据集的监督学习算法。</em></p><p id="4434" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> 4) </em> <a class="ae je" href="https://datacamp.pxf.io/e4A6nr" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">数据角力和操纵数据帧使用熊猫</em></strong></a><strong class="ih hj"><em class="jd">:</em></strong><em class="jd">这个神奇的课程将帮助你在 python 中进行数据角力和数据预处理。而一个数据科学家大部分时间都在做预处理和数据角力。所以这门课对初学者来说可能会很方便。</em></p><p id="b6af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> 5) </em>本课程教你中级<em class="jd"/><a class="ae je" href="https://datacamp.pxf.io/x9N1Ay" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">Python for Data Science</strong></a><strong class="ih hj"/>如果你想开始你的数据科学家职业生涯，并使用 Python <strong class="ih hj">学习当今业界所需的所有基础知识，那么本基础课程<strong class="ih hj"> </strong> <a class="ae je" href="https://datacamp.pxf.io/x9N1Ay" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">数据科学家与 python </strong> </a> <strong class="ih hj"> </strong>是最佳选择。</strong> <em class="jd"> 6)最近</em><a class="ae je" href="https://datacamp.pxf.io/x9N1Ay" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">data camp</strong></a><em class="jd">开始了一项新的计划，他们在课程中提供各种真实世界的</em> <a class="ae je" href="https://datacamp.pxf.io/x9N1Ay" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">项目</em> </strong> </a> <em class="jd">和问题陈述，帮助数据爱好者建立强大的实用数据科学基础。所以试试这些</em> <a class="ae je" href="https://datacamp.pxf.io/x9N1Ay" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">项目中的任何一个</strong> </a> <em class="jd">出来。这肯定非常令人兴奋，会帮助你学得更快更好。最近我完成了一个关于</em> <a class="ae je" href="https://datacamp.pxf.io/MXNran" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">的项目，探索 Linux </strong> </a> <strong class="ih hj"> </strong>的进化，这是一次令人惊异的经历<strong class="ih hj">。</strong></p><p id="f7d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">R 用户，不要担心，我也有一些精心挑选的最佳 R 课程供你开始构建数据科学和机器学习基础，并同时使用这个令人惊叹的 <a class="ae je" href="https://datacamp.pxf.io/QO6kV6" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">数据科学和 R </strong> </a> <strong class="ih hj"> </strong>课程，它将教你完整的基础知识。相信我，这个值得你花费时间和精力。<em class="jd"> 8)这个课程也是理解 R 中机器学习基础知识最好的课程之一，叫做</em> <a class="ae je" href="https://datacamp.pxf.io/QO6kV6" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">机器学习工具箱<em class="jd">。</em> </strong> </a></p><p id="c75c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> 9) </em> <em class="jd">所有的数据科学项目都是从探索数据开始的，对数据科学家来说最重要的任务之一就是彻底了解数据集，所以这个关于</em> <a class="ae je" href="https://datacamp.pxf.io/0Jd40N" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">探索性数据分析的可爱课程使用 R </em> </strong> </a> <strong class="ih hj"> <em class="jd">是</em> </strong> <em class="jd">启动任何数据分析和数据科学项目</em> <strong class="ih hj"> <em class="jd">。</em> </strong> <em class="jd">同样，本课程对 R</em><strong class="ih hj"><em class="jd"/></strong><a class="ae je" href="https://datacamp.pxf.io/P0651R" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="jd">中的统计建模</em></strong></a><strong class="ih hj"><em class="jd"/></strong><em class="jd">会对所有像我一样有抱负的数据科学家有用。统计学是数据科学的基础。</em></p><p id="9f83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> 10) </em> <a class="ae je" href="https://datacamp.pxf.io/a1W0NW" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">机器学习任务的数据预处理</em></strong></a><strong class="ih hj"><em class="jd"/></strong><em class="jd">是</em> <strong class="ih hj"> <em class="jd"> </em> </strong> <em class="jd">针对深度学习和 ML 爱好者的另一个方便的课程，因为它是您为任何数据科学项目执行的最重要和首要的任务之一。</em></p><p id="4f66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我仍然在使用<a class="ae je" href="https://datacamp.pxf.io/x9NMDy" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"/></a><strong class="ih hj">数据营，并在空闲时间继续上课。实际上，我坚持认为，读者可以根据自己的兴趣尝试上述任何课程，以便开始学习机器学习和数据科学，并为其打下良好的基础。DataCamp </strong>  <strong class="ih hj">提供的这些课程最棒的一点是，他们以一种非常优雅和不同的方式解释它，平衡地关注实践和概念知识，最后总是有一个案例研究。这是我最喜欢他们的一点。这些课程确实值得你花费时间和金钱。这些课程肯定会帮助你更好地理解和实现</strong><a class="ae je" href="https://datacamp.pxf.io/3PdGkK" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"/></a><strong class="ih hj">、机器学习，并在</strong><a class="ae je" href="https://datacamp.pxf.io/x9N1Ay" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">Python</strong></a><strong class="ih hj">或</strong> <a class="ae je" href="https://datacamp.pxf.io/x9N1Ay" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> R </strong> </a> <strong class="ih hj">中实现。我他妈的肯定你会喜欢它，我是从我个人的观点和经验来说的。</strong></p><p id="25fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回到主题-</p><h1 id="c2e6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">什么是优化算法</strong></h1><p id="97d2" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">优化算法帮助我们将<strong class="ih hj"><em class="jd"/></strong><strong class="ih hj">目标</strong>函数(<em class="jd"/><strong class="ih hj"><em class="jd">误差</em> </strong> <em class="jd">函数</em> ) <strong class="ih hj"> E(x) </strong>最小化(或最大化)<strong class="ih hj">目标</strong>函数(<em class="jd">的另一个名称)它只是一个数学函数，依赖于模型的内部<strong class="ih hj">可学习的</strong> <strong class="ih hj">参数</strong>，这些参数用于计算目标值(例如，我们将神经网络的<strong class="ih hj">权值(W) </strong>和<strong class="ih hj">偏差(b) </strong>值称为其内部可学习的<em class="jd">参数</em>，这些参数用于计算输出值，并通过网络的训练过程朝着最优解的方向学习和更新，即最小化<strong class="ih hj">损失</strong>，并且在<strong class="ih hj"> <em class="jd">训练</em> </strong>中也起主要作用</em></p><p id="043e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型的内部参数对于高效、有效地训练模型并产生准确的结果起着非常重要的作用。这就是为什么我们使用各种优化策略和算法来更新和计算这种模型参数的适当和最佳值，这些参数影响我们的模型的学习过程和模型的输出。</em> </strong> <strong class="ih hj">优化算法类型</strong>？</p><p id="d4f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化算法分为两大类</p><p id="1207" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<strong class="ih hj">一阶优化算法</strong> —这些算法使用损失函数<strong class="ih hj"> E(x) </strong>相对于参数的<strong class="ih hj"><em class="jd"/></strong>梯度值来最小化或最大化该损失函数。最广泛使用的一阶优化算法是<strong class="ih hj">梯度下降法。</strong>一阶导数告诉我们函数在某一点是减少还是增加。一阶导数基本上给我们一条<strong class="ih hj">线</strong>，它与误差面上的一点<strong class="ih hj"><em class="jd"/></strong><em class="jd">相切。</em></p><p id="81cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">什么是函数的梯度？</em></strong><strong class="ih hj">梯度</strong>就是一个向量，它是一个<strong class="ih hj"> <em class="jd">导数</em> </strong> ( <strong class="ih hj"> dy/dx </strong>)的多变量推广，它是<em class="jd"/><strong class="ih hj"><em class="jd">y 相对于 x 的瞬时变化率</em> </strong> <em class="jd">不同的是，要计算一个依赖于一个以上变量或多个变量的函数的导数，一个</em> <strong class="ih hj">并且使用偏导数计算梯度。</strong><em class="jd"/><strong class="ih hj"><em class="jd"/></strong><em class="jd">和一个</em><strong class="ih hj"><em class="jd"/></strong><em class="jd">的另一个主要区别是，一个</em> <strong class="ih hj"> <em class="jd">渐变</em> </strong> <em class="jd">的函数产生一个</em> <strong class="ih hj"> <em class="jd">矢量场</em> </strong> <em class="jd">。</em></p><p id="575f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个<strong class="ih hj">梯度</strong>用一个<strong class="ih hj"> <em class="jd">雅可比</em> </strong>矩阵来表示——它就是一个简单的由<strong class="ih hj"> <em class="jd">一阶偏导数(梯度)</em> </strong>组成的矩阵。</p><p id="c4fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，综上所述，导数是简单地为依赖于单个变量的函数定义的，而梯度是为依赖于多个变量的函数定义的。现在我们不要再深入微积分和物理了。</p><p id="3200" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。二阶优化算法</strong> —二阶方法使用<strong class="ih hj">二阶导数</strong>，也称为<strong class="ih hj">海森</strong>来最小化或最大化</p><p id="6272" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">损失</strong>功能。<strong class="ih hj"> Hessian </strong>是<strong class="ih hj"> <em class="jd">二阶偏导数</em> </strong>的矩阵。<strong class="ih hj"> <em class="jd">由于二阶导数计算成本高，所以二阶用得不多</em> </strong>。二阶导数告诉我们<strong class="ih hj"> <em class="jd">一阶导数</em> </strong>是增加还是减少，这暗示了函数的曲率。二阶导数为我们提供了一个与<strong class="ih hj">误差面</strong>的曲率相接触的<strong class="ih hj">二次面</strong>。</p><h1 id="7fa3" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">二阶优化优于一阶优化的一些优势—</h1><p id="cca4" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">虽然二阶导数的查找和计算可能有点费钱，但是一个<strong class="ih hj"> <em class="jd">二阶</em> </strong>的优点</p><p id="5b3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">优化</em> </strong> <em class="jd">的技巧在于不忽视或忽略</em> <strong class="ih hj"> <em class="jd">曲面的曲率</em> </strong> <em class="jd">。其次，就步进式性能而言，它们更好。</em></p><p id="e0a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里搜索更多关于二阶优化算法的信息【https://web.stanford.edu/class/msande311/lecture13.pdf T42</p><h1 id="9d2b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">那么使用哪种订单优化策略呢？</strong></h1><p id="359c" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">1.现在<strong class="ih hj">一阶优化</strong>技术易于计算，耗时更少，在大型数据集上收敛相当快。</p><p id="7e8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">二阶技术</strong>只有在<strong class="ih hj">二阶导数</strong>已知的情况下才更快，否则，这些方法在计算时间和内存方面总是较慢且成本较高。</p><p id="ff42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">虽然有时</em> <strong class="ih hj"> <em class="jd">牛顿二阶优化</em> </strong> <em class="jd">技术有时能胜过</em> <strong class="ih hj"> <em class="jd">一阶梯度下降</em> </strong> <em class="jd">技术因为二阶技术不会卡在围绕</em> <strong class="ih hj"> <em class="jd">鞍点</em> </strong> <em class="jd">的缓慢收敛的路径上而</em> <strong class="ih hj"> <em class="jd">梯度下降</em></strong></p><p id="5d2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想知道哪个收敛快，最好的方法就是自己去尝试。</p><h1 id="f499" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">现在，神经网络中使用的优化算法有哪些不同类型？</strong></h1><h1 id="4bc6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">梯度下降</h1><p id="fb75" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">梯度下降</em> </strong>是最重要的技术，也是我们如何训练和优化<strong class="ih hj"> <em class="jd">智能系统的基础。</em> </strong>什么是<strong class="ih hj"><em class="jd">——</em></strong></p><blockquote class="kr ks kt"><p id="37be" class="if ig jd ih b ii ij ik il im in io ip ku ir is it kv iv iw ix kw iz ja jb jc hb bi translated">“哦，梯度下降—找到最小值，控制方差，然后更新模型参数，最终引导我们收敛”</p></blockquote><p id="8472" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">θ=θ-η</strong>⋅∇<strong class="ih hj">j(θ)</strong>—是参数更新的公式，其中'<strong class="ih hj"> η' </strong>是学习率,'∇ <strong class="ih hj"> J(θ)' </strong>是<strong class="ih hj"> <em class="jd">的<strong class="ih hj">梯度</strong>损失函数</em>(</strong>θ<strong class="ih hj">)<em class="jd"/></strong>w . r . t<em class="jd">参数-【θ</em></p><p id="7b23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是用于优化神经网络的最流行的优化算法。现在梯度下降主要用于在神经网络模型中进行<strong class="ih hj">权重更新</strong>，即在一个方向上更新和调整模型的参数，以便我们可以最小化<strong class="ih hj">损失函数</strong>。现在，我们都知道神经网络通过一种著名的技术训练，称为<strong class="ih hj">反向传播，</strong>其中，我们首先向前传播计算输入信号及其相应权重的点积，然后将<strong class="ih hj"> <em class="jd">激活函数</em> </strong>应用于这些乘积的总和，这将输入信号转换为输出信号，并且对复杂的非线性函数建模也很重要，并且将<strong class="ih hj">非线性</strong>引入到模型中，使模型能够学习几乎任何<em class="jd">任意函数映射。</em>之后，我们在网络中向后传播<strong class="ih hj"/></p><p id="17de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">携带<strong class="ih hj">误差</strong>项并使用<em class="jd">梯度下降更新<strong class="ih hj">权重</strong>值，其中我们计算</em> <strong class="ih hj"> <em class="jd">误差(E)函数</em> </strong> <em class="jd">相对于</em> <strong class="ih hj"> <em class="jd">权重</em></strong><em class="jd">(</em><strong class="ih hj"><em class="jd">W</em></strong><em class="jd">)或参数的梯度</em></p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es kx"><img src="../Images/5b28aa0fe813e6da80242590a880bdd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSaMSrcdBk49b63GF0aFkQ.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">权重在梯度的相反方向更新-Source-Google.com</figcaption></figure><p id="fe65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">上图显示了在误差 w.r.t 的梯度向量与网络权重的相反方向上的权重更新过程。</em> <strong class="ih hj"> <em class="jd"> U 形</em> </strong> <em class="jd">曲线就是坡度(斜率)。可以注意到，如果权重</em> <strong class="ih hj"> <em class="jd"> (W) </em> </strong> <em class="jd">值太小或太大，那么我们会有大的误差，所以想要更新和优化权重，使得它既不太小也不太大，所以我们与梯度相反地向下下降，直到我们找到局部最小值。</em></p><h1 id="78f1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">梯度下降的变体-</h1><p id="6faa" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">传统的<em class="jd">批量梯度下降</em>将计算整个数据集的梯度，但是<em class="jd">将仅执行</em> <strong class="ih hj"> <em class="jd">一次更新，因此对于非常非常大并且不适合存储器</em> </strong> <em class="jd">的数据集来说，它可能非常慢并且难以控制。</em>更新的大小由学习率决定- <strong class="ih hj"> η、</strong>和<strong class="ih hj"> </strong>保证对于凸误差曲面收敛到<strong class="ih hj"> <em class="jd">全局最小值</em> </strong> <em class="jd"> </em>，对于非凸误差曲面收敛到<strong class="ih hj"><em class="jd"/></strong>。使用标准批量梯度下降的另一个好处是，它可以计算大型数据集的冗余更新。</p><p id="213c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准梯度下降的上述问题在随机梯度下降中得到纠正。</p><h1 id="fd13" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">1.随机梯度下降</h1><p id="5ab3" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">另一方面，随机梯度下降(SGD)为每个训练示例执行<strong class="ih hj">的参数更新。这通常是一种更快的技术。它一次执行一次更新。</strong></p><p id="06d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">θ=θη</strong>⋅∇<strong class="ih hj">j(θ；x(一)；y(i))，其中{x(i)，y(i)}为训练示例</strong>。<em class="jd">现在由于这些</em> <strong class="ih hj"> <em class="jd">频繁更新</em> </strong> <em class="jd">，参数更新有</em> <strong class="ih hj"> <em class="jd">高方差</em> </strong> <em class="jd">并导致</em> <strong class="ih hj"> <em class="jd">损失函数波动到不同强度</em> </strong> <em class="jd">。这实际上是一件好事，因为它帮助我们</em> <strong class="ih hj"> <em class="jd">发现新的和可能更好的局部最小值</em> </strong> <em class="jd">，而标准梯度下降只会收敛到如上所述的盆地的最小值。</em></p><p id="1719" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是 SGD 的问题是，由于频繁的更新和波动，它最终会使收敛复杂化到精确的最小值，并且会由于频繁的波动而保持超调。T75】</p><p id="3834" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管已经表明，当我们缓慢降低学习速率- <strong class="ih hj"> η </strong>时，SGD 显示出与标准梯度下降相同的收敛模式。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es ln"><img src="../Images/03a995582268508626c9f8fde593172e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4OTAd4-hHB8jbFp-3oF17w.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">每个训练示例的高方差参数更新导致损失函数剧烈波动，由于这种原因，我们可能无法获得给出最小损失值的参数的最小值。</figcaption></figure><p id="bbd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">高方差参数更新和不稳定收敛的问题可以在另一种称为<em class="jd">小批量梯度下降</em>的变体中得到纠正。</strong></p><h1 id="8564" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.小批量梯度下降</h1><p id="fcbe" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">为了避免 SGD 和标准梯度下降的所有问题和缺点，一种改进是使用<strong class="ih hj">小型批量梯度下降</strong>，因为它采用了两种技术的最佳技术，并在每批中使用 n 个训练样本对每批进行更新。</p><h1 id="2518" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用小批量梯度下降的优点是—</h1><p id="a02b" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">1.它减少了参数更新中的方差，这最终可以使我们更好、更稳定地收敛。</p><p id="7ab2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.可以利用最先进的深度学习库中常见的高度优化的矩阵优化，使计算梯度 w.r.t. a 小批量非常高效。</p><p id="b75b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.通常小批量的大小范围从 50 到 256，但可以根据应用和要解决的问题而变化。</p><p id="30f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.如今，当训练神经网络时，小批量梯度下降是典型的算法选择</p><p id="da7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">P . S——实际上，当使用小批量梯度下降时，也使用术语 SGD。</strong></p><h1 id="3b3e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用梯度下降及其变体时面临的挑战—</h1><p id="07a5" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">1.选择一个合适的学习速度可能很困难。学习率过小会导致收敛<em class="jd">缓慢，即</em>会导致<strong class="ih hj">过小</strong>逐步找到最佳参数值，使损失最小化，并找到直接影响整体训练时间的谷值，该谷值会变得过大。而过大的学习率会阻碍收敛，并导致损失函数在最小值附近波动，甚至发散。</p><p id="1275" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.此外，相同的学习率适用于所有参数更新。如果我们的数据很稀疏，并且我们的要素具有非常不同的频率，我们可能不希望将所有要素更新到相同的程度，而是对很少出现的要素执行更大的更新。</p><p id="7d7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.最小化神经网络常见的高度非凸误差函数的另一个关键挑战是避免陷入其众多的<strong class="ih hj"> <em class="jd">次优局部最小值</em> </strong>。实际上，困难实际上不是来自局部极小值，而是来自<strong class="ih hj"> <em class="jd">鞍点</em> </strong>，即<em class="jd">点，其中一个维度向上倾斜，另一个维度向下倾斜</em>。这些鞍点通常被相同误差的平台所包围，这使得</p><p id="4c20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SGD 逃逸，因为梯度在所有维度上都接近于零。</p><h1 id="da43" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">优化梯度下降</h1><p id="9e95" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">现在我们将讨论用于进一步优化梯度下降的各种算法。</p><h1 id="2a9b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">动力</h1><p id="f6ce" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated"><em class="jd">SGD 中的高方差振荡使得难以达到收敛</em>，因此发明了一种称为<strong class="ih hj"> <em class="jd">动量</em> </strong>的技术，其中<strong class="ih hj"> <em class="jd">通过沿着相关方向导航来加速</em> </strong> <strong class="ih hj"> SGD </strong>并软化不相关的振荡</p><p id="2a2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">方向。换句话说，它所做的只是将过去步骤的更新向量的一部分'<strong class="ih hj"> γ </strong>'加到当前更新向量上。</p><p id="e2d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">V(t)=γV(t1)+η</strong></p><p id="0c6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> J(θ) </strong>。</p><p id="2e52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们通过<strong class="ih hj">θ=θV(t)更新参数。</strong></p><p id="ff4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">动量项<strong class="ih hj"> γ </strong>通常设定为 0.9 或类似的值。<em class="jd">这里的</em> <strong class="ih hj"> <em class="jd">动量</em> </strong> <em class="jd">与经典物理学中的动量相同，当我们把球扔下山时，它聚集动量，其速度不断增加。</em></p><p id="3a85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样的事情也发生在我们的参数更新上—</p><p id="520f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.它导致更快和更稳定的收敛。</p><p id="aff4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.减少振荡</p><p id="1c14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于梯度指向相同方向的维度，<strong class="ih hj">动量</strong>项<strong class="ih hj"> γ </strong>增加，对于梯度改变方向的维度，<em class="jd">减少更新</em>。<em class="jd">这意味着它仅对相关示例进行参数更新。这减少了不必要的参数更新，从而导致更快和稳定的收敛，并减少振荡</em>。</p><h1 id="5c31" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">内斯特罗夫加速梯度</h1><p id="e5aa" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">一位名叫尤里·内斯特罗夫的研究员发现了动量的问题</p><p id="c7af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，一个滚下山坡的球，盲目地顺着斜坡，是极不令人满意的。我们希望有一个更聪明的球，一个知道自己要去哪里的球，这样它就知道在山坡再次向上倾斜之前减速。</p><p id="71aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际发生的情况是，当我们到达最小值，即曲线上的最低点时，<strong class="ih hj">动量</strong>相当高，并且它不知道在那个点上<em class="jd"> </em> <strong class="ih hj"> <em class="jd">缓慢</em> </strong> <em class="jd">下降</em>，因为高动量<em class="jd">可能导致它完全错过最小值并继续向上移动。这个问题被</em>内斯特罗夫<strong class="ih hj">尤里注意到了。</strong></p><p id="d7f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他在 1983 年发表了一篇研究论文，解决了这个动量问题，我们现在称这个策略为<strong class="ih hj"> <em class="jd">内斯特罗夫加速梯度。</em>T55】</strong></p><p id="1d82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在该方法中，他建议我们首先在先前动量的基础上进行一次大的跳跃，然后计算梯度，然后进行修正，导致参数更新。现在，这种预期更新防止我们走得太快，不会错过最小值，并使它对变化更敏感。T3】</p><p id="587b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">【内斯特罗夫加速渐变(NAG) </em> </strong>是一种给我们动量术语的这种先见之明。我们知道，我们将使用动量项<strong class="ih hj">γV(t1)</strong>来移动参数<strong class="ih hj"> θ </strong>。因此，计算<strong class="ih hj">θγV(t1)</strong>为我们提供了参数 <em class="jd">的下一个位置的<em class="jd">近似值，这为我们提供了参数将达到</em>的</em> <em class="jd">粗略概念。<strong class="ih hj">我们现在可以有效地向前看，不是通过计算相对于我们当前参数θ的梯度，而是相对于我们参数的近似未来位置的梯度:</strong></em></p><p id="df6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">V(t)=γV(t1)+η</strong></p><p id="3084" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">J(θγV(t 1))</strong>然后使用<strong class="ih hj">θ=θV(t)更新参数。</strong></p><p id="8da1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">可以参考更多关于</em> <strong class="ih hj"> <em class="jd">的唠叨</em> </strong>这里<a class="ae je" href="http://cs231n.github.io/neuralnetworks-3/." rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/neuralnetworks-3/.</a></p><p id="35b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们能够使我们的更新适应我们的误差函数的斜率，并依次加速 SGD，我们<em class="jd">还想使我们的更新适应每个单独的参数，以根据它们的重要性执行更大或更小的更新。</em></p><h1 id="f4e5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">阿达格拉德</h1><p id="8ba4" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">它只是允许学习率- <strong class="ih hj"> η </strong>根据参数来<strong class="ih hj">调整</strong>。所以它对不频繁的参数进行大的更新，对频繁的参数进行小的更新。因此，它非常适合处理稀疏数据。</p><p id="3975" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">它对每个参数</em> </strong> <em class="jd"> </em> <strong class="ih hj"> <em class="jd"> θ使用不同的学习率，时间步长基于为该参数计算的过去梯度。</em>T51】</strong></p><p id="abdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之前，我们一次对所有参数<strong class="ih hj"> θ </strong>进行更新，因为每个参数<strong class="ih hj"> θ(i) </strong>使用相同的学习速率<strong class="ih hj"> η </strong>。由于<strong class="ih hj"> <em class="jd"> Adagrad </em> </strong>在每个时间步<strong class="ih hj"> t </strong>对每个参数<strong class="ih hj"> θ(i) </strong>使用不同的学习率，我们首先显示 Adagrad 的每个参数更新，然后我们将其矢量化。为简洁起见，我们将<strong class="ih hj"> g(t，i) </strong>设为损失函数  w.r.t 的<strong class="ih hj"> <em class="jd">梯度。</em></strong></p><p id="c08d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到时间步<strong class="ih hj"> t . </strong>的参数<strong class="ih hj"> θ(i) </strong></p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es lo"><img src="../Images/5651275e448cd136b6590f867a370bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*smGqiXNDqdTTdojrWXNdFg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">参数更新的公式(来源:谷歌)</figcaption></figure><p id="c52f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd"> Adagrad </em>基于已经为θ(i)计算的过去梯度，在每个时间步长 t 为每个参数θ(i)修改总学习率η。</strong></p><p id="2570" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"/><strong class="ih hj"><em class="jd">Adagrad</em></strong><em class="jd">的主要好处是我们不需要手动调学习速率。</em> <em class="jd">大多数实现使用默认值 0.01，并保持不变。</em></p><h1 id="aae3" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">劣势—</h1><p id="6459" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">1.它的主要弱点是它的学习率- <strong class="ih hj"> η </strong>总是在递减和衰减。</p><p id="eaec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是由于分母中每个平方梯度的累积，因为每个增加的项都是正的。积累的总和在训练中不断增长。这进而导致<em class="jd">学习率收缩，最终变得如此之小，</em> <strong class="ih hj">以至于模型完全停止学习，停止获取新的额外知识。因为我们知道，随着学习速率变得越来越小，模型快速学习的能力下降，这使得收敛非常慢，并且需要很长时间来训练和学习，即学习速度下降。</strong>这个<strong class="ih hj">学习率</strong>衰减的问题在另一个叫做<strong class="ih hj"> AdaDelta 的算法中得到纠正。</strong></p><h1 id="ea57" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">阿达德尔塔</h1><p id="14cf" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">它是 AdaGrad 的扩展，旨在消除 AdaGrad 的学习率衰减问题。<strong class="ih hj"> <em class="jd"> Adadelta </em> </strong>将累积的过去梯度的窗口限制为某个固定大小<strong class="ih hj"> w </strong>，而不是累积所有先前平方的梯度。</p><p id="c424" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不是低效地存储<strong class="ih hj"> w </strong>先前的平方梯度，梯度的和被递归地定义为所有过去的平方梯度的<em class="jd">衰减</em> <strong class="ih hj"> <em class="jd">平均值</em> </strong>。时间步长<strong class="ih hj"> t </strong>处的移动平均值<strong class="ih hj">E【g】(t)</strong>然后仅取决于(作为分数<strong class="ih hj"> γ </strong>类似于动量项)<em class="jd">先前的平均值和当前的梯度。</em></p><p id="453d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> E[g ](t)=γ。e[g](t1)+(1γ)。g (t)，</strong>我们将<strong class="ih hj"> γ </strong>设为与动量项相似的值，约为 0.9。</p><p id="d86d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">δθ(t)=-η</p><p id="d41c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">g(t，I)。θ(t+1)=θ(t)+δθ(t)。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lp"><img src="../Images/47837d4636c202097c0cae8d71499ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*brufR2JEH-KziQAhtxopsQ.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><strong class="bd jq">参数更新的最终公式(来源:谷歌)</strong></figcaption></figure><p id="e921" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">AdaDelta 的另一个优点是我们甚至不需要设置默认的学习速率</strong></p><h1 id="d5c4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">到目前为止，我们做了哪些改进</h1><p id="7c9f" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">1.我们正在为每个参数计算<em class="jd">不同的学习率</em>。</p><p id="6359" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.我们也在计算<em class="jd">动量</em>。</p><p id="1f06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.防止<strong class="ih hj">学习率消失(衰减)</strong>。</p><p id="0545" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还能做哪些改进？</p><p id="986e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们正在为每个参数计算单独的<strong class="ih hj"> <em class="jd">学习率</em> </strong>，为什么不为每个参数计算单独的<strong class="ih hj">动量</strong>变化并分别存储它们。这就是一种被称为<strong class="ih hj"> Adam 的新改良技术和改进发挥作用的地方。</strong></p><h1 id="0054" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><p id="0f25" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">Adam 代表<strong class="ih hj">自适应矩估计。</strong>自适应矩估计(Adam)是另一种计算每个参数的自适应学习率的方法。除了存储像<strong class="ih hj"> AdaDelta </strong>、<strong class="ih hj"> <em class="jd"> Adam </em> </strong> <em class="jd">这样的过去平方梯度的指数衰减平均值之外，还保存过去梯度 M</em><strong class="ih hj"><em class="jd">(t)</em></strong><em class="jd">的指数衰减平均值，类似于动量</em>:</p><p id="6846" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> M(t)和 V(t) </strong>分别是<em class="jd">梯度</em>的<strong class="ih hj"> <em class="jd">均值</em> </strong>和<strong class="ih hj"> <em class="jd">无中心方差</em> </strong>的一阶矩值。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es lq"><img src="../Images/db402b7eabfe302e164ca125352467da.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*W3NMTJao5xADBmbq5yKqwg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><strong class="bd jq">梯度的一阶矩(均值)和二阶矩(方差)的公式</strong></figcaption></figure><p id="603f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么参数更新的最终公式是:</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es lr"><img src="../Images/7e234808df79ad71b78ca604d0cd5088.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*ZxigpkZ7j0K0Xa2dN5v5jA.png"/></div></figure><p id="41a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">β1 的值是 0.9，β2 的值是 0.999，而<strong class="ih hj"> ϵ </strong>的值是(10 x exp(-8))。<strong class="ih hj"/></p><h1 id="99b2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="ae54" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">我们应该使用哪种优化器？</p><p id="69b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">问题是为我们的神经网络模型选择最佳的优化器，以便快速收敛并适当地学习和调整内部参数，从而最小化损失函数。</p><p id="8d09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">亚当</strong> <em class="jd">在实践中表现良好，优于其他自适应技术。</em></p><p id="73fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你的输入数据是稀疏的，那么像<strong class="ih hj"> SGD、NAG 和 momentum </strong>这样的方法是次等的，并且表现很差。<strong class="ih hj">对于稀疏数据集，应使用<em class="jd">自适应学习率</em>方法</strong>中的一种。一个额外的好处是，我们不需要调整学习率，但可能会达到默认值的最佳结果。</p><p id="3e15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果想要快速收敛并训练深度神经网络模型或高度复杂的神经网络，那么应该使用<strong class="ih hj"> Adam 或任何其他自适应学习速率技术</strong>，因为它们优于所有其他优化算法。</p><p id="0769" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望你们喜欢这篇文章，并且能够对不同优化算法的不同行为有一个很好的直觉。</p><h1 id="d4a5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">参考文献—</h1><p id="e3ce" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">1.优化梯度下降-【http://sebastianruder.com/optimizing-gradient-descent/ T2】</p><p id="c5a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.Dean，j .，Corrado，G. S .，Monga，r .，Chen k .，Devin，m .，Le，q .</p><p id="9086" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">… Ng，A. Y. (2012)。大规模分布式深度网络。</p><p id="e8f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NIPS 2012:神经信息处理</p><p id="6cf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">系统。【http://doi.org/10.1109/ICDAR.2011.95 T4】</p><p id="4540" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.约夫，s .，&amp;塞格迪，C. (2015)。批量标准化:通过减少内部协变量转移加速深度网络训练。arXiv 预印本 arXiv:1502.03167v3。</p><p id="c9c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.钱嫩(1999)。梯度下降学习算法中的动量项。神经网络:美国神经科学学会官方杂志</p><p id="89f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">国际神经网络学会，12(1)，145-</p><p id="cadb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">151.<a class="ae je" href="http://doi.org/10.1016/S0893-6080(98)00116-6" rel="noopener ugc nofollow" target="_blank">http://doi . org/10.1016/s 0893-6080(98)00116-6</a></p><p id="6abb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.金马博士，&amp;巴，法学博士(2015)。亚当:一种方法</p><p id="d29d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机优化。学习表征国际会议</p><p id="9117" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.Zaremba，w .，&amp; Sutskever，I. (2014 年)。学习执行，1–</p><p id="19c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">25.从 http://arxiv.org/abs/1410.4615<a class="ae je" href="http://arxiv.org/abs/1410.4615" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="f072" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.张，s，乔洛曼斯卡，a，&amp;乐村，y</p><p id="4f4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(2015).弹性平均 SGD 深度学习。神经的</p><p id="3a23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息处理系统会议</p><p id="8fda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2015).从 http://arxiv.org/abs/1412.6651<a class="ae je" href="http://arxiv.org/abs/1412.6651" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="4f1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8.达肯，c，常，j .，&amp;穆迪，J. (1992)。快速随机梯度搜索的学习速率表。信号处理的神经网络 II 1992 年 IEEE 会议录，</p><p id="6251" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(九月)。<a class="ae je" href="http://doi.org/10.1109/NNSP.1992.253713" rel="noopener ugc nofollow" target="_blank">http://doi.org/10.1109/NNSP.1992.253713</a></p></div></div>    
</body>
</html>