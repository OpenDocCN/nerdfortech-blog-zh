<html>
<head>
<title>Reinforcement Learning: Deep Q-Learning with Atari games</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:用雅达利游戏进行深度Q学习</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/reinforcement-learning-deep-q-learning-with-atari-games-63f5242440b1?source=collection_archive---------2-----------------------#2021-07-08">https://medium.com/nerd-for-tech/reinforcement-learning-deep-q-learning-with-atari-games-63f5242440b1?source=collection_archive---------2-----------------------#2021-07-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f1b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我之前的帖子<a class="ae jd" rel="noopener" href="/nerd-for-tech/first-look-at-reinforcement-learning-67688f36413d">中，我尝试使用深度Q学习来解决侧翻问题。在这篇文章中，我将进一步探索深度Q学习，但在雅达利游戏的背景下。</a></p><p id="b43b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2013年，论文由Deepmind团队<a class="ae jd" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank">用深度强化学习</a>玩雅达利(Mnih et。al)探索了在雅达利游戏上使用深度Q学习的概念。目标是提出一个深度学习模型，该模型可以处理高维输入，并在不同的Atari游戏中实现超人的基准，而无需调整模型的底层架构。读完这篇论文后，我想尝试并实现这样一个模型，并希望能够训练一个代理来玩一些经典的Atari游戏，如Breakout或Pong。我还将实施发表在英国《自然》杂志上的后续论文<a class="ae jd" href="http://files.davidqiu.com//research/nature14236.pdf" rel="noopener ugc nofollow" target="_blank">通过深度强化学习进行人类水平的控制</a>中的想法。此外，我还将谈到一个我们可以添加的修改，称为双DQN。</p><p id="cc30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我需要为这个问题选择一个环境。当我第一次开始这个项目时，我想尝试解决网球这个游戏。然而，在对我的模型进行了几个小时的训练后，结果并不比随机游戏好多少。看看我的训练参数，我只训练了我的代理人大约50集，其中每集最多需要200帧才能完成。所以总共大约10，000帧。看看Deepmind的论文，他们已经训练了他们的代理超过1000万帧。虽然我的10，000帧需要大约1.5小时来运行，但是1，000万帧需要2个月！最后，我决定选择Pong，这是一种比较容易学的游戏。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/4ec445f5b06d2168f76d962d80ee162a.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/0*7CZhntvUjDQ4iVGd.gif"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">一个机器人(左)对抗一个RL代理(右)</figcaption></figure></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><h1 id="347d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">环境设置</strong></h1><p id="9cd2" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">对于这个实验，我将使用OpenAI的健身房库和预构建的环境。注意，目前OpenAI的atari-py包中唯一的环境是俄罗斯方块，所以你必须从其他地方导入rom。我将在文章的最后把这个文件包含在我的代码中。这里是环境:</p><pre class="jf jg jh ji fd la lb lc ld aw le bi"><span id="dd5e" class="lf jy hi lb b fi lg lh l li lj">#initial environment<br/>env = gym.make('PongNoFrameskip-v4')</span><span id="b8f1" class="lf jy hi lb b fi lk lh l li lj">#Atari preprocessing wrapper<br/>env = gym.wrappers.AtariPreprocessing(env, noop_max=30, frame_skip=4, screen_size=84, terminal_on_life_loss=False, grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)</span><span id="8e8e" class="lf jy hi lb b fi lk lh l li lj">#Frame stacking<br/>env = gym.wrappers.FrameStack(env, 4)</span></pre><p id="457d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第一行中，我初始化了环境‘PongNoFrameskip-v4’。gym采用的格式是['Game]['NoFrameskip '，' Deterministic '，None]['-v0 '，'-v4']的串接字符串。以下是对每个术语的快速解释:</p><ul class=""><li id="c962" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">“无帧跳过”:环境的每一步都是一帧</li><li id="3060" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">“确定性”:每一步对k帧执行相同的动作，并返回第k帧。k = 4。</li><li id="3c6e" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">无:与“确定性”相同，但k是从[2，5]中采样的。</li><li id="df1b" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">“-v0”:每一步都有0.25的概率重复前面的动作</li><li id="06d8" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">“v4”:每一步都将遵循您发布的操作</li></ul><p id="a701" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第二行中，我应用了一些在《自然》杂志论文中详述的预处理。Atari包装器遵循Machado等人(2018)“重温街机学习环境:通用代理的评估协议和开放问题”中的指南。每个参数的快速解释:</p><ul class=""><li id="fbd1" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">NoopReset:通过在复位时随机选择no-ops(无动作)来获得初始状态。</li><li id="611f" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">缺省情况下跳帧:4</li><li id="9457" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">Max-pooling:采用最近观察的最大像素值来消除一些Atari游戏中的闪烁效果。</li><li id="2b54" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">失去生命时的终止信号:默认关闭。Machado等人(2018)不推荐。</li><li id="bbe8" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">默认情况下，调整为正方形图像的大小:84x84</li><li id="052b" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">灰度观察:将观察转换为灰度</li><li id="e746" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">缩放观察值:将观察值缩放到[0，1]</li></ul><p id="7616" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我们不会缩放观察值，并将值类型保持为uint8，以获得更有效的内存存储，并且只会在将它输入到模型中之前进行缩放。</p><p id="eb98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三行将每k帧堆叠成一次观察。这将使模型了解一个动作是如何进行的，而不仅仅是输入是一个静止的帧。请注意，这并不意味着我们将每k帧分组在一起，每帧的观察值都是堆叠的，因此它们会重叠。我们的观察维度应该是(4，84，84)，其中输入是大小为84x84的图像，并与最后4帧堆叠在一起。关于预处理后的观察，值得注意的一件有趣的事情是，《自然》杂志选择保留游戏中的分数，而不是将其剔除。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lz"><img src="../Images/9ec2efbe6bf053f79182a90f3c75a7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*ALngFl2kDNAYIP5lmUzICA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">分数用黑色方框标出</figcaption></figure><h1 id="8f1b" class="jx jy hi bd jz ka ma kc kd ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku bi translated"><strong class="ak">实现深度Q学习</strong></h1><p id="3436" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">我们将使用epsilon-greedy算法来选择最佳动作，其中有epsilon机会从动作空间中随机抽取一个动作。根据Deepmind的规范，我们将使用线性退火在一百万帧内将ε从1减少到0.1，而不是使用ε衰减。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mf"><img src="../Images/f4e5f5bf696b031c19940d505ae3a112.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*oOnAlIDHVPRaEb5E1AA2xg.png"/></div></figure><p id="b48e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似于我在以前的帖子中描述的基线深度Q学习算法，我们将使用神经网络来学习特定状态的Q值，而不是查找Q表。这是因为对于更复杂的环境，例如Atari游戏，存储所有可能状态所需的存储器效率太低，并且Q值收敛需要长得多的时间。我们将使用下面的等式来获得目标Q值，并让网络更新其权重。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mg"><img src="../Images/e421b9ed7895f05b676b07b132f6dd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W1XNDmp5q8itrR1J.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">给定状态和动作的目标q值</figcaption></figure><p id="2057" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们处理作为输入的图像时，网络将是一个卷积神经网络。正如《自然》杂志论文中所详述的，</p><blockquote class="ml mm mn"><p id="78a9" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">神经网络的输入由预处理图w产生的84 × 84 × 4图像组成。第一个隐藏层将32个步长为4的8×8滤波器与输入图像进行卷积，并应用整流器非线性。第二个隐藏层将64个4×4的滤波器与步长2进行卷积，之后是整流器非线性。其后是第三个卷积层，它将64个3×3的滤波器进行卷积，步长为1，其后是一个整流器。最后一个隐藏层是全连接的，由512个整流单元组成。输出图层是一个完全连接的线性图层，每个有效动作都有一个输出。</p></blockquote><p id="9c22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在Keras中的实现如下所示:</p><pre class="jf jg jh ji fd la lb lc ld aw le bi"><span id="b06d" class="lf jy hi lb b fi lg lh l li lj">model = Sequential(<br/>    [<br/>        Lambda(lambda tensor: tf.transpose(tensor, [0, 2, 3, 1]), output_shape=(84, 84, 4), input_shape=(4, 84, 84)),<br/>        Conv2D(32, kernel_size=(8, 8), strides=4, activation="relu", input_shape=(4, 84, 84)),<br/>        Conv2D(64, kernel_size=(4, 4), strides=2, activation="relu"),<br/>        Flatten(),<br/>        Dense(512, activation="relu"),<br/>        Dense(env.action_space.n, activation="linear"),<br/>    ]<br/>)</span><span id="2a54" class="lf jy hi lb b fi lk lh l li lj">rms = tf.keras.optimizers.RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01)<br/>model.compile(loss="mse", optimizer=rms)</span></pre><p id="67f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的实现中，我包含了一个Lambda层，它将NCWH(批处理、通道、宽度、高度)格式的输入转换为NWHC。这是因为当我用CPU训练我的模型时(无法配置GPU)，不支持NCWH格式。如果是在GPU上训练，那么可以忽略Lambda层，用(4，84，84)作为第一个卷积层的输入形状。《自然》杂志的论文中详细描述了优化器的规格。关于RMSprop的更多信息，我发现这篇文章非常有帮助。</p><p id="3353" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还将使用经验回放，并在“记忆”数组中跟踪过去的转换。该论文中描述的参数在存储器中存储多达100万个跃迁。存储器将被随机播放的50000个过渡预填充。在小批量更新期间，将从存储器中统一采样一批用于训练。该论文还提到了一个更复杂的策略，强调模型学习最多的过渡，类似于<a class="ae jd" href="https://link.springer.com/article/10.1007/BF00993104" rel="noopener ugc nofollow" target="_blank">优先清扫</a>。另一个修改在论文中有详细描述:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es ms"><img src="../Images/a8448527ae53add2d278c340408ca2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otZunpxjrrlowHEIVDrs8A.png"/></div></div></figure><p id="f207" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">修改声明在每次C更新时克隆网络，并使用克隆的网络作为目标Q值的目标网络，并训练在线网络，而不是对两个任务使用相同的网络。这是为了减少网络的不稳定性，因为每次我们更新网络时，目标也会改变。使用目标网络将确保目标不会因C更新而改变。如下所示，微分损失函数将模型参数θᵢ用于当前q值，将克隆模型参数θᵢ⁻用于目标q值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mt"><img src="../Images/fb25285881832b5cee1225e8b4c41ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cf69tmyvWLh3g9-OoRD2XA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">对损失函数求微分时的梯度</figcaption></figure><p id="9baf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该算法的摘要如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mu"><img src="../Images/17f8bf0a2a8705a4a87a03089a352a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljyyQeR1cQ-sFJERtantHw.png"/></div></div></figure><h1 id="d95d" class="jx jy hi bd jz ka ma kc kd ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku bi translated">双DQN</h1><p id="35c1" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">我们可以对该算法进行的另一种修改称为双重深度Q学习。在论文<a class="ae jd" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank">中用双Q学习进行深度强化学习</a> (Hasselt et .al 2015)，解决了高估q值的问题。尽管拥有过于乐观的价值观不一定是件坏事，但如果行动价值观不一致，它会对最终的政策产生负面影响。</p><p id="f8e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文中提出的变更是关于培训期间的目标值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mv"><img src="../Images/e5d8e58581b1052cc38349353c54c769.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*iQayJ3PkXUS8onv8-SsJXg.png"/></div></div></figure><p id="910b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们使用神经网络来训练我们的代理时，我们希望基于我们的模型认为它的Q值与我们的模型认为它的Q值之间的差异来更新我们的模型。上面的等式表明，我们的目标是奖励+贴现的未来效用，其中我们使用神经网络来预测行动值，并选择最高的一个。这里的问题是我们使用了max操作符，在这里我们使用相同的值来选择一个动作并对该动作求值。这可能导致过于乐观的价值估计，因此我们需要一种方法来将选择与评估分离。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mw"><img src="../Images/bc5924c163ceaa64f966b9259d433247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*9odKBFz1Tt2XWRz2lgW0NA.png"/></div></figure><p id="bde6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的等式是我们如何调整DQN目标等式的。我们没有使用max运算符来选择最佳操作并同时获取操作的值，而是使用argmax使用网络q和权重θₜ来选择操作，然后使用网络q和另一组权重θ′ₜ.来评估该操作本文将双q学习的概念与DQN相结合来创建一个简单的双DQN修改，其中我们可以使用目标网络作为权重θ′ₜ，使用在线网络作为权重θₜ.</p><p id="560a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们仔细看看高估是如何导致次优政策的。以文中提出的这个定理为例:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mx"><img src="../Images/39499a7f7d46c496e521c8284cfc591d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*OqE9GqCiQQabROeFwQ_HIQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">证明可以在论文中找到(查看下面的链接)</figcaption></figure><p id="df43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们具有无偏误差，使得真实最优网络v和任意网络Qₜ之间的动作值的差之和均方误差C &gt; 0时，我们可以导出一个紧下界:√(C/m-1)，其中m是动作的数目。使用双Q学习，误差的下限是0。</p><pre class="jf jg jh ji fd la lb lc ld aw le bi"><span id="1ef9" class="lf jy hi lb b fi lg lh l li lj">#action selection with online network<br/>best_future_action = np.argmax(model.predict(np.expand_dims(new_state, axis=0)))</span><span id="e214" class="lf jy hi lb b fi lk lh l li lj">#action evaluation with target network<br/>target = reward + discount_factor * target_model.predict(np.expand_dims(new_state, axis=0))[0][best_future_action]</span></pre><h1 id="9ecb" class="jx jy hi bd jz ka ma kc kd ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku bi translated">我的结果</h1><p id="f72a" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">实现算法后，我准备训练我的代理。但是，我确实严重低估了训练时间。在《自然》杂志的论文中，他们训练了代理人5000万帧，相当于38天的播放时间。至于我，我选择使用更简单的Atari游戏，它需要更短的时间来收敛，根据我看到的一些例子，大约是750，000帧。即使减少了训练时间，在我的本地CPU上进行训练也要花费几天时间，而且我遇到了内存问题。我决定用200，000帧来训练我的模型，其他超参数被缩放到训练帧的数量。</p><p id="0eac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们看看随机策略的效果。在150次游戏中，随机策略的平均分数为-20.273，分数由Cpu分数-代理分数计算得出，游戏在21分结束。正如预期的那样，与人类基准-3相比，随机策略不是很好。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es my"><img src="../Images/94413786bded04c0e1bb02edd255bfeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OXGbiKw449ABBJS8gkL-pg.png"/></div></figure><p id="e47f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练我的模型之后，我没有得到我期望的结果。分数并没有随着时间的推移而提高，最终的策略和随机的策略一样好。检查培训损失历史，看起来模型确实在减少损失。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es my"><img src="../Images/02c5296a5f6fe467ece4618eda0bc545.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*VGII_ZDOUPzQKX6gB252-A.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">训练Pong超过150集</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mz"><img src="../Images/fb751b3bd6b16634990da67c5c784532.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*wps_eTl2xBHxPIXIAnixOQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">乒乓球训练损失</figcaption></figure><p id="fa6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那为什么我的模特没有训练呢？一些关于为什么会发生这种情况的假设。首先，我缩小了所有的超参数，从1000万帧训练到20万帧。尽管一切仍按比例进行，但有一个主要问题。解决问题所需要的帧数还是一样的。当我在前20，000帧中减少ε时，学习策略仍然和随机策略一样好。</p><p id="bae7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其次，这可能是由于Pong中奖励的稀疏性。在CartPole环境中，我们奖励代理人杆子直立的每一个时间步。这种反馈允许代理快速区分好的和坏的行为。然后，我们修改了奖励，使其与极角成比例，以获得更精确的反馈。然而，DQN的情况不同。DQN最初是作为一种通用解决方案提出的，用于解决给定图像输入的所有Atari游戏环境。因此，我们不能分配更精确的奖励，Pong的奖励系统是-1或1分。这意味着代理人在每次演练中学习的奖励要少得多，并且在每次奖励之间有大量的帧，这使得代理人更难掌握一个动作的未来效用。</p><p id="b962" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在使用与上一篇文章相似的超参数的CartPole环境进行健全性检查之后，我们得到了以下结果:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es na"><img src="../Images/90e3c07c4240649a4066c75475e785db.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*Pb6etgeVdbzT-AKi9AeWDg.png"/></div></figure><p id="f9f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">唷。至少算法是可行的，代理将在一个更容易的环境中学习。将此与2013年报告中详述的基线DQN实施进行比较，我们得到了类似的结果，10集的平均得分为476。</p><p id="52a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，这是一次有趣的经历，我对DQN算法进行了更深入的研究，增加了目标网络和一个简单的双Q学习实现。仍然有可以改进的地方，比如优先清扫和决斗网络，但这是未来要探索的。尽管DQN是第一个使用图像输入解决Atari游戏的模型，但事实仍然是，DQN确实有很长的训练时间和缓慢的收敛速度。在未来，我将探索其他种类的算法，如用于连续动作的深度确定性策略梯度(DDPG)、近似策略优化、A3C等等！</p><p id="4522" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的代码:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Q%20Learning/Atari_DQN.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Q % 20 learning/Atari _ dqn . ipynb</a></p><p id="c200" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来源:</p><ul class=""><li id="204d" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><a class="ae jd" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank">用深度强化学习玩雅达利</a> (Mnih et。铝2013)</li><li id="406c" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="http://files.davidqiu.com//research/nature14236.pdf" rel="noopener ugc nofollow" target="_blank">通过深度强化学习的人类级控制</a> (Mnih et。铝2015)</li><li id="e408" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank">采用双Q学习的深度强化学习</a> (Hasselt et。铝2015)</li></ul></div></div>    
</body>
</html>