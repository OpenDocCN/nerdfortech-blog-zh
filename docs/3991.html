<html>
<head>
<title>Linear regression for 5-year-olds</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5岁儿童的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/linear-regression-for-5-year-olds-2e8d83e9a680?source=collection_archive---------12-----------------------#2021-07-02">https://medium.com/nerd-for-tech/linear-regression-for-5-year-olds-2e8d83e9a680?source=collection_archive---------12-----------------------#2021-07-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fcca" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">1/3部分:直觉</h2></div><blockquote class="ix iy iz"><p id="65f7" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">第2/3部分的链接:<a class="ae jx" href="https://mohitpatil246.medium.com/linear-regression-for-5-year-olds-1a854c88bbb5" rel="noopener">算法背后的数学</a> <br/>第3/3部分的链接:<a class="ae jx" href="https://mohitpatil246.medium.com/linear-regression-for-5-year-olds-1bdc5d0badc4" rel="noopener">从头开始用Python实现</a></p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es jy"><img src="../Images/3c2437b61ebdfcee9fa344079dbeaf76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NBDL2Iva7VKEN-zYeFY4hA.jpeg"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx translated">图片来自<a class="ae jx" href="https://www.pexels.com/photo/light-sea-city-road-8355939/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae jx" href="https://www.pexels.com/@mikhail-nilov?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Mikhail Nilov </a></figcaption></figure><p id="9267" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi kr translated">大家好！这是5岁儿童线性回归的第一部分，涵盖了线性回归的基础知识，这将带你进入第二部分，包括线性回归背后的一些推导和数学。所以…让我们开始吧！</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><p id="3bdb" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">回归就是寻找一个依赖特性和一个或多个独立特性之间的关系。现在，当我们谈论线性回归时，我们评估/找到一个从属特征和一个独立特征之间的关系，并试图找出如何绘制一条最适合我们数据的线。</p><p id="96aa" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">有多种方法可以拟合这条线，但在这里我们将看看<strong class="jd hj">普通最小二乘法(OLS) </strong>的工作原理。在这种方法中，我们试图通过<strong class="jd hj">最小二乘法</strong>的原理来估计参数<em class="jc">(系数&amp; y截距)</em>:最小化给定数据集中观察到的从属特征与由独立特征的线性函数预测的特征之间的差的平方和。</p><p id="aeff" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">在数学术语中，我们将简单的线性回归方程表示为(标量形式)</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lh"><img src="../Images/e94c3da9dc31647cf8eac783280ad298.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*SxPmx9YOneKqe_CFP_myGw.png"/></div></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es li"><img src="../Images/615a320adc4258b6c5f5fb9e5054e010.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*vbOTQM1LWHnTcBknYrf4-Q.png"/></div></figure><p id="4a9d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">简单来说，<strong class="jd hj"> B </strong>表示直线指向哪个方向，<strong class="jd hj"> a </strong>表示y轴截距，表示直线在y轴<em class="jc">上的交点(即x为零时y的值是多少)。</em></p><p id="3c0c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">这个方程也可以用矩阵形式表示为</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lj"><img src="../Images/7f51bfa97b7d8531e664874e19f87176.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*2RvaXnM6ZeAQVDLhIsOpQA.png"/></div></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lk"><img src="../Images/f35cc0d7a2385b568147daa441e4d2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNp-FgTcyF_XKZ4LV2GnSw.png"/></div></div></figure><p id="2a14" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">让我们举一个非常普通的例子-</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ll"><img src="../Images/e7873e2a0692bb1e626afcc261b11ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*8ph3WKz2po4rGpYhRGw8VA.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">作者图片</figcaption></figure><p id="9d3a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">假设我们有一家冰淇淋店，我们记录了每天的平均温度和每天售出的冰淇淋数量。我们注意到，温度和冰淇淋销售额之间存在正线性关系，也就是说，随着温度的升高，销售额也会增加。</p><p id="af6e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">我们通过计算实际值和预测值之间的平方差之和来估计参数并找到最佳拟合线，即，我们记录每个数据点的实际值和预测值之间的差异(d1、d2、d3…)，对它们求平方并相加(d1 +d2 +d3 +…)。主要目标是估计使该值最小的参数。这是线性回归的成本函数。</p><blockquote class="ix iy iz"><p id="afaf" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">成本/损失函数是衡量机器学习模型性能的东西。它告诉我们，在估计我们的独立特征和从属特征之间的关系时，我们的模型有多好。</p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lm"><img src="../Images/c83c6305e8eec6af3d137054d37966c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*U-9AyV60Pe1bSSVGIRNuUQ.png"/></div></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ln"><img src="../Images/4ce61dca81cafc73730c7b88e41d736b.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*VXrxv3yLmrtEo8Ft4LayDw.png"/></div></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="41c4" class="lo lp hi bd lq lr ls lt lu lv lw lx ly ko lz ma mb kp mc md me kq mf mg mh mi bi translated"><strong class="ak">但在使用该算法之前，需要考虑一些假设</strong>:</h2><blockquote class="ix iy iz"><p id="755e" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jd hj">注:</strong>如果违反了这些假设中的任何一个，并不意味着我们的线性回归模型就行不通，只是我们的参数可能不准确(或者我们可以说，我们的预测可能不准确)。</p></blockquote><ul class=""><li id="eebc" class="mj mk hi jd b je jf jh ji ko ml kp mm kq mn jw mo mp mq mr bi translated"><strong class="jd hj">线性- </strong>线性回归假设从属特征和独立特征之间的关系是线性的。这可以通过查看散点图来检查。请注意，异常值也会影响数据的线性，因此在分析数据之前处理异常值非常重要。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ms"><img src="../Images/c7a8103ff936a6d7270a65f6945a6a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*veuq3Y5m1BsUnrtb1ZxEhA.png"/></div></figure><p id="b813" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">通过观察散点图，我们可以说数据遵循线性分布。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mt"><img src="../Images/820227a06532a8f417a392360e84963f.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*yZTEVN2LiUFScJE49mAJOA.png"/></div></figure><p id="28a6" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">从这个散点图中，我们可以说数据不遵循线性分布。</p><blockquote class="ix iy iz"><p id="272f" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">注意:如果数据不是线性分布的，可以使用其他一些非线性估计，如多项式回归，它也可以拟合曲线数据！</p></blockquote><ul class=""><li id="04d3" class="mj mk hi jd b je jf jh ji ko ml kp mm kq mn jw mo mp mq mr bi translated"><strong class="jd hj">独立残差- </strong>拟合模型后，残差(实际值-预测值)应该是独立的，即连续的残差之间应该没有相关性。这个概念也被称为<strong class="jd hj">自相关</strong>。<em class="jc">指同一变量</em>的值之间的相关程度。我们可以借助残差图(独立变量对残差)来检测自相关。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mu"><img src="../Images/cc7eb54ee748b8a7b1c6ec34591b2ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*yqswpSF33QgZnokczB9zqQ.png"/></div></figure><p id="7dcc" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">从这个图中，我们可以说残差是独立的，并且是随机分布的。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mu"><img src="../Images/5c45b7e014ff30074ceed9b5d55ef73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*MvCAOz-bUounBuuhIPqCSw.png"/></div></figure><p id="a103" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">通过看曲线，我们可以说残差不是相互独立的，不是随机分布的。</p><blockquote class="ix iy iz"><p id="919c" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">自相关也可以使用Durbin-Watson测试来检测，其值范围为0-4。<br/>接近2的值= &gt;更少的自相关<br/>接近0或4的值= &gt;高+ve /高-ve自相关</p></blockquote><ul class=""><li id="c9e6" class="mj mk hi jd b je jf jh ji ko ml kp mm kq mn jw mo mp mq mr bi translated"><strong class="jd hj">同异方差——</strong>异方差(同异方差的反义词)发生在Y给定X的方差(V(Y|X))不是常数的时候。这些是同方差和异方差数据的残差对拟合值图</li></ul><div class="jz ka kb kc fd ab cb"><figure class="mv kd mw mx my mz na paragraph-image"><img src="../Images/81fe9a8deb5abc080841ff19ae71d9c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*v_GsH7V4CY96mgfX4SJ2rA.png"/></figure><figure class="mv kd mw mx my mz na paragraph-image"><img src="../Images/0c9a441f4a56c6dc6f25b1af4fe0af64.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*SSCzqATC-RNRLFuDdAWijg.png"/><figcaption class="kk kl et er es km kn bd b be z dx nb di nc nd translated">作者图片</figcaption></figure></div><p id="7f93" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ko jl jm jn kp jp jq jr kq jt ju jv jw hb bi translated">在第二个图中，我们可以看到残差类似于漏斗形状，这在线性回归中是不期望的。为了处理这种情况，我们可以取因变量的对数，这在这种情况下很有用。</p><ul class=""><li id="5afb" class="mj mk hi jd b je jf jh ji ko ml kp mm kq mn jw mo mp mq mr bi translated"><strong class="jd hj">正态性(可选)- </strong> OLS不要求误差项遵循正态分布来产生无偏估计。然而，如果误差项遵循正态分布，我们可以执行统计假设检验，并生成可靠的置信区间。我们可以借助Q-Q图(分位数-分位数图)来检验正态性</li></ul><div class="jz ka kb kc fd ab cb"><figure class="mv kd mw mx my mz na paragraph-image"><img src="../Images/8c7730bc8912d4760bc554aed4a8ad0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*cjeno-eDP0Zac6ykVmNL4A.png"/></figure><figure class="mv kd mw mx my mz na paragraph-image"><img src="../Images/66ba3f9dc3d5d33bdc118bdbf63ce8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*oONcghdFPjAU2lrrZ-U7ew.png"/><figcaption class="kk kl et er es km kn bd b be z dx nb di nc nd translated">作者图片</figcaption></figure></div><ul class=""><li id="bd17" class="mj mk hi jd b je jf jh ji ko ml kp mm kq mn jw mo mp mq mr bi translated"><strong class="jd hj">多重共线性- </strong>当我们处理多个独立特征时，我们必须考虑这个假设。当两个特征具有+1或-1的<strong class="jd hj">皮尔逊相关系数</strong>时，完美相关发生。当变量完全相关时，普通的最小二乘法无法区分它们。所以，这样的特征应该从模型中移除。</li></ul></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="1a8d" class="ne lp hi bd lq nf ng nh lu ni nj nk ly io nl ip mb ir nm is me iu nn iv mh no bi translated">摘要</h1><p id="6acb" class="pw-post-body-paragraph ja jb hi jd b je np ij jg jh nq im jj ko nr jm jn kp ns jq jr kq nt ju jv jw hb bi translated">总之，我们看了什么是线性回归，线性回归的数学方程，成本函数，以及线性回归的各种假设。不要忘记查看该系列的下一部分。</p></div></div>    
</body>
</html>