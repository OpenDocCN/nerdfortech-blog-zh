<html>
<head>
<title>Building a Personal AI Assistant: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建个人人工智能助手:第2部分</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/building-a-personal-ai-assistant-part-2-afb26c2a3b5b?source=collection_archive---------4-----------------------#2021-09-02">https://medium.com/nerd-for-tech/building-a-personal-ai-assistant-part-2-afb26c2a3b5b?source=collection_archive---------4-----------------------#2021-09-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4908" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">利用意图分类和实体抽取理解自然语言——意图分类</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/707d20bedcb65e1ea16abcf36c09bf70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oe6tojQwtgmcwhTh5A6WLg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">托马斯·科尔诺斯基在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片，经过编辑</figcaption></figure><p id="347c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">欢迎回来！这是我正在撰写的关于构建个人虚拟助理的迷你系列的第二部分。我建议在开始这篇文章之前先阅读上一篇文章，因为它包含了重要的代码。如果你来自我之前的文章，很高兴再次见到你！是时候使用我们花了这么长时间准备和预处理的数据集了。是时候建立我们的模型了。</p><h1 id="5635" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">变形金刚(电影名)</h1><p id="8113" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">我们的意图分类模型将由一个预训练的转换器和一些附加层组成。变形金刚是机器学习模型，特别擅长理解<strong class="jq hj">自然语言</strong>。这些模型发展了对语言的统计理解，这使得它们能够理解单词背后的含义。然而，变形金刚的缺点是它们通常很庞大，无论是在层数还是训练数据方面。例如，GPT-2使用了超过40GB的原始训练数据。正因为如此，最好使用<strong class="jq hj">预训</strong>车型。</p><h2 id="5629" class="lh kl hi bd km li lj lk kq ll lm ln ku jx lo lp kw kb lq lr ky kf ls lt la lu bi translated">预训练模型</h2><p id="55b7" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">预先训练的变形金刚，如伯特，是已经接受过各种任务训练的变形金刚。这意味着其他人已经为我们完成了大部分培训工作。我们所需要做的就是微调模型，使其最适合我们的用例。在本文中，我将使用distilBERT作为我的预训练转换器。DistilBERT是一个基于更大的BERT模型的转换器。通过使用distilBERT而不是BERT，我们将有一个运行速度比<strong class="jq hj">快60%</strong>的模型，而且几乎没有任何负面影响。</p><h2 id="0410" class="lh kl hi bd km li lj lk kq ll lm ln ku jx lo lp kw kb lq lr ky kf ls lt la lu bi translated">构建我们的模型</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lv"><img src="../Images/3b12d9c9572f988b0e0634c55d0e1e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-vnAhryrfyv6k3uUvbaXww.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">意图分类转换器的布局。</figcaption></figure><p id="13e5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们的意图分类模型将接受两个输入，<em class="lw">输入标识</em>和<em class="lw">输入注意。</em>这些输入由tokenizer生成，包含distilBERT的重要信息。两个输入的大小都是128，这对应于我们在记号赋予器中指定的填充。</p><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="1f74" class="lh kl hi ly b fi mc md l me mf">import tensorflow as tf<br/>from transformers import TFDistilBertModel</span><span id="5c4c" class="lh kl hi ly b fi mg md l me mf"># define the input layers<br/>input_ids_layer = <br/>tf.keras.layers.Input(<br/>    shape=(128,),           # shape of 128 coresponds to the padding<br/>    name='input_ids',<br/>    dtype='int32',<br/>)</span><span id="b3d1" class="lh kl hi ly b fi mg md l me mf">input_attention_layer = <br/>tf.keras.layers.Input(<br/>    shape=(128,),           # shape of 128 coresponds to the padding<br/>    name='input_attention',<br/>    dtype='int32',<br/>)</span></pre><p id="6922" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然后我们可以创建<em class="lw">distilbert-base-uncased</em>transformer，它是模型的核心。两个输入层都被输入到这个变换器中，这产生了<em class="lw">最后_隐藏_状态</em>层。该图层的大小为[ <em class="lw"> batch_size，128，768 </em> ]。DistilBert接受我们的文本输入，并为每个令牌返回768个唯一向量。</p><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="5382" class="lh kl hi ly b fi mc md l me mf"># create the pre-trained model<br/>transformer = TFDistilBertModel.from_pretrained('distilbert-base-uncased')</span><span id="ab6b" class="lh kl hi ly b fi mg md l me mf"># feed the inputs into the pre-trained model<br/># results in a layer of shape (<br/>#    batch_size, <br/>#    sequence_length, <br/>#    hidden_size=768)</span><span id="8238" class="lh kl hi ly b fi mg md l me mf">last_hidden_state = transformer([<br/>    input_ids_layer, <br/>    input_attention_layer])[0]</span></pre><p id="d596" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了使我们的分类器更容易，我们将只使用第一批768个向量。我们使用第一批并不重要，这只是常规。然后，这一层将被输入到一个稠密层中，作为我们的输出。输出层应用softmax函数将标量输出转换为概率，以便我们可以比较每个意图分类的可能性。</p><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="e771" class="lh kl hi ly b fi mc md l me mf"># the cls token contains a condensed representation of the entire last_hidden_state tensor<br/>cls_token = last_hidden_state[:, 0, :]</span><span id="c8f1" class="lh kl hi ly b fi mg md l me mf"># create the output layer<br/>intent_output = tf.keras.layers.Dense(<br/>    intent_class_count,<br/>    activation='softmax',<br/>    kernel_initializer=weight_initializer,<br/>    kernel_constraint=None,<br/>    bias_initializer='zeros',<br/>    name='intent_output'<br/>)(cls_token)</span></pre><p id="6981" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，可以定义一个Keras模型来包含我们所有的层。</p><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="6c78" class="lh kl hi ly b fi mc md l me mf"># define the model<br/>model = tf.keras.Model(<br/>    [input_ids_layer, input_attention_layer], <br/>    [intent_output])</span><span id="af68" class="lh kl hi ly b fi mg md l me mf">print(model.summary())</span></pre><p id="8146" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对模型做的最后一件事是编译它。我发现与<a class="ae jn" href="https://stats.stackexchange.com/questions/260505/should-i-use-a-categorical-cross-entropy-or-binary-cross-entropy-loss-for-binary#answer-410165" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">分类交叉熵</strong> </a> <strong class="jq hj"> </strong>损失函数配对的<a class="ae jn" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">亚当</strong> </a>优化器效果最好。尽管可以随意试验优化器，但是最好保持损失函数不变。</p><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="da27" class="lh kl hi ly b fi mc md l me mf">model.compile(<br/>    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5),<br/>    loss      = tf.keras.losses.CategoricalCrossentropy(),<br/>    metrics   = [tf.keras.metrics.CategoricalAccuracy('categorical_accuracy')])</span></pre><h2 id="3940" class="lh kl hi bd km li lj lk kq ll lm ln ku jx lo lp kw kb lq lr ky kf ls lt la lu bi translated">训练模型</h2><p id="f4c5" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">在编译完意图分类模型后，您应该会得到一个类似于<a class="ae jn" href="https://colab.research.google.com/drive/10h9idWFH5sromz6Oy41mq1HUirKI3Rb7?authuser=1#scrollTo=yQviC7KCdSQp" rel="noopener ugc nofollow" target="_blank">这个</a>的脚本。在我们的数据集上运行<em class="lw"> model.fit </em>在使用<strong class="jq hj"> GPU </strong>运行时的Google Colab中每个历元需要大约四十秒。除非添加了许多新的意图，否则该模型往往会在两个时代后收敛。</p><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="0805" class="lh kl hi ly b fi mc md l me mf">#* train the model<br/>history = model.fit(<br/>    x = [x_train_ids, x_train_attention],<br/>    y = [y_train_intents],<br/>    epochs = 2,<br/>    batch_size = 16,<br/>    verbose = 1<br/>)</span></pre><p id="6671" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">需要注意的一点是，没有提供验证数据。这是因为通过交互可以更容易地看到模型的效果。通过与模型交互并主动寻找错误，可以将目标句子模板添加到训练数据集中。</p></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="5c74" class="kk kl hi bd km kn mo kp kq kr mp kt ku io mq ip kw ir mr is ky iu ms iv la lb bi translated">结果</h1><p id="9513" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">为了与模型进行交互，您首先需要编写一个快速交互脚本。由于这与本文的主题没有直接关系，我建议您从提供的Google Colab中复制最后两个<a class="ae jn" href="https://colab.research.google.com/drive/10h9idWFH5sromz6Oy41mq1HUirKI3Rb7?authuser=1#scrollTo=QIdUZj_zhwuG" rel="noopener ugc nofollow" target="_blank">代码块</a>。一旦在你的脚本中，运行整个程序，看看你的意图分类器工作的有多好。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/9ffbf77e868ba2aed0d0f2a50a546799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVRTKAD6ZCCDrvRFOU04Hw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">与意图分类模型交互，测试每个意图。</figcaption></figure><p id="ab82" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果你运气好的话，那么你的意图分类器应该可以第一次尝试。如果没有，下面是我遇到的几个问题。</p><ul class=""><li id="f022" class="mu mv hi jq b jr js ju jv jx mw kb mx kf my kj mz na nb nc bi translated"><strong class="jq hj">无法导入变形金刚:</strong>用pip安装变形金刚库。这在Colab笔记本中也是需要的。</li></ul><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="1fe9" class="lh kl hi ly b fi mc md l me mf">!pip install transformers</span></pre><ul class=""><li id="0a0c" class="mu mv hi jq b jr js ju jv jx mw kb mx kf my kj mz na nb nc bi translated"><strong class="jq hj"> Model not training: </strong>确保您使用的填充为128，并且模型输入的形状为(128，)。</li><li id="e3fe" class="mu mv hi jq b jr nd ju ne jx nf kb ng kf nh kj mz na nb nc bi translated"><strong class="jq hj">训练时间长</strong>:确认你的运行时使用的是GPU。这可以在运行时&gt;中改变运行时类型。</li><li id="7221" class="mu mv hi jq b jr nd ju ne jx nf kb ng kf nh kj mz na nb nc bi translated"><strong class="jq hj">损失低，但模型交互被破坏</strong>:确保在分类函数中，您正在将令牌化的model_input从字典转换为数组。</li></ul><pre class="iy iz ja jb fd lx ly lz ma aw mb bi"><span id="b29c" class="lh kl hi ly b fi mc md l me mf">model_input = [<br/>    model_input['input_ids'], <br/>    model_input['attention_mask']]</span></pre></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><p id="2c0b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这就是一切！既然已经完成了意图分类，为什么不尝试添加更多的功能，比如文本解析或实体提取呢？哦，等等——实际上我现在正在为这两个网站写文章！请务必关注我，这样当我发布这些内容时，你会得到通知，这样你就可以将这个虚拟助手变成一个可以与Siri或Google home相媲美的助手了！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/670e1c620e65375877c07a01a574743d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*NJoApEmFfm1z5xv5hL5DoQ.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">下面先睹为快！</figcaption></figure><blockquote class="nj nk nl"><p id="cc50" class="jo jp lw jq b jr js ij jt ju jv im jw nm jy jz ka nn kc kd ke no kg kh ki kj hb bi translated">感谢阅读我的文章！请随意查看我的<a class="ae jn" href="https://tks.life/profile/robert.macwha#portfolio" rel="noopener ugc nofollow" target="_blank">作品集</a>，如果你有什么要说的，请在<a class="ae jn" href="https://www.linkedin.com/in/robert-macwha-0555141b6/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我发消息，或者在Medium上关注我，以便在我发布另一篇文章时得到通知！</p></blockquote><h2 id="bb1c" class="lh kl hi bd km li lj lk kq ll lm ln ku jx lo lp kw kb lq lr ky kf ls lt la lu bi translated">重要链接</h2><ul class=""><li id="e0ea" class="mu mv hi jq b jr lc ju ld jx np kb nq kf nr kj mz na nb nc bi translated"><strong class="jq hj"> Colab笔记本</strong>带代码的交互意图分类模型:<br/><a class="ae jn" href="https://colab.research.google.com/drive/10h9idWFH5sromz6Oy41mq1HUirKI3Rb7?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/drive/10 H9 id wf H5 sromz 6 oy 41 m1 huirk i3 Rb 7？usp =共享</a></li><li id="4804" class="mu mv hi jq b jr nd ju ne jx nf kb ng kf nh kj mz na nb nc bi translated"><strong class="jq hj">用于创建自定义意图分类数据集的代码库</strong>包括文档:<br/>T18】https://github . com/Robert-MAC wha/NLP-Intent-Classification/tree/Dataset-Generation</li><li id="f84d" class="mu mv hi jq b jr nd ju ne jx nf kb ng kf nh kj mz na nb nc bi translated"><strong class="jq hj">用于意图分类的代码库</strong>—文档包括:<br/><a class="ae jn" href="https://github.com/Robert-MacWha/NLP-Intent-Classification/tree/Intent-Classification" rel="noopener ugc nofollow" target="_blank">https://github . com/Robert-MAC wha/NLP-Intent-class ification/tree/Intent-class ification</a></li></ul></div></div>    
</body>
</html>