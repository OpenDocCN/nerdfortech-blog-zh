<html>
<head>
<title>Using RNN to generate text — using Fast AI &amp; Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用RNN生成文本—使用快速人工智能和Pytorch</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/using-rnn-to-generate-text-using-fast-ai-pytorch-29b77179791?source=collection_archive---------13-----------------------#2021-05-21">https://medium.com/nerd-for-tech/using-rnn-to-generate-text-using-fast-ai-pytorch-29b77179791?source=collection_archive---------13-----------------------#2021-05-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RNNs(递归神经网络)</strong>对于许多机器学习和自然语言处理任务来说，功能极其多样，非常强大——尤其是当你处理非结构化的复杂数据集时——这是大多数数据集。</p><p id="5765" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在互联网上发现了这个智慧的金块——Siri和谷歌语音搜索只是冰山一角</p><blockquote class="jd je jf"><p id="7dd0" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">递归<strong class="ih hj">神经网络</strong> ( <strong class="ih hj"> RNN </strong>)是最先进的序列数据算法，被苹果的Siri和谷歌的语音搜索所使用。由于有内部存储器，它是第一个记住其输入的算法，这使它非常适合涉及顺序数据的<strong class="ih hj">机器学习</strong>问题。</p></blockquote><p id="81e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ARK Investment Management是华尔街表现最好的基金之一，它认为深度学习是值得关注的软件突破，这是正确的。点击这里查看更多:<a class="ae jk" href="https://ark-funds.com/" rel="noopener ugc nofollow" target="_blank">https://ark-funds.com/</a></p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/7e22c43e8d3c37aabc35f239fcdde6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XpclAEbwTSUFtGLWWiQQ8g.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">来源:方舟投资管理有限责任公司，2020年</figcaption></figure><p id="40be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一篇介绍使用<strong class="ih hj"> Pytorch </strong>和<strong class="ih hj"> FastAI </strong>(我最喜欢的深度学习库)构建RNN的小文章。<a class="ae jk" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>网站将其定义为“加速从研究原型到生产部署路径的开源机器学习框架”。PyTorch日益成为研究人员构建深度学习算法的首选框架。完全是开源的，让我好开心:)。本文基于杰瑞米·霍华德关于语言建模的演讲。参观https://www.fast.ai/</p><p id="89d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语言建模的维基百科定义是这样的:</p><blockquote class="jd je jf"><p id="aef5" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">统计语言模型是单词序列的概率分布<a class="ae jk" href="https://en.wikipedia.org/wiki/Probability_distribution" rel="noopener ugc nofollow" target="_blank">。给定这样一个序列，比如长度为<em class="hi"> m </em>，它给整个序列分配一个概率。</a></p><p id="42d5" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">语言模型提供<a class="ae jk" href="https://en.wikipedia.org/wiki/Context_(language_use)" rel="noopener ugc nofollow" target="_blank">上下文</a>来区分听起来相似的单词和短语。例如，在美国英语中,“识别语音”和“破坏美丽的海滩”听起来很相似，但意思不同。</p><p id="3d6d" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">数据稀疏是构建语言模型的一个主要问题。大多数可能的单词序列在训练中没有观察到。一种解决方案是假设一个单词的概率只取决于前面的<em class="hi"> n </em>个单词。当<em class="hi"> n </em> = 1时，这被称为<a class="ae jk" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> <em class="hi"> n </em> -gram </a>模型或unigram模型。unigram模型也被称为<a class="ae jk" href="https://en.wikipedia.org/wiki/Bag_of_words_model" rel="noopener ugc nofollow" target="_blank">单词包模型</a>。</p><p id="a2c8" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">估计不同短语的<a class="ae jk" href="https://en.wikipedia.org/wiki/Relative_likelihood" rel="noopener ugc nofollow" target="_blank">相对可能性</a>在许多<a class="ae jk" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>应用中是有用的，尤其是那些生成文本作为输出的应用。语言建模用于<a class="ae jk" href="https://en.wikipedia.org/wiki/Speech_recognition" rel="noopener ugc nofollow" target="_blank">语音识别</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Language_model#cite_note-1" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Machine_translation" rel="noopener ugc nofollow" target="_blank">机器翻译</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Language_model#cite_note-Semantic_parsing_as_machine_translation-2" rel="noopener ugc nofollow" target="_blank">、【2】、</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank">词性标注</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Parsing" rel="noopener ugc nofollow" target="_blank">句法分析</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Language_model#cite_note-Semantic_parsing_as_machine_translation-2" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Optical_Character_Recognition" rel="noopener ugc nofollow" target="_blank">光学字符识别</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Handwriting_recognition" rel="noopener ugc nofollow" target="_blank">手写识别</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Language_model#cite_note-3" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae jk" href="https://en.wikipedia.org/wiki/Information_retrieval" rel="noopener ugc nofollow" target="_blank">信息检索</a>等"</p></blockquote><p id="a50f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章一点也不全面。但这是一种快速接触DL代码的方法。让我们暂时忘记数据，只看模型框架和它是如何构建的，以及它看起来是什么样子。你也可以在这里听讲座:<a class="ae jk" href="https://youtu.be/l1rlFh0PmZw" rel="noopener ugc nofollow" target="_blank">https://youtu.be/l1rlFh0PmZw</a>。</p><p id="7f72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是首先:关于ReLu的一个说明:</p><h1 id="803c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">整流线性激活功能或<strong class="ak"> ReLU </strong></h1><p id="2246" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">整流线性激活函数简称<strong class="ih hj"> ReLU </strong>是一个分段线性函数，如果输入为正，则直接输出，否则输出零。它已经成为许多类型的神经网络的默认激活函数，因为使用它的模型更容易训练，并且通常可以实现更好的性能。</p><p id="776f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">校正的线性单元是深度学习模型中最常用的激活函数。如果接收到任何负输入，函数返回0，但是对于任何正值xx，它返回该值。所以可以写成</p><p id="4350" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jg"> f(x)=max(0，x) </em></p><p id="5fe5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图形看起来是这样的</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es le"><img src="../Images/7b1b192cb4a90065f81b7cc698938e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aL4akJnlbmPR441qNnDCRw.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">热卢</figcaption></figure><p id="7227" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jk" href="https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/dans Becker/rectified-linear-units-relu-in-deep-learning</a></p><p id="d1fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是你唯一需要的进口货。除了Pytorch (obvs)</p><pre class="jm jn jo jp fd lf lg lh li aw lj bi"><span id="0905" class="lk kc hi lg b fi ll lm l ln lo">from fastai.text import *</span></pre><h1 id="fbdd" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">型号</strong></h1><p id="6a33" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">如果我们知道一个短语的前3个字符，让我们试着预测它的第4个字符。让我们举个例子，比如，“你过得怎么样？”。那么你要预测的字符是' '，前面三个字符是' h '，' o '，和' w '。这些是字符级模型，不要被误认为是单词级模型。单词级模型将按照“如何”、“是”、“你”的顺序预测“你”。字符级模型和单词级模型构成了文本生成问题的基础。获取数据集——看看UCI知识库或Kaggle。</p><p id="d717" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:batch norm 1d</strong>——对构建神经网络有帮助的最重要的一个函数——是一个FastAI模块。剩下的是本地PyTorch函数。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lp"><img src="../Images/ebb3749787d78d0ac202624303f2671c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5tPppdGaugegjRZ6hkePg.png"/></div></div></figure><pre class="jm jn jo jp fd lf lg lh li aw lj bi"><span id="4b97" class="lk kc hi lg b fi ll lm l ln lo">class Model0(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.i_h = nn.Embedding(nv,nh)  # green arrow<br/>        self.h_h = nn.Linear(nh,nh)     # brown arrow<br/>        self.h_o = nn.Linear(nh,nv)     # blue arrow<br/>        self.bn = nn.BatchNorm1d(nh)<br/>        <br/>    def forward(self, x):<br/>        h = self.bn(F.relu(self.i_h(x[:,0])))<br/>        if x.shape[1]&gt;1:<br/>            h = h + self.i_h(x[:,1])<br/>            h = self.bn(F.relu(self.h_h(h)))<br/>        if x.shape[1]&gt;2:<br/>            h = h + self.i_h(x[:,2])<br/>            h = self.bn(F.relu(self.h_h(h)))<br/>        return self.h_o(h)</span></pre><h1 id="fc45" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">多全连接模型</h1><p id="768c" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">之前，我们只是预测一行文本中的最后一个单词。给定70个代币，代币71是什么？这种方法丢弃了大量数据。为什么不从令牌1预测令牌2，然后预测令牌3，再预测令牌4，以此类推？为此，我们将修改我们的模型</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lq"><img src="../Images/efc1a85442f4f77ee25425468a56043f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0PQbbG6LC-HDLqwGce2XZw.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">多全连接模型:预测单词2到n，使用单词1到n-1</figcaption></figure><pre class="jm jn jo jp fd lf lg lh li aw lj bi"><span id="b79a" class="lk kc hi lg b fi ll lm l ln lo">To address this issue, let’s keep the hidden state from the previous line of text, so we are not starting over again on each new line of text.</span><span id="ce42" class="lk kc hi lg b fi lr lm l ln lo">class Model3(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.i_h = nn.Embedding(nv,nh)<br/>        self.h_h = nn.Linear(nh,nh)<br/>        self.h_o = nn.Linear(nh,nv)<br/>        self.bn = nn.BatchNorm1d(nh)<br/>        self.h = torch.zeros(bs, nh).cuda()<br/>        <br/>    def forward(self, x):<br/>        res = []<br/>        h = self.h<br/>        for i in range(x.shape[1]):<br/>            h = h + self.i_h(x[:,i])<br/>            h = F.relu(self.h_h(h))<br/>            res.append(self.bn(h))<br/>        self.h = h.detach()<br/>        res = torch.stack(res, dim=1)<br/>        res = self.h_o(res) //this is the part?<br/>        return res</span></pre><h1 id="315e" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">皮托尔彻·RNN</h1><p id="608a" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">Pytorch RNN没有BatchNorm1dFlat，这会增加计算时间并降低精度。当你有很长的时间尺度和更深的网络时，这些就变得不可能训练了。解决这个问题的一个方法是添加mini-NN来决定保留多少绿色箭头和多少橙色箭头。这些小型nn可以是gru或LSTMs。这些将在另一篇文章中讨论。</p><pre class="jm jn jo jp fd lf lg lh li aw lj bi"><span id="cf02" class="lk kc hi lg b fi ll lm l ln lo">class Model4(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.i_h = nn.Embedding(nv,nh)<br/>        self.rnn = nn.RNN(nh,nh, batch_first=True)<br/>        self.h_o = nn.Linear(nh,nv)<br/>        self.bn = BatchNorm1dFlat(nh)<br/>        self.h = torch.zeros(1, bs, nh).cuda()<br/>        <br/>    def forward(self, x):<br/>        res,h = self.rnn(self.i_h(x), self.h)<br/>        self.h = h.detach()<br/>        return self.h_o(self.bn(res))</span></pre></div></div>    
</body>
</html>