<html>
<head>
<title>Review —DFN: Discriminative Feature Network (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述—DFN:区别特征网络(语义分割)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-dfn-discriminative-feature-network-semantic-segmentation-eb3714482270?source=collection_archive---------13-----------------------#2021-04-25">https://medium.com/nerd-for-tech/review-dfn-discriminative-feature-network-semantic-segmentation-eb3714482270?source=collection_archive---------13-----------------------#2021-04-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b7fe" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak">拥有平滑网络&amp;边界网络，胜过</strong><a class="ae ix" rel="noopener" href="/@sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90">DeepLabv3+</a><a class="ae ix" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank">PSPNet</a><a class="ae ix" rel="noopener" href="/@sh.tsang/resnet-38-wider-or-deeper-resnet-image-classification-semantic-segmentation-f297f2f73437">ResNet-38</a><a class="ae ix" href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------" rel="noopener" target="_blank">RefineNet</a><a class="ae ix" href="https://towardsdatascience.com/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=post_page---------------------------" rel="noopener" target="_blank">GCN</a><a class="ae ix" rel="noopener" href="/@sh.tsang/review-resnet-duc-hdc-dense-upsampling-convolution-and-hybrid-dilated-convolution-semantic-c4208227b1ca">DUC</a><a class="ae ix" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank">DeepLabv2</a><a class="ae ix" rel="noopener" href="/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990?source=post_page---------------------------">parse net</a><a class="ae ix" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc">DPN</a><a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank">FCN</a>。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es iy"><img src="../Images/3ee132eebff9fcf1b46776be606863a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*OeGhMXJ60VpmZ5WCikDeeQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">语义分割中的硬例子</strong></figcaption></figure><p id="7e4b" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">在</span>这个故事里，<strong class="jn hj">学习一个用于语义切分的判别特征网络</strong>(DFN)，由华中科技大学、北京大学和旷视科技公司(Face++合作完成。在本文中:</p><ul class=""><li id="8494" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj">区别特征网络(DFN) </strong>有2个子网络。</li><li id="fc41" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">一个是<strong class="jn hj">平滑网络</strong>，用通道注意块和全局平均池来处理类内不一致性问题，以选择更具区分性的特征。</li><li id="9767" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">一个是<strong class="jn hj">边界网络</strong>，用深度语义边界监督使边界的双边特征可区分。</li></ul><p id="cae8" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">这是一篇在<strong class="jn hj"> 2018年CVPR </strong>发表的论文，有超过<strong class="jn hj"> 360条引文。(</strong><a class="le lf ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----eb3714482270--------------------------------" rel="noopener" target="_blank"><strong class="jn hj">Sik-Ho Tsang</strong></a><strong class="jn hj">@ media)</strong></p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="470f" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak">轮廓</strong></h1><ol class=""><li id="db0f" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg mj kw kx ky bi translated"><strong class="jn hj"> DFN:网络架构</strong></li><li id="a857" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">平滑网络</strong></li><li id="5b64" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">边境网络</strong></li><li id="c6a3" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">消融研究</strong></li><li id="2fc9" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">实验结果</strong></li></ol></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="b07b" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 1。DFN:网络架构</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mk"><img src="../Images/e98dbd1f6d992a1dc10b460b28e29643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hYmGXmbT3Rd64G0rNNJfbg.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk"> DFN:整体网络架构</strong></figcaption></figure><ul class=""><li id="5be2" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj"> ImageNet预先训练</strong><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jn hj">ResNet</strong></a><strong class="jn hj">-101被使用。</strong> <a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="jn hj"> FCN以</strong> </a> <strong class="jn hj"> 4作为基础分割框架。</strong></li><li id="09dd" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>因要素图大小不同而有多个阶段。</li><li id="cee5" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj">损失函数由光滑网络的分割损失<em class="mp"> ls </em>和边界网络的边界损失<em class="mp">lb</em>:</strong>组成</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mq"><img src="../Images/f74c6f1eeb12c64e97bf2e6882818c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*DQ-yTfv4Fn99_r8ppaP1QQ.png"/></div></figure><ul class=""><li id="9808" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">这两个网络或损失将在下文提及。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="5dbf" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 2 .平滑网络</strong></h1><ul class=""><li id="add7" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">总体网络架构图可见，平滑网络由<strong class="jn hj">通道关注块(CABs) </strong>和<strong class="jn hj">细化残差块(rrb)组成。</strong></li></ul><h2 id="befd" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">2.1.频道关注块(CABs)</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nf"><img src="../Images/d04b0e3c390a6e2070811d205049fba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*J2wGvMR4W0MHQX4J3STWXg.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">通道注意阻滞(CAB) </strong></figcaption></figure><ul class=""><li id="f60e" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">在<a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> FCN </a>架构中，卷积运算符输出得分图，该得分图给出每个像素处每个类别的概率。</li><li id="1342" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">评分图的最终评分仅在要素图的所有通道上求和:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ng"><img src="../Images/fd7effefc6483c368ae902da20864680.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*wCWaHXz7amJt1xFCqQqqRg.png"/></div></figure><ul class=""><li id="14b3" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">其中<em class="mp"> x </em>为网络的输出特征，<em class="mp"> w </em>为卷积核。<em class="mp"> K </em>为通道数。<em class="mp"> D </em>是像素位置的集合。</li></ul><blockquote class="nh ni nj"><p id="a7c1" class="jl jm mp jn b jo jp ij jq jr js im jt nk jv jw jx nl jz ka kb nm kd ke kf kg hb bi translated">然而，上面的等式隐含地表明不同通道的权重是相等的。</p></blockquote><ul class=""><li id="125b" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">对于CAB，<strong class="jn hj"> <em class="mp"> α </em>是CAB基于特征映射响应</strong>估计的权重，如上图所示:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nn"><img src="../Images/fe373c9254b48d4da6d3eef3128e1498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*DwLdGC_Zil6_rED9dkjAWg.png"/></div></figure><ul class=""><li id="d868" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">其中<em class="mp"> α </em>在与输入相乘之前是s形的:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es no"><img src="../Images/8cec1b3bf2a4eda722960cd7aaa11a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*rjTMzQOQwCtWN9QDaz5Z1Q.png"/></div></figure><ul class=""><li id="c1e6" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">这个想法起源于塞涅特。(有兴趣请随意阅读SENet。)</li></ul><blockquote class="nh ni nj"><p id="a18c" class="jl jm mp jn b jo jp ij jq jr js im jt nk jv jw jx nl jz ka kb nm kd ke kf kg hb bi translated">利用CAB得到<strong class="jn hj">类内一致性预测</strong>，提取<strong class="jn hj">鉴别特征</strong>，抑制<strong class="jn hj">非鉴别特征。</strong></p></blockquote><h2 id="bca8" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">2.2.细化残差块</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es np"><img src="../Images/01bf37582768d598c461057286cffb36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*oUJ_IrfYJ2tThT4touKPuA.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">细化残差块(RRB) </strong></figcaption></figure><ul class=""><li id="ce82" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">RRB的第一个组件是一个<strong class="jn hj"> 1×1卷积层</strong>。通道数统一为512。同时，它可以<strong class="jn hj">组合所有通道的信息。</strong></li><li id="ffa1" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">然后下面是一个基本的残差块，可以细化特征图。</li></ul><blockquote class="nh ni nj"><p id="1925" class="jl jm mp jn b jo jp ij jq jr js im jt nk jv jw jx nl jz ka kb nm kd ke kf kg hb bi translated">因此，该块可以<strong class="jn hj">加强各阶段</strong>的识别能力，灵感来自<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a>的架构。</p></blockquote></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="2e8e" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">3.边界网络</h1><ul class=""><li id="4a5f" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">边界网络，如总体网络架构图所示，用于<strong class="jn hj">扩大特征的类间区分。</strong></li><li id="cf16" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">为了<strong class="jn hj">提取准确的语义边界</strong>，<strong class="jn hj">应用语义边界的显式监督</strong>，使网络学习到一个类间区分能力强的特征。</li><li id="0698" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">该网络能够同时从低层获得准确的边缘信息和从高层获得语义信息，消除了一些缺乏语义信息的原始边缘。</li><li id="4f58" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj">高级的语义信息可以逐级细化低级的细节边缘信息。</strong></li><li id="28d7" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj">该网络的监督信号</strong>是用传统的图像处理方法，如<strong class="jn hj"> Canny </strong>从语义分割的基础上得到的。</li><li id="ca1d" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">为了补救阳性和阴性样本的不平衡，使用了RetinaNet中的焦点损失:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nq"><img src="../Images/b3cc011ece3d6bb4cea9d9efc742e723.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*YDfnZmlSR8pV4G5BCRMM9g.png"/></div></figure><ul class=""><li id="f972" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">其中<em class="mp"> pk </em>是类别<em class="mp"> k </em>的估计概率。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="c776" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">4.消融研究</h1><h2 id="5cd5" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">4.1.平滑网络</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nr"><img src="../Images/5d67cd13c55373854ac74e679569d7a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*h9FFGThFamV_Jp3ofpC7YA.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">PASCAL VOC 2012基准性能</strong></figcaption></figure><ul class=""><li id="5a08" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">底座<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> -101的性能如上图所示。</li><li id="dda5" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">用5个标度{0.5，0.75，1，1.5，1.75}，<strong class="jn hj">在PASCAL VOC 2012上达到72.86% </strong> mIOU。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ns"><img src="../Images/4510f526cd983971c322663f5545fc42.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*g-elrVtgfUeInDf7QdiUGw.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">拟建平滑网络的详细性能对比</strong>。<strong class="bd jk"> RRB </strong>:细化残差块。<strong class="bd jk"> GP </strong>:全球统筹分支。<strong class="bd jk">驾驶室</strong>:通道注意块。<strong class="bd jk"> DS </strong>:深度监督。</figcaption></figure><ul class=""><li id="b332" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj"> RRB </strong>:采用RRB，性能从72.86%提高到<strong class="jn hj"> 76.65% </strong>。</li><li id="1f18" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj"> GP </strong>:全球平均池<strong class="jn hj">引入最强的一致性来指导其他阶段</strong>。这使得性能从76.65%提高到<strong class="jn hj"> 78.20% </strong>，这是一个明显的提高。</li><li id="01b9" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj"> DS </strong>:通过深度监控，这进一步将性能<strong class="jn hj">提高了近0.4%。</strong></li><li id="a1f2" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj"> CAB </strong> : CAB <strong class="jn hj">利用高阶段引导低阶段</strong>用通道注意力向量增强一致性，使性能从78.51%提高到<strong class="jn hj"> 79.54% </strong>。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nt"><img src="../Images/ee73cf221895fc8eb94c6f3d5fa53531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*4snoDcLhlkeAowEBEspe8g.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">PASCAL VOC 2012上平滑网络的结果</strong></figcaption></figure><h2 id="463c" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">4.2.边界网络</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nu"><img src="../Images/6f6367bc43167ed5c7b50ed664a60cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*buv7At6q2ChVFzCFcowvwQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">结合边界网络和平滑网络作为区别特征网络</strong>。<strong class="bd jk"> SN </strong>:平滑网络。<strong class="bd jk"> BN </strong>:边界网络。<strong class="bd jk"> MS翻转</strong>:增加多刻度输入和左右翻转输入。</figcaption></figure><ul class=""><li id="34e1" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">随着<strong class="jn hj">边界网络</strong>集成到平滑网络中，这将性能从79.54%提高到<strong class="jn hj"> 79.67% </strong>。</li><li id="f7cb" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">边界网络<strong class="jn hj">优化了语义边界</strong>，语义边界是整个图像中相对较小的一部分，所以这个设计做了一个小的改进。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nv"><img src="../Images/d4164f92fdfe3c0aeaa7c0498481b71a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*O2sU5zTWh-rODqF1aRns4w.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">预测边界由边界网络细化。</strong></figcaption></figure><ul class=""><li id="2afb" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">如上所示，边界网络不仅细化了边界，还细化了预测。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nw"><img src="../Images/ac8fa3797659b0bab60cc0c8ea29e990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*dC0Z2Q6moixfHCD7lNhUwg.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">PASCAL VOC 2012数据集上边界网络的边界预测</strong></figcaption></figure><ul class=""><li id="a299" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">如上图所示，第三列是Canny算子从Ground-truth中提取的<strong class="jn hj">语义边界。</strong></li><li id="4bc6" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">最后一栏是边界网络的预测结果。</li></ul><h2 id="f779" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">4.3.区别特征网络</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nx"><img src="../Images/d54a0d0169f1a7e1bdf15f5f9447c7c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*NcU6XV_jGwhXCwKkuLHuhg.png"/></div></figure><ul class=""><li id="41e8" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">针对总损失测试不同的平衡值。</li><li id="a631" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">选择<strong class="jn hj"> <em class="mp"> λ </em> =0.1 </strong>，因为它得到了最好的结果。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ny"><img src="../Images/a0c074e2ca53f88c5ad0193795e7791e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*-V0G6zsR25SO7ScUUJnZag.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated">PASCAL VOC 2012数据集上的分阶段细化过程。</figcaption></figure><ul class=""><li id="0f68" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj">下级</strong>中的分割预测是<strong class="jn hj">更空间粗</strong>，而<strong class="jn hj">上级</strong>是<strong class="jn hj">更精细</strong>。</li><li id="a71a" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">较低阶段的<strong class="jn hj">边界预测包含<strong class="jn hj">更多不属于语义边界</strong>的边，而较高阶段的语义边界更纯粹。</strong></li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="ad25" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">5.实验结果</h1><h2 id="dd83" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">5.1.帕斯卡VOC 2012</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nz"><img src="../Images/eafd25a6ca7a97edceb23707a1a7fc59.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*WFIuVXROHw0RrN3k0haXYA.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">PASCAL VOC 2012数据集上的验证策略</strong>。<strong class="bd jk"> MS Flip </strong>:多尺度和翻转评估。</figcaption></figure><ul class=""><li id="ea0b" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj"> Train_data </strong>:模型在PASCAL VOC 2012车组上进一步微调。</li><li id="86df" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj"> MS_Flip </strong>:应用多刻度输入{0.5，0.75，1，1.5，1.75}和水平翻转。</li><li id="8932" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">在DFN的验证集上获得了80.6%的准确率。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es oa"><img src="../Images/55d988c033d73455c2508083353c7f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*np1wnGoLXbwFR4mO7nAx-A.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">在PASCAL VOC 2012测试集上的性能</strong></figcaption></figure><ul class=""><li id="6b0c" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">PASCAL VOC 2012 trainval集合用于进一步微调所提出的方法。最终DFN在有MS-COCO微调和没有MS-COCO微调的情况下分别达到<strong class="jn hj"> 82.7% </strong>和<strong class="jn hj"> 86.2% </strong> <strong class="jn hj">的性能，如上图。</strong></li><li id="ebef" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">注意，<a class="ae ix" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv1 </a>中的Dense-CRF后处理不用于DFN。</li><li id="ee14" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">最后，DFN的表现优于<a class="ae ix" rel="noopener" href="/@sh.tsang/review-deeplabv3-atrous-separable-convolution-semantic-segmentation-a625f6e83b90"> DeepLabv3+ </a>、<a class="ae ix" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>、<a class="ae ix" rel="noopener" href="/@sh.tsang/resnet-38-wider-or-deeper-resnet-image-classification-semantic-segmentation-f297f2f73437"> ResNet-38 </a>、<a class="ae ix" href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------" rel="noopener" target="_blank"> RefineNet </a>、<a class="ae ix" href="https://towardsdatascience.com/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=post_page---------------------------" rel="noopener" target="_blank"> GCN </a>、<a class="ae ix" rel="noopener" href="/@sh.tsang/review-resnet-duc-hdc-dense-upsampling-convolution-and-hybrid-dilated-convolution-semantic-c4208227b1ca"> DUC </a>、<a class="ae ix" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv2 </a>、<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990?source=post_page---------------------------"> ParseNet </a>、<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc"> DPN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> FCN </a>。</li></ul><h2 id="eb4c" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">5.2.城市景观</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ob"><img src="../Images/310b065ae0c445c725d09b531eaec3a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*IWzymMizoZSmVfIybw8I1Q.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">在Cityscapes测试集上的性能</strong></figcaption></figure><ul class=""><li id="4613" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">同样，在城市风景上，DFN的表现也优于<a class="ae ix" href="https://towardsdatascience.com/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d?source=post_page---------------------------" rel="noopener" target="_blank"> PSPNet </a>、<a class="ae ix" rel="noopener" href="/@sh.tsang/review-resnet-duc-hdc-dense-upsampling-convolution-and-hybrid-dilated-convolution-semantic-c4208227b1ca"> DUC </a>、<a class="ae ix" href="https://towardsdatascience.com/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------" rel="noopener" target="_blank"> RefineNet </a>、<a class="ae ix" href="https://towardsdatascience.com/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------" rel="noopener" target="_blank"> DeepLabv2 </a>、<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990?source=post_page---------------------------"> ParseNet </a>、<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-dpn-deep-parsing-network-semantic-segmentation-2f740ced6edc"> DPN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------" rel="noopener" target="_blank"> FCN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------" rel="noopener" target="_blank"> CRF-RNN </a>。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es oc"><img src="../Images/df0c61a5072681e237de75e6d4ceb132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mY7hyOOViS9O2xdF975jHg.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">city scapes数据集上DFN的示例结果。</strong></figcaption></figure></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="3d65" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">参考</h2><p id="eef8" class="pw-post-body-paragraph jl jm hi jn b jo me ij jq jr mf im jt ju od jw jx jy oe ka kb kc of ke kf kg hb bi translated">【2018 CVPR】【DFN】<br/><a class="ae ix" href="https://arxiv.org/abs/1804.09337" rel="noopener ugc nofollow" target="_blank">学习用于语义切分的判别特征网络</a></p><h2 id="4fe2" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated">语义分割</h2><p id="d8b2" class="pw-post-body-paragraph jl jm hi jn b jo me ij jq jr mf im jt ju od jw jx jy oe ka kb kc of ke kf kg hb bi translated">)(我)(们)(都)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(就)(是)(这)(些)(事)(,)(我)(们)(还)(没)(想)(要)(到)(这)(里)(来)(,)(我)(们)(就)(没)(想)(到)(这)(些)(事)(了)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(们)(们)(还)(有)(什)(么)(好)(好)(的)(</p><h2 id="eb56" class="mr lo hi bd jk ms mt mu ls mv mw mx lw ju my mz ly jy na nb ma kc nc nd mc ne bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>