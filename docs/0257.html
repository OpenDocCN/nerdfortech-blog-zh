<html>
<head>
<title>Overcoming Parquet Schema Issues</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">克服拼花图案问题</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/overcoming-parquet-schema-issues-4f7ae38d1c4a?source=collection_archive---------0-----------------------#2020-08-20">https://medium.com/nerd-for-tech/overcoming-parquet-schema-issues-4f7ae38d1c4a?source=collection_archive---------0-----------------------#2020-08-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c753bbee3b813343b0aafd9508f86da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uN3G3dR-_2Brm4OzxfF8Uw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Genessa panainite在<a class="ae iu" href="https://unsplash.com/s/photos/spark?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="332b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于我们在使用Pandas和Spark数据框架时如何克服拼花模式相关问题的几种方法。</p><p id="fe97" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我目前的项目中，我们完全依赖于parquet文件进行所有的数据处理。最初，我们将数据从MS SQL Server提取到。csv (dat)文件和模式转换成。sch文件。使用Spark和iconv，我们将这些文件转换成parquet文件，确保在保存到Hadoop数据存储之前应用模式。</p><p id="1010" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最近我们转而使用Pandas(带有pyodbc)进行提取，并使用Pandas to_parquet()方法直接上传到HDFS作为拼花。这些文件需要再进行一轮处理才能应用模式，现在它们安全地存储在一个中央模式存储库中。这是我们遇到如下问题的地方</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5c5a" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">TypeError</strong>: field ActiveFlag: <strong class="jy hj">IntegerType can not accept object</strong> 1.0 <strong class="jy hj">in type &lt;class 'float'&gt;</strong></span></pre></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="b188" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我解释一下我的分析</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/0642726142f51b0285f283b0b8b84611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*kHQbBUMIJmJRT7FmKzOL8g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="3243" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在截图中，您可以看到一个虚拟表(EMP ),它是我用列ActiveFlag作为tinyint创建的。预期值为0、1或null</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/7648033a4fc4ac4bd1afa4c1e2d0cc06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*vptsNQTTB1yN99aJt_aVbA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="df6d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">创建了一些虚拟数据来填充表。突出显示的记录是我们感兴趣的记录。<br/>正如我们所见，ActiveFlag对于最后一条记录有整数值0、1和null。</p></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="51a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我读取Pandas数据帧中的数据，显示记录和模式，并将其写出到一个parquet文件中。</p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="kr ks l"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/247cc93f7a86df50372d1efec3dc8b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*0RXDPnTuaaFT5a70C0NIMQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="5e34" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如上面突出显示的，ActiveFlag列存储为float64。像0、1和null这样的值被转换为0.0、1.0和NaN。这是因为Pandas限制在int64 Dtype系列中存储null。</p></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="21ed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将文件读入Spark数据帧，打印记录并检查模式。我们看到ActiveFlag为double，所有列都标记为nullable = true</p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="kr ks l"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/72c2d218c274c7b5083ef514692d47e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*KgC616nylHOsda3gnxIAcw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="cc2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们创建一个模式，并将其应用于Spark数据帧。模式应用成功，但当我们执行显示或写入时，Spark将抛出一个TypeError，表明它无法将float转换为IntegerType。</p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="kr ks l"/></div></figure></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="a719" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi kv translated">一种方法是在应用所需模式时从Pandas数据帧创建PyArrow表，然后将其转换为Spark数据帧。<br/>如下所示，PyArrow表正确显示了模式和数据</p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="kr ks l"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es le"><img src="../Images/63d559ccb62d64fad9acdf83e5b72cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*HFXt3ZhH0jNvIxaGAuxqJg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="bcd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您搜索将PyArrow表转换为Spark数据帧的方法，您将最常见地看到PyArrow表的to_pandas()方法被调用，然后Spark的createDataFrame方法在pandas数据帧上被调用。但是如果我们再次转换回Pandas，那么模式将会丢失，数据将会混乱。所以回顾可用的选项，我们看到PyArrow表上的to_pydict()方法。这将把表转换成有序的字典，并正确地保留模式和数据。然后从这里我们可以创建一个Spark数据框架并应用我们的模式。</p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="kr ks l"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/8833c0aaaaa6922c0507c18dff446d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*rKBjDiwei_4EJcK2AP8AJQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="af12" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi kv translated">我最近想到的另一种方法是使用熊猫1.0.0中新提供的Int64 Dtype。当保存拼花文件时，我们可以在Pandas数据帧上直接使用<a class="ae iu" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html" rel="noopener ugc nofollow" target="_blank"> convert_dtypes() </a>，它会正确地存储数据和数据类型。然后我们只需在Spark中读取它并应用我们的模式。完成的工作:)</p><figure class="jt ju jv jw fd ij"><div class="bz dy l di"><div class="kr ks l"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/a329a1b2e90a3ea82248cbd30ede286d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*ZH5yU3ig_3lY2tQZHhDBYw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="fc4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考文献:</strong> <br/> 1。<a class="ae iu" href="https://stackoverflow.com/questions/48578787/convert-ordered-dictionary-to-pyspark-dataframe" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/48578787/convert-ordered-dictionary-to-py spark-data frame</a><br/>2 .<a class="ae iu" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html" rel="noopener ugc nofollow" target="_blank">https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas。data frame . convert _ dtypes . html</a></p></div></div>    
</body>
</html>