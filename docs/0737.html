<html>
<head>
<title>Weight Initializations in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的权重初始化</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/weight-initializations-in-neural-networks-b66555a29e50?source=collection_archive---------11-----------------------#2021-02-09">https://medium.com/nerd-for-tech/weight-initializations-in-neural-networks-b66555a29e50?source=collection_archive---------11-----------------------#2021-02-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/01a4f21b7508ec76eb42810c0bfc1d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YN_D0vz8qjTKm61F-rkMA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">在<a class="ae hv" href="https://unsplash.com/s/photos/random-matrix?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae hv" href="https://unsplash.com/@frostroomhead?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Rodion Kutsaev </a>拍照</figcaption></figure><div class=""/><p id="255a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">像随机梯度下降(SGD)这样的优化算法依赖于参数的初始值。初始值，当明智地选择时，有助于避免缓慢收敛，也确保我们不会一直振荡在最小值之外。简单地说，权重初始化防止激活输出在神经网络的正向传递期间爆炸或消失。在这篇博客中，我们将看看一些初始化技术:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jt"><img src="../Images/9fa09b2e31bdf3741c21197335d179fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iJ5u2l0QRRLSGg-rYtS9RA.png"/></div></div></figure><p id="e43d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">零初始化</strong> —权重初始化为零。当我们用 0 初始化时，它相对于损失函数的导数将是相同的，因此所有权重在随后的迭代中具有相同的值。它将表现得像一个线性模型，神经元将学习相同的特征。</p><blockquote class="jy jz ka"><p id="6058" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">太小的初始化会导致渐变消失&amp;学习缓慢，而太大的初始化会导致渐变爆炸。</p></blockquote><pre class="ju jv jw jx fd kf kg kh ki aw kj bi"><span id="8797" class="kk kl hy kg b fi km kn l ko kp">x = torch.randn(64)<br/>for i in range(100):<br/>  a = torch.randn(64,64)<br/>  x = a @ x<br/>  if torch.isnan(x.std()):<br/>    break<br/>print(i)</span></pre><p id="674d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的代码块中，我们大量的初始化导致了渐变的爆发。系统无法表示 40 次迭代后的值，因为值的连续乘法使其变大。</p><pre class="ju jv jw jx fd kf kg kh ki aw kj bi"><span id="2a9a" class="kk kl hy kg b fi km kn l ko kp">x = torch.randn(64)<br/>for i in range(100):<br/>  a = torch.randn(64,64)*0.01<br/>  x = a @ x<br/>  if i&lt;10:<br/>    print(x.mean(), x.std())</span></pre><p id="66de" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的示例代码中，如果我们使用小的初始化，梯度将在几次迭代中变得非常小。在上面的代码中，在最初的几次迭代之后，平均值和标准偏差变为零。</p><p id="1a7f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">随机初始化</strong> —权重被随机初始化为接近零(非零)。在进行随机初始化时，我们确保平均值应该为零&amp;方差在各层之间应该相同。</p><p id="a0ab" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> Xavier 初始化</strong> —最常用于 sigmoid &amp; tanh 激活功能。所有权重都是从均值为 0、方差为 1/(前一层神经元数)的正态分布中随机选取的。</p><pre class="ju jv jw jx fd kf kg kh ki aw kj bi"><span id="643a" class="kk kl hy kg b fi km kn l ko kp">x = torch.randn(64)<br/>for i in range(100):<br/>  a = torch.Tensor(torch.randn(64,64)).uniform_(-1, 1)*math.sqrt(6/(64+64))<br/>  x = torch.sigmoid(a @ x)</span><span id="2916" class="kk kl hy kg b fi kq kn l ko kp">x.mean(), x.std()</span></pre><p id="b0cd" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是相同的初始化不能很好地与 ReLU 一起工作。</p><pre class="ju jv jw jx fd kf kg kh ki aw kj bi"><span id="2843" class="kk kl hy kg b fi km kn l ko kp">x = torch.randn(64)<br/>for i in range(100):<br/>  a = torch.Tensor(torch.randn(64,64)).uniform_(-1, 1)*math.sqrt(6/(64+64))<br/>  x = torch.relu(a @ x)</span><span id="c2b5" class="kk kl hy kg b fi kq kn l ko kp">x.mean(), x.std()</span></pre><p id="5577" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">初始化</strong> —权重随机初始化，但其范围不同，且基于神经元的前一层。此外，这种初始化是激活感知的。它最常用于 ReLU 激活。</p><pre class="ju jv jw jx fd kf kg kh ki aw kj bi"><span id="569a" class="kk kl hy kg b fi km kn l ko kp">x = torch.randn(64)<br/>for i in range(100):<br/>  a = torch.randn(64,64)*math.sqrt(2/64)<br/>  x = torch.relu(a @ x)<br/>x.mean(), x.std()</span></pre><p id="fb42" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望有帮助！！</p></div></div>    
</body>
</html>