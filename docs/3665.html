<html>
<head>
<title>Deep Learning — a foundation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习——基础</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/deep-learning-a-foundation-645b612f4e54?source=collection_archive---------13-----------------------#2021-06-18">https://medium.com/nerd-for-tech/deep-learning-a-foundation-645b612f4e54?source=collection_archive---------13-----------------------#2021-06-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4039" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习背后的整个想法是让计算机人工模拟生物自然智能，所以让我们对生物神经元如何工作有一个大致的了解。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/9afaa2f14fd087039d6af3015c4f9c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*zflioNp7JqQQsNQlVAylgQ.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1:生物神经元</figcaption></figure><ul class=""><li id="7534" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">这是生物神经元，这里要注意的是各个部分都与细胞核相连，轴突负责从细胞核输出的部分。</li></ul><p id="8e2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">.</p><p id="cef2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">.</p><p id="21d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">.</p><p id="afa4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jy"><img src="../Images/5ac46ff0212bbf13f4064a841e2fbdb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tf7NC8Hg1r9vAh7qYpl4JQ.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图2:核心</figcaption></figure><ul class=""><li id="65b1" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">这是一个生物学的例子，树突带来输入信号，然后将信号传递给细胞核，在那里进行数学计算，轴突将输出传递给另一个树突。</li><li id="64ee" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">这是生物神经元模型的基本思想，在感知器发挥作用的地方，它被转换成数学神经元。</li><li id="b4fa" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">感知器是神经网络的一种形式，由弗兰克·T2·罗森布拉特于1958年发明。他当时看到了巨大的潜力，并引用<em class="ki">的话说，“感知机最终可能能够学习、决策和翻译语言”。一个很好的例子是谷歌翻译使用神经网络来翻译语言。</em></li><li id="d11c" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">1969年马文·明斯基和西蒙·派珀特出版了他们的书<strong class="ih hj"><em class="ki"/></strong>。他们在书中提出的主要缺点是计算能力，回到1970年，计算能力非常低，所以很难建立多层神经网络。流行起来需要时间。</li><li id="6a74" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">下一个问题来了<em class="ki">‘既然我们有ML，为什么我们还需要DL？’</em></li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kj"><img src="../Images/f86ac549b3706b0ada7d23f700937f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6R4eyDo1xrsoLCdXpY3qg.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图3:ML和DL的比较。</figcaption></figure><ul class=""><li id="dbb6" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">机器学习性能在特定数据量时会饱和，而深度学习性能随着数据量呈指数增长。</li><li id="3847" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">对于小的训练集，性能是相同的。这个时代充斥着数据，所以DL在从数据中获取洞察方面起着至关重要的作用。</li><li id="75ae" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">现在我们可以把生物神经元替换成数学单位。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kk"><img src="../Images/61c3933da93cec5918393e97c9053d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Us7Muw7csHRcuILFRQJARQ.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图4:单一感知器</figcaption></figure><ul class=""><li id="c447" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">这是单个神经元，它有两个输入特征和一个输出。</li></ul><p id="a22e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">.</p><p id="7830" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kl"><img src="../Images/3812d1afce89c85824881c4f7b0b3edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BqKWtJOOrXcuorDmE_3SDg.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图5:单一感知器的工作。</figcaption></figure><ul class=""><li id="f8df" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">权重与输入特征相乘，因此我们可以优化权重值以减少损失。</li><li id="4e5e" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">该项增加了一个偏差，以固定输出的最小值。通过梯度下降找到权重和偏差的优化值，它可以是负值也可以是正值。</li><li id="dc07" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">神经元执行线性计算和激活功能，该输出作为另一个神经元的输入传递。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es km"><img src="../Images/19d460bed1881703f5b51cefe3c5b57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sjBB_eUqUoFUWOLX"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图6:线性函数的通用公式。</figcaption></figure><ul class=""><li id="b594" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">单个神经元不足以学习复杂的系统。幸运的是，我们可以扩展单个感知器的思想，创建一个多层感知器模型。</li><li id="bbf4" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated"><strong class="ih hj">神经网络</strong>是一种模仿生物网络的机器学习架构。</li><li id="687d" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated"><strong class="ih hj">深度学习</strong>是训练超过一个隐层的神经网络的过程。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kn"><img src="../Images/01cc2cd4bfc4b98ef15cfbb0c6703f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*RJHx8yi16nlZTWyBJScIkw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图7:多层感知器</figcaption></figure><ul class=""><li id="fedb" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">为了建立一个感知器网络，我们可以使用一个<strong class="ih hj">多层感知器模型</strong>连接多层感知器。</li><li id="da6b" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">第一层是<strong class="ih hj">输入层</strong>，它直接接受真实数据值。</li><li id="212b" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">最后一层是<strong class="ih hj">输出层</strong>，它可以是多个神经元，具体取决于多标签预测。</li><li id="fd28" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">输入层和输出层之间的层是<strong class="ih hj">隐藏层。</strong>隐藏层很难解释，因为它们具有高度的互连性，并且远离已知的输入或输出值。如果神经网络包含2个或更多隐藏层，则成为<strong class="ih hj">“深度神经网络”</strong>。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ko"><img src="../Images/496e29b8d2a4a8ccc89fa6030aa78df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*WED3SsSJ-45q4CkTROFE-w.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图8:非深度神经网络vs深度神经网络。</figcaption></figure><ul class=""><li id="d848" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">我们看到感知器本身包含一个非常简单的求和函数f(x)。然而，对于大多数用例来说，这是没有用的，我们希望能够为我们的输出值设置约束，尤其是在分类任务中。</li><li id="de57" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">让所有输出落在0和1之间将是有用的。这就是<strong class="ih hj">激活功能</strong>发挥作用的地方。</li></ul><h2 id="6f26" class="kp kq hi bd kr ks kt ku kv kw kx ky kz iq la lb lc iu ld le lf iy lg lh li lj bi translated">激活功能:</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lk"><img src="../Images/6c1aeb81f8abb8e7f332ca0dc0072df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L7bHZ-cnp1WigwJQ8wQiew.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图9: Sigmoid函数</figcaption></figure><ul class=""><li id="8282" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">Sigmoid函数或Logit函数给出介于0和1之间的值。它的截止点是0.5。</li><li id="a779" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">问题是。值&gt; 0.5为1，概率。值&lt; 0.5 is 0.</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ll"><img src="../Images/d392505493e085f45e483a7a852b39d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hqx0Xdp46r77yLMZPx7Jvg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">Figure 10 : Tanh function</figcaption></figure><ul class=""><li id="e2a2" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">Hyperbolic Tanget function or Tanh function gives the output between -1 and 1 instead of 0 to 1.</li><li id="b40d" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">The major drawback with Tanh and sigmoid function it get saturated at particular point.</li><li id="1752" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">During back propagation the derivative value becomes smaller, which slows down the process of gradient descent optimisation. To overcome this we have some other activation function like <strong class="ih hj"> ReLU </strong>和<strong class="ih hj">泄漏ReLU </strong>。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lm"><img src="../Images/1a46197ef83ebc94627394f4d8ebf838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*Na4mu5K9-ypPkA-QQ3ifCg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图11: ReLU</figcaption></figure><ul class=""><li id="de38" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">整流线性单元是相对简单的单元。</li><li id="0f10" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">它给出的输出为</li></ul><p id="8c3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果值Z ≤ 0，则为0</p><p id="241e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果值Z &gt; 0，则返回Z。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ln"><img src="../Images/cdcbd896ca1d9911a4819746b9b2cccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_HJnZDYLDeuW11kwQjF7fw.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图12:泄漏的ReLU</figcaption></figure><p id="fc6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">.</p><ul class=""><li id="3c44" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">Leaky- ReLU类似于ReLU，只是值小于或等于零，它给出的输出为0.01*x，从而加快了优化过程。</li><li id="92a5" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">ReLu和Leaky-ReLu已经被发现具有非常好的性能，特别是在处理<strong class="ih hj">消失梯度</strong>的问题时，这种问题是由于sigmoid和tanh的导数值很小而导致的，这会减慢优化过程。</li></ul><p id="4b76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">成本函数:</strong></p><ul class=""><li id="764b" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">神经网络接受输入，将它们乘以权重，并向它们添加偏差。然后，这个结果通过一个激活函数，该函数在所有层的末尾导致一些输出。</li><li id="6983" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">我们需要获取网络的估计输出，然后将它们与标签的真实值进行比较。</li><li id="3045" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">成本函数必须是平均值，因此它可以输出单个值。损失函数是为1个时期(一个正向传播+一个反向传播)计算的单个值。</li></ul><p id="ddb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ y代表真实值</p><p id="cb59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ a代表神经元的预测</p><p id="d329" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ w*x + b = z</p><p id="e90a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→将z传递给激活函数σ(z) = a</p><ul class=""><li id="99b2" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">我们简单地计算实际值y(x)与预测值a(x)之间的差值。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/528a353ba8dba928ce0d745e1815158e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*WbbvjkAHMNyTddySMvWK5A.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图13:平方成本函数。</figcaption></figure><ul class=""><li id="ad56" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">平方这为我们做了2件有用的事情，让一切保持积极，并且<strong class="ih hj">惩罚</strong>大错误！</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lp"><img src="../Images/e955203ef0d62248bf777cec35737b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*5ybb0oB_hDYODxIbmm5g6Q.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图14:带参数的多层网络。</figcaption></figure><ul class=""><li id="8842" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">在一个真实的例子中，这意味着我们有一些成本函数<strong class="ih hj"> C </strong>依赖大量的权重！<strong class="ih hj"> C(w1，w2，w3，…。wn) </strong></li><li id="9720" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">我们如何计算出哪种重量使我们的成本最低？</li><li id="79f7" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated"><strong class="ih hj">梯度下降</strong>用于寻找具有低成本函数的权重和偏差的优化值。</li><li id="6a6d" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">我们在插图中展示的学习率是恒定的(每个步长都是相等的)。但是我们可以聪明地在前进的过程中调整我们的步长</li><li id="ff3a" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">我们可以从更大的步长开始，然后当我们意识到斜率越来越接近零时，我们可以变得更小。这被称为<strong class="ih hj">自适应梯度下降。</strong></li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/ddff4c49489ad70b1d9eba6d8e3cb944.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/0*1CTBrc1N2-VPQzDi"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图15:梯度下降性能。</figcaption></figure><ul class=""><li id="ccc8" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">Adam与其他梯度下降算法。</li><li id="b443" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">对于二元分类，我们使用二元交叉熵。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/b0659635d7f06b5e9ab6dd6b85a532f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/0*BW4h1HeuXqtliPJq"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图16:二元交叉熵。</figcaption></figure><ul class=""><li id="eb94" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">对于大于2的类，我们使用分类交叉熵。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/03f346fe242179c510632224f2379e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*IsfCRWNHKuVbO3-o"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图17:分类交叉熵。</figcaption></figure><ul class=""><li id="ca5d" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">一旦我们得到了成本/损失值，我们实际上如何回过头来调整我们的权重和偏差呢？这是<strong class="ih hj">反向传播。</strong></li></ul><p id="7f3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关于衍生品的直觉:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lt"><img src="../Images/1816eeece2e7d446b7b6a0626b0612c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L7cm__PY8866dUowZR2C6A.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图18:导数计算</figcaption></figure><ul class=""><li id="6c12" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">当我们计算函数的导数时，它给出了输入的变化如何影响输出的值。</li></ul><p id="2ca6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">反向传播</strong>:</p><ul class=""><li id="248a" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">我们想知道成本函数结果如何相对于网络中的权重变化，因此我们可以更新权重以最小化成本函数</li></ul><div class="je jf jg jh fd ab cb"><figure class="lu ji lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><img src="../Images/93307ec064d06a5d54c08f6134ebdbc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SAqWXPdS60kYa1JewBl74g.jpeg"/></div></figure><figure class="lu ji lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><img src="../Images/8ba491c5f8d422a79ca658253094c994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mERv6cscSNOehTdFJZoZ3w.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx ma di mb mc translated">图19:反向传播的步骤</figcaption></figure></div><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es md"><img src="../Images/5be449f91dbf5c261e39a1cfed4a5616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8uMFiUKi9_-iJhfoswq7YQ.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图20:最终导数。</figcaption></figure><p id="8580" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">逻辑回归的计算图</strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es me"><img src="../Images/98ca5b9ff03664f7dd9392b0e29a845e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4BTpU8s-hqgdgazb8TZCUw.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图21:反向传播导数</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mf"><img src="../Images/95a12b245f7661c75b1a5eebacda0a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEMgGhoNGFbA-M1zwt6iUQ.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图22:最终导数</figcaption></figure><ul class=""><li id="d7f9" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">逻辑回归检查损失函数，然后反向传播以分配优化值。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mg"><img src="../Images/b048fa2331d926ab0c7690a1beb54900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m_eKGqxEH5WF43P2Wvs2Pg.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图23:神经网络的计算图。</figcaption></figure><ul class=""><li id="eeeb" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">每个神经元具有不同的权重和偏差值。</li><li id="9a4a" class="jp jq hi ih b ii kd im ke iq kf iu kg iy kh jc ju jv jw jx bi translated">随机初始化权重和偏差值以加速优化。</li></ul><p id="f23f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考以下内容，了解如何构建基本人工神经网络模型:</p><p id="8ae1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mh" href="https://github.com/Rishikumar04/Deep-Learning/blob/main/Basic%20ANN.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/rishikumar 04/Deep-Learning/blob/main/Basic % 20 ann . ipynb</a></p><p id="3f5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。希望你对神经网络如何工作有了基本的直觉。</p></div></div>    
</body>
</html>