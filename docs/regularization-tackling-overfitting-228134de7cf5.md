# 正规化—解决过度拟合问题

> 原文：<https://medium.com/nerd-for-tech/regularization-tackling-overfitting-228134de7cf5?source=collection_archive---------10----------------------->

![](img/c2a1e71265adc8d894f34e8778263abd.png)

照片由[马克·达弗尔](https://unsplash.com/@2mduffel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/rules-and-regulations?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄

**正则化**是一种惩罚复杂模型的原则，这样它们可以更好地推广。它可以防止过度拟合。在这个博客中，我们将访问常见的正则化技术。

> 你的神经网络只和你输入的数据一样好。

## 数据扩充

深度学习神经网络的性能通常会随着可用数据量的增加而提高。但是我们通常没有大量的数据。数据扩充是一种从现有训练数据中人工创建新训练数据的技术。根据我们何时应用这些变换，我们有两种类型的增强:

**在线—** 预先执行所有必要的转换

**离线—** 在将小批量产品送入 ML 模型之前，对其进行转换

**图像数据增强的例子**

> PCA
> 翻转、旋转
> 裁剪、缩放

有条件的 GANs 或风格转移也可以用来生成更多的数据。

**自然语言处理数据扩充示例**

> 同义词替换
> 随机插入/删除
> 单词嵌入

**数值数据示例**

> 重击

## 拒绝传统社会的人

Dropout 是一种正则化技术，在训练期间将随机选择的神经元的激活值归零。在神经网络中，每一层都实现了丢失。不同的图层类型应有不同的漏失。

注意:退出将增加收敛所需的迭代次数。但是由于它减少了计算，每次迭代都更快。

## 全体

打包——结合优秀的学习者，平滑他们的预测

Bagging 是一种减少预测方差的方法，它通过使用重复组合从数据集生成用于训练的附加数据，以产生多组原始数据。通过增加训练集的大小，您只是减少了方差，将预测调整到预期的结果。《出埃及记》**随机森林**

提升——将弱学习者组合成强学习者

升压是减少偏差的一种方式。子集的创建不是随机的，而是取决于先前模型的性能。每个新的子集都包含被以前的模型错误分类的元素。《出埃及记》 **XGBoost**

## 提前停止

在模型建立期间，在每个时期之后，在单独的验证数据集上评估模型。如果验证数据集上的模型的验证损失(或任何其他度量)开始下降，则训练过程停止。这个过程叫做提前停止。如果模型在当前数据集上的性能比前一时期更好，则我们保存模型权重。最后，选择具有最佳性能的权重。这里重要的一点是选择最能定义模型性能的正确的性能指标。

早停可以通用。

## 添加噪声

噪声的添加具有正则化效果，并且反过来提高了模型的鲁棒性。噪声(高斯噪声)阻止模型记忆训练样本。一般在输入层加噪声，但不限于。它可以随机添加到网络的其他部分。

> **激活** —在深度网络中有用
> **权重—** 对 RNNs 有用。
> **梯度** —适用于深度全连接网络

注意:在添加噪声之前，应该对相关参数进行缩放。

## **批量归一化**

隐藏层激活的值在训练过程中会发生变化。在批处理规范化中，我们通过使用当前批处理中值的平均值和标准差(或方差)来规范化每个图层的输入。基本上，我们正在进行标准化，不仅是在开始，而是在整个网络。每个小批量使用其平均值和标准偏差进行缩放。这给每个层引入了一些噪声，提供了一种正则化效果。

## L1(拉索)正则化

当要素数量较多时，L1 正则化是一个很好的选择，L1 提供了一个稀疏的解决方案，即移除要素。它在计算上不太昂贵。

## L2(岭)正则化

相互依赖倾向于增加系数方差，使系数不可靠/不稳定，这损害了模型的通用性。性别和怀孕特征对是相互依赖特征的一个例子。L2 减少了这些估计的方差，抵消了相互依存的影响。