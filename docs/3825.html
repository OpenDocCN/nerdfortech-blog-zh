<html>
<head>
<title>Types of Optimization Algorithm used to Train Neural Network.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于训练神经网络的优化算法类型。</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/types-of-optimization-algorithm-used-to-train-neural-network-ea3becc2222a?source=collection_archive---------17-----------------------#2021-06-25">https://medium.com/nerd-for-tech/types-of-optimization-algorithm-used-to-train-neural-network-ea3becc2222a?source=collection_archive---------17-----------------------#2021-06-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0fa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化器是一种算法或方法，用于改变神经网络的属性，如权重和学习速率，以减少损失。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/3866219e77f524389c776b22037daa47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KJWwaa4yO28vD9AX.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图1:达到全局最小值。</figcaption></figure><p id="3d30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你应该如何改变你的神经网络的权重或学习速率来减少损失是由你使用的优化器定义的。优化算法或策略负责减少损失，并尽可能提供最准确的结果。</p><p id="f527" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将了解不同类型的优化器及其优势:</p><h2 id="704d" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">梯度下降:</h2><ul class=""><li id="59a8" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">梯度下降是最常用的算法，它相当常用于线性和逻辑回归技术。</li><li id="fad9" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">也被称为万尼拉梯度下降。</li><li id="9735" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">梯度下降是一阶导数，其中它在神经网络中反向传播，以通过优化权重(w)和偏差(b)来最小化损失。</li></ul><p id="24ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">算法:<strong class="ih hj"> θ=θ−α⋅∇J(θ) </strong></p><p id="928f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优势:</strong></p><ul class=""><li id="b50d" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">非常容易实现</li><li id="380f" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">简单易懂的方法</li></ul><p id="8a3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>:</p><ul class=""><li id="34bb" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">需要整个数据集来获得全局最小值，这需要大量的计算能力和耗时的过程。</li><li id="17c2" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">它可能在达到全局最小值之前就陷入局部最小值。</li></ul><p id="2f11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机梯度下降:</strong></p><ul class=""><li id="d680" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">它优化了训练数据集中每一行的权重和损失。如果数据有1000行，它将在一个周期内改变为每1000行计算的权重和偏差，而梯度下降需要整个数据，因此它需要较少的计算能力，但消耗更多的运行时间。</li></ul><p id="8279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">θ=θ−α⋅∇j(θ；x(一)；y(i))，其中{x(i)，y(i)}是训练示例</strong></p><p id="d5d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优势:</strong></p><ul class=""><li id="dec7" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">它需要更少的计算能力。</li></ul><p id="4461" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>:</p><ul class=""><li id="cd86" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">因为它是对每一行进行优化，所以需要更多的时间来收敛到全局最小值。</li><li id="1d71" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">它可能会达到局部最小值。</li><li id="33db" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">它在达到全局最小值时有更多的噪声。</li></ul><h2 id="8782" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">小批量梯度下降:</h2><ul class=""><li id="6de7" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">它是所有梯度下降算法中最好的。这是对SGD和标准梯度下降的改进。它会在每次批处理后更新模型参数。因此，数据集被分成不同的批次，在每一批次之后，参数被更新。</li></ul><p id="2990" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">θ=θ−α⋅∇j(θ；B(i))，其中{B(i)}为训练样本的批次</strong>。</p><p id="13f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点</strong>:</p><ul class=""><li id="1765" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">SGD达到全局极小值所需的时间更少。</li><li id="29c0" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">它需要中等的计算能力。</li></ul><p id="afa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>:</p><ul class=""><li id="2395" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">虽然它消耗的时间比SGD少，但通常它需要更多的时间来优化参数。</li><li id="053d" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">它有更多的噪音。</li></ul><h2 id="ac4e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated"><strong class="ak">气势如虹的新币:</strong></h2><ul class=""><li id="641d" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">具有动量的SGD是帮助在正确的方向上加速梯度向量的方法，从而导致更快的收敛。</li><li id="b729" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">SGD和小批量SGD的主要缺点是噪声，因为它是以批量方式计算的，所以波动较大，所以我们转向动量SGD。</li><li id="6718" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">它使用指数加权移动平均来避免计算梯度下降时的噪声。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lh"><img src="../Images/e5fff14393262b0c20e4fdd4cc6c2bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IYRMeLKLRHckwenC.png"/></div></div></figure><p id="7d08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点</strong>:</p><ul class=""><li id="738a" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">减少参数的振荡和高方差。</li><li id="4d53" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">在所有GD算法中，它消耗的时间较少。</li></ul><p id="df9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">所有类型的梯度下降都有一些挑战:</strong></p><ol class=""><li id="b44e" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc li kw kx ky bi translated">选择学习率的最佳值。如果学习率太小，梯度下降可能需要很长时间才能收敛。</li><li id="541f" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc li kw kx ky bi translated">对所有参数都有一个恒定的学习率。可能有些参数我们不想以同样的速度改变。</li><li id="80a9" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc li kw kx ky bi translated">可能会陷入局部最小值。</li></ol><h2 id="c10d" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated"><strong class="ak"> AdaGrad(自适应渐变):</strong></h2><ul class=""><li id="30c5" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">所有梯度优化算法的主要缺点之一是定义学习率，该学习率对于每个周期是恒定的。</li><li id="5962" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">AdaGrad的关键思想是为每个权重设定一个自适应的学习速率。</li><li id="a52b" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">权重的学习速率将随着迭代次数而降低。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/73c3fe5b7ec0d09043cfde5ca7fc84cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/0*-o0OfE2dtF6CoV-S.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">给定参数在给定时间t的损失函数的导数。</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/bb9e50d1ab8daba51e96adc2372cc538.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*lc8Gzkzz2QUAgZf4.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">为给定输入I和在时间/迭代t更新参数</figcaption></figure><p id="4c76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点</strong>:</p><ul class=""><li id="7673" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">它在每次迭代中自适应地更新学习率，我们不需要提及参数。</li><li id="4b81" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">它适用于稀疏数据。</li></ul><p id="c989" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>:</p><ul class=""><li id="00cf" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">当迭代次数变得非常大时，学习率降低到非常小的数值，这导致收敛缓慢。</li><li id="c929" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">计算昂贵的过程，因为它涉及大量的数学计算</li></ul><h2 id="aab6" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">Adadelta:</h2><ul class=""><li id="a953" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">它是AdaGrad的扩展，旨在消除AdaGrad的学习率衰减问题。<strong class="ih hj"> <em class="ll"> Adadelta </em> </strong>将累积的过去梯度的窗口限制为某个固定大小<strong class="ih hj"> w </strong>，而不是累积所有先前平方的梯度。在此，使用指数移动平均值，而不是所有梯度的总和。</li></ul><p id="9973" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> E[g ](t)=γ。e[g](t1)+(1γ)。克(特)</strong></p><p id="c5b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优势</strong>:</p><ul class=""><li id="f33b" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">现在学习速度不衰减，训练不停止。</li></ul><p id="3053" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>:</p><ul class=""><li id="fe56" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">计算开销很大。</li></ul><p id="dd27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adadelta和Rmsprop彼此相似，在Adam加入之前工作得很好。</p><h2 id="5d9e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">亚当:</h2><ul class=""><li id="7249" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">自适应矩估计(Adam)适用于一阶和二阶动量。</li><li id="97d8" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">Adam背后的直觉是，我们不希望滚动得太快，因为我们可以跳过最小值，我们希望稍微降低速度，以便仔细搜索。</li><li id="47a3" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">除了存储像<strong class="ih hj"> AdaDelta </strong>、<strong class="ih hj">、<em class="ll"> Adam </em>、</strong>、<em class="ll">、</em>这样的过去平方梯度的指数衰减平均值之外，还保存过去梯度的指数衰减平均值<strong class="ih hj"> M(t)。</strong></li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/201028b7c224d729f0b6beded23e48ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/0*RO6FokQAAsYTLryT.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">优化公式</figcaption></figure><p id="2d58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点</strong>:</p><ul class=""><li id="a44e" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">快速收敛，克服了AdaGrad的缺点</li><li id="1823" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">纠正消失学习率，高方差。</li></ul><p id="6051" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>:</p><ul class=""><li id="de68" class="ko kp hi ih b ii ij im in iq le iu lf iy lg jc kv kw kx ky bi translated">计算成本高。</li></ul><h2 id="4f06" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">各种优化器之间的比较:</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/def46bd05d0fffe27f98182c6ac2137c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*3VcdNTs5IssQLxjf.gif"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">不同优化算法的比较</figcaption></figure><h2 id="7a41" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">结论:</h2><p id="0f82" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">→亚当是最好的优化者。如果一个人想在更短的时间内比亚当更有效地训练神经网络，那么他就是优化者。</p><p id="f69b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→它是最好的、广泛使用的优化器</p><p id="5cfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望你们喜欢这篇文章，并且能够对不同优化算法的不同行为有一个很好的直觉。</p></div></div>    
</body>
</html>