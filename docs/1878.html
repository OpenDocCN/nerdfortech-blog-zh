<html>
<head>
<title>Review — CenterNet: Keypoint Triplets for Object Detection (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾—中心网:用于对象检测的关键点三元组(对象检测)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-centernet-keypoint-triplets-for-object-detection-object-detection-26feee780efc?source=collection_archive---------6-----------------------#2021-04-11">https://medium.com/nerd-for-tech/review-centernet-keypoint-triplets-for-object-detection-object-detection-26feee780efc?source=collection_archive---------6-----------------------#2021-04-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7e9d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><span class="l ix iy iz bm ja jb jc jd je di"> D </span>检测物体为关键点三元组，优于<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener"> RefineDet </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener"> CoupleNet </a>、<a class="ae jf" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-grf-dsod-grf-ssd-improving-object-detection-from-scratch-via-gated-feature-reuse-495c11b627d3" rel="noopener"> GRF-DSOD </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>、<a class="ae jf" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a>、<a class="ae jf" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>、<a class="ae jf" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank"> YOLOv2 </a>、<a class="ae jf" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank"> G-RMI </a>、<a class="ae jf" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------"/></h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/e50e7ffd86d8147fac13e00b6d922bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*JnC8qb2xGCBi_JqYWn17uA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js"> CenterNet:用于对象检测的关键点三元组</strong>(红色阴影区域是中心区域。)</figcaption></figure><p id="4e22" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi kp translated"><span class="l ix iy iz bm ja jb jc jd je di">在</span>这个故事中，回顾了由中国科学院大学、华中科技大学、华为诺亚方舟实验室、中国科学院和程鹏实验室合作的<strong class="jv hj">CenterNet:key point Triplets for Object Detection</strong>(CenterNet)。在这个故事中:</p><ul class=""><li id="2dd5" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">中心网<strong class="jv hj">将每个物体检测为三个一组的关键点</strong>，而不是像角落网那样的一对关键点，这提高了精确度和召回率。</li><li id="bcee" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">两个定制模块<strong class="jv hj">级联角池</strong>和<strong class="jv hj">中心池</strong>，这两个模块<strong class="jv hj">丰富了由左上角和右下角</strong>和<strong class="jv hj">收集的信息，提供了来自中心区域</strong>的更多可识别信息。</li></ul><p id="da8b" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">这是一篇在<strong class="jv hj"> 2019 ICCV </strong>超过<strong class="jv hj"> 380 次引用</strong>的论文。(<a class="le lf ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----26feee780efc--------------------------------" rel="noopener" target="_blank">曾植和</a> @中)</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="82c7" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">概述</h1><ol class=""><li id="420a" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko mj kw kx ky bi translated"><strong class="jv hj">中心网:总体流程</strong></li><li id="0c05" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">规模感知中心区域探测(CRE) </strong></li><li id="4ca7" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">中心池(CTP) </strong></li><li id="446e" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">梯级转角汇集(CCP) </strong></li><li id="0196" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">实验结果</strong></li></ol></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="38d6" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 1。中心网:整体程序</strong></h1><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mk"><img src="../Images/c5a4077831ae0d90e44052ddf14467b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sx16jnNjC-SlF1PjmOu7uA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js"> CenterNet:网络架构</strong></figcaption></figure><h2 id="fe69" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">1.1.网络体系结构</h2><ul class=""><li id="6af8" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko kv kw kx ky bi translated"><a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">角网</a>用作基线。因此<strong class="jv hj">主干</strong>类似于<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">角网</a>。 <a class="ae jf" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank"> <strong class="jv hj">纽厄尔 ECCV【16】</strong></a><strong class="jv hj"/>中使用的<strong class="jv hj">堆叠沙漏网络以 52 层和 104 层为骨干。</strong></li><li id="a7ab" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>为每个对象检测一对角。</li><li id="159c" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">相比之下，对于 CenterNet，<strong class="jv hj">每个对象使用一个中心关键点和一对角。</strong></li><li id="ccae" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><strong class="jv hj">基于<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">角网</a>嵌入中心关键点</strong>的热图，并预测中心关键点的偏移量。</li></ul><h2 id="34f5" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">1.2.总体程序</h2><ul class=""><li id="b6d7" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko kv kw kx ky bi translated">一般检测步骤如下:</li></ul><ol class=""><li id="0233" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko mj kw kx ky bi translated">根据得分选择<strong class="jv hj">-<em class="nd">k</em>个中心关键点</strong>。</li><li id="09d2" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">相应的偏移量用于将这些中心关键点重新映射到输入图像。</strong></li><li id="c352" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated">为每个边界框定义一个中心区域，并检查该中心区域是否包含中心关键点。并且<strong class="jv hj">中心区域和中心关键点的类别应该相同。</strong></li><li id="29bc" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">如果在中心区域检测到中心关键点，则保留边界框。</strong>边界框的分数由三重点的平均分数代替。</li><li id="2140" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko mj kw kx ky bi translated"><strong class="jv hj">如果在中心区域没有检测到中心关键点，边界框将被移除。</strong></li></ol><ul class=""><li id="dfbd" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">(因为整个程序是基于<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>。如果有兴趣，请先阅读<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>。)</li></ul><h2 id="7559" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">1.3.损失函数</h2><ul class=""><li id="cc9d" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko kv kw kx ky bi translated">为了训练网络，使用类似于<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"><strong class="jv hj">CornerNet</strong></a><strong class="jv hj">的</strong>损失</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ne"><img src="../Images/fc9787fbbed6a75759674ec47ae1f894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*JVBjjuOL4yJFt4xXI8WGhw.png"/></div></figure><ul class=""><li id="0505" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">其中<em class="nd"> co </em>表示拐角，<em class="nd"> ce </em>表示中心。</li><li id="6185" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">损耗基本相同，除了<strong class="jv hj">有与中心关键点相关的附加项。</strong></li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="97da" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">2.规模感知中心区域探索(CRE)</h1><ul class=""><li id="9fa8" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko kv kw kx ky bi translated">边界框中中心区域的大小会影响检测结果。</li><li id="9578" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">小的中心区域导致小边界框的低召回率，而大的中心区域导致大边界框的低精度。</li></ul><blockquote class="nf ng nh"><p id="917c" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">一个比例敏感的中心区域，以适应边界框的大小。</p><p id="a92c" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">比例感知中心区域倾向于<strong class="jv hj">为小边界框</strong>生成相对较大的中心区域，而<strong class="jv hj">为大边界框</strong>生成相对较小的中心区域。</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nl"><img src="../Images/881393c78a49035ed8ea1f02db504132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0q0J6S2Ck1Y_8Ouv9kiWKQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js"> (a)当 n = 3 时中央区域为小包围盒。(b)对于大边界框，当 n = 5 时的中心区域。</strong></figcaption></figure><ul class=""><li id="0b9c" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">对于包围盒<em class="nd"> i </em>，让<em class="nd"> tlx </em>和<em class="nd"> tly </em>表示<em class="nd"> i </em>的左上角坐标，<em class="nd"> brx </em>和<em class="nd"> bry </em>表示<em class="nd"> i </em>的右下角坐标。</li><li id="1aec" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">对于一个中心区域<em class="nd"> j </em>，让<em class="nd"> ctlx </em>和<em class="nd">简洁地表示<em class="nd"> j </em>左上角的坐标，<em class="nd"> cbrx </em>和<em class="nd"> cbry </em>表示<em class="nd"> j </em>右下角的坐标。</em></li><li id="3f52" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">则<em class="nd"> tlx </em>、<em class="nd"> tly </em>、<em class="nd"> brx </em>、<em class="nd"> bry </em>、<em class="nd"> ctlx </em>、<em class="nd">ctlx</em>、<em class="nd"> cbrx </em>和<em class="nd"> cbry </em>应满足以下关系:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nm"><img src="../Images/f814bbd7ca099f2f9924771e0a1f7e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*zT5dBLEdrl6u2Whkl32Vaw.png"/></div></figure><ul class=""><li id="bc1f" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">其中<em class="nd"> n </em>为奇数，决定中心区域<em class="nd"> j </em>的比例。</li><li id="4794" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">本文中，<em class="nd"> n </em>对于小于和大于 150 的包围盒比例分别设为 3 和 5，如上图。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="4652" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">3.中心池</h1><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nn"><img src="../Images/863f2248b49d77488da9b86d88b9846c.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*F93sVxeLdGCQLUm7bMuxSg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">中心汇集取水平和垂直方向的最大值。</strong></figcaption></figure><ul class=""><li id="2848" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">物体的几何中心并不总是传达非常容易辨认的视觉模式。(例如，人的头部包含强烈的视觉图案，但是中心关键点通常在人体的中间。)</li><li id="393c" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><strong class="jv hj">中心池用于捕捉更丰富、更易识别的视觉模式。</strong></li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es no"><img src="../Images/7720ae16b419c1204c49bcba57df539a.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*SYl_nStQjPfpIZdlgzNgDA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">中心统筹</strong></figcaption></figure><ul class=""><li id="7b16" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">主干输出特征图，并确定特征图中的像素是否是中心关键点。</li><li id="5f80" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><strong class="jv hj"> CenterNet 找到水平和垂直方向的最大值，并将这些值相加。</strong></li><li id="6ec6" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">通过这样做，<strong class="jv hj">中心汇集有助于改进中心关键点的检测。</strong></li></ul><blockquote class="nf ng nh"><p id="caf7" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">在上面的示例中，头部和肩部在特征图上可能具有较大的值，CenterNet 找到它们，并将它们加在一起作为中心热图。</p></blockquote><ul class=""><li id="7132" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">执行这些步骤是为了生成中心热图。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="83d0" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">4.级联角池(CCP)</h1><h2 id="48fe" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">4.1.<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">角池<strong class="ak">角网</strong>角网</a></h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es np"><img src="../Images/5ecf21448a49f8d3a72280008da1dfb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*45RurfoyRA_t6mE2lkCTZA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> <strong class="bd js">角网</strong> </a> <strong class="bd js">:角池只取边界方向的最大值</strong></figcaption></figure><ul class=""><li id="e935" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">角点往往在物体外部，缺乏局部外观特征。这个问题也发生在<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>中。</li></ul><blockquote class="nf ng nh"><p id="0db4" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">例如，头部和肩部的边界可能分别具有比头部和肩部的中心更弱的特征响应。</p></blockquote><h2 id="d009" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">4.2.提议的级联角池(CCP)</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nq"><img src="../Images/4cb6ed395abb863aed33914f850600d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*pZBCRE_VZX_ckb40CON7DQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">提出的级联角池在对象的边界方向和内部方向都取最大值</strong></figcaption></figure><ul class=""><li id="3f4e" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">为了解决这个问题，<strong class="jv hj"> CenterNet 使角点能够从中心区域提取特征</strong>，如上图所示。</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nr"><img src="../Images/48e559cbb7fbe668cfb730de4eb161fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*BbF1K5JRmeisKNin3yZY5A.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">级联角池</strong></figcaption></figure><ul class=""><li id="156c" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">级联角池<strong class="jv hj">首先沿着边界寻找最大边界值</strong>，然后<strong class="jv hj">沿着边界最大值</strong>的位置在框内寻找内部最大值。</li><li id="64cb" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">最后，将两个最大值<strong class="jv hj">相加</strong>。</li></ul><blockquote class="nf ng nh"><p id="4df0" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">通过级联角点池，角点获得对象的边界信息和视觉模式。</p></blockquote></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="b8a5" class="ln lo hi bd js lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">5.实验结果</h1><h2 id="5d3a" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">5.1.消融研究</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es ns"><img src="../Images/715822b5272d360ee6435b5ab51efc89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VK3nmb5kk8Ux-TdvqVPvA.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">在 MS-COCO 验证数据集上对 centernet 511–52 的主要组件进行消融研究</strong></figcaption></figure><ul class=""><li id="e00d" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">测试了 CenterNet 对象检测的三个组件，即<strong class="jv hj">中心区域探测(CRE)、中心池(CTP)和级联角池(CCP) </strong>。</li><li id="9b52" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">基线是角网 511–52。</li><li id="4eb7" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated"><strong class="jv hj">全部启用时，可获得最佳结果。</strong></li></ul><h2 id="206f" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">5.2.与 SOTA 方法的比较</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nt"><img src="../Images/07e1c7eb17dd0647f910a4e2f8187f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9MtKL2rFf4ZKpC8WmYFGbw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">在 MS-COCO 测试开发数据集上与最先进方法的性能比较(%)</strong></figcaption></figure><ul class=""><li id="5331" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated"><strong class="jv hj"> CenterNet511-52 </strong>(表示输入图像的分辨率为 511 × 511，主干为沙漏-52)报告了<strong class="jv hj">单尺度</strong>测试 AP 为<strong class="jv hj"> 41.6% </strong>，比 37.8%提高了 3.8%<strong class="jv hj">多尺度</strong>测试 AP 为<strong class="jv hj"> 43.5% </strong>，比 39 提高了 4.1%</li><li id="608c" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">使用<strong class="jv hj">更深主干</strong>(即沙漏-104)时，在<strong class="jv hj">单尺度</strong>和<strong class="jv hj">多尺度</strong>测试下，AP 相对于<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>的提升分别为 4.4%(从 40.5%到<strong class="jv hj"> 44.9% </strong>)和 4.9%(从 42.1%到<strong class="jv hj"> 47.0% </strong>)。</li></ul><blockquote class="nf ng nh"><p id="abac" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">这些结果证明了 CenterNet 的有效性。</p></blockquote><ul class=""><li id="1d2d" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">另外，<strong class="jv hj">最伟大的来自小物件。</strong>例如，<strong class="jv hj">centernet 511–52 将小物体的 AP 提高了 5.5%(单尺度)和 6.4%(多尺度)。</strong></li><li id="c0cd" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">对于主干沙漏-104，改进分别为 6.2%(单尺度)和 8.1%(多尺度)。</li></ul><blockquote class="nf ng nh"><p id="fbaf" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">这一好处源于由中心关键点建模的中心信息。</p><p id="b707" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">并且 CenterNet 的表现优于<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener"> RefineDet </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener"> CoupleNet </a>、<a class="ae jf" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-grf-dsod-grf-ssd-improving-object-detection-from-scratch-via-gated-feature-reuse-495c11b627d3" rel="noopener"> GRF-DSOD </a>、<a class="ae jf" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>、<a class="ae jf" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank"> DSSD </a>、<a class="ae jf" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>、<a class="ae jf" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank"> YOLOv2 </a>、<a class="ae jf" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank"> G-RMI </a>、<a class="ae jf" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------"> TDM </a>、<a class="ae jf" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank">FPN</a><a class="ae jf" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"/></p></blockquote><h2 id="77d8" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">5.3.推理速度</h2><ul class=""><li id="136f" class="kq kr hi jv b jw me jz mf kc mg kg mh kk mi ko kv kw kx ky bi translated"><a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"><strong class="jv hj">corner net</strong></a><strong class="jv hj">511–104</strong>每张图像的平均推断时间为<strong class="jv hj"> 300ms </strong>，centernet 511–104 每张图像的平均推断时间为<strong class="jv hj"> 340ms </strong>。</li><li id="807c" class="kq kr hi jv b jw kz jz la kc lb kg lc kk ld ko kv kw kx ky bi translated">同时，使用沙漏-52 主干可以加快推理速度。<strong class="jv hj">centernet 511–52</strong>处理每张图像平均需要<strong class="jv hj"> 270ms </strong>，比<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"><strong class="jv hj">CornerNet</strong></a><strong class="jv hj">511–104<strong class="jv hj">更快更准确。</strong></strong></li></ul><h2 id="2ef8" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">5.4.不正确的边界框缩小</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nu"><img src="../Images/0de6a462e5c016df7748d500fd55fca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*BHwXuhXTuxazxjL05GYK6Q.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">MS-COCO 验证数据集上</strong><a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"><strong class="bd js">corner net</strong></a><strong class="bd js">和 CenterNet 的误发现率(%)比较。</strong></figcaption></figure><ul class=""><li id="9f9e" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated"><a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener"> CornerNet </a>即使在 IoU = 0.05 的阈值下也会产生许多不正确的边界框，即<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">corner net</a>511–52 和<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">corner net</a>511–104 分别获得 35.2%和 32.7%的 FD 率。</li></ul><blockquote class="nf ng nh"><p id="05f0" class="jt ju nd jv b jw jx ij jy jz ka im kb ni kd ke kf nj kh ki kj nk kl km kn ko hb bi translated">CenterNet 通过探索中心区域降低了所有标准下的 FD 发生率。</p></blockquote><ul class=""><li id="c868" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">小边界框的 FD 率下降最多，centernet 511–52 下降了 9.5%，centernet 511–104 下降了 9.6%。</li></ul><h2 id="4ebf" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">5.5.误差分析</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nv"><img src="../Images/544fa3e30fe30c401e7eafadec91f553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*bafL4Pqb3-zZORIVTC3hBg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">使用地面真值的中心关键点的误差分析。</strong></figcaption></figure><ul class=""><li id="1c4f" class="kq kr hi jv b jw jx jz ka kc ks kg kt kk ku ko kv kw kx ky bi translated">随着中心关键点的地面真实值的使用，AP 增加了很多，这意味着仍有改进的空间。</li></ul><h2 id="3f9e" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">5.6.定性结果</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nw"><img src="../Images/70a26bf9c6204d261556cacb0512bd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_BkyJ8hE3ioP1TfktoBrw.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><strong class="bd js">MS-COCO 验证数据集上的定性检测结果。仅显示分数高于 0.5 的检测。</strong></figcaption></figure></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="e127" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">参考</h2><p id="a4ee" class="pw-post-body-paragraph jt ju hi jv b jw me ij jy jz mf im kb kc nx ke kf kg ny ki kj kk nz km kn ko hb bi translated">【2019 ICCV】【CenterNet】<br/><a class="ae jf" href="https://arxiv.org/abs/1904.08189" rel="noopener ugc nofollow" target="_blank">CenterNet:用于对象检测的关键点三元组</a></p><h2 id="c137" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated">目标检测</h2><p id="00fd" class="pw-post-body-paragraph jt ju hi jv b jw me ij jy jz mf im kb kc nx ke kf kg ny ki kj kk nz km kn ko hb bi translated"><strong class="jv hj"> 2014 </strong> : [ <a class="ae jf" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------">过食</a>][<a class="ae jf" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------">R-CNN</a>]<br/><strong class="jv hj">2015</strong>:[<a class="ae jf" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快 R-CNN </a> ] [ <a class="ae jf" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">快 R-CNN</a>][<a class="ae jf" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae jf" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a><br/><strong class="jv hj">2016 [<a class="ae jf" href="https://towardsdatascience.com/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac?source=post_page---------------------------" rel="noopener" target="_blank">GBD-网/GBD-v1&amp;GBD-v2</a>][<a class="ae jf" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae jf" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a><br/><strong class="jv hj">2017</strong>:[<a class="ae jf" rel="noopener" href="/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a?source=post_page---------------------------">NoC</a>][<a class="ae jf" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae jf" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------">TDM</a>[<a class="ae jf" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>[<a class="ae jf" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 900] [</a><a class="ae jf" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener">couple net</a>]<br/><strong class="jv hj">2018</strong>:[<a class="ae jf" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolov 3</a>][<a class="ae jf" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">Cascade R-CNN</a>][<a class="ae jf" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae jf" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>][<a class="ae jf" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener">refined et</a>][<a class="ae jf" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">corner net</a>]【T78</strong></p><h2 id="a352" class="mp lo hi bd js mq mr ms ls mt mu mv lw kc mw mx ly kg my mz ma kk na nb mc nc bi translated"><a class="ae jf" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>