<html>
<head>
<title>Linear Regression From Scratch PT2: The Gradient Descent Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始线性回归 PT2:梯度下降算法</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/linear-regression-from-scratch-pt2-the-gradient-descent-algorithm-f30d42fea40c?source=collection_archive---------6-----------------------#2021-04-25">https://medium.com/nerd-for-tech/linear-regression-from-scratch-pt2-the-gradient-descent-algorithm-f30d42fea40c?source=collection_archive---------6-----------------------#2021-04-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5c99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我上一篇关于线性回归的<a class="ae jd" href="https://mardiyyah.medium.com/linear-regression-from-scratch-pt1-484ff41e5a3" rel="noopener">文章</a>中，我简要介绍了线性回归、直觉、假设以及两种最常用的解决线性回归问题的方法。前一篇文章关注其中一种方法；“封闭形式的解决方案(分析方法)”。在本文中，重点将放在第二种方法:“数值方法”，具体来说，一种类型的数值方法称为“<strong class="ih hj">梯度下降(GD 算法)</strong>”。</p><h1 id="2a34" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">用数值方法求解线性回归</h1><p id="c564" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在现实生活中，大多数问题(或者至少是我们对机器学习感兴趣的问题)没有封闭形式解决方案所提出的精确解决方案。除此之外，使用封闭形式的公式可能会导致一些复杂情况(如下图所示)。其中一个问题是当 X 是大样本时，计算的复杂性和求逆矩阵(X.T.X)的费用。另一个关注点是矩阵不可逆或不存在的可能情况(<em class="kh">是的，这种情况会发生！！参考下面的材料，了解为什么</em></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="ab fe cl kn"><img src="../Images/c0204c306d7b2e863670124531438a20.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Bt1LjOBnr9vkwPTotwM2yA.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">封闭形式的方程。来源:<a class="ae jd" href="https://cdn-images-1.medium.com/max/498/1*Bt1LjOBnr9vkwPTotwM2yA.png" rel="noopener">此处</a></figcaption></figure><p id="8333" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，与“封闭形式的解决方案”不同，数值方法有一个权重的更新规则(θ⍬).它使用迭代过程来寻找使成本函数最小化的最佳θ值，或者损失最小的最佳θ值。我们可以称之为“优化”。</p><blockquote class="ku kv kw"><p id="a2eb" class="if ig kh ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated">“优化是机器学习的核心”。寻找给定问题的最佳/最优解决方案。</p></blockquote><p id="17ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有几种数值方法用于线性回归。然而，我将把重点放在优化技术的“梯度下降”类。在本文中，我们将只探讨一种变体(批量梯度下降(BGD))，并在后续文章中更多地讨论其他变体。</p><p id="7297" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度下降作为优化算法</strong></p><p id="f2d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是一种迭代一阶优化技术，用于最小化函数。它寻找凸函数的全局最小值(<em class="kh">哎呀！！行话。请参考资料以了解这些术语</em>。它假设函数是凸的。</p><blockquote class="ku kv kw"><p id="43c7" class="if ig kh ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated">如果一个函数的二阶导数大于或等于零(0)，则称该函数是“凸的”。即 f ⎖⎖( x) ≥ 0。如果-f ⎖⎖( x) ≥ 0，那么它是一个凹函数(上升)</p></blockquote><p id="3074" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是机器学习中使用的最流行的优化技术之一(尤其是在深度学习领域)。已经发表了几篇研究论文，证明这种算法对大多数问题都是最好的(至少就我们目前所知)。</p><p id="8e73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它被认为是一种自然算法，在成本函数的最陡下降方向上重复采取步骤</p><p id="2414" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是如何工作的？</p><p id="fa99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，对于数值方法，使用迭代过程，使得模型的权重(θ)在每次迭代时递增地(逐步地)更新，试图找到损失最小的最佳值。</p><p id="0672" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于梯度下降，假设凸函数，损失函数相对于权重被微分以计算梯度。这个梯度乘以一个参数(⍺-学习率或步数)并从先前的权重值中减去。(<em class="kh">毕竟是下降</em>)。在每一步(迭代)计算损失。这个过程重复迭代直到收敛。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es la"><img src="../Images/0aa8598e6bebd34638bbdd7081a43dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JL_gHyy05McHQ7m9.jpg"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">来源:<a class="ae jd" href="https://i.ytimg.com/vi/b4Vyma9wPHo/maxresdefault.jpg" rel="noopener ugc nofollow" target="_blank">图片</a></figcaption></figure><p id="64f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，等式右侧的 w 是新的θ值，左侧是先前/旧的θ值。</p><p id="5fc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习 rate(⍶)确定沿着斜坡要采取的步骤以实现目标。过大的步长会导致跳过或错过全局最小值点(也称为过冲)，而过小的步长会导致实现目标的过程非常缓慢。这是一个需要调整的超参数。在实践中，人们往往从 0.01 开始，并相应地减少或增加。</p><blockquote class="ku kv kw"><p id="8177" class="if ig kh ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated">所有问题的学习速度并不一致。一个值可能对一组问题有效，但对另一组问题无效。寻找最佳学习速率是机器学习中的一场持久战。</p></blockquote><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lf"><img src="../Images/639fe75fd63313710b9f16b732d817e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/0*CEvswZfGLuOlaH65.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">图片来源:Github</figcaption></figure><p id="c5b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度下降的变体</strong></p><p id="639f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降算法通常有三(3)种变体；</p><ul class=""><li id="e787" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc ll lm ln lo bi translated">批量梯度下降</li><li id="245e" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">随机梯度下降</li><li id="9053" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">小批量梯度下降</li></ul><p id="c188" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在批量梯度下降(这将是本文的代码实现用例)中，所有样本(X)在每次迭代中用于计算梯度和更新权重。对于大型数据集，这往往计算量很大，而且非常慢。这实际上是它的缺点之一(对于较大的样本量，计算速度(时间复杂度))。</p><p id="76b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个算法的伪代码是:</p><ul class=""><li id="6d93" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc ll lm ln lo bi translated">初始化权重的随机猜测(从θ的随机猜测开始)</li><li id="bb66" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">使用所有样本计算梯度(x)</li><li id="3ddb" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">使用更新规则更新权重</li><li id="07a7" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">重复直到收敛</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lu"><img src="../Images/39fd36ddd04b99e43cba409a7ca1d829.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*Nl6GziKrXrfBNI3q.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">来源:<a class="ae jd" href="https://medium.datadriveninvestor.com/the-math-and-intuition-behind-gradient-descent-13c45f367a11" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="034e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">太好了！现在让我们通过代码来看看这一点。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="fe7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">批量梯度下降算法的代码实现</strong></p><p id="6922" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用本系列第一部分中使用的相同数据(在这里找到链接(<a class="ae jd" href="https://mardiyyah.medium.com/linear-regression-from-scratch-pt1-484ff41e5a3" rel="noopener"> <em class="kh">将鼠标悬停在这里</em> </a>)。</p><p id="c2bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，将跳过数据导入、预处理等基本步骤。我们将直接构建批量梯度下降(BGD)模型。</p><p id="69c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤:</p><ol class=""><li id="d9de" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc mc lm ln lo bi translated">定义模型类和它将采用的超参数(纪元和学习率)</li></ol><blockquote class="ku kv kw"><p id="2b93" class="if ig kh ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated"><strong class="ih hj">时期</strong>是将整个数据传递一次所需的时间。这与<strong class="ih hj">迭代</strong>不同。在这里，这并不重要，因为我们在每一步都要考虑所有的数据样本。然而，当我们谈到梯度下降算法的其他变体时，我们会注意到两个术语(时期和迭代)之间的差异</p></blockquote><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es md"><img src="../Images/13ed593a36bb761e36a6cba5672b066f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fftKuK4u3PReffj1PB7oEw.png"/></div></div></figure><p id="968b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.定义训练方法及其将采用的参数(X 和 Y)</p><p id="147f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.I)使用 X 特征形状的随机变量和偏差初始化θ(其原因是θ和 X 之间的矩阵乘法。记住矩阵乘法需要<em class="kh"> nxm - &gt; mxn </em>条件。</p><blockquote class="me"><p id="ab17" class="mf mg hi bd mh mi mj mk ml mm mn jc dx translated">也请记住这一点，在可能出现“<strong class="ak"> matmul </strong>”错误的情况下，请检查正在执行矩阵乘法的变量的形状。手动计算将帮助你更快地调试，因为有时你可能需要转置一个矩阵，以便乘法是可能的。还要记住表示/矩阵的放置方式决定了乘法的计算方式。</p></blockquote><p id="42bd" class="pw-post-body-paragraph if ig hi ih b ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc hb bi translated">ii)计算梯度</p><p id="461c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iii)定义更新规则并更新 theta</p><p id="48af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iv)计算培训期间的损失</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mt"><img src="../Images/7398d62e79c0a92901654c3b43ab1d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQFzA84rd8GmDTWPkcs5Ow.png"/></div></div></figure><p id="efaf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您会注意到上面代码中的一些事情。名为“precision”的变量被添加到拟合函数将采用的参数集中。这是可选的，它有点像阈值，当我们想要设置收敛点并跳出循环时使用(注意设置阈值条件的代码行)。</p><p id="3256" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kh">提示:尝试使用带精度和不带精度的模型，查看差异</em> </strong></p><p id="b04b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.定义预测方法(与之前相同)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mu"><img src="../Images/5f2907f633847e58d8100461ee1bcd5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_83PHk4dIml0YgASrydIw.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">预测函数</figcaption></figure><p id="e88a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.根据测试数据计算 MSE</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mv"><img src="../Images/01f183b02359ba9c947775d08a9a80a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXnGafTo72fwfaq_6JGz4w.png"/></div></div></figure><p id="a82d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.用超参数初始化模型(设置学习率和选择的历元大小)并拟合数据。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mw"><img src="../Images/cbe4ffb6b00658249dcd639a4ca1a62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uBEG8X-l-q8r_2q0jWn0eA.png"/></div></div></figure><p id="83d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.根据测试数据进行预测。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mx"><img src="../Images/4cbf300a0886038ae1a11bc47c8a5c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*olOonw8CYV8d5qxwsXMJgg.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">根据测试数据进行预测</figcaption></figure><p id="cb89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.获得 MSE(测试损失)</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es my"><img src="../Images/2de11c3ebcf666a714d77b5f1edc65b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*UN3RnSB6blYpq2UVsHTgzg.png"/></div></figure><p id="930c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">太好了！您刚刚从头开始构建了一个批量梯度下降模型。恭喜你！！</p><p id="6406" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你喜欢读这篇文章，并有所收获。下一次，我们将使用相同的数据，但使用梯度下降算法的另一种变体-“<strong class="ih hj">随机梯度下降</strong>”。到时候见！感谢您的阅读！</p><p id="f1d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">找到下面代码的 GitHub 库链接；</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="2fb9" class="je jf hi bd jg jh mz jj jk jl na jn jo jp nb jr js jt nc jv jw jx nd jz ka kb bi translated">GITHUB 资源库链接</h1><div class="ne nf ez fb ng nh"><a href="https://github.com/Aminah92/Foundations-of-Machine-Learning/blob/main/Linear-Models/LINEAR%20REGRESSION%20FROM%20SCRATCH%20PT%202-%20Gradient%20Descent.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hj fi z dy nm ea eb nn ed ef hh bi translated">amina h92/机器学习基础</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">在 GitHub 上创建一个帐户，为 amina h92/机器学习基础开发做贡献。</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">github.com</p></div></div></div></a></div></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="bcc5" class="je jf hi bd jg jh mz jj jk jl na jn jo jp nb jr js jt nc jv jw jx nd jz ka kb bi translated"><strong class="ak">上一篇文章(从零开始线性回归 PT1) </strong></h1><div class="ne nf ez fb ng nh"><a href="https://mardiyyah.medium.com/linear-regression-from-scratch-pt1-484ff41e5a3" rel="noopener follow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hj fi z dy nm ea eb nn ed ef hh bi translated">从头开始线性回归 PT1</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">线性回归被认为是建模数据的最自然的学习算法，主要是因为它很容易…</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">mardiyyah.medium.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ko nh"/></div></div></a></div></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="8622" class="je jf hi bd jg jh mz jj jk jl na jn jo jp nb jr js jt nc jv jw jx nd jz ka kb bi translated">在社交媒体上与我联系</h1><p id="f030" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">推特:@diyyah92</p><p id="6b8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">领英:【https://www.linkedin.com/in/aminah-mardiyyah-rufa-i/ T2】</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="3d5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">资源和参考资料</p><div class="ne nf ez fb ng nh"><a href="https://stats.stackexchange.com/questions/278755/why-use-gradient-descent-for-linear-regression-when-a-closed-form-math-solution" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hj fi z dy nm ea eb nn ed ef hh bi translated">当封闭形式的数学解可用时，为什么使用梯度下降进行线性回归？</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">感谢您为交叉验证提供答案！请务必回答问题。提供详细信息并分享…</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">stats.stackexchange.com</p></div></div><div class="nq l"><div class="nw l ns nt nu nq nv ko nh"/></div></div></a></div><div class="ne nf ez fb ng nh"><a href="https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hj fi z dy nm ea eb nn ed ef hh bi translated">通过闭合形式方程拟合模型，梯度下降法，随机梯度下降法…</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">为了解释估计模型参数的不同方法之间的差异，让我们以…</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">sebastianraschka.com</p></div></div></div></a></div><div class="ne nf ez fb ng nh"><a href="https://towardsdatascience.com/understand-convexity-in-optimization-db87653bf920" rel="noopener follow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hj fi z dy nm ea eb nn ed ef hh bi translated">为什么凸性是最优化的关键</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">使用凸成本函数是很容易的</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nx l ns nt nu nq nv ko nh"/></div></div></a></div><div class="ne nf ez fb ng nh"><a href="https://vitalflux.com/local-global-maxima-minima-explained-examples/" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab dw"><div class="nj ab nk cl cj nl"><h2 class="bd hj fi z dy nm ea eb nn ed ef hh bi translated">举例说明局部和全局最小值-数据分析</h2><div class="no l"><h3 class="bd b fi z dy nm ea eb nn ed ef dx translated">在这篇文章中，你将通过插图和例子学习局部和全局最小值的概念…</h3></div><div class="np l"><p class="bd b fp z dy nm ea eb nn ed ef dx translated">vitalflux.com</p></div></div><div class="nq l"><div class="ny l ns nt nu nq nv ko nh"/></div></div></a></div></div></div>    
</body>
</html>