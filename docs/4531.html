<html>
<head>
<title>Demystifying Differentiation and Optimisers in Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭开神经网络中微分和优化器的神秘面纱</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/demystifying-differentiation-and-optimisers-in-neural-network-510c54f693c?source=collection_archive---------4-----------------------#2021-07-25">https://medium.com/nerd-for-tech/demystifying-differentiation-and-optimisers-in-neural-network-510c54f693c?source=collection_archive---------4-----------------------#2021-07-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="054f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">直观地理解为什么在神经网络中使用微分，并了解可用的不同优化器。</h2></div><p id="5164" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上一篇文章中，我谈到了可用的不同损失函数，并了解了梯度函数。梯度函数使用损失相对于权重的微分来计算网络必须如何改变权重以更接近预期输出。</p><p id="10a1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是我意识到我忽略了分化在帮助神经网络学习中的作用。因此，我将在这篇文章中这样做，并讨论我们可以为神经网络选择的不同优化器。剧透警告:使用亚当乐观者。</p><h1 id="5e27" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">区别</h1><p id="45be" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">微分是一个如此抽象的概念，以至于我们不知道为什么要使用它，也不知道为什么x的微分是2x而x是1。如果我们理解使用差异化背后的原因，我相信它不会显得令人生畏。</p><blockquote class="kq"><p id="3b72" class="kr ks hi bd kt ku kv kw kx ky kz js dx translated"><em class="la">在深度学习中使用微分的根本原因是为了找到曲线中非直线的任何一点的切线斜率。</em></p></blockquote><p id="537f" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">让我们解开那个句子。</p><p id="6969" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个<strong class="iz hj">斜率</strong>是一条直线的陡度。如果斜率为0，则线是水平的，这意味着线完全是平的。另一方面，如果直线是垂直的，则称该直线的斜率为无穷大。</p><p id="3d96" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">求直线的斜率很容易。选取任意两个点，并除以y坐标和x坐标之间的差值。现在，如果我们想知道直线上任意一点的斜率，我们可以确定它与直线的斜率相同，如下图所示。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lg"><img src="../Images/dc37a5a427e689d93c5728a6d59ff6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qD2u2eiiPlcCp_6U"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">红色线的斜率</figcaption></figure><p id="7aa2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，如果我们想求出曲线或圆上某一点的斜率，这个方程就不成立了。考虑下面的y = x的图形。</p><p id="00ef" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑直线上的4个点，A，B，C和D。A和B之间的斜率与C和d不同，这使得我们很难自信地说出曲线上任意一点的斜率。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lw"><img src="../Images/c8ac5ed3f26b19bbeaaf0702fc3cb57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*25nTUrz_XDGxiYVa"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">曲线的斜率</figcaption></figure><p id="3ab7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们了解如何找到曲线的斜率之前，我们首先需要知道为什么我们需要斜率。</p><p id="3a95" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你还记得<a class="ae lx" rel="noopener" href="/nerd-for-tech/never-forget-gradient-descent-and-loss-function-ever-again-e593936a3bf8">上一篇文章</a>中的梯度下降，我们的目标是尽可能减少损失。为此，我们需要到达曲线的底部。</p><p id="c61d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">难道我们不能选择值而不是让模型来选择吗？</p><p id="1ff4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">答案是否定的！如果你只有两个特征，就很容易画出一个二维图并选择最小值。但是，如果您的数据集中有100个或1000个要素，您需要一台计算机来执行这些计算，因为我们的大脑无法理解三维以外的任何东西。</p><h2 id="b3b2" class="lz ju hi bd jv ma mb mc jz md me mf kd jg mg mh kf jk mi mj kh jo mk ml kj mm bi translated">斜率如何帮助我们找到最佳值？</h2><p id="344d" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">斜坡告诉我们是否在正确的轨道上。如果当前点的斜率没有前一点陡，我们知道我们在正确的路上。类似地，如果斜率接近于零，我们可以有理由相信我们已经到达最低点，并且该点是我们模型的最佳可能值。</p><h2 id="ad8d" class="lz ju hi bd jv ma mb mc jz md me mf kd jg mg mh kf jk mi mj kh jo mk ml kj mm bi translated">我们如何找到曲线在一点的斜率？</h2><p id="1d1f" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">如果我们放大一个圆，在某一点上，我们可以看到曲线开始像一条直线。我们可以用这条直线求出曲线的斜率。这叫做曲线的<strong class="iz hj">切线</strong>。然而，这些点彼此非常接近，几乎相互重叠；我们对此表示如下:</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mn"><img src="../Images/01189b21f32137940e3b963b4b735adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/0*u8ErjcJLX6FF_blI"/></div></figure><p id="119a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以这样写</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mo"><img src="../Images/c8a3eab191d0f38bd6970db04d6adfd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/0*d-CyhVH5qpiYGqsU"/></div></figure><p id="d4a0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是没有任何限制的最简单形式的微分公式。如果你想详细了解差异化，可以查看这个<a class="ae lx" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="9e89" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要求曲线任一点的斜率，必须对曲线方程求微分，代入x的值。</p><p id="3af1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一些需要记住的简便公式。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mp"><img src="../Images/60d336ee45b345ad92fb7ad4313ebc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/0*aUH_k-kbEPT8tUL7"/></div></figure><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mq"><img src="../Images/a1e4c33d8338041afe7e048f53504543.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/0*mMf0hCq2KJK5KuGn"/></div></figure><p id="80a7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">常数微分为零，因为常数是一个点，而点没有斜率。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mr"><img src="../Images/5924d363ed7b8787246197e8f288d383.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/0*Fb3YkDRrX77Ub9wP"/></div></figure><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es ms"><img src="../Images/2bfcd08b985cfa3813e3e72bbacb4495.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/0*vpg0BS8-nXwO4rGO"/></div></figure><p id="3f1f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一条线的斜率是一个常数，因为它不变，这和我们之前的观察是一致的。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mt"><img src="../Images/e1c8207049dfc9efce83d8e0e0cd8553.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/0*bhFP_aDFKVgb40vd"/></div></figure><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mu"><img src="../Images/c42421b448dc1331f87af99d001dfd05.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/0*oLB8SfF3NHppSxoM"/></div></figure><p id="b1d2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">曲线在任一点的斜率都不是常数。因此，我们得到一个等式，可以代入x的值，求出该点的斜率。</p><p id="2505" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，要了解更多关于差异化的信息，您可以查看此<a class="ae lx" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="6de8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在对分化及其在神经网络中的作用有了足够好的理解。我们现在可以看看可以用来更新权重的各种优化器。</p><h1 id="f1e9" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">乐观主义者</h1><p id="7430" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">在我们使用优化器之前，等式的一个主要部分是de/dw，它是误差相对于权重的变化率，在<strong class="iz hj"> <em class="ly">反向传播</em> </strong>步骤中计算。它是高度数学密集型的，超出了本文的范围。如果你很好奇，可以去看看<a class="ae lx" href="https://www.youtube.com/watch?v=Ilg3gGewQ5U" rel="noopener ugc nofollow" target="_blank">这个</a>了解更多。</p><p id="38dc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">经过反向传播后，我们现在有了更新权重的所有原材料。最简单的方法是遵循<strong class="iz hj">随机梯度下降</strong>公式。内容如下。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mv"><img src="../Images/3307cefb9a5e92876856859a593d680e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DiEiBtajCafHxGyl"/></div></figure><p id="bfa5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所有其他优化技术给这个等式增加了其他东西。</p><p id="a345" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SGD方法的一个缺点是它的更新方向完全依赖于当前批处理，因此它的更新是不稳定的。解决这个问题的一个简单方法是引入动量。</p><h2 id="b332" class="lz ju hi bd jv ma mb mc jz md me mf kd jg mg mh kf jk mi mj kh jo mk ml kj mm bi translated">SDG +势头</h2><p id="6ab3" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">Momentum计算一组更新次数的梯度的滚动平均值，并将该平均值与每一步的单独梯度相结合。另一种思考方式是想象一个球滚下山坡；即使它遇到一个小洞或一座小山，动量也会推动它径直穿过小洞或小山，到达一个更低的最小值——山的底部。如果你陷入局部最小值(一个洞)并且来回跳动，这可能是有用的。有了动量，模型更有可能通过局部最小值，进一步降低损失。简单地说，动量可能仍然指向全球梯度下降的方向。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mw"><img src="../Images/da6c1b2191e2a029798c5c684aedb5a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v1Bsn5BQ_mFhWlKS"/></div></div></figure><p id="953f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们有两个超参数，动量(<em class="ly"> m </em>)和学习率(/eta)。超参数就像一个旋钮。如果你旋转一个旋钮，这个模型可以学得更好或更差。它让我们能够控制优化过程。</p><p id="31d6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">实际上，带有动量的SGD乐观者经常是乐观者的两个主要选择之一，还有亚当乐观者，我们稍后会讨论。首先，我们将通过两个以上的优化。AdaGrad是随机梯度下降的下一个增强。</p><p id="233b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Adagrad和RMSProp差不多。我说几乎是因为他们之间唯一的区别是他们计算学习率的方式。两者都关注于改变每个参数的学习率，而不是对所有参数都保持相同。这允许不经常改变的参数跟上变化，有效地利用更多的神经元进行训练。</p><h2 id="49e6" class="lz ju hi bd jv ma mb mc jz md me mf kd jg mg mh kf jk mi mj kh jo mk ml kj mm bi translated">阿达格拉德</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mx"><img src="../Images/b7342050e192a86e9479e0baa7a556cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y5Z1bxq15YMolTzS"/></div></div></figure><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es my"><img src="../Images/aadde95daf11a51d6e76bbaaf95c800c.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/0*yhtWmE3kGhHbkBJp"/></div></figure><p id="8efe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，g是当前周期的梯度。ε是防止被0除的超参数。ε值通常是一个小值，比如1e-7。</p><h2 id="6a5b" class="lz ju hi bd jv ma mb mc jz md me mf kd jg mg mh kf jk mi mj kh jo mk ml kj mm bi translated">RMSProp</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mz"><img src="../Images/85fa461b690ad6485b7b846430179a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/0*en1jArV5H2caod5_"/></div></figure><p id="abbb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Roh是超参数；默认值为0.9。我们可以根据你的需要来改变它。</p><p id="90b5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结合RMSProp和基于动量的SGD的最佳特性，我们得到了Adam。</p><h2 id="d2de" class="lz ju hi bd jv ma mb mc jz md me mf kd jg mg mh kf jk mi mj kh jo mk ml kj mm bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h2><p id="9453" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">Adam是Adaptive Momentum的缩写，是最广泛使用的基于RMSProp的优化器，并且重新引入了SGD的Momentum概念。这意味着我们将不再使用当前的梯度，而是使用像SGD优化器中的动量，然后是像RMSProp中的缓存的每权重自适应学习速率。</p><p id="2dc2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个等式有点复杂，如果你感兴趣，你可以通过这个<a class="ae lx" href="https://www.youtube.com/watch?v=JXQT_vxqwIs" rel="noopener ugc nofollow" target="_blank">视频</a>了解更多。</p><h2 id="8f10" class="lz ju hi bd jv ma mb mc jz md me mf kd jg mg mh kf jk mi mj kh jo mk ml kj mm bi translated">如何选择优化器？</h2><ul class=""><li id="92dd" class="na nb hi iz b ja kl jd km jg nc jk nd jo ne js nf ng nh ni bi translated">如果数据稀疏，使用自适用的方法，即Adagrad、Adadelta、RMSprop、Adam。</li><li id="3f28" class="na nb hi iz b ja nj jd nk jg nl jk nm jo nn js nf ng nh ni bi translated">RMSprop，Adadelta，Adam在很多情况下都有类似的效果。</li><li id="b5bd" class="na nb hi iz b ja nj jd nk jg nl jk nm jo nn js nf ng nh ni bi translated">Adam只是在RMSprop的基础上增加了偏差修正和动量，</li><li id="b57f" class="na nb hi iz b ja nj jd nk jg nl jk nm jo nn js nf ng nh ni bi translated">随着梯度变得稀疏，Adam的性能将优于RMSprop。</li></ul><p id="49dd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">总的来说，亚当是最好的选择。</p><h1 id="78e8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">结论</h1><p id="59d6" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">在本文中，我们讨论了各种优化器，也使区分变得更容易接受。我们还了解到，Adam可以作为首选优化器用于几乎所有的深度学习项目，如果这不起作用，你可以寻找替代方案。在下面的文章中，我们将最终编写一些代码并实现我们的第一个神经网络。一定要让我知道你下一步想看什么作品。</p></div></div>    
</body>
</html>