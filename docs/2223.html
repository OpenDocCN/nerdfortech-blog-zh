<html>
<head>
<title>An Overview of Pipeline Parallelism and its Research Progress</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">流水线并行性及其研究进展综述</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/an-overview-of-pipeline-parallelism-and-its-research-progress-7934e5e6d5b8?source=collection_archive---------1-----------------------#2021-04-27">https://medium.com/nerd-for-tech/an-overview-of-pipeline-parallelism-and-its-research-progress-7934e5e6d5b8?source=collection_archive---------1-----------------------#2021-04-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="de85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关键词:深度神经网络，分布式系统，流水线并行，GPipe，PipeDream，DAPPLE，PipeMare </strong></p><h1 id="e901" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">简介</strong></h1><p id="50e6" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">现在，现代深度神经网络和训练数据的规模变得非常大。在单个GPU节点上训练新的DNN模型变得越来越困难。因此，我们需要建立一个大型分布式机器学习系统，该系统可以使用大规模集群，用大数据训练模型，并获得与单个节点相当甚至更好的性能。为了实现这一目标，我们需要优化机器学习系统的并行策略，以获得更高的速度和更有效的训练。该领域的相关工作可以分为以下几类:模型并行、数据并行和混合并行。</p><p id="bfa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据并行是最广泛使用的策略。它用于训练数据的大小很大并且无法放入单台机器的情况。为了解决这个问题，数据并行性允许我们将数据划分为多个分片，并将它们分发到不同的节点。每个节点首先利用规模较小的局部数据训练一个子模型，并与其他节点进行通信，以确保来自每个节点的训练结果在一定时间内能够被整合，最终获得全局模型。数据并行的参数更新策略(机器学习/深度学习的SGD)可以分为异步更新和同步更新两类。数据并行的缺点也很明显。由于每个子模型在每次迭代训练后都需要提交梯度，网络通信开销非常大。</p><p id="8027" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型并行用于模型非常大并且不能存储在本地内存中的情况。在这种情况下，我们需要将模型分割成不同的模块(例如，DNN中的不同层)。然后，可以将每个模块放入不同的节点进行训练。此时，可能需要不同节点之间频繁的节点间通信。模型并行的性能取决于两个方面，连接结构和操作的计算需求。虽然模型并行可以解决大模型训练的问题，但是也会给我们带来网络流量低，训练时间增加的问题。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/5260f4d034a4bacd6af63e9b8fc6805f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xrsh4A21yj_N6HpSujtRg.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图一。数据并行和模型并行。数据并行化:并行化小批量梯度计算，模型复制到所有机器。模型并行性:跨机器划分模型并复制数据。[1]</figcaption></figure><p id="fdf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">混合并行是数据并行和模型并行的结合。混合并行的一个很好的例子是DistBelief，它是Google提出的一个经典软件框架。它包含两个部分:(1) Downpour SGD，一种异步随机梯度下降过程，支持大量模型副本，这些副本在异步执行中结合了模型和数据并行性，以及(Sandblaster，一种支持各种分布式批处理优化过程的框架，包括L-BFGS的分布式实现。关于混合并行，可以阅读《优化深度学习训练的多GPU并行化策略》。了解更多信息。</p><p id="abc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本报告中，我们将讨论一种新的并行策略:流水线并行。管道并行性同时发表在Gpipe和PipeDream论文中，是一种在多个GPU上训练大型模型的有效技术。接下来的部分将介绍流水线并行的一般原理，然后比较和讨论基于流水线并行的四个框架:PipeDream、Gpipe、DAPPLE和PipeMare。</p><h1 id="c0fc" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">流水线并行度</strong></h1><p id="e3ad" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">深层神经网络可以定义为一系列层。每一层都由一个正演计算函数和一组相应的参数组成。模型的大小受层数和每层神经元数量的影响。正如我们上面讨论的，对于不适合一台机器的本地存储器的大型深度神经网络，模型并行性被采用，其中模型的某些部分被放置在不同的机器上。每一层的参数将被平衡到相应的机器上。模型并行性的训练过程是双向的。它包括用于处理一个小批量的正向路径和反向路径。如图2所示，标准模型并行导致计算资源的严重利用不足。此外，通常不清楚如何在不同的机器之间分割模型以获得最佳性能。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/8bac31b78571d8dd4680e49608ff9b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xHqENMwwkutHmqJF5BWuEg.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图二。有4名工人的模拟平行培训。数字表示批次ID，向后传递花费的时间是向前传递的两倍。为了简单起见，作者假设在工人之间交流激活/梯度没有开销。[3]</figcaption></figure><p id="9a46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这个问题，迪帕克·纳拉亚南，亚伦·哈莱普。等人(管道梦)和黄雁萍。GPipe等人同时提出了流水线并行的概念。流水线并行的关键思想是在训练期间利用流水线并行。它首先将训练示例分成小批(GPipe中的微批和PipeDream中的小批)，然后使用批间或批内并行性将每组小批的执行流水线化到单元上。流水线并行的效率可以通过总的存储器使用和流水线利用率来评估。管道利用率(Util)是在任何给定时间非空闲(停止)的管道阶段的百分比。与数据并行类似，流水线并行也可以分为同步流水线和异步流水线，因为它们的更新策略不同。</p><p id="70af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> GPipe </strong> — <strong class="ih hj">同步管道</strong></p><p id="7472" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPipe将输入小批分成多个微批，并通过管道在多台机器上执行这些微批。渐变在最后同步应用。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kx"><img src="../Images/f7aac4d87167f2b33d0098c6ece31cc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bWis3uiTSMRCmhAfKyQ-Fg.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图3。(a)具有连续层的示例神经网络跨四个加速器划分。(b)由于网络的顺序依赖性，朴素模型并行策略导致严重的利用不足。(c)流水线并行将输入小批量分成更小的微批量，使得不同的加速器能够同时处理不同的微批量。渐变在最后同步应用。[4]</figcaption></figure><p id="e9e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图3中，他们将每个小批量分成4个小批量。设备0、1、2、3分别存储层0、1、2、3的参数，并向前和向后传递中间变量。其工作方式如下:</p><p id="965c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">微批处理0首先传递到设备0，并通过设备0的转发功能进行计算。</p><p id="6350" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设备1从设备0接收微批次0，计算并将其传输到设备2。同时，微批次1传递到设备0。</p><p id="5eb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设备2从设备1接收微批次0，计算并将其传输到设备3。同时，设备1从设备0接收微批次1，计算并将其传送到设备2。同时，微批次2传送到设备0。</p><p id="f9dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设备3从设备2接收微批次0，计算并将其传送到设备4。设备2从设备1接收微批次1，计算并将其传送到设备3。同时，设备1从设备0接收微批量2，计算并将其传送到设备2。同时，微批次3传送到设备0。</p><p id="5cb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">……</p><p id="bd62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">渐变在最后同步应用。更新每个层/设备的参数。然后，开始下一个管道。</p><p id="dd80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了总结GPipe的优势，作者列出了三个关键属性:(1)效率:GPipe使用一种新颖的批量分裂流水线算法，随着器件数量的增加，实现了几乎线性的加速比。(2)灵活性:GPipe支持任何可以表示为一系列层的深度网络。(3)可靠性:GPipe利用同步梯度下降，无论分区数量多少，都能保证训练的一致性。由于其可靠性和比模型并行更好的训练速度，GPipe在业界得到广泛应用，并有GPU集群和TPU集群两个版本。</p><p id="47d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">pieddream—异步流水线</strong></p><p id="19ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然PipeDream与GPipe同时期出版，但其并行思想与GPipe大相径庭。PipeDream将前向传递的执行流水线化，并将它们与后向传递穿插在一起，试图最大化硬件利用率和吞吐量。它将小批连续插入管道，并在反向传递后异步更新参数。PipeDream和GPipe之间的区别很明显:PipeDream应用异步向后更新，而GPipe应用同步向后更新。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ky"><img src="../Images/ff25bdd5c1d4ec07fb773cf7a16ace03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gf3535whIzrWPZZ8MNx4_A.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图4。PipeDream:为了确保在任何时间点都没有GPU空闲，框架将多个小批量一个接一个地注入到管道中。在完成小批量的正向传递后，每个阶段将输出激活异步发送到下一个阶段，同时开始执行另一个小批量的工作。类似地，在完成小批量的反向传递之后，每个阶段异步地将输出梯度发送到前一阶段，同时开始另一个小批量的计算。这确保了不同的GPU同时处理不同的小批量。[3]</figcaption></figure><p id="54dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PipeDream的基本框架如图4所示。它结合了批内并行和批间并行。如何将不同批次的正向和反向传递分配给每个工人是一个优化问题。PipeDream选择大致均匀的分割，以最小化计算节点之间的通信，同时平衡它们之间的计算负载，使用动态程序来解决这一优化问题。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kz"><img src="../Images/f0e511a0d3171e9705265217e726a3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UegDyompRYad-VRagpYq3w.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">T^k (i → j，m)表示跨越层I到j的单个级向前和向后传递所花费的总时间，使用带宽B_k复制overm工作器</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es la"><img src="../Images/8456413c86e7cc427caf7a2c1ab09879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JL_n-M5WKNEQA23q1ouHEQ.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">这是PipeDream的DP定义:max内的第一项是具有m个m’工作者的层I和s之间的最优子管道的最慢阶段所花费的时间，第二项是在层s和s + 1之间传递激活和大小梯度所花费的时间，第三项是在m’工作者的数据并行配置中包含层s + 1到j的单个阶段所花费的时间。</figcaption></figure><p id="069c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，流水线并行计算涉及将DNN模型的层划分成多个阶段，其中每个阶段由模型中的一组连续层组成。每个阶段映射到一个单独的设备，该设备为该阶段中的所有层执行正向传递(和反向传递)。同时，参数更新将不是同步的。例如，在图4中，在工作1对小批量1的反向传递之后，小批量5的新正向传递将使用更新的参数。因此，PipeDream在每台机器上维护多个版本的参数(每个正在进行的小批量一个版本)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lb"><img src="../Images/ee3fd4ea68eb76e560f8a4da5400ae52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*am9GutIjCqvZl5DWuzJyLw.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图5。重量藏在管道梦里。[3]</figcaption></figure><p id="927c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，图4的设计遭受由异步向后更新引入的权重陈旧。为了解决这个问题，PipeDream使用了一种叫做重量堆积的技术。砝码存储维护多个版本的砝码，每个有效迷你批次一个。每个阶段使用正向传递中可用的最新版本砝码处理一个小批次。完成向前传递后，PipeDream会存储该迷你批次所用的重量。然后，相同的重量版本用于计算重量更新和微型批次后向传递中的上游重量梯度。</p><p id="bd66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，PipeDream是分布式训练的一个有用的异步流水线并行工具。在吞吐量和加速方面，它比GPipe更好。但是由于使用了加权存储，它也需要更多的存储资源。</p><p id="31fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">斑纹</strong>——<strong class="ih hj">改进型</strong> <strong class="ih hj">同步流水线</strong></p><p id="c5bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">DAPPLE是一个同步分布式训练框架，结合了大型DNN模型的数据并行和流水线并行。</p><p id="61c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与GPipe相比，DAPPLE使用了更好的内存调度和相等的气泡空闲。如图6所示，GPipe一次注入所有微批处理，而DAPPLE有一个早期向后调度。DAPPLE的向后调度可以释放用于存储由相应的向前阶段产生的激活的存储器。优化步骤类似于PipeDream:使用动态编程找到最佳分区、复制和放置策略。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lc"><img src="../Images/95278a0bd73cc79de11196c7480d7e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*K-2c97-qS_2WwXYUe02Jdg.jpeg"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图6。GPipe (a)和DAPPLE (b)之间的不同调度以及它们的内存消耗。(作者有一个笔误:在下面的内存比较图例中，红线应该是斑纹线，而蓝线应该是GPipe线)[6]</figcaption></figure><p id="513c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在同步训练场景下，DAPPLE planner始终比PipeDreams planner生成的策略快3.23倍，DAPPLE scheduler比GPipe快1.6倍的训练吞吐量，同时节省12%的内存消耗。</p><p id="f9af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">pipe mare——改进的异步流水线</strong></p><p id="302c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PipeMare是一个提高异步流水线训练统计效率的新框架。在模型需要使用时，它使用内存中的任何权重计算梯度，这避免了存储模型权重的额外副本(无权重存储)或引入气泡，但比PipeDream实现了更好的性能。</p><p id="eb14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PipeMare使用两种技术来提高异步管道并行性。它们是(1)学习率重新安排，和(2)差异校正。学习率重调度是解决延迟更新的一种新方法。它设计了一个步长方案来控制训练过程中的步长，因为步长与异步更新的衰减相关。误差校正取代了重量存储。它根据权重的最近平均轨迹，通过外推前向过程中的权重来调整后向过程中使用的权重值。它不是保存多个版本的权重，而是只使用一点额外的内存来保存权重速度的近似值。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ld"><img src="../Images/3acd5ff2d6c7b0688ab2676c54ef540c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YDDFAHlE-mlK54v13yluow.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图7。将PipeMare与模型并行性和PipeDream进行比较。PipeMare的内存效率更高，并且仍然保持高吞吐量。[7]</figcaption></figure><p id="e342" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作者将PipeMare与PipeDream和GPipe进行了比较。他们发现PipeMare实现了与GPipe相似的最终模型质量，但实现了更高效的端到端训练。</p><h1 id="ce95" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">讨论</strong></h1><p id="4da5" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">训练神经网络时的管道并行性使得更大的模型能够在空间上被划分，从而导致更低的网络通信和整体更高的硬件利用率。流水线并行有两种方法。诸如GPipe的同步流水线并行需要相邻训练迭代之间的必要梯度同步，以确保收敛。在运行时，它调度尽可能多的并发管道阶段，以最大化设备利用率。实际上，这种调度策略会导致显著的内存消耗高峰。诸如PipeDream之类的异步流水线并行将小批量连续地插入到流水线中，并丢弃原始的同步操作以实现最大的吞吐量。然而，它需要很高的内存消耗来避免权重过时。研究人员已经为同步流水线并行和异步流水线并行开发了一个内存高效的版本。例如，DAPPLE使用了比GPipe更好的内存调度，并在加速和收敛方面都取得了良好的性能，而PipeDream-2BW和PipeMare的内存效率比PipeDream更好。</p><p id="4d2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，流水线并行的研究人员正试图在两个方向上改进他们的框架。(1)提高记忆效率；(2)保持较高的管道利用率。有趣的是，无论是同步管道还是异步管道的改进版本，都试图将对方的研究思路融入到自己的框架中。例如，DAPPLE应用动态编程来优化同步管道的内存调度，这类似于PipeDream中的优化问题。他们的研究思路类似于寻找同步SGD和异步SGD在数据并行性上的取舍。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es le"><img src="../Images/8dcf6c3cc56cc7e02e2168c1d97d9828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KcjbrWS0HKmF0aU1lstKQg.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图8。GPipe和PipeDream之间的差异类似于同步SGD和异步SGD在数据并行性方面的差异。(此图参考Parijat Dube教授的讲稿)</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lf"><img src="../Images/e5238f05b3c33290eaf5b24b80f52cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-s9JEE1yiB2HzRKsaXt0A.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">表1。这是我自己总结的一个对比表。从低到高的比较值意味着从最差到最好。加速(1-&gt;4:低-&gt;高)；内存效率(1-&gt;4:高-&gt;低)；收敛(1-&gt;4:低-&gt;高)</figcaption></figure><p id="2034" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">未来前景</strong></p><p id="0911" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为这方面的初学者，我将根据所有这些论文谈谈我的一些想法。同步管道和异步管道各有优缺点，适用于不同的应用场景。在我看来，同步流水线和异步流水线的取舍取决于如何控制收敛和加速来进行训练。同步流水线和异步流水线都属于极端情况:同步流水线有很高的收敛但代价是速度，异步流水线很快但代价是收敛低。能不能把它们结合起来，在中间取得更好的表现？我觉得一个可能的研究方向是建立一个跨同步管道和异步管道的决策代理。在训练期间，流水线可以在同步和异步之间动态切换。为了跨同步管道和异步管道训练决策模型，我们可以使用强化学习或元学习。</p><p id="9866" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以想象的是，作为机器学习系统的一种新的并行方法，流水线并行将成为一个长期的热门话题。</p><p id="a6a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">实施</strong></p><p id="c06b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPipe和PipeDream已经与Pytorch集成。DAPPLE在GitHub中也有开源版本。</p><p id="48d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">相关博客</strong></p><p id="c4d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您在阅读我的报告后对流水线并行感兴趣，这里有一些相关技术博客的链接:</p><p id="55c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPipe:<a class="ae lg" href="https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2019/03/introducing-GPipe-open-source-library . html</a></p><p id="0ea9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPipe:<a class="ae lg" href="https://towardsdatascience.com/explained-gpipe-training-giant-neural-nets-using-pipeline-parallelism-341b63bfc74b" rel="noopener" target="_blank">https://towards data science . com/explained-GPipe-training-giant-neural-nets-using-pipeline-parallelism-341 b 63 bfc 74 b</a></p><p id="7906" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PipeDream:<a class="ae lg" href="https://www.microsoft.com/en-us/research/blog/pipedream-a-more-effective-way-to-train-deep-neural-networks-using-pipeline-parallelism/" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/research/blog/PipeDream-a-more-effective-way-to-training-deep-neural-networks-using-pipeline-parallelism/</a></p><h1 id="ebb3" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">致谢</strong></h1><p id="cf83" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">本文基于NYU·库兰特计算机科学系的课程CSCI-GA 3033–091深度学习系统介绍和相关参考资料撰写。课程的老师是Parijat Dube教授，助教是Kshitij Bharat Sanghvi。感谢他们精彩的课程让我了解到深度学习系统的研究进展。</p><h1 id="5f77" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">参考:</strong></h1><p id="de42" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">[1]迪恩，杰弗里，等，“大规模分布式深度网络”乳头。2012.</p><p id="1d05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2] Pal，Saptadeep，等，“优化深度学习训练的多GPU并行化策略。”<em class="lh"> IEEE微</em>39.5(2019):91–101。</p><p id="dfe8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3] Narayanan，Deepak，等人，“pipe dream:DNN训练的广义流水线并行性”第27届美国计算机学会操作系统原理会议录。2019.</p><p id="ba17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4]黄，严平，等.“Gpipe:利用流水线并行实现巨型神经网络的高效训练”arXiv预印本arXiv:1811.06965 (2018)。</p><p id="f80f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5] Harlap，Aaron等，“pipe dream:DNN培训的流水线并行性”第一届系统和机器学习会议录。2018.</p><p id="9f0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6]范，，等.“DAPPLE:一种用于训练大模型的流水线数据并行方法”第26届ACM SIGPLAN并行编程原理与实践研讨会会议录。2021.</p><p id="fe75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7]杨，鲍文，等.“Pipemare:异步流水线并行dnn训练.”机器学习与系统会议录3 (2021)。</p><p id="f4dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[8] Narayanan，Deepak，等人，“内存高效的流水线并行dnn训练”arXiv预印本arXiv:2006.09503 (2020)。</p><p id="b6ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[9]<a class="ae lg" href="https://pytorch.org/docs/stable/pipeline.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/pipeline.html</a></p><p id="1ac7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lg" href="https://github.com/AlibabaPAI/DAPPLE" rel="noopener ugc nofollow" target="_blank">https://github.com/AlibabaPAI/DAPPLE</a></p></div></div>    
</body>
</html>