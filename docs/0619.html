<html>
<head>
<title>Activation Function — Secret sauce</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能—秘制酱</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/activation-function-secret-sauce-f283bfe160cb?source=collection_archive---------4-----------------------#2021-01-28">https://medium.com/nerd-for-tech/activation-function-secret-sauce-f283bfe160cb?source=collection_archive---------4-----------------------#2021-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/e503f3c85e2f952e53b8555702a0d225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D4xyhPzzXD5yUoBQJ8ljOg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">在<a class="ae hv" href="https://unsplash.com/s/photos/transform?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae hv" href="https://unsplash.com/@mcarsience_photography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Meagan carscience</a>拍摄的照片</figcaption></figure><div class=""/><p id="b34c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">激活函数，也称为传递函数，决定神经元的输入是否相关。这些函数应用于隐藏层以引入非线性。这种非线性有助于理解复杂的关系。激活功能也用于输出层。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jt"><img src="../Images/0a841d455426946282c975625a8d6525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3zrvUvZHO9rpLMYhddh-ug.png"/></div></div></figure><p id="38cf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">激活功能具有以下基本要求。</p><blockquote class="jy jz ka"><p id="d7c3" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">计算成本低廉的<br/>微分<br/>零居中</p></blockquote><p id="b6c8" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下图给了我们一个粗略的概念，基于神经网络的类型，何时在隐藏层使用哪个激活函数。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kf"><img src="../Images/86feb7cb5323682276f124a6e25a6b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*64as7a9u81XnFkaL2ogy3g.png"/></div></div></figure><p id="975e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下图根据我们试图解决的网络问题类型，给出了在输出层使用哪个激活函数的想法。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kg"><img src="../Images/a3fc5424db6b2fa02d3b688877a804f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywkZ6k4YZR2UZzbZvF-zEA.png"/></div></div></figure><p id="5272" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们逐一了解每个激活功能。</p><h2 id="dce1" class="kh ki hy bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated">乙状结肠的</h2><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lc"><img src="../Images/4fcfef63e48ebaee254d6dfb8300768d.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*5VwevmMsvpadH19XnMZaoA.png"/></div></figure><blockquote class="jy jz ka"><p id="213a" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">1.在 0 和 1 之间转换值。<br/> 2。函数的梯度值在-3 到 3 的范围内是显著的。超出此范围的梯度值变为零。<br/> 3。以 0.5 左右为中心。这使得所有神经元的输出具有相同的符号。</p></blockquote><h2 id="5620" class="kh ki hy bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated">双曲正切</h2><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ld"><img src="../Images/2fc1e2c1d8a8b1cf485495796ea8eeff.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*bPuekGfnB4yh33-WIVbaew.png"/></div></figure><blockquote class="jy jz ka"><p id="eb69" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">1.在-1 和 1 之间转换值。<br/> 2。零居中。因此梯度向两个方向移动。</p></blockquote><p id="26b6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Sigmoid 和 tanh 激活函数的一个普遍问题是“<strong class="ix hz">饱和</strong>”。大值转换为 1 &amp;，小值转换为 0 或-1。这些函数只在中点(0.50 或 0)附近的区域敏感。这使得体重适应非常困难。</p><p id="70d6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于与非线性激活函数(sigmoid &amp; tanh)相关的<strong class="ix hz">消失梯度</strong>问题，它们在具有许多层的网络中不是优选的。我们使用线性近似法。精确校正线性单位。</p><h2 id="d871" class="kh ki hy bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated">整流线性单位</h2><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es le"><img src="../Images/8884fe68bc5789dab17fe1d15654ec25.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*TTjvJXOogaKeYbPSvTnusA.png"/></div></figure><blockquote class="jy jz ka"><p id="b2d3" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">1.所有正值为线性，所有负值为零。因此，这是稀疏激活。<br/> 2。计算成本低。<br/> 3。收敛速度更快(几乎比 Sigmoid 或 tanh 快 6 倍)<br/> 4。与 Sigmoid 或 tanh 不同，它没有消失梯度问题<br/> 5。患有一种叫做“死亡再路”的疾病。</p></blockquote><p id="2686" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">垂死的 ReLU: </strong>一旦一个神经元变成阴性，它就不太可能恢复。因此所有这样的神经元它们的权重将在下降过程中被调整&amp;那些神经元变得被动&amp;不起任何作用。这通常发生在学习率很高或者有很大的负偏差的时候。</p><h2 id="7d12" class="kh ki hy bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated">LeakyReLU</h2><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lf"><img src="../Images/4cf755efe54f1d4def633c65fb96aff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*YngUqInWGtt_2ducQqZNsQ.png"/></div></figure><blockquote class="jy jz ka"><p id="be6f" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">1.克服了垂死的 ReLU 问题。leaky ReLU 不是负值的斜率为零，而是负值的斜率较小。<br/> 2。它比 ReLU 快。<br/> 3。它比 ReLU 更平衡。</p></blockquote><h2 id="8f80" class="kh ki hy bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated">卢瑟</h2><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lg"><img src="../Images/4047975a57195cf65ad12ddb2dd3dfd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*IddFSAK3zCO1Wh44GmmJUg.png"/></div></figure><p id="c42e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它结合了 ReLU 和 Leaky ReLU 的优点。它使用<strong class="ix hz">标度(对数曲线)</strong>表示负值。因为它对于负值具有大的负斜率，所以它使那些神经元不活跃。在计算上，它比 ReLU 或 Leaky ReLU 稍微贵一点，但是它训练得更快。由于其平均值更接近于零，因此具有更好的推广性。它解决了两个消失&amp;爆炸渐变问题。</p><blockquote class="lh"><p id="d16c" class="li lj hy bd lk ll lm ln lo lp lq js dx translated">由于 ReLU 及其变体的输出在正域中是无界的，因此 ReLU 或其变体被认为不太适合在 RNN 使用。RNN 更喜欢有界激活函数。</p></blockquote><p id="428a" class="pw-post-body-paragraph iv iw hy ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated"><strong class="ix hz">输出层激活功能</strong></p><h2 id="850d" class="kh ki hy bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated">Softmax</h2><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lw"><img src="../Images/7962ddc2a48c982eb4e3d31fcfd544d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*bzw5YlJRYoYROr2qgylPEQ.png"/></div></figure><blockquote class="jy jz ka"><p id="233d" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">1.将数字转换成概率分布<br/> 2。给出每个类别的“可信度分数”<br/> 3。分数总和为 1</p></blockquote><h2 id="1722" class="kh ki hy bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated">Softplus</h2><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lx"><img src="../Images/9eef423dbfd370f60afdc5caeead7a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*jkoqJZtfOAHW069bKf_SEg.png"/></div></figure><blockquote class="jy jz ka"><p id="07a5" class="iv iw kb ix b iy iz ja jb jc jd je jf kc jh ji jj kd jl jm jn ke jp jq jr js hb bi translated">以(0，+∞) <br/>的比例产生输出，还没有注意到被使用得多</p></blockquote><p id="f81b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望对你有用。</p></div></div>    
</body>
</html>