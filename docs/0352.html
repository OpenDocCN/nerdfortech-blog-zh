<html>
<head>
<title>What’s the KNN?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是KNN？</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/whats-the-knn-74e84458bd24?source=collection_archive---------0-----------------------#2020-10-22">https://medium.com/nerd-for-tech/whats-the-knn-74e84458bd24?source=collection_archive---------0-----------------------#2020-10-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e037" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过从头开始创建来理解懒惰学习者算法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/cf565cef754b51953bd16288f19d5e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*cT30BuMLRwZRDLMS9gitKw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">具有3个最近邻的Iris数据集上最近邻决策的决策边界</figcaption></figure><h1 id="48e1" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">什么是KNN算法？</strong></h1><p id="7807" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">k近邻(KNN)是一种非参数的监督机器学习算法。(有监督的机器学习是指机器根据标记的训练数据学习将输入映射到输出。)它是机器学习中用于回归和分类的最简单的算法之一。KNN遵循“物以类聚”的策略来确定新数据的适用范围。KNN使用所有可用的数据，并根据相似性度量或距离函数对新数据或案例进行分类。然后将新数据分配给大多数邻居所属的类。</p><h1 id="43a4" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">为什么KNN被称为“懒惰的学习者”？</h1><p id="4837" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">KNN经常被称为<strong class="ih hj">懒惰的学习者。这意味着该算法不使用训练数据点来进行任何归纳。换句话说，没有明确的训练阶段。缺乏通用性意味着KNN保留了所有的训练数据。它是一个<strong class="ih hj">非参数学习算法</strong>，因为它没有对底层数据做任何假设。</strong></p><h1 id="9c6e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">如何使用KNN算法进行预测？</strong></h1><p id="d824" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">为了将由一些特征向量表示的未知实例分类为特征空间中的点，KNN分类器计算该点和训练数据集中的点之间的距离。通常，<strong class="ih hj">欧几里德距离</strong>被用作距离度量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/b7d8e7438b181d7f72bdc0d9862955d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jyZpd_nenl5vPfCEgZtEiw.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">找到新数据最接近的标签模式。</figcaption></figure><h2 id="04ba" class="kx jq hi bd jr ky kz la jv lb lc ld jz iq le lf kd iu lg lh kh iy li lj kl lk bi translated">距离公式</h2><p id="5214" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">以下是KNN算法中使用的其他距离公式。</p><ul class=""><li id="85ed" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><strong class="ih hj">闵可夫斯基距离</strong>是一个赋范向量空间，可以认为是欧几里德距离和曼哈顿距离的推广。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/b4a48d31e0e74c68f483e3e1ec08727c.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*HZOTXvlxSg1yQAz2yqziIA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">闵可夫斯基距离</figcaption></figure><ul class=""><li id="27b9" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><strong class="ih hj">欧几里德距离</strong>是计算KNN时最常用的距离公式。欧几里得距离公式找到A点和b点之间的最短距离。这有时被称为勾股定理。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/ede4c83fe21145574dbd208d638ea6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/0*k8xusTDhxapvgVFk.jpg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">欧几里得距离</figcaption></figure><ul class=""><li id="42cb" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">另一个距离公式是<strong class="ih hj">曼哈顿距离</strong>公式。该公式取坐标差的绝对值之和。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/4afcfd3606c4091b064a40b578f9dcea.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/1*LlJuGVWywBoU2MGwoSGnPA.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">曼哈顿距离</figcaption></figure><ul class=""><li id="f9b9" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">其他公式包括切比雪夫，Wminkowski，Seuclidean，Mahalanobis。</li></ul><p id="e836" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以阅读这篇关于距离公式在计算KNN中的重要性的文章。<a class="ae lx" href="https://arxiv.org/pdf/1708.04321.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1708.04321.pdf</a></p><h1 id="f1b2" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">如何选择k的值？</strong></h1><p id="99fa" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">KNN中的“K”代表模型用来计算如何为新数据分配类的最近邻的数量。这是提高模型精度的一个重要参数。虽然没有找到K的最佳值的结构化方法，但这里有一些建议。</p><ol class=""><li id="b7d2" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc ly lr ls lt bi translated">取N的平方根，其中N是训练数据中的样本数(k=sqrt(N))</li><li id="ec40" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc ly lr ls lt bi translated">选择一个奇数。这有助于避免两类数据之间的混淆。</li><li id="5f58" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc ly lr ls lt bi translated">交叉验证。使用训练数据的一个小子集(称为验证集)，测试K的不同可能值，然后选择为验证集产生最佳结果的K。</li></ol><h1 id="e321" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">实现KNN算法</strong></h1><p id="e7c3" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">KNN模式可以通过4个简单的步骤来实施。为了演示KNN算法，我在虹膜数据集上使用它。这是来自sklearn库的预加载数据集。鸢尾数据集包括三种鸢尾属物种——Setosa、Versicolor和Virginica，每种都有50个样本。对于每个样本，我们有萼片长度，宽度，花瓣长度和宽度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es me"><img src="../Images/9387181036f5f62d70cbb2645ab8853b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bnLKsChXq94QjtAiRn40w.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae lx" href="https://www.datacamp.com/community/tutorials/machine-learning-in-r" rel="noopener ugc nofollow" target="_blank">面向初学者的R中的机器学习</a></figcaption></figure><ol class=""><li id="b58e" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc ly lr ls lt bi translated"><strong class="ih hj">看数据</strong></li></ol><p id="a330" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN算法将使用4个特征之间的关系，并对测试数据进行预测。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mf"><img src="../Images/11f0f76e6f9db0bd8152989ea00f4e4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdQKUm-DwPn30GlyQJYzZA.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mg"><img src="../Images/b5fa804e242186085e9dd7c2bd8c9341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAKkuTFBL9jFq1RCj3jYzQ.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jr">虹膜特征方框图</strong></figcaption></figure><pre class="je jf jg jh fd mh mi mj mk aw ml bi"><span id="f258" class="kx jq hi mi b fi mm mn l mo mp"># Load Data<br/># Import library<br/>from sklearn.datasets import load_iris</span><span id="c240" class="kx jq hi mi b fi mq mn l mo mp">iris = load_iris()</span><span id="5cb3" class="kx jq hi mi b fi mq mn l mo mp"><strong class="mi hj"># Separate into target and features<br/># Scale features</strong><br/>X = scale(iris.data)<br/>y = iris.target</span></pre><p id="fcd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1a。将K初始化为您选择的邻居数量</p><pre class="je jf jg jh fd mh mi mj mk aw ml bi"><span id="8a4c" class="kx jq hi mi b fi mm mn l mo mp"><strong class="mi hj">Instantiate model</strong><br/>clf = KNN(k=13)<br/>I used 13 because this is approximately the sqrt(n) as mentioned above.</span></pre><p id="3b0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1b。符合模型</p><pre class="je jf jg jh fd mh mi mj mk aw ml bi"><span id="84be" class="kx jq hi mi b fi mm mn l mo mp"><strong class="mi hj">Fit on X_train and y_train</strong><br/>clf.fit(X_train, y_train)</span></pre><p id="a564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了配合，KNN只是将训练数据添加到内存中。没有活跃的训练阶段。</p><p id="31fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">对于数据中的每个示例:</strong></p><ul class=""><li id="2bef" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">根据数据计算查询示例和当前示例之间的距离。</li><li id="97ad" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">将示例的距离和索引添加到有序集合中</li></ul><pre class="je jf jg jh fd mh mi mj mk aw ml bi"><span id="3f9a" class="kx jq hi mi b fi mm mn l mo mp"><strong class="mi hj"># Distance Formula<br/>def</strong> euclidean_distance(self, point1, point2, length):<br/>     distance = 0</span><span id="fb04" class="kx jq hi mi b fi mq mn l mo mp">     for x in range(length):<br/>          distance += (point1[x]-point2[x])**2</span><span id="aee1" class="kx jq hi mi b fi mq mn l mo mp">     return np.sqrt(distance)</span><span id="bdb8" class="kx jq hi mi b fi mq mn l mo mp"><strong class="mi hj"># Calculate distances between X_train and X_test<br/>def</strong> _get_distance(self, X_test):</span><span id="5a6a" class="kx jq hi mi b fi mq mn l mo mp">     length = X_test.shape[1]</span><span id="6337" class="kx jq hi mi b fi mq mn l mo mp">     <em class="mr"># Initialize distance array<br/>     </em>distances = []</span><span id="c33f" class="kx jq hi mi b fi mq mn l mo mp">     for idx in range(len(X_test)):<br/>          <em class="mr"># record X_test id <br/>          # initialize an array to hold distances<br/>          </em>distances.append([ X_test[idx], [] ])</span><span id="808b" class="kx jq hi mi b fi mq mn l mo mp"><em class="mr">          # Loop through each row in x_train<br/>          </em>for row in self.X_train:</span><span id="516d" class="kx jq hi mi b fi mq mn l mo mp"><em class="mr">              # find the euclidean distance <br/>              # append to distance list<br/>              </em>dist = self.euclidean_distance(row, X_test[idx],   <br/>                                             length)<br/>              distances[idx][1].append(dist)</span><span id="6d31" class="kx jq hi mi b fi mq mn l mo mp">      return distances</span></pre><p id="3187" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。找到邻居</strong></p><ul class=""><li id="9e87" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">按距离从小到大(按升序)对距离和索引的有序集合进行排序</li><li id="38a3" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">从排序的集合中挑选前K个条目</li><li id="5f29" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">获取所选K个条目的标签</li></ul><pre class="je jf jg jh fd mh mi mj mk aw ml bi"><span id="3648" class="kx jq hi mi b fi mm mn l mo mp"><strong class="mi hj"># Get Predictions<br/>def</strong> _get_labels(self, distances):<br/>     <em class="mr"># Initialize y_pred array</em>     <br/>     y_pred = []</span><span id="a1c5" class="kx jq hi mi b fi mq mn l mo mp">     for row in range(len(distances)):<br/>     <br/>          <em class="mr"># sort distances and record up to k values<br/>          </em>distance = distances[row]<br/>          y_indices = np.argsort(distance[1])[:self.k] <br/>          </span><span id="f829" class="kx jq hi mi b fi mq mn l mo mp"><em class="mr">         #  find the classes that correspond with nearest neighbors<br/>         </em>k_nearest_classes = [self.y_train[i%len(self.y_train)] for <br/>                              i in y_indices]<br/>         <em class="mr"># get label based on the mode of the classes<br/>         label</em> = [stats.mode(k_nearest_classes)][0][0][0]</span><span id="7401" class="kx jq hi mi b fi mq mn l mo mp">         y_pred.append(label)<br/>     <br/>     return y_pred</span></pre><p id="635b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4a。如果分类，返回K标签的模式。</strong></p><p id="4c4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4b。如果回归，返回K个标签的平均值</strong></p><pre class="je jf jg jh fd mh mi mj mk aw ml bi"><span id="5e9c" class="kx jq hi mi b fi mm mn l mo mp"><strong class="mi hj">def</strong> predict(self, X_test):<br/>     distances = self._get_distance(X_test)<br/>     return self._get_labels(distances)</span><span id="31e7" class="kx jq hi mi b fi mq mn l mo mp"><strong class="mi hj">Predict on X_test</strong><br/>predict = clf.predict(X_test)</span><span id="f4eb" class="kx jq hi mi b fi mq mn l mo mp"><strong class="mi hj">This returns:<br/></strong>[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0]</span><span id="172d" class="kx jq hi mi b fi mq mn l mo mp"><strong class="mi hj">Calculate accuracy</strong><br/>accuracy_score(y_test, predict)</span><span id="9e8c" class="kx jq hi mi b fi mq mn l mo mp">KNN model accuracy: 1.0</span></pre><h1 id="4fa7" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">用K值优化</strong></h1><p id="e009" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">您可以检查不同k值的错误率。这将有助于确定k的选择是否最适合您的模型。</p><pre class="je jf jg jh fd mh mi mj mk aw ml bi"><span id="7409" class="kx jq hi mi b fi mm mn l mo mp">k_range = range(1, 26)<br/>scores = {}<br/>scores_list = []</span><span id="2cd4" class="kx jq hi mi b fi mq mn l mo mp">for k in k_range:<br/>    knn = KNeighborsClassifier(n_neighbors = k)<br/>    knn.fit(X_train, y_train)<br/>    y_pred = knn.predict(X_test)<br/>    scores[k] = metrics.accuracy_score(y_test, y_pred)<br/>    scores_list.append(metrics.accuracy_score(y_test, y_pred))</span><span id="c53a" class="kx jq hi mi b fi mq mn l mo mp"># plot the relationship between K and the testing accuracy<br/>plt.plot(k_range, scores_list)</span><span id="7f64" class="kx jq hi mi b fi mq mn l mo mp">plt.xlabel("Value of K for KNN")<br/>plt.ylabel("Testing Accuracy")</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5f71c585270bdc86e3e943724716e6c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*3EgxQg-MjI6aeblnOs5N7w.png"/></div></figure><h1 id="38b1" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">如何使用KNN？</strong></h1><p id="0b21" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">KNN是一个简单而强大的算法，因为它不需要进行预测训练。下面是KNN算法的一些应用。</p><ul class=""><li id="5cfb" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">推荐系统:KNN用于创建简单的推荐系统，如亚马逊或网飞使用的推荐系统，以提供电影建议或购买建议。</li><li id="3f40" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">信用评级:KNN算法可以用来找到一个人的信用评级，方法是对一个人的财务信息进行比较，并将其与其他具有类似信息的人进行比较。</li><li id="80c4" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">银行业:银行应该给个人贷款吗？这个人会拖欠贷款吗？</li><li id="0f0c" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">其他示例包括手写检测、图像识别，甚至视频识别。</li></ul><h1 id="3b40" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">何时使用KNN？</h1><p id="dff0" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">KNN是一种强大的算法，但它不是所有机器学习模型的最佳算法。那么我们应该什么时候使用它呢？KNN对于无噪声、小且有标签的数据很有用。</p><h1 id="91d7" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">KNN的利与弊</strong></h1><p id="0083" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated"><strong class="ih hj">优点</strong></p><ul class=""><li id="44c7" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">它很容易使用，只需要k和距离公式。</li><li id="3fd7" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">计算时间快，精度较高。因为它是一个懒惰的学习者，所以没有训练的必要。</li><li id="8bfc" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">多用途，可用于回归或分类。</li></ul><p id="d33b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong></p><ul class=""><li id="448e" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">计算量大，因为它必须存储所有的训练数据，并且需要很高的存储容量。</li><li id="f5be" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">对不相关的特征和数据规模敏感。</li><li id="76b3" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated">可能会很慢，这取决于数据的大小。</li></ul><h1 id="0558" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">摘要</h1><p id="62c0" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">KNN是一种简单、强大且灵活的算法，可用于回归和分类问题。</p><p id="5dd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4个简单的步骤:</strong></p><ol class=""><li id="6362" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc ly lr ls lt bi translated">看数据。</li><li id="85f8" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc ly lr ls lt bi translated">计算距离。</li><li id="6c72" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc ly lr ls lt bi translated">找邻居。</li><li id="0ec6" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc ly lr ls lt bi translated">找出最常见的标签。</li></ol></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><h1 id="0db4" class="jp jq hi bd jr js mz ju jv jw na jy jz ka nb kc kd ke nc kg kh ki nd kk kl km bi translated"><span class="l ne nf ng bm nh ni nj nk nl di">F</span>T10】ind代码:</h1><p id="467a" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">在<a class="ae lx" href="https://github.com/jiobu1/CS_Build_Week_1" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到代码。</p><p id="6c46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">具体文件:</strong></p><p id="511b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里找到我的KNN算法<a class="ae lx" href="https://github.com/jiobu1/CS_Build_Week_1/blob/main/KNN/knn.py" rel="noopener ugc nofollow" target="_blank">的代码</a>。</p><p id="d9fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">找到我的KNN测试的代码。py文件<a class="ae lx" href="https://github.com/jiobu1/CS_Build_Week_1/blob/main/KNN/knn_test.py" rel="noopener ugc nofollow" target="_blank">此处</a>或作为jupyter笔记本<a class="ae lx" href="https://github.com/jiobu1/CS_Build_Week_1/blob/main/KNN/knn_test.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。(我将我的算法与sklearn库中的KNeighborsClassifier进行比较)。</p><p id="93f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">点击查看我的虹膜数据集<a class="ae lx" href="https://github.com/jiobu1/CS_Build_Week_1/blob/main/KNN/Exploratory_Analysis/iris_visuals.ipynb" rel="noopener ugc nofollow" target="_blank">的数据可视化。</a></p><h1 id="2b9f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">来源</strong>:</h1><ul class=""><li id="e735" class="ll lm hi ih b ii kn im ko iq nm iu nn iy no jc lq lr ls lt bi translated"><a class="ae lx" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . neighbors . kneighborsclassifier . html</a></li><li id="df35" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated"><a class="ae lx" href="https://arxiv.org/pdf/1708.04321.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1708.04321.pdf</a></li><li id="545b" class="ll lm hi ih b ii lz im ma iq mb iu mc iy md jc lq lr ls lt bi translated"><a class="ae lx" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Algorithm" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/K-nearest _ neighbors _ Algorithm # Algorithm</a></li></ul></div></div>    
</body>
</html>