<html>
<head>
<title>Ceph-Ansible Deployment &amp; Testing Using Vagrant (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Ceph-Ansible 部署和测试(第 1 部分)</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/ceph-ansible-deployment-testing-using-vagrant-8205a9f39f2d?source=collection_archive---------7-----------------------#2021-03-07">https://medium.com/nerd-for-tech/ceph-ansible-deployment-testing-using-vagrant-8205a9f39f2d?source=collection_archive---------7-----------------------#2021-03-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b18c3e915f6e0054aa753452a05f6664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gq6BilhX0LgWE9R86bDk0g.png"/></div></div></figure><h1 id="d608" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是 Ceph，为什么是 Ceph？</h1><p id="f8c1" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">简而言之，Ceph 是一个开源的软件定义存储(SDS)解决方案，用于从单个系统提供高度可扩展的基于对象、数据块和文件的存储。</p><p id="0828" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Ceph 旨在成为一个多用途的分布式软件，更是一个“一站式解决方案”,以克服一般企业的数据中心架构的横向扩展观念。</p><p id="4b8b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Ceph 集群可以将这些基本节点以及更多节点组合在一起:</p><ul class=""><li id="8d19" class="kr ks hi jq b jr km jv kn jz kt kd ku kh kv kl kw kx ky kz bi translated"><strong class="jq hj">监视器</strong> (MON)维护集群节点状态图。</li><li id="8310" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated"><strong class="jq hj">管理器</strong> (MGR)与监控器一起运行，守护进程&amp;提供外部监控和管理接口。</li><li id="9f8b" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated"><strong class="jq hj">元数据服务器</strong> (MDS)存储集群的元数据，主要用于基于文件的存储。</li><li id="59a6" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated"><strong class="jq hj">对象存储设备</strong> (OSD)，实际存储驱动器。</li></ul><p id="1743" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在使用 Red Hat 的 Ceph 存储时，我研究了许多可用于使用和部署 Ceph 的部署，主要用于测试目的。到目前为止，可用部署的列表相当大，包含 7 个不同的部署选项。</p><p id="4948" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Ceph-Ansible 是当今最著名和最流行的部署之一。Ceph-Ansible 部署以其多功能性而闻名，可以轻松地更新和升级使用的集群。</p><p id="9524" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在这个由两部分组成的演示中，我将重点介绍 Ceph-Ansible 部署方法，并演示我们如何使用 vagger &amp; Virtualbox 作为提供者，轻松地执行各种日常任务，例如 Ceph 的主要组件配置。</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><p id="3557" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在这个过程中，我将部署 Ceph-ansible (Nautilus)的上游版本，使用社区项目的 Github 存储库的稳定版本 4.0。</p><h2 id="fc49" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">先决条件:</h2><ul class=""><li id="1418" class="kr ks hi jq b jr js jv jw jz ma kd mb kh mc kl kw kx ky kz bi translated">RHEL 8</li><li id="c386" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated">流浪者 2.2</li><li id="867f" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated">虚拟盒子 6.1</li><li id="100a" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated">Ansible 2.9</li></ul><blockquote class="md me mf"><p id="c483" class="jo jp mg jq b jr km jt ju jv kn jx jy mh ko kb kc mi kp kf kg mj kq kj kk kl hb bi translated"><em class="hi">在此过程中，下面提到的虚拟机部署了 1GB 内存。</em></p></blockquote><p id="ee11" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">首先，将使用这些节点部署群集。</p><ul class=""><li id="5c59" class="kr ks hi jq b jr km jv kn jz kt kd ku kh kv kl kw kx ky kz bi translated">1 个月</li><li id="513c" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated">3 个 OSD</li><li id="5b5d" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated">2 名 rgw</li><li id="17c8" class="kr ks hi jq b jr la jv lb jz lc kd ld kh le kl kw kx ky kz bi translated">1 MGRs</li></ul><h2 id="2f20" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">部署</h2><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="8d4c" class="lm ir hi mp b fi mt mu l mv mw">$ git clone <a class="ae mx" href="https://github.com/ceph/ceph-ansible.git" rel="noopener ugc nofollow" target="_blank">https://github.com/ceph/ceph-ansible.git</a><br/>$ cd ceph-ansible<br/>$ git checkout stable-4.0</span></pre><p id="f0ff" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在将社区存储库克隆到我们的主机上之后，创建一个“vagger _ variables _ yml”文件的副本，并更改以下内容:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="ac47" class="lm ir hi mp b fi mt mu l mv mw">$ cp vagrant_variables.yml.sample vagrant_variables.yml<br/>$ cp site.yml.sample site.yml<br/>$ vim vagrant_variables.yml</span></pre><p id="09d7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在本文的开始，我们想要部署上面提到的虚拟机。我们应该如下更新流浪者变量文件(不需要编辑任何其他的。):</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="8544" class="lm ir hi mp b fi mt mu l mv mw">mon_vms: 1 <br/>osd_vms: 3 <br/>mds_vms: 0 <br/>rgw_vms: 1 <br/>nfs_vms: 0 <br/>grafana_server_vms: 0<br/>rbd_mirror_vms: 0 <br/>client_vms: 1 <br/>iscsi_gw_vms: 0 <br/>mgr_vms: 1</span></pre><p id="0ce1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，我们的浮动变量文件已经根据我们的需求设置好了，我们可以通过编辑“groups_vars/all.yml”文件开始设置我们的 Ceph 配置，如下所示:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="ed7e" class="lm ir hi mp b fi mt mu l mv mw">---<br/>ceph_origin: repository<br/>ceph_repository: community<br/>ceph_stable_release: nautilus<br/>public_network: "192.168.42.0/24"<br/>cluster_network: "192.168.43.0/24"<br/>monitor_interface: eth1<br/>devices:<br/>  - '/dev/sda'<br/>  - '/dev/sdb'<br/>dashboard_enabled: False</span></pre><p id="41e9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">编辑完所需的文件后，从 ceph-ansible 文件夹中运行下一个命令来启动集群部署:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="f022" class="lm ir hi mp b fi mt mu l mv mw">[root@RHEL ceph-ansible]# vagrant up --provision</span></pre><p id="a8b4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这个配置为 OSD 的守护进程部署了一个带有 LVM 方法的 Ceph Nautilus 上游版本。如下图所示，该配置包含 3 个 OSD 虚拟机，每个虚拟机包含 2 个 OSD，因此它是一个 6 OSD Ceph 集群。另外，看看其他正在运行的服务，比如 Monitor(MON)、Manager(MGR)和 Rados Gateway (RGW)。</p><figure class="mk ml mm mn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/600a6d47704af3367075b2cbedec943f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*envMZ2v7ObsTKdWr.png"/></div></div></figure></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="fa25" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">扩展 Ceph 集群容量</h2><p id="b084" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">可以通过添加新的 OSD 服务器或者向现有的 OSD 服务器添加新的存储设备来实现集群容量的扩展。容量的扩展有两种方式，自动或手动。(在尝试以下操作之前，请确保添加额外的存储设备)</p><p id="924f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在我的测试集群上执行的手动或自动可用的案例。</p><p id="abb8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">要手动添加一个新的 OSD 存储守护进程，我们首先必须创建一个逻辑卷来寻址新的 OSD 守护进程。我们可以通过执行以下操作来实现这一点，从而生成一个物理卷:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="ef5c" class="lm ir hi mp b fi mt mu l mv mw"><strong class="mp hj">[vagrant@osd1 vagrant]$ vagrant ssh osd1</strong><br/><strong class="mp hj">[vagrant@osd1 vagrant]$ su</strong> <br/><strong class="mp hj">[root@osd1 vagrant]# pvs</strong><br/>  PV         VG                                        Fmt  Attr PSize   PFree  <br/>  /dev/sdb   ceph-51819e22-b1b9-4d6b-955b-22d9724569f1 lvm2 a--  &lt;10.74g      0 <br/>  /dev/sdc   ceph-c3307bd4-3b8b-4e40-8756-7cddfed1d34a lvm2 a--  &lt;10.74g      0<br/><strong class="mp hj">[root@osd1 vagrant]# parted /dev/sdd</strong><br/>GNU Parted 3.1<br/>Using /dev/sdd<br/>Welcome to GNU Parted! Type 'help' to view a list of commands.<br/>(parted) <strong class="mp hj">mklabel </strong>                                                         <br/>New disk label type? <strong class="mp hj">gpt</strong>                                                  <br/>(parted) <strong class="mp hj">mkpart</strong>                                                           <br/>Partition name?  []? <strong class="mp hj">ceph</strong>                                                 <br/>File system type?  [ext2]? <strong class="mp hj">xfs</strong>                                            <br/>Start? <strong class="mp hj">2048s</strong>                                                              <br/>End? <strong class="mp hj">10.8GB</strong>                                                               <br/>(parted) <strong class="mp hj">p</strong>                                                                <br/>Model: VBOX HARDDISK (scsi)<br/>Disk /dev/sdd: 11.5GB<br/>Sector size (logical/physical): 512B/512B<br/>Partition Table: gpt<br/>Disk Flags:Number  Start   End     Size    File system  Name  Flags<br/> 1      1049kB  10.8GB  10.8GB               ceph</span></pre><p id="d037" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">当物理卷存在时，我们可以开始创建一个卷组，然后创建一个逻辑卷，并添加 OSD 守护进程。</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="6fd6" class="lm ir hi mp b fi mt mu l mv mw"><strong class="mp hj">[root@osd1 vagrant]# pvs</strong><br/>  PV         VG                                        Fmt  Attr PSize   PFree  <br/>  /dev/sdb   ceph-51819e22-b1b9-4d6b-955b-22d9724569f1 lvm2 a--  &lt;10.74g      0 <br/>  /dev/sdc   ceph-c3307bd4-3b8b-4e40-8756-7cddfed1d34a lvm2 a--  &lt;10.74g      0 <br/>  /dev/sdd1                                            lvm2 ---  &lt;10.06g &lt;10.06g<br/><strong class="mp hj">[root@osd1 vagrant]# vgcreate ceph /dev/sdd1<br/></strong>  Volume group "ceph" successfully created<br/><strong class="mp hj">[root@osd1 vagrant]# vgs</strong><br/>  VG                                        #PV #LV #SN Attr   VSize   VFree <br/>  ceph                                        1   0   0 wz--n-  10.05g 10.05g<br/>  ceph-51819e22-b1b9-4d6b-955b-22d9724569f1   1   1   0 wz--n- &lt;10.74g     0 <br/>  ceph-c3307bd4-3b8b-4e40-8756-7cddfed1d34a   1   1   0 wz--n- &lt;10.74g     0 <br/><strong class="mp hj">[root@osd1 vagrant]# lvcreate -n ceph -L 10GB ceph</strong><br/>  Rounding up size to full physical extent 10.05 GiB<br/>  Logical volume "lvol0" created.<br/><strong class="mp hj">[root@osd1 vagrant]# lvs</strong><br/>  LV                                             VG                                        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert<br/>ceph                                             ceph                                      -wi-a-----  10.05g                                                    <br/>  osd-block-865339b0-91fb-47f1-8fe0-abe0c9682b68 ceph-51819e22-b1b9-4d6b-955b-22d9724569f1 -wi-ao---- &lt;10.74g                                                    <br/>  osd-block-b69b3062-fa9f-417e-bd62-2f6cc7f28bee ceph-c3307bd4-3b8b-4e40-8756-7cddfed1d34a -wi-ao---- &lt;10.74g</span></pre><p id="aed1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">既然我们已经在新设备上配置了一个逻辑卷，我们就可以开始使用下面的命令将 Ceph OSD 守护进程添加到我们的新机器中了:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="6330" class="lm ir hi mp b fi mt mu l mv mw"><strong class="mp hj">[root@osd1 vagrant]# ceph-volume lvm prepare --bluestore --data ceph/ceph</strong><br/><strong class="mp hj">[root@osd1 vagrant]# ceph-volume lvm activate --all<br/>[vagrant@mon0 ~]$ sudo ceph -s</strong><br/> cluster:<br/> id: 6bf530b3–64b7–4a09-b24e-af3202f2230a<br/> health: HEALTH_OK<br/> <br/> services:<br/> mon: 1 daemons, quorum mon0 (age 22m)<br/> mgr: mgr0(active, since 20m)<br/> osd: 7 osds: 7 up (since 20m), 6 in (since 20m)<br/> rgw: 1 daemon active (rgw0.rgw0) <br/> <br/> data:<br/> pools: 4 pools, 128 pgs<br/> objects: 12 objects, 1.2 KiB<br/> usage: 6.0 GiB used, 58 GiB / 64 GiB avail<br/> pgs: 128 active+clean</span></pre><p id="c1c6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">正如我们所看到的，一个新的 OSD 使用手动方法成功启动并运行。</p><p id="6699" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">第二种自动方式是修改“groups_vars/all.yml”文件，并添加一个新行，该行带有一个破折号(-)，后跟存储设备位置路径，如下所示:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="11c9" class="lm ir hi mp b fi mt mu l mv mw">devices:<br/>  - '/dev/sda'<br/>  - '/dev/sdb'<br/>  - '/dev/sdc'</span></pre><p id="8623" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">修改完成后，转到 ceph-ansible 主文件夹，重新运行 site.yml ansible-playbook，使用 After 命令更新集群配置:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="97b3" class="lm ir hi mp b fi mt mu l mv mw">[razmab@RHEL ceph-ansible]$ vagrant provision</span></pre><p id="c593" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">接下来，ssh 到 MON0 VM 并检查结果。</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="d557" class="lm ir hi mp b fi mt mu l mv mw"><strong class="mp hj">[root@RHEL ceph-ansible]# vagrant ssh mon0</strong><br/><strong class="mp hj">[vagrant@mon0 vagrant]$ su<br/>[root@mon0 ~]$ sudo ceph -s<br/>[vagrant@mon0 ~]$ sudo ceph -s</strong><br/>  cluster:<br/>    id:     6bf530b3-64b7-4a09-b24e-af3202f2230a<br/>    health: HEALTH_OK<br/> <br/>  services:<br/>    mon: 1 daemons, quorum mon0 (age 7h)<br/>    mgr: mgr0(active, since 6h)<br/>    osd: 8 osds: 8 up (since 5h), 8 in (since 5h)<br/>    rgw: 1 daemon active (rgw0.rgw0)<br/> <br/>  task status:<br/> <br/>  data:<br/>    pools:   4 pools, 128 pgs<br/>    objects: 187 objects, 1.2 KiB<br/>    usage:   9.1 GiB used, 85 GiB / 94 GiB avail<br/>    pgs:     128 active+clean</span></pre><h2 id="b6c3" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">多个 RGW 的实例</h2><p id="4160" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">现在让我们允许每个 RGW 服务器有 2 个 Rados 网关守护进程。这可以通过再次修改 groups_vars/all.site.yml 文件并添加下一个<strong class="jq hj">粗体</strong>行来完成。</p><p id="7fe6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">RGW 的多实例可以为我们提供读/写工作负载性能的提升。在这里阅读更多<a class="ae mx" href="https://www.redhat.com/en/blog/red-hat-ceph-storage-rgw-deployment-strategies-and-sizing-guidance" rel="noopener ugc nofollow" target="_blank"/>。</p><blockquote class="md me mf"><p id="182c" class="jo jp mg jq b jr km jt ju jv kn jx jy mh ko kb kc mi kp kf kg mj kq kj kk kl hb bi translated"><em class="hi">该配置为每个 RGW 服务器设置了 2 个 RGW 守护进程实例，默认为 1 个。</em></p></blockquote><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="fa6d" class="lm ir hi mp b fi mt mu l mv mw">---<br/>ceph_origin: repository<br/>ceph_repository: community<br/>ceph_stable_release: nautilus<br/>public_network: "192.168.42.0/24"<br/>cluster_network: "192.168.43.0/24"<br/>monitor_interface: eth1<br/>devices:<br/>  - '/dev/sda'<br/>  - '/dev/sdb'<br/>  - '/dev/sdc'<br/>dashboard_enabled: False<br/><strong class="mp hj">radosgw_num_instances: 2</strong></span></pre><p id="469e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">完成上述更新后，再次运行 site.yml 行动手册，如前面所示，并查看结果:</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="c2fe" class="lm ir hi mp b fi mt mu l mv mw"><strong class="mp hj">[vagrant@mon0 ~]$ sudo ceph -s</strong><br/>  cluster:<br/>    id:     6bf530b3-64b7-4a09-b24e-af3202f2230a<br/>    health: HEALTH_OK<br/> <br/>  services:<br/>    mon: 1 daemons, quorum mon0 (age 7h)<br/>    mgr: mgr0(active, since 6h)<br/>    osd: 8 osds: 8 up (since 5h), 8 in (since 5h)<br/>    rgw: 2 daemons active (rgw0.rgw0, rgw0.rgw1)<br/> <br/>  task status:<br/> <br/>  data:<br/>    pools:   4 pools, 128 pgs<br/>    objects: 187 objects, 1.2 KiB<br/>    usage:   9.1 GiB used, 85 GiB / 94 GiB avail<br/>    pgs:     128 active+clean</span></pre><h2 id="fd96" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">监视器守护进程增加</h2><p id="86a5" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">与 ceph-ansible 的大多数日常操作一样，添加一个 monitor 守护进程相对容易，并且可以使用一个简单的预先准备好的 ansible-playbook 自动完成。在我们的例子中，我们希望向 MGR 节点添加一个 MON 守护进程。我们将编辑浮动的 ansible 清单，并在“[mons]”行下添加管理器节点主机名(“mgr0”)。</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="e3b2" class="lm ir hi mp b fi mt mu l mv mw"><strong class="mp hj">[root@RHEL ceph-ansible]# vim .vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory<br/></strong>[mons]<br/>mon0<br/><strong class="mp hj">mgr0</strong></span><span id="4630" class="lm ir hi mp b fi mz mu l mv mw">[osds]<br/>osd0<br/>osd1<br/>osd2</span><span id="b4ba" class="lm ir hi mp b fi mz mu l mv mw">[rgws]<br/>rgw0</span><span id="b403" class="lm ir hi mp b fi mz mu l mv mw">[mgrs]<br/>mgr0</span></pre><p id="786d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">添加主机名后，保存更改并运行“add-mon.yml”脚本，使用节点的私钥来应用更改。</p><pre class="mk ml mm mn fd mo mp mq mr aw ms bi"><span id="bf48" class="lm ir hi mp b fi mt mu l mv mw"><strong class="mp hj">[root@RHEL ceph-ansible]#</strong> ansible-playbook --private-key=~/.vagrant.d/insecure_private_key -u vagrant -i ~/Desktop/ceph-ansible/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory infrastructure-playbooks/add-mon.yml -e mons<br/><strong class="mp hj">[vagrant@mon0 ~]$ sudo ceph -s</strong><br/>  cluster:<br/>    id:     6bf530b3-64b7-4a09-b24e-af3202f2230a<br/>    health: HEALTH_OK<br/> <br/>  services:<br/>    <strong class="mp hj">mon: 2 daemons, quorum mon0,mgr0 (age 100s)</strong><br/>    mgr: mgr0(active, since 79s)<br/>    osd: 9 osds: 9 up (since 20s), 9 in (since 20h)<br/>    rgw: 2 daemons active (rgw0.rgw0, rgw0.rgw1)<br/> <br/>  task status:<br/> <br/>  data:<br/>    pools:   4 pools, 128 pgs<br/>    objects: 219 objects, 1.2 KiB<br/>    usage:   9.1 GiB used, 85 GiB / 94 GiB avail<br/>    pgs:     128 active+clean</span></pre><p id="382c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">添加了一个新的 mon 守护程序，使集群中有 2 个 MON 和 2 个 RGW 守护程序，同时使其对于故障更加冗余。</p><h2 id="52b0" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">结论</h2><p id="df72" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这是第一部分演示的部署和日常任务部分。第二部分将讨论和覆盖更多关于 Ceph 对象存储能力，使用 rados 网关，同时运行 HAProxy 负载均衡器并在后台保持活动，以防止集群的故障转移。</p></div></div>    
</body>
</html>