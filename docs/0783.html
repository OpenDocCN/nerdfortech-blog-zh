<html>
<head>
<title>Understanding Loss Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解损失函数</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/understanding-loss-functions-cfbb44b5d17f?source=collection_archive---------7-----------------------#2021-02-13">https://medium.com/nerd-for-tech/understanding-loss-functions-cfbb44b5d17f?source=collection_archive---------7-----------------------#2021-02-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c144c49ee3276fb4e53e0b2b968d647f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2_FeOBWWFb5L8XJGJon1g.png"/></div></div></figure><p id="019c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失函数量化了模型表现的好坏。就优化而言，这是一个收敛指标。选择正确的损失函数变得非常关键。在这个博客中，我们将看到一些最常见的损失函数。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="4f44" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">from</strong> <strong class="jt hj">sklearn.metrics</strong> <strong class="jt hj">import</strong> hinge_loss<br/><strong class="jt hj">from</strong> <strong class="jt hj">sklearn.metrics</strong> <strong class="jt hj">import</strong> log_loss<br/><strong class="jt hj">from</strong> <strong class="jt hj">sklearn.metrics</strong> <strong class="jt hj">import</strong> mean_absolute_error<br/><strong class="jt hj">from</strong> <strong class="jt hj">sklearn.metrics</strong> <strong class="jt hj">import</strong> mean_squared_error<br/><strong class="jt hj">from</strong> <strong class="jt hj">sklearn.metrics</strong> <strong class="jt hj">import</strong> mean_squared_log_error<br/><strong class="jt hj">from</strong> <strong class="jt hj">scipy.stats</strong> <strong class="jt hj">import</strong> entropy</span></pre><h2 id="4f62" class="jx jy hi bd kd ke kf kg kh ki kj kk kl jb km kn ko jf kp kq kr jj ks kt ku kv bi translated">回归损失</h2><blockquote class="kw kx ky"><p id="6e6f" class="iq ir kz is b it iu iv iw ix iy iz ja la jc jd je lb jg jh ji lc jk jl jm jn hb bi translated">预测连续值</p></blockquote><p id="ae85" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">均方误差</strong>—也称为L2损失。如果你有高斯分布，这被使用。这个损失函数将严重惩罚高估。这是最常见的损失函数之一。此外，这也是一个很好的起点。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="1071" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">def</strong> mse(y_predicted, y_actual):<br/>    <strong class="jt hj">return</strong> np.sum((y_predicted - y_actual)**2) / y_actual.size</span></pre><p id="6fd2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">平均绝对误差</strong> —也称为最小绝对偏差(L1)损失。如果您有带有异常值的高斯分布，则使用此选项。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="274d" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">def</strong> mae(y_predicted, y_actual):<br/>    <strong class="jt hj">return</strong> np.sum(np.absolute(y_predicted - y_actual)) / y_actual.size</span></pre><p id="8dda" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">Huber</strong>—MSE和MAE的结合。这意味着当误差较小时，它表现得像MSE，否则，它表现得像MAE。阈值(δ)被确定为超参数。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="662b" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">def</strong> Huber(y_predicted, y_actual, delta=1.):<br/>    <strong class="jt hj">return</strong> np.where(np.abs(y_actual-y_predicted) &lt; delta,.5*(y_actual-y_predicted)**2 , delta*(np.abs(y_actual-y_predicted)-0.5*delta))</span></pre><p id="519e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">均方对数误差</strong> —当高斯分布具有较大值时使用。这个损失函数对低估的惩罚大于对高估的惩罚。</p><h2 id="1d50" class="jx jy hi bd kd ke kf kg kh ki kj kk kl jb km kn ko jf kp kq kr jj ks kt ku kv bi translated">分类损失</h2><blockquote class="kw kx ky"><p id="0ec3" class="iq ir kz is b it iu iv iw ix iy iz ja la jc jd je lb jg jh ji lc jk jl jm jn hb bi translated">预测离散值</p></blockquote><p id="6a4e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">二元交叉熵</strong>——也称对数损失。如果您的预测目标值在集合(0，1)中，则使用此选项。对于NN输出层，应激活sigmoid或使用logits作为true。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="a1bf" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">def</strong> cross_entropy(y_predicted, y_actual):<br/>    <strong class="jt hj">if</strong> y_actual== 1:<br/>      <strong class="jt hj">return</strong> -log(y_predicted)<br/>    <strong class="jt hj">else</strong>:<br/>      <strong class="jt hj">return</strong> -log(1 - y_predicted)</span></pre><p id="0192" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">铰链损耗</strong> —最初主要用于SVM。当集合中有目标值(-1，1)时使用。在神经网络输出层应具有双曲正切激活，使值在上述范围内。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="795d" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">def</strong> Hinge(y_predicted, y_actual):<br/>    <strong class="jt hj">return</strong> np.max(0, y_actual - (1-2*y)*y_predicted)</span></pre><p id="5ac4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">分类交叉熵</strong> —目标值在集合{0，1，3，…，n}中，其中每个类被分配一个唯一的整数值。在NN输出层应该有softmax激活。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="44ae" class="jx jy hi jt b fi jz ka l kb kc">def categorical_cross_entropy(y_actual, y_predicted):<br/>    sum_score = 0.0<br/>    for i in range(len(y_actual)):<br/>        for j in range(len(y_actual[i])):<br/>            sum_score += y_actual[i][j] * log(1e-15 + y_predicted[i][j])<br/>    mean_sum_score = 1.0 / len(y_actual) * sum_score<br/>    return -mean_sum_score</span></pre><p id="16ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">kull back Leiber Divergence—</strong>简而言之，它比较实际值和预测值的分布。它告诉你，当你改变概率分布时，你损失或获得了多少熵。也称为相对熵。较低的值表示较好的预测。与GAN和自动编码器一起使用。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="6781" class="jx jy hi jt b fi jz ka l kb kc"><strong class="jt hj">def</strong> KLDivergence(y_predicted, y_actual):<br/>    <strong class="jt hj">return</strong> np.sum(y_predicted * np.log((y_predicted / y_actual)))</span></pre></div></div>    
</body>
</html>