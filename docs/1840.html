<html>
<head>
<title>Decision Trees — Algorithmic Working</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树——算法工作</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/decision-trees-algorithmic-working-1aa837397476?source=collection_archive---------23-----------------------#2021-04-09">https://medium.com/nerd-for-tech/decision-trees-algorithmic-working-1aa837397476?source=collection_archive---------23-----------------------#2021-04-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="15b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们尝试理解决策树算法的内部工作原理，即算法为我们完成的任务，让我们的生活变得更加轻松。让我们从UCI机器学习库中选取名为“葡萄酒质量”的公共数据集。这在<a class="ae jd" href="https://archive.ics.uci.edu/ml/datasets/wine+quality" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/wine+quality</a>有售</p><p id="4716" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">对于这个例子，让我们先考虑3个解释变量，即pH值、硫酸盐和酒精。</strong>以下是数据集的局部视图-</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/f3aa6302970f521ee1995e03fc8de324.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*oKAPvGiEkA9cAupkAROGXg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">示意图1.0-数据集的局部视图</figcaption></figure><p id="81ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">算法工作(这是算法如何处理手头的问题以得出决策树)</strong></p><p id="6c43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤1:识别解释变量和目标变量</strong></p><p id="1094" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一步是计算观察次数。在这种情况下，有1200个观察值。下一步是评估列。在这种情况下，有12列。每一列的数据类型与目标/响应变量一起被评估。在这种情况下，响应变量是“葡萄酒质量”。分析目标变量。这种情况下的目标变量是一个介于0和10之间的值(序数值)。计算目标变量“葡萄酒质量”的平均值，在这种情况下恰好是5.6。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jq"><img src="../Images/da1571a604b5c1c72dfc56853fa72e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iysBLpoYD31p2o-p_Vd81g.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jv">示意图1.1 -分析数据集以评估解释变量和目标变量</strong></figcaption></figure><p id="657e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤2:为每个决策节点选择分裂参数</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jw"><img src="../Images/ce99d5b71e357c7268caf2c878403f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*Yxw_9EOV8TdBBVRddmHXdA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jv">示意图1.2-了解算法如何找到第一个决策节点(示意图1.1) </strong></figcaption></figure><p id="aef9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从示意图1.2中，我们看到中点是在成对的连续值之间计算的。一旦完成，分裂点值3.35、3.23、3.33、3.51、3.34和3.37(参考示意图1.2)根据它们提高均一性/减少杂质的程度进行排序。让我们看看示意图1.3中的一个这样的分割点- 3.5。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/e4bf6b1e3083fc047ffb6150068d1b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*DSNz6s1lOxT_7L-SDjeUig.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jv">示意图1.3——了解算法如何处理分割点</strong></figcaption></figure><p id="2b20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们假设的排名如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jy"><img src="../Images/6c305e03f514e40a9fee345e42d1e12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*RIwIkd07UqgvnsR6wXtbrg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">示意图1.4-确定分裂参数的分裂点排序</figcaption></figure><p id="1cdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据示意图1.4，我们知道变量“pH”的分裂参数是3.5。</p><p id="8601" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jz">对数据集中所有可用的解释变量继续步骤2。</em> </strong>对于数据集中可用的每个决策变量，选择分裂参数。在变量“pH”的情况下，分裂参数是3.5，因为在这一点上，不同区域之间具有最大的异质性，而在一个区域内具有最大的同质性。决策树本质上是二进制的，该算法重复地将记录分成两个区域，以便在区域内实现最大的同质性。同质性(样本内)最大的值被选为<strong class="ih hj">分裂条件</strong>。这被称为<strong class="ih hj">递归分区</strong>或<strong class="ih hj">贪婪逐步下降</strong>。</p><p id="7a96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤3:确定决策变量</strong></p><p id="4cce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦为数据集中的每个解释变量(预测变量/列)选择了拆分参数，算法就会对所有变量进行彻底扫描，以查看哪个变量产生的杂质最少。</p><p id="ffb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jz">‘杂质最少’是什么意思？</em>T15】</strong></p><p id="8b40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，数据集中有各种预测变量。选择能够准确分割变量以使信息增益最大并且分割尽可能异质的预测变量作为第一节点。</p><p id="a58a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">杂质通过残差∑(预测值-目标值/实际值)的平方和来衡量。选择产生最小/最小误差平方和的变量。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ka"><img src="../Images/81e98efd66a73d618926d302fe894fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CM0nZbOeBZ1f9JQSNronIQ.png"/></div></div></figure><p id="e74c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择的变量:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kb"><img src="../Images/9bd9e7544695fd938c7ae480fe0a43ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*o5aQ2Qqn-MLWJY5rba767Q.png"/></div></figure><p id="8ee5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">过拟合的概念:</strong></p><p id="f619" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当算法考虑数据集中的所有变量时，会产生多个决策边界。这些界限中的每一个都集中于创建同质样本，这导致预测测试数据结果的不准确性。这种现象被称为‘过拟合’。如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kc"><img src="../Images/167eab3dba3c14a2fe4161b9c0d363cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Au-wb5IjmSAPHFZ6WIt_8A.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kd"><img src="../Images/7afe51494f5ed25f18df65f6a264ddd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yfsbwk2CEsDU-ICMc7zm3w.png"/></div></div></figure><p id="606d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们假设这些点用值‘3’、‘4’、‘7’和‘8’代表目标变量(质量)。当呈现具有相似分裂条件的测试数据点时，算法将简单地预测质量为4。因此，由于过度拟合，复杂性增加。过度拟合带来的模式/决策规则更可能是噪声，而不是有用的决策规则/模式。</p><p id="1643" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">修剪:</strong></p><p id="2000" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">修剪是为了克服过度拟合的问题。修剪包括以下步骤:</p><p id="a296" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤1:删除离叶节点最近的决策节点</p><p id="e787" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步:计算准确性、错误率和成本复杂性。</p><p id="c1b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤3a:如果错误率比未修剪的树低，继续步骤1和2。</p><p id="9c84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤3b:如果错误率与未修剪的树相比更多，则停止修剪。</p><p id="d7c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用rpart修剪:</strong></p><p id="b1cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.rpart的算法对提供的数据执行K倍交叉验证(默认为10倍)。</p><p id="319e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.然后，它对K-1个数据子集应用完全生长的树。(将它们视为训练数据)</p><p id="13c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.然后将树模型应用于第k个子集(将其视为验证数据)</p><p id="2618" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.计算如下:-</p><p id="e69a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-相对误差(用训练数据估计)</p><p id="0044" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-交叉验证误差(x_error/X-val相对误差)(用验证数据估计)</p><p id="87e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-标准偏差(x_std)(用验证数据估算)</p><p id="130f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-复杂性参数</p><p id="1433" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.然后，最接近叶节点的决策树被移除，并且这被应用于9个数据子集。</p><p id="3ffc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.步骤1至3再次应用于修剪后的树。</p><p id="3c3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.然后对结果进行绘图，以查看x_error最小的树的大小。</p><p id="1cc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8.然后，这被认为是最终的修剪最优树模型。</p><p id="9cd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:</strong>R中的plotcp()给出了——树的大小、复杂度参数和交叉验证误差的图。</p><p id="32c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它由X值相对误差表示，不应与相对误差混淆。</p><p id="113b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">成本复杂度参数:</strong></p><p id="4ffc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成本复杂性参数是一个建议</p><p id="e2e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分配给树的惩罚值。它是由</p><blockquote class="ke kf kg"><p id="1328" class="if ig jz ih b ii ij ik il im in io ip kh ir is it ki iv iw ix kj iz ja jb jc hb bi translated">Cp = Err(Tree)+ alpha*[L(T)]</p></blockquote><p id="0b8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里</p><p id="b4c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Err (Tree)是分类树(or)情况下验证数据集中的误分类率</p><p id="8ca4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回归树情况下的1-(R)项。</p><p id="0dac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Alpha是基于树中节点数量给出的惩罚值。α介于0和1之间；0表示未修剪树的惩罚项；1表示只有一个节点的树的惩罚项。因此，α在0和1之间变化。</p><p id="fadb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">L(t) —树中终端/叶节点的数量</p><p id="6e59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结束语</strong></p><p id="95cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的步骤中我们看到，算法为我们做了大部分的重量提升工作，因此我们可以毫不费力地构建决策树模型(不可否认，在构建模型之前，我们需要进行大量的数据清理和数据转换)。</p></div></div>    
</body>
</html>