<html>
<head>
<title>Malicious Webpage Classifier using DNN [PyTorch]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用DNN的恶意网页分类器</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/malicious-webpage-classifier-using-dnn-pytorch-ee6402663c58?source=collection_archive---------13-----------------------#2021-05-28">https://medium.com/nerd-for-tech/malicious-webpage-classifier-using-dnn-pytorch-ee6402663c58?source=collection_archive---------13-----------------------#2021-05-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><a href="https://ophtek.com/wp-content/uploads/2019/08/website-malware-removal-tool.png"><div class="er es if"><img src="../Images/0f8c1399ef944e3d3f15ee51a035d2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3oyI4dF_mZCg1CPO-I1J5A.png"/></div></a></figure><p id="d846" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">恶意网页是在您的系统上安装恶意软件的页面，这些恶意软件会破坏计算机的运行，收集您的个人信息，最糟糕的情况还有很多。对互联网上的这些网页进行分类是为用户提供安全浏览体验的一个非常重要的方面。</p><p id="8ec9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">目标是将网页分为两类<em class="jk">恶意</em>【不良】和<em class="jk">良性</em>【良好】网页。针对特定目标，首先对数据集进行预处理，并训练DNN模型，该模型在Pytorch中实现。</p><p id="d461" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">项目中使用的数据集取自<a class="ae jl" href="https://data.mendeley.com/datasets/gdx3pkwp47/2" rel="noopener ugc nofollow" target="_blank">门德利数据</a>。数据集包含原始网页内容、地理位置、javascript长度、网页的模糊JavaScript代码等特征。该数据集包含大约150万个网页，其中120万个用于训练，30万个用于测试。数据集的片段如下所示。</p><figure class="jn jo jp jq fd ij er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jm"><img src="../Images/289ab14272e4d146636bc10041a509fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wxmH6YH2lNUxOwCRABCmNQ.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">资料组</figcaption></figure><p id="70e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据集高度倾斜，97.73%的数据集是良性网页，2.27%是恶意网页，因此仔细选择评估指标非常重要，因为仅仅是准确性并不能给出正确的评估，所以我们将使用f1_score、recall和混淆矩阵。</p><figure class="jn jo jp jq fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/3b27274f70b4349a3dd519ccbe24f06c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*IC2waAUmblewzPG0mufVDw.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">高度倾斜的数据集</figcaption></figure><p id="69e2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，首先我们将导入所有需要的库。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="a809" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Importing the Required Libraries</em><br/><br/>import pandas as pd<br/>import numpy as np<br/><br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>from torch.utils.data import Dataset, DataLoader<br/>from sklearn import metrics<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler</span></pre><p id="9b93" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在导入所有需要的库之后，我们将导入数据集，然后对其进行预处理并向数据集添加一些特性。【在预处理和特征工程之前进行大量探索性数据分析，并在<a class="ae jl" href="https://www.kaggle.com/sumitm004/malicious-webpage-classifier-using-dnn-pytorch" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的完整笔记本中给出。我们将添加的功能是“网络类型”、“原始内容中特殊字符的数量”和“内容的长度”。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="f657" class="kf kg hi kb b fi kh ki l kj kk"># Importing the dataset<br/>df_train=pd.read_csv("../input/Webpages_Classification_train_data.csv")</span><span id="c86d" class="kf kg hi kb b fi kl ki l kj kk">df_test=pd.read_csv("../input/Webpages_Classification_test_data.csv")</span><span id="9abe" class="kf kg hi kb b fi kl ki l kj kk"># Dropping the redundant column<br/>df_train.drop(columns = "Unnamed: 0", inplace = True)<br/>df_test.drop(columns = "Unnamed: 0", inplace = True)</span></pre><p id="6f8d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们将编写一些函数来添加提到的功能，这些函数包含在名为“preproc”的类中，第一个函数是计算原始内容中的特殊字符，第二个函数是使用IP地址分配网络的类型(“A”、“B”和“C”)，有关更多信息，请访问<a class="ae jl" href="https://docs.oracle.com/cd/E19504-01/802-5753/6i9g71m2o/index.html#planning3-fig-11" rel="noopener ugc nofollow" target="_blank">网络类</a>。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="24de" class="kf kg hi kb b fi kh ki l kj kk">class <strong class="kb hj">preproc</strong>:<br/>    <br/>    <em class="jk"># Counting the Special Characters in the content</em><br/>    def count_special(string):<br/>        count = 0<br/>        for char <strong class="kb hj">in</strong> string:<br/>            if <strong class="kb hj">not</strong>(char.islower()) <strong class="kb hj">and</strong> <strong class="kb hj">not</strong>(char.isupper()) <strong class="kb hj">and</strong> <strong class="kb hj">not</strong>(char.isdigit()):<br/>                if char != ' ':<br/>                    count += 1<br/>        return count<br/>    <br/>    <em class="jk"># Identifying the type of network [A, B, C]</em><br/>    def network_type(ip):<br/>        ip_str = ip.split(".")<br/>        ip = [int(x) for x <strong class="kb hj">in</strong> ip_str]<br/><br/>        if ip[0]&gt;=0 <strong class="kb hj">and</strong> ip[0]&lt;=127:<br/>            return (ip_str[0], "A")</span><span id="4f61" class="kf kg hi kb b fi kl ki l kj kk">        elif ip[0]&gt;=128 <strong class="kb hj">and</strong> ip[0]&lt;=191:<br/>            return (".".join(ip_str[0:2]), "B")</span><span id="61b0" class="kf kg hi kb b fi kl ki l kj kk">        else:<br/>            return (".".join(ip_str[0:3]), "C")</span></pre><p id="42c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，使用函数生成特征，并获取原始内容的长度。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="9297" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Adding Feature that shows the Network type</em></span><span id="29df" class="kf kg hi kb b fi kl ki l kj kk">df_train['Network']= df_train['ip_add'].apply(lambda x : preproc.network_type(x))</span><span id="29ea" class="kf kg hi kb b fi kl ki l kj kk">#Getting the Network type</span><span id="99d5" class="kf kg hi kb b fi kl ki l kj kk">df_train['net_part'], df_train['net_type'] = zip(*df_train.Network)<br/>df_train.drop(columns = ['Network'], inplace = True)<br/><br/><em class="jk"># Adding Feature that shows the Number of Special Character in the Content</em></span><span id="45fa" class="kf kg hi kb b fi kl ki l kj kk">df_train['special_char'] = df_train['content'].apply(lambda x: preproc.count_special(x))</span><span id="1b41" class="kf kg hi kb b fi kl ki l kj kk"><em class="jk"># Length of the Content</em></span><span id="cadb" class="kf kg hi kb b fi kl ki l kj kk">df_train['content_len'] = df_train['content'].apply(lambda x: len(x))</span></pre><p id="7a8b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，在标注编码和归一化之前将要素添加到数据集中，</p><figure class="jn jo jp jq fd ij er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es km"><img src="../Images/982712fed5fe4c36a85422e77a98e6ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQG97bSnHd7Tr3QWFCra1A.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">特征工程后的训练数据</figcaption></figure><p id="0079" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将预处理数据集并删除不需要的列，我们将使用scikit-learn LabelEncoder进行标签编码，使用标准标量进行标准化。我们将删除列—<em class="jk">【URL】</em><em class="jk">【IP _ add】</em>和<em class="jk">【内容】</em>。并对其进行预处理。这里，<em class="jk"> le_dict </em>和<em class="jk"> ss_dict </em>具有标签编码器实例和标准标量实例，因此我们可以对测试数据集使用相同的实例。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="8e1f" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># This le_dict will save the Label Encoder Class so that the same Label Encoder instance can be used for the test dataset</em></span><span id="7288" class="kf kg hi kb b fi kl ki l kj kk">le_dict = {}<br/><br/>for feature <strong class="kb hj">in</strong> ls:<br/>    le = LabelEncoder()<br/>    le_dict[feature] = le<br/>    df_train[feature] = le.fit_transform(df_train[feature])<br/><br/># Encoding the Labels<br/>df_train.label.replace({'bad' : 1, 'good' : 0}, inplace = True)</span><span id="a753" class="kf kg hi kb b fi kl ki l kj kk"><em class="jk"># Normalizing the 'content_len' and 'special_char' in training data</em><br/>ss_dict = {}<br/><br/>for feature <strong class="kb hj">in</strong> ['content_len', 'special_char']:<br/>    ss = StandardScaler()<br/>    ss_fit = ss.fit(df_train[feature].values.reshape(-1, 1))<br/>    ss_dict[feature] = ss_fit<br/>    d = ss_fit.transform(df_train[feature].values.reshape(-1, 1))<br/>    df_train[feature] = pd.DataFrame(d, index = df_train.index, columns = [feature])</span></pre><p id="e11d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">预处理后的训练数据，</p><figure class="jn jo jp jq fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/fce59b7c84eea356450e9bf655d56b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*wSO61LYG1Xs_2m3M9O3reg.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">预处理后的训练数据</figcaption></figure><p id="8a19" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将绘制要素相关性热图，以查看要素与标注的相关性。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="da07" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Pearson Correlation Heatmap</em><br/>plt.rcParams['figure.figsize'] == [18, 16]<br/>sns.set(font_scale = 1)<br/><br/>sns.heatmap(df_train.corr(method = 'pearson'), annot = True, cmap = "YlGnBu");</span></pre><p id="4ea2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有一些有趣的事情需要注意，<em class="jk"> 'content_len' </em>，<em class="jk"> 'special_char' </em>，<em class="jk"> 'js_obf_len' </em>，<em class="jk"> 'js_len </em>'与标签的正相关度非常高。对恶意网页和良性网页有更多有趣的发现。(请查看完整的<a class="ae jl" href="https://www.kaggle.com/sumitm004/malicious-webpage-classifier-using-dnn-pytorch" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>以了解广泛的EDA)</p><figure class="jn jo jp jq fd ij er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ko"><img src="../Images/9230cffd40f4f1a3a2caa5f110d87cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*er88Z8UZnUzkKcIDm8siLg.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">特征关联热图</figcaption></figure><p id="4ac1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在继续预处理，我们将对测试数据应用相同的函数并执行相同的过程。[我将展示如何使用相同的<em class="jk"> le_dict </em>和<em class="jk"> ss_dict </em>进行标签编码和标准化，因为前面的特征工程步骤是相同的。]</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="f008" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Using the same label encoders for the features as used in the training dataset</em></span><span id="526d" class="kf kg hi kb b fi kl ki l kj kk">for feature <strong class="kb hj">in</strong> ls:<br/>    le = le_dict[feature]<br/>    df_test[feature] = le.fit_transform(df_test[feature])<br/><br/>df_test.label.replace({'bad' : 1, 'good' : 0}, inplace = True)</span><span id="aa86" class="kf kg hi kb b fi kl ki l kj kk"><em class="jk"># Normalizing the 'content_len' and 'special_char' in testing data</em></span><span id="9ae2" class="kf kg hi kb b fi kl ki l kj kk">ss_fit = ss_dict['content_len']</span><span id="a9ed" class="kf kg hi kb b fi kl ki l kj kk">d = ss_fit.transform(df_test['content_len'].values.reshape(-1, 1))<br/>df_test['content_len'] = pd.DataFrame(d, index = df_test.index, columns = ['content_len'])<br/><br/>ss_fit = ss_dict['special_char']</span><span id="b23e" class="kf kg hi kb b fi kl ki l kj kk">d = ss_fit.transform(df_test['special_char'].values.reshape(-1, 1))<br/>df_test['special_char'] = pd.DataFrame(d, index = df_test.index, columns = ['special_char'])</span></pre><p id="9fde" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">预处理后的测试数据，</p><figure class="jn jo jp jq fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/cf02ae394010129c920d9aa668b83a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*hH3AKFoDlSlDMxS6AuC6rw.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">预处理后测试数据</figcaption></figure><p id="312f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在所有的预处理都完成了，我们将使用PyTorch进行建模，首先，我们必须定制数据集和数据加载器。</p><p id="31a9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将设置配置类，包括<em class="jk">批量</em>、<em class="jk">设备</em> (CPU或GPU)、<em class="jk">学习速率</em>和<em class="jk">时期</em>。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="05c9" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Configuration Class</em><br/>class <strong class="kb hj">config</strong>:<br/>    BATCH_SIZE = 128<br/>    DEVICE =  torch.device("cuda:0" if torch.cuda.is_available() else "cpu")<br/>    LEARNING_RATE = 2e-5<br/>    EPOCHS = 20</span></pre><p id="6414" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们将使用PyTorch的“dataset”和“data loader”创建自定义数据集和数据加载器。为了进行测试，我们将批大小设置为1。<em class="jk"> df_train_loader </em>和<em class="jk"> df_test_loader </em>是我们将用于训练和测试的最终数据加载器。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="00da" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Making the custom dataset for pytorch</em></span><span id="62e3" class="kf kg hi kb b fi kl ki l kj kk">class <strong class="kb hj">MaliciousBenignData</strong>(Dataset):<br/>    def __init__(self, df):<br/>        self.df = df<br/>        self.input = self.df.drop(columns = ['label']).values<br/>        self.target = self.df.label<br/>        <br/>    def __len__(self):<br/>        return (len(self.df))<br/>    <br/>    def __getitem__(self, idx):<br/>        return (torch.tensor(self.input[idx]), torch.tensor(self.target[idx]))</span><span id="122e" class="kf kg hi kb b fi kl ki l kj kk"><em class="jk"># Creating the dataloader for pytorch</em></span><span id="9733" class="kf kg hi kb b fi kl ki l kj kk">def create_dataloader(df, batch_size):<br/>    cls = MaliciousBenignData(df)<br/>    return DataLoader(<br/>        cls,<br/>        batch_size = batch_size,<br/>        num_workers = 0<br/>    )<br/><br/>df_train_loader = create_dataloader(df_train, batch_size=config.BATCH_SIZE)<br/>df_test_loader = create_dataloader(df_test, batch_size = 1) <em class="jk"># Here for testing using the batch size as 1</em></span></pre><p id="dd49" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">定制数据加载器已经完成了！！现在是制作模型的时候了，我们暂时保持DNN的简单。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="173b" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Making the DNN model</em></span><span id="a519" class="kf kg hi kb b fi kl ki l kj kk">class <strong class="kb hj">dnn</strong>(nn.Module):<br/>    def __init__(self):<br/>        super(dnn, self).__init__()<br/><br/>        self.fc1 = nn.Linear(10, 64)<br/>        self.fc2 = nn.Linear(64, 128)<br/>        self.fc3 = nn.Linear(128, 128)<br/>        self.out = nn.Linear(128, 1)<br/><br/>        self.dropout1 = nn.Dropout(p = 0.2)        <br/>        self.dropout2 = nn.Dropout(p = 0.3)<br/>        self.batchn1 = nn.BatchNorm1d(num_features = 64)<br/>        self.batchn2 = nn.BatchNorm1d(num_features = 128)<br/><br/>    def forward(self, inputs):<br/><br/>        t = self.fc1(inputs)<br/>        t = F.relu(t)<br/>        t = self.batchn1(t)<br/>        t = self.dropout1(t)<br/>        t = self.fc2(t)<br/>        t = F.relu(t)<br/>        t = self.batchn2(t)<br/>        t = self.dropout2(t)<br/>        t = self.fc3(t)<br/>        t = F.relu(t)<br/>        t = self.out(t)<br/><br/>        return t</span></pre><p id="3fa1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们将把模型转移到适当的设备上(在config类中给出)。模型的标准和优化器将分别是<a class="ae jl" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html" rel="noopener ugc nofollow" target="_blank"><em class="jk">BCEWithLogitsLoss</em></a>和<a class="ae jl" href="https://pytorch.org/docs/master/generated/torch.optim.Adam.html" rel="noopener ugc nofollow" target="_blank"> <em class="jk">亚当</em> </a>。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="d759" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Transfer the model on the device -- 'GPU' if available or Default 'CPU'</em><br/>model = dnn()<br/>model.to(config.DEVICE)</span><span id="1956" class="kf kg hi kb b fi kl ki l kj kk"><em class="jk"># Criterion and the Optimizer for the model</em><br/>criterion = nn.BCEWithLogitsLoss()<br/>optimizer = optim.Adam(model.parameters(), lr= config.LEARNING_RATE)</span></pre><p id="3895" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将编写一个非常基本的二进制精度函数，它将计算每个历元的二进制精度。这里PyTorch的sigmoid和round函数用于获得预测。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="a8fe" class="kf kg hi kb b fi kh ki l kj kk">def binary_acc(predictions, y_test):<br/>    y_pred = torch.round(torch.sigmoid(predictions))<br/>    correct = (y_pred == y_test).sum().float()<br/>    acc = torch.round((correct/y_test.shape[0])*100)</span><span id="0e05" class="kf kg hi kb b fi kl ki l kj kk">    return acc</span></pre><p id="d78a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">最后，我们将编写训练函数和评估函数(在PyTorch中非常常见)。</p><p id="c73d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">训练函数将接受<em class="jk">【设备】</em><em class="jk">【数据加载器】</em><em class="jk">【优化器】</em><em class="jk">【准则】</em>和<em class="jk">【模型】</em>本身作为参数。我们将使用BCEWithLogitsLoss计算纪元损失，并使用我们编写的二进制精度函数计算精度。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="07c5" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Training function</em><br/><br/>def train_model(model, device, data_loader, optimizer, criterion):</span><span id="069f" class="kf kg hi kb b fi kl ki l kj kk">    <em class="jk"># Putting the model in training mode</em><br/>    model.train()<br/><br/>    for epoch <strong class="kb hj">in</strong> range(1, config.EPOCHS+1):<br/>        epoch_loss = 0<br/>        epoch_acc = 0<br/>        for X, y <strong class="kb hj">in</strong> data_loader:<br/><br/>            X = X.to(device)<br/>            y_ = torch.tensor(y.unsqueeze(1), dtype = torch.float32)<br/>            y = y_.to(device)<br/><br/>            <em class="jk"># Zeroing the gradient</em><br/>            optimizer.zero_grad()<br/><br/>            predictions = model(X.float())<br/><br/>            loss = criterion(predictions, y)<br/>            acc = binary_acc(predictions, y)<br/><br/>            loss.backward() <em class="jk"># Calculate Gradient</em><br/>            optimizer.step() <em class="jk"># Updating Weights</em><br/><br/>            epoch_loss += loss.item()<br/>            epoch_acc += acc.item()<br/><br/>        print (f"Epoch -- <strong class="kb hj">{</strong>epoch<strong class="kb hj">}</strong> | Loss : <strong class="kb hj">{</strong>epoch_loss/len(data_loader)<strong class="kb hj">:</strong> .5f<strong class="kb hj">}</strong> | Accuracy : <strong class="kb hj">{</strong>epoch_acc/len(data_loader)<strong class="kb hj">:</strong> .5f<strong class="kb hj">}</strong>")</span></pre><p id="d132" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">评估功能将非常相似，但我们首先将模型置于评估模式。我们将使用<em class="jk"> torch.no_grad() </em>来减少内存消耗。评估函数将返回<em class="jk"> 'y_test_al' </em>和<em class="jk"> 'y_pred' </em>，它们分别是真实标签和预测值。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="dde1" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Evaluation Function</em><br/><br/>def eval_model(model, device, data_loader):<br/>    <em class="jk"># Putting the model in evaluation mode</em><br/>    model.eval()<br/><br/>    y_pred = []<br/>    y_test_al = []<br/><br/>    with torch.no_grad():<br/>        for X_test, y_test <strong class="kb hj">in</strong> data_loader:<br/>            X_test = X_test.to(device)<br/><br/>            predictions = model(X_test.float())<br/>            pred = torch.round(torch.sigmoid(predictions))<br/><br/>            y_test_al.append(y_test.tolist())<br/>            y_pred.append(pred.tolist())<br/><br/>        <em class="jk"># Changing the Predictions into list </em><br/>        y_test_al = [ele[0] for ele <strong class="kb hj">in</strong> y_test_al]<br/>        y_pred = [int(ele[0][0]) for ele <strong class="kb hj">in</strong> y_pred] <em class="jk"># the format of the prediction is [[[0]], [[1]]]</em><br/><br/>        return (y_test_al, y_pred)</span></pre><p id="5344" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，一切都已完成，我们现在要做的就是训练模型，评估模型并检查其性能。对于训练，我们只调用训练函数。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="e4ce" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Training the Model</em><br/>train_model(model, config.DEVICE, df_train_loader, optimizer, criterion)</span></pre><figure class="jn jo jp jq fd ij er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kq"><img src="../Images/90db0b0364fb509067783f25f67b9258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldXiajsuLPbT82aD-AQdhg.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">培训报告(20个时期)</figcaption></figure><p id="fef2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于评估，调用评估函数。我们将拥有列表形式的真实标签和预测，并且可以使用来自sklearn的指标。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="60fc" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Evaluating the model and getting the predictions</em><br/>y_test, preds = eval_model(model, config.DEVICE, df_test_loader)</span></pre><p id="63ac" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用来自<em class="jk"> sklearn.metrics </em>的分类报告，并使用<em class="jk"> seaborn </em>绘制混淆矩阵的热图。</p><pre class="jn jo jp jq fd ka kb kc kd aw ke bi"><span id="51ae" class="kf kg hi kb b fi kh ki l kj kk"><em class="jk"># Classification Report</em><br/>cls_report = metrics.classification_report(y_test, preds)<br/><br/>print ("")<br/>print (f"Accuracy : <strong class="kb hj">{</strong>metrics.accuracy_score(y_test, preds)*100 <strong class="kb hj">:</strong> .3f<strong class="kb hj">}</strong> %") <br/>print ("")<br/>print ("Classification Report : ")<br/>print (cls_report)</span><span id="b573" class="kf kg hi kb b fi kl ki l kj kk"># Setting the params for the plot<br/>plt.rcParams['figure.figsize'] = [10, 7]<br/>sns.set(font_scale = 1.2)<br/><br/><em class="jk"># Confusion Matrix</em><br/>cm = metrics.confusion_matrix(y_test, preds)<br/><br/><em class="jk"># Plotting the Confusion Matrix</em><br/>ax = sns.heatmap(cm, annot = True, cmap = 'YlGnBu')<br/>ax.set(title = "Confusion Matrix", xlabel = 'Predicted Labels', ylabel = 'True Labels');</span></pre><figure class="jn jo jp jq fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/c84ed142ce1e79e0d6e7f988344eb2af.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*EG-CtOSnWUX6_rrok2YpQQ.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">分类报告</figcaption></figure><figure class="jn jo jp jq fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/7e2826962e43eee4fce1ec6b6223744b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*I2TXYyXour7PxYUyq6an4A.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">混淆矩阵</figcaption></figure><p id="7a94" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，我们可以忽略准确性，因为它只是有偏差，但我们可以看到f1_score和recall score非常好，根据混淆指标，我们的模型可以很好地对网页进行分类。因为我们知道我们的数据集是有偏差的，所以我们忽略了准确性。</p><p id="9684" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里最有趣的是‘special _ char’，‘content _ len’，‘js _ len’和‘js _ obf _ len’与标签具有非常高的正相关性，因此它们是判断模型性能的最重要的特征。有许多有趣的发现，恶意和良性网页，由于我们的模型能够执行好。(我强烈推荐查看<a class="ae jl" href="https://www.kaggle.com/sumitm004/malicious-webpage-classifier-using-dnn-pytorch" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>中的EDA)。</p><p id="bec8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我还在数据集上训练了3个机器学习模型进行比较，并使用Flask和PyWebIO部署了它们。我只运行了DNN模型的kaggle笔记本。所有模型的全部代码和部署都在我的<a class="ae jl" href="https://github.com/SumitM0432/Malicious-Webpage-Classifier" rel="noopener ugc nofollow" target="_blank"> Github </a>上。</p><p id="b396" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">另外，我还在学习，所以非常感谢你的反馈:)</p><p id="1867" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">链接:</strong></p><p id="4d00" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[1]Github—<a class="ae jl" href="https://github.com/SumitM0432/Malicious-Webpage-Classifier" rel="noopener ugc nofollow" target="_blank">https://github.com/SumitM0432/Malicious-Webpage-Classifier</a></p><p id="6ef6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[2] Kaggle笔记本—<a class="ae jl" href="https://www.kaggle.com/sumitm004/malicious-webpage-classifier-using-dnn-pytorch" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/sumitm 004/恶意-网页-分类器-使用-dnn-pytorch </a></p><p id="5729" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[3]数据集—【https://data.mendeley.com/datasets/gdx3pkwp47/2 T4】</p></div></div>    
</body>
</html>