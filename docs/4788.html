<html>
<head>
<title>Review — Pelee: A Real-Time Object Detection System on Mobile Devices</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述——Pelee:移动设备上的实时目标检测系统</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/review-pelee-a-real-time-object-detection-system-on-mobile-devices-65fa30fa93c2?source=collection_archive---------7-----------------------#2021-08-07">https://medium.com/nerd-for-tech/review-pelee-a-real-time-object-detection-system-on-mobile-devices-65fa30fa93c2?source=collection_archive---------7-----------------------#2021-08-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="da66" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">Pelee =提议的 PeleeNet 作为主干+修改的<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>作为对象检测网络(图像分类&amp;对象检测)</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/ffd275e4fba16d5258d39ce3a29f163a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F_P-yLaMkh7Z2N8A"/></div></div></figure><p id="4901" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi kg translated"><span class="l kh ki kj bm kk kl km kn ko di">在</span>这个故事中，对西安大略大学的<strong class="jm hj"> Pelee:一个移动设备上的实时物体检测系统</strong>(Pelee&amp;PeleeNet)进行了回顾。在本文中:</p><ul class=""><li id="6b64" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated"><strong class="jm hj"> PeleeNet </strong>被提出，它只使用<strong class="jm hj">常规卷积</strong>，是<strong class="jm hj">DenseNet</strong><a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jm hj"/></a><strong class="jm hj"/>架构的变体，用于移动设备。</li><li id="dbc4" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hj"> Pelee </strong>，以 PeleeNet 为骨干，以<strong class="jm hj">改进型</strong><a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jm hj">SSD</strong></a><strong class="jm hj"/>为目标检测网络，成为一款高速目标检测器。(这就像 YOLO 物体探测网络，他们使用暗网作为主干。)</li></ul><p id="84a5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这是一篇发表在<strong class="jm hj"> 2018 NeurIPS </strong>的论文，被引用超过<strong class="jm hj"> 200 次</strong>。(<a class="ld le ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----65fa30fa93c2--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="6374" class="lm ln hi bd lo lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">概述</h1><ol class=""><li id="b223" class="kp kq hi jm b jn me jq mf jt mg jx mh kb mi kf mj kv kw kx bi translated"><strong class="jm hj"> PeleeNet:修改后的</strong><a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jm hj">DenseNet</strong></a></li><li id="9de2" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf mj kv kw kx bi translated"><strong class="jm hj"> Pelee:修改过的</strong> <a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="jm hj"> SSD </strong> </a></li><li id="5262" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf mj kv kw kx bi translated"><strong class="jm hj"> PeleeNet:网络架构</strong></li><li id="2f12" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf mj kv kw kx bi translated"><strong class="jm hj">骨盆网:消融研究</strong></li><li id="3439" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf mj kv kw kx bi translated"><strong class="jm hj"> PeleeNet:图像分类结果</strong></li><li id="1008" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf mj kv kw kx bi translated"><strong class="jm hj"> Pelee:物体检测结果</strong></li></ol></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="efee" class="lm ln hi bd lo lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">1.<strong class="ak"> PeleeNet:改良型</strong> <a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a></h1><ul class=""><li id="d685" class="kp kq hi jm b jn me jq mf jt mg jx mh kb mi kf ku kv kw kx bi translated">在 PeleeNet 中修改了<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>的多个位置。</li></ul><h2 id="160a" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated"><strong class="ak"> 1.1。双向密集层</strong></h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es my"><img src="../Images/b2760bf0dd0f02b0d2ad90c1ce65c6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mCUWm87xjeM37CR9KgcHrQ.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">双向密集层结构</strong></figcaption></figure><ul class=""><li id="6277" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">由<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7"> GoogLeNet </a>，<strong class="jm hj">一个双向致密层</strong>激发得到不同规模的感受野，如上。</li><li id="a688" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hj">该层的一种方式</strong>使用<strong class="jm hj">一个 3×3 的内核大小。</strong></li><li id="b369" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hj">该层的另一种方式</strong>使用<strong class="jm hj">两个堆叠的 3×3 卷积</strong>来学习大对象的视觉模式。</li></ul><h2 id="9843" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">1.2.茎块</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nd"><img src="../Images/6fe1e4c3db17adba040918a32e2663b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*4jXvm4_iFpLdBH59HubshQ.png"/></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">阀杆块的结构</strong></figcaption></figure><ul class=""><li id="39da" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">受<a class="ae ix" href="https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc?source=post_page---------------------------" rel="noopener" target="_blank"> Inception-v4 </a>和<a class="ae ix" href="https://sh-tsang.medium.com/review-dsod-learning-deeply-supervised-object-detectors-from-scratch-object-detection-43393dcb31bd" rel="noopener"> DSOD </a>、<strong class="jm hj">的启发，如上所述，在第一个致密层之前设计了一个具有成本效益的阀杆块</strong>。</li><li id="1a87" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">这个词干块可以<strong class="jm hj">在不增加太多计算成本的情况下有效提高特征表达能力</strong>。</li></ul><h2 id="9f2b" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">1.3.瓶颈层中的动态通道数</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ne"><img src="../Images/aa99cf81c4442cee17c78655258724da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ars_z0-CmdVP-wT10jqKuA.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">瓶颈层动态通道数</strong></figcaption></figure><ul class=""><li id="b3c6" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">在<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>中，对于前几个密集层，瓶颈通道的数量远大于其输入通道的数量，这意味着对于这些层来说，瓶颈层增加了计算成本而不是降低了成本。</li><li id="a62f" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">为了保持架构的一致性，<strong class="jm hj">瓶颈层</strong>仍然添加到所有密集层，但是数量<strong class="jm hj">根据输入形状动态调整</strong>，<strong class="jm hj">保证通道数量不超过输入通道。</strong></li><li id="15e7" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">实验表明，与原有的<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>结构相比，这种方法在对精度影响不大的情况下<strong class="jm hj">可以节省高达 28.5%的计算成本。</strong></li></ul><h2 id="3a7b" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">1.4.无压缩过渡层</h2><ul class=""><li id="8152" class="kp kq hi jm b jn me jq mf jt mg jx mh kb mi kf ku kv kw kx bi translated"><a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>提出的压缩因子伤害了特征表达。</li><li id="93b6" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hj">在过渡层中，输出通道的数量始终保持与输入通道的数量相同。</strong></li></ul><h2 id="eebf" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">1.5.复合函数</h2><ul class=""><li id="3020" class="kp kq hi jm b jn me jq mf jt mg jx mh kb mi kf ku kv kw kx bi translated">使用后激活(Conv- <a class="ae ix" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a> -ReLU)代替<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>中使用的预激活。</li><li id="d7fb" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">在这种情况下，<a class="ae ix" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN </a>可以在推理阶段与卷积层合并，这样可以大大加快速度。</li><li id="63be" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">为了补偿这种变化对精度造成的负面影响，当<strong class="jm hj">使用浅而宽的网络结构</strong>时<strong class="jm hj">，在最后一个密集块</strong>后增加一个 1×1 卷积层，以获得更强的表示能力。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="9c19" class="lm ln hi bd lo lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 2。Pelee:修改过的</strong> <a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a></h1><ul class=""><li id="ff9e" class="kp kq hi jm b jn me jq mf jt mg jx mh kb mi kf ku kv kw kx bi translated">PeleeNet 中修改了多处<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a>。</li></ul><h2 id="8bb1" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">2.1.特征地图选择</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nf"><img src="../Images/67ae73f55f55f770b2ff21c9aa68b78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjOtmRwam-KfMBBCGtJYZA.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">特征地图和默认框的比例</strong></figcaption></figure><ul class=""><li id="7e5c" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">精心挑选的一套<strong class="jm hj"> 5 比例尺特征图(19×19、10×10、5×5、3×3、1×1)。</strong></li><li id="0b85" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">为了降低计算成本，我们不使用 38×38 的特征图。</li></ul><h2 id="852a" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">2.2.残差预测块</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ng"><img src="../Images/757a3c265b5e4436b908fc58dc234420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ja8j2PdOuz5glHA_Lu9Oiw.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">残差预测块</strong></figcaption></figure><ul class=""><li id="4500" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">对于用于检测的每个特征图，<strong class="jm hj">在进行预测之前构建残差块(ResBlock)。</strong></li></ul><h2 id="d4e3" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">2.3.用于预测的小卷积核</h2><ul class=""><li id="3039" class="kp kq hi jm b jn me jq mf jt mg jx mh kb mi kf ku kv kw kx bi translated">残差预测块使得<strong class="jm hj">应用 1×1 卷积核来预测类别分数和盒子偏移量成为可能。</strong></li><li id="5cdf" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">实验表明，使用 1×1 核的模型<strong class="jm hj">与使用 3×3 核的模型</strong>的精度几乎相同。</li><li id="7162" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">然而，<strong class="jm hj"> 1×1 核减少了 21.5%的计算量。</strong></li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="46f0" class="lm ln hi bd lo lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">3.PeleeNet:网络架构</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nh"><img src="../Images/27f00cba72549d835bc863c41ee9c053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9e3sLE2D3zBlTASENcwu3Q.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">PeleeNet 架构概述</strong></figcaption></figure><ul class=""><li id="2c4b" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">整个网络由<strong class="jm hj">一个词干块</strong>和<strong class="jm hj">四级特征提取器组成。</strong></li><li id="bb05" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">除了最后一个阶段，每个阶段的最后一层是步长为 2 的平均池层。</li><li id="e0fc" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">前两个阶段的层数被特别控制在可接受的范围内。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="f194" class="lm ln hi bd lo lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">4.PeleeNet:消融研究</h1><ul class=""><li id="5807" class="kp kq hi jm b jn me jq mf jt mg jx mh kb mi kf ku kv kw kx bi translated">根据斯坦福犬使用的 ImageNet，ILSVRC 2012 的子集。这些图像是<strong class="jm hj">犬种</strong>的图像。训练数据和验证数据都是从 ILSVRC 2012 数据集中精确复制的。</li><li id="b58f" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hj">类别数量</strong> : <strong class="jm hj"> 120 </strong></li><li id="f5f1" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hj">训练</strong>图像数量:<strong class="jm hj">150466</strong></li><li id="fa02" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated"><strong class="jm hj">验证</strong>图像数量:<strong class="jm hj">6000</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ni"><img src="../Images/793f2e5f995bcd5ff17107e5f59cbe89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVORXhlC1e3LsAde8Ro_8g.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">各种设计选择和组件对性能的影响</strong></figcaption></figure><ul class=""><li id="b094" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">一个名为<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a> -41 的类似<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>的网络被用作基线。该型号与最初的<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>有两处不同:</li></ul><ol class=""><li id="bb68" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf mj kv kw kx bi translated">第一个是第一 conv 层的参数。第一 conv 层上有 24 个通道<strong class="jm hj">而不是 64 个</strong>，并且<strong class="jm hj">内核大小</strong>也从 7×7 变为 3×3 。</li><li id="4cc2" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf mj kv kw kx bi translated">第二个是<strong class="jm hj">调整每个密集块中的层数以满足计算预算。</strong></li></ol><ul class=""><li id="9954" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">综合所有设计选择后，<strong class="jm hj"> PeleeNet 在 Stanford Dogs </strong>上达到 79.25%的准确率，比<a class="ae ix" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jm hj">DenseNet</strong></a><strong class="jm hj">-41</strong>在<strong class="jm hj">更少的计算开销下，准确率提高了<strong class="jm hj">4.23%。</strong></strong></li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="4573" class="lm ln hi bd lo lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 5。图像分类结果</strong></h1><h2 id="c713" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">5.1.ImageNet</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nj"><img src="../Images/325d22f24fbee41b8f72e13ccdfe68be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uy46DX2TyxeyhQyv-qj8dw.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">ImageNet ils vrc 2012 的结果</strong></figcaption></figure><ul class=""><li id="fd81" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">PeleeNet 比<a class="ae ix" href="https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jm hj">mobilenetv 1</strong></a><strong class="jm hj">和</strong><a class="ae ix" href="https://towardsdatascience.com/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jm hj">shuffle net V1</strong></a><strong class="jm hj"/>【在<strong class="jm hj">不超过模型大小</strong>的 66%的情况下，实现了<strong class="jm hj">更高的精度和<strong class="jm hj">更低的计算成本。</strong></strong></li><li id="4c64" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">PeleeNet 的模型尺寸只有<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------"> VGG </a> 16 的 1/49。</li></ul><h2 id="be36" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">5.2.<strong class="ak">NVIDIA TX2 上的速度</strong></h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nk"><img src="../Images/9d0cfa1a05606cbe851a312f2b499cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BbNMDDg5A_tUky03AGoqIg.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">NVIDIA TX2 上的速度(越大越好)基准测试工具采用 NVIDIA TensorRT4.0 库构建。</strong></figcaption></figure><ul class=""><li id="9305" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">PeleeNet 比 TX2 上的 MoibleNetV1、<a class="ae ix" href="https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jm hj">mobilenet v2</strong></a><strong class="jm hj">快得多。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nl"><img src="../Images/a8b6ab3e86dd8dfc03475febe60930e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvpbsWdKCaJYmMi3O2eEpw.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">NVIDIA TX2 上的速度</strong></figcaption></figure><ul class=""><li id="4283" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated"><strong class="jm hj">使用半精度浮点(FP16)，PeleeNet 在 FP16 模式下的运行速度比在 FP32 模式下快 1.8 倍。</strong></li><li id="6fc6" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">相比之下，用深度方向可分离卷积构建的网络很难受益于 TX2 半精度(FP16)推理引擎。在 FP16 模式下运行的<a class="ae ix" href="https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69?source=post_page---------------------------" rel="noopener" target="_blank"> MobileNetV1 </a>和<a class="ae ix" href="https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c?source=post_page---------------------------" rel="noopener" target="_blank"> MobileNetV2 </a>的速度与在 FP32 模式下运行的速度几乎相同。</li></ul><h2 id="f116" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">5.3.iPhone 8 上的速度</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nm"><img src="../Images/d1d7e8f4d759147d93c87feb9e987003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxxeWU7OHqYHxX4603BS6Q.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">iPhone 8 上的速度(越大越好)基准测试工具是用 CoreML 库构建的</strong></figcaption></figure><ul class=""><li id="3823" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">类似地，PeleeNet 以较小的模型尺寸获得较高的精度。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h1 id="24b0" class="lm ln hi bd lo lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">6.<strong class="ak">物体检测结果</strong></h1><h2 id="9c66" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">6.1.各种设计选择的效果</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nn"><img src="../Images/218cfba6f6b24ba8269837125e56b81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6anfRHf7BcsyJeP_wHXUw.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">各种设计选择对性能的影响</strong></figcaption></figure><ul class=""><li id="4ab4" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">具有<strong class="jm hj">残差预测块</strong>的模型实现了比没有残差预测块的模型高 2.2% 的<strong class="jm hj">。</strong></li><li id="3104" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">使用 1×1 核函数预测的模型精度与使用 3×3 核函数预测的模型精度几乎相同。然而，<strong class="jm hj"> 1×1 核减少了 21.5%的计算量和 33.9%的模型规模。</strong></li></ul><h2 id="d758" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">6.2.帕斯卡 VOC 2007</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es no"><img src="../Images/b5ef3fb9e2f20ae45e8d92b99bf29781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sweUesPAmHvLi9WfkUJrYg.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">PASCAL VOC 2007 的结果</strong></figcaption></figure><ul class=""><li id="73f2" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">Pelee 的精度比 Tiny <a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank"> YOLOv2 </a>高 13.8%，比<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>+<a class="ae ix" href="https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69?source=post_page---------------------------" rel="noopener" target="_blank">MobileNetV1</a>高 2.9%。</li><li id="e0ea" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">它甚至高于<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2</a>–288 的计算量，而其计算量仅为<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2</a>–288 的 14.5%。</li><li id="c487" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">当我们采用在第 3.3 节中描述的 COCO trainval35k 上训练的模型并在 07+12 数据集上对其进行微调时，Pelee 实现了 76.4%的 mAP。</li></ul><h2 id="061e" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">6.3.真实设备上的速度</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es np"><img src="../Images/0179a6c397f58887dfb1f32bf2d46284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z87MsuzmC5y_4zbltnJ8DQ.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">真实设备上的速度</strong></figcaption></figure><ul class=""><li id="a775" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">虽然 Pelee 中使用的残差预测块增加了计算成本，但 Pelee 在 iPhone 上和 FP32 模式下的 TX2 上运行速度仍然比<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a> +MobileNetV1 快。</li><li id="f1b9" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">还有，Pelee 在 FP16 模式下相对于<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank"> SSD </a> +MobileNetV1 和<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>Lite+<a class="ae ix" href="https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c?source=post_page---------------------------" rel="noopener" target="_blank">MobileNetV2</a>有更大的速度优势。</li><li id="6b93" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">我们提出的物体检测系统 Pelee 可以在 iPhone 8 上运行<strong class="jm hj"> 23.6 FPS，在 NVIDIA TX2 </strong>上运行<strong class="jm hj"> 125 FPS，精度很高。</strong></li></ul><h2 id="0540" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">6.4.椰子树</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nq"><img src="../Images/003e26ff1fe3f473e02302f5c42b51c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mo2nssAz2p6dTVwBnINsPA.png"/></div></div><figcaption class="mz na et er es nb nc bd b be z dx translated"><strong class="bd lo">COCO 测试结果-dev2015 </strong></figcaption></figure><ul class=""><li id="6412" class="kp kq hi jm b jn jo jq jr jt kr jx ks kb kt kf ku kv kw kx bi translated">Pelee 不仅比<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>+<a class="ae ix" href="https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69?source=post_page---------------------------" rel="noopener" target="_blank">MobileNetV1</a>更准确，而且在 mAP @ 0.5:0.95 和 mAP@0.75 两种情况下都比<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank"> YOLOv2 </a>更准确。</li><li id="0db8" class="kp kq hi jm b jn ky jq kz jt la jx lb kb lc kf ku kv kw kx bi translated">同时，Pelee 的速度比<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank"> YOLOv2 </a>快 3.7 倍，模型尺寸小 11.3 倍。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="09ac" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">参考</h2><p id="52da" class="pw-post-body-paragraph jk jl hi jm b jn me ij jp jq mf im js jt nr jv jw jx ns jz ka kb nt kd ke kf hb bi translated">【2018 neur IPS】【Pelee &amp; PeleeNet】<br/><a class="ae ix" href="https://arxiv.org/abs/1804.06882" rel="noopener ugc nofollow" target="_blank">Pelee:移动设备上的实时物体检测系统</a></p><h2 id="9ca1" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">图像分类</h2><p id="69e2" class="pw-post-body-paragraph jk jl hi jm b jn me ij jp jq mf im js jt nr jv jw jx ns jz ka kb nt kd ke kf hb bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(强)(的)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)( )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(没)(想)(要)(到)(这)(个)(人)(,)(我)(们)(们)(都)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(都)(不)(想)(到)(这)(里)(去)(。</p><h2 id="f133" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated">目标检测</h2><p id="f63c" class="pw-post-body-paragraph jk jl hi jm b jn me ij jp jq mf im js jt nr jv jw jx ns jz ka kb nt kd ke kf hb bi translated">… <br/> <strong class="jm hj"> 2018 </strong> : [ <a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">约洛夫 3 </a> ] [ <a class="ae ix" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">级联 R-CNN</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener">refined et</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">corner net</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-pelee-a-real-time-object-detection-system-on-mobile-devices-65fa30fa93c2" rel="noopener">Pelee&amp;PeleeNet</a><br/><strong class="jm hj">2019</strong>:[<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/review-dcnv2-deformable-convnets-v2-object-detection-instance-segmentation-3d8a18bee2f5"/></p><h2 id="d1ca" class="mk ln hi bd lo ml mm mn ls mo mp mq lw jt mr ms ly jx mt mu ma kb mv mw mc mx bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>