<html>
<head>
<title>Quora Question Pairs Similarity Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Quora问题对相似性问题</h1>
<blockquote>原文：<a href="https://medium.com/nerd-for-tech/quora-question-pairs-similarity-problem-5868b7a33e1e?source=collection_archive---------7-----------------------#2021-05-02">https://medium.com/nerd-for-tech/quora-question-pairs-similarity-problem-5868b7a33e1e?source=collection_archive---------7-----------------------#2021-05-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3cdf" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用文本分析和经典的机器学习方法解决现实世界的问题！</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/4142161a3c4d8e0bbab6771318b481bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MEA47DZk97EUV-oJXboBIw.jpeg"/></div></div></figure><p id="27d3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi kf translated">uora是一个美国问答网站，在这里，互联网用户以事实或观点的形式提出、回答、关注和编辑问题。</p><p id="fa85" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">每月有超过1亿人访问Quora，所以很多人问类似措辞的问题也就不足为奇了。具有相同意图的多个问题会导致搜索者花费更多的时间来寻找问题的最佳答案，并使作者感到他们需要回答同一问题的多个版本。Quora关注规范问题，因为它们为积极的搜索者和作者提供了更好的体验，并为这两个群体提供了更大的长期价值。</p><h1 id="d815" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">商业问题</h1><p id="318b" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">确定Quora上问的哪些问题是已经问过的问题的重复。</p><p id="dbd3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这对于立即提供已经回答的问题的答案可能是有用的。我们的任务是预测一对问题是否重复。</p><h1 id="766f" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">用户指南</h1><p id="7485" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">在我的GitHub仓库中，<a class="ae ll" href="https://github.com/Priyanka-Dandale/Quora-Questions-Pairs-Kaggle-Competition" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj"> Colab笔记本</strong> </a>可用于这个真实世界的用例！</p><h1 id="d0cd" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">限制</h1><ol class=""><li id="0171" class="lm ln hi jl b jm lg jp lh js lo jw lp ka lq ke lr ls lt lu bi translated">错误分类的代价可能非常高。</li><li id="4f8c" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke lr ls lt lu bi translated">你需要一对重复问题的概率，这样你就可以选择任何选择的阈值。比如说，如果你选择概率阈值为0.95，那么Prob(问题1类似于问题2) ≥0.95，那么只合并问题1和问题2的答案，否则不合并。</li><li id="9b0e" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke lr ls lt lu bi translated">没有严格的延迟问题。</li><li id="973f" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke lr ls lt lu bi translated">可解释性是部分重要的。</li></ol><h1 id="644d" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">理解数据</h1><p id="dc8c" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">您可以从<a class="ae ll" href="https://www.kaggle.com/c/quora-question-pairs/data" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj"><em class="ma">Kaggle</em></strong></a><strong class="jl hj"><em class="ma">或我的</em></strong><a class="ae ll" href="https://github.com/Priyanka-Dandale/Quora-Questions-Pairs-Kaggle-Competition" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj"><em class="ma">GitHub</em></strong></a><strong class="jl hj"><em class="ma">资源库下载用例的数据集。</em>T25】</strong></p><p id="177d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">数据概述</strong></p><p id="7459" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">数据将在一个文件<strong class="jl hj"> <em class="ma"> train.csv. </em> </strong>中总共有<em class="ma"> 404，290 </em>个观测值或行数。它包含<em class="ma"> 6 </em>列:id(观察id)，qid1(第一个问题id)，qid2(第二个问题id)，question1(第一个问题的文本内容)，question2(第二个问题的文本内容)，is_duplicate是问题是否重复的标签，分别用1或0表示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/98fc43704d1f20020970af609ea5f550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7XDgfn-qDG-7acqK.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">前10个数据观察</figcaption></figure><h1 id="d6ae" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">ML问题的类型</h1><p id="eca8" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">这是一个<strong class="jl hj"><em class="ma"/></strong>二元分类问题，对于给定的一对问题，我们需要预测它们是否重复。</p><h1 id="5b5d" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">探索性数据分析</h1><p id="7c30" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated"><strong class="jl hj">用于训练的问题对总数是多少？</strong>———404290。</p><p id="b046" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">数据中是否有空值或缺失值？ — —问题1有1个空对象或缺失值，问题2有2个空对象或缺失值。我们将用空白字符串填充缺失的值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/4fba7e5ca6b9f306f5d1c847b9affb0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NWj3k8tr2eWKci0K.png"/></div></div></figure><p id="d52d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">最终类标签分布如何？</strong></p><p id="a153" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">重复(相似或1)问题数为<em class="ma"> 149263 </em>，非重复(不相似或0)问题数为<em class="ma"> 255027 </em>，分别占数据的36.92%和63.08%。只有37%左右的问题对是相似的！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/f9f297b2729f7f4eb1a6f3beb9b27966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z0094fSxxu7UR7dm.png"/></div></div></figure><p id="961f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">独特题有多少？</strong>———537933</p><p id="9d9b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">查找独特问题的图示:在下图的虚拟数据中，请注意问题的id在第4行和第5行重复出现。观察的总数是5，但是唯一的问题数是6，分别是1、2、3、4、5和6。为了找到问题的唯一数量，我们应该考虑来自qid1和qid2的唯一问题id的数量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/dfe4bafc320e52bc41d783a037522872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2428DBGA0ECiSp0v.png"/></div></div></figure><p id="0290" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">大约21%的独特问题被重复多次。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/3dc078dc3bdb243c71956871bb0a58a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*r9UdVZ66aM41OV__.png"/></div></div></figure><p id="2ba7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">重复次数最多的问题是哪个？“减肥的最好方法是什么？”问题是否重复了157次。似乎人们对如何减肥更感兴趣！:P</strong></p><p id="bb88" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">有没有重复的成对题？不！</strong></p><p id="a6ce" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">查看是否有任何重复问题对的图示:在下图的虚拟数据中，有2个重复问题对，一个问题对的id为0和4，另一个问题对的id为1和2。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/63e7aefa48826e252b30f211e4747443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tSwtyfx_ZVXR6a-g.png"/></div></div></figure><p id="31a4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">每个问题出现的次数是多少？</p><p id="87b5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">注意y轴是对数轴，10的0次方是1，依此类推……注意有一个问题重复了160次左右。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/be7992831a83aad437474499660ca9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ea6VTxrfUlkZxRQI.png"/></div></div></figure><h1 id="8a18" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">数据清洗前的特征提取</h1><p id="cce5" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">让我们从特征工程开始。我们将提取11个可能的特征，这些特征中有些可能有用，有些可能根本没用。在构造了下面的特性之后，我们总共有17列，包括输出特性is_duplicate。考虑以下虚拟数据进行说明:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/012169fbdaea601515df9fdbc3a8a36b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QbwMJ2_1OzpD5CM9.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/e08cd96f04a37c319dcadd10d6c8e53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*HdeXTpvZSqQT3E3M-kJq3g.png"/></div></figure><h2 id="57a9" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">对上面提取的特征的分析</h2><ul class=""><li id="09d7" class="lm ln hi jl b jm lg jp lh js lo jw lp ka lq ke mv ls lt lu bi translated">问题1有67个问题，问题2有24个问题，只有一个词！</li><li id="2919" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">“word_share”功能有很强的预测能力，因为它善于将重复问题与非重复问题区分开来。左侧的小提琴图显示，重复问题比非重复问题共享更多的常用词，因此重复问题的word_share更高。标准化word_share的分布在最右侧有一些重叠，即有相当多的问题具有高的单词相似性。</li><li id="7d1f" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">有趣的是，它似乎很擅长识别绝对不同的问题，但不太擅长发现绝对重复的问题。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/179a9d390b7d9a28c72f122212b2bcf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*spsKqcBcpBST5w3DAzEpsQ.png"/></div></div></figure><ul class=""><li id="1422" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">在下图中，word_Common特征在相似和不相似问题中的分布高度重叠。因此，这个特性对我们来说并不是很有用。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/03e706315a3716a63b9927704c3cdde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7YfkG-bULgWFZzFnmPcqA.png"/></div></div></figure><ul class=""><li id="6ab4" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">在下图中，特征词_Total的分布高度重叠。因此，这个特征对于区分重复和非重复问题毫无帮助。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/959578a552c5a266993fc602552d621c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_QJD8cqw-wJNJGF6K_ZfBg.png"/></div></div></figure><ul class=""><li id="b237" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">下面的相关图还显示，并非所有提取的特征都有助于区分问题是否重复。唯一的word_share与类标签的相关性稍高。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/5e96ebf9076699e2e25085347c0551f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*L8EQEOqmaRRuG9VeZnKSpg.png"/></div></figure><h2 id="a149" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">语义分析</h2><p id="cd8a" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">接下来，我们将看看不同标点在问题中的用法——这可能为后面一些有趣的功能奠定基础。</p><ul class=""><li id="9d47" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">带问号的问题:99.87%</li><li id="d228" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">带有[数学]标签的问题:0.12%</li><li id="146d" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">带句号的问题:6.31%</li><li id="0102" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">首字母大写的问题:99.81%</li><li id="0ea6" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">大写字母的问题:99.95%</li><li id="75b6" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">带数字的问题:11.83%</li></ul><h1 id="5038" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">数据清理后的高级特征提取(NLP和模糊特征)</h1><p id="8796" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">考虑“什么是数据科学？”用于说明的句子:</p><ul class=""><li id="8f69" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated"><strong class="jl hj">令牌</strong>:将一句话用空格分割，得到一个令牌。</li></ul><p id="a743" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">《出埃及记》:'什么'，'是'，'数据'，'科学？'是四个代币。</p><ul class=""><li id="0acf" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated"><strong class="jl hj">停用词</strong>:根据NLTK停用词。</li></ul><p id="274d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">《出埃及记》:这里“is”是停用词。</p><ul class=""><li id="d489" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated"><strong class="jl hj">字</strong>:不是停用字的令牌。</li></ul><p id="b3f6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">《出埃及记》:'什么'，'数据'，'科学？'是单词。</p><p id="ef9a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">以下是我们将提取的21项新功能:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/39b30f5d90a822d31bf096fa6ec0c647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*3qMxFj2EvJiU2uHy1BXb_g.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/2f7f02432528a03325ce4ce29e13e836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LP-zkjRCwV-rGK2X6RAfkw.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/917d80d658679a0b82f74205561d7468.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2GuSKEkGc5iiTWZsXl3ZiA.png"/></div></div></figure><p id="1385" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">更多关于<a class="ae ll" href="https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj"> fuzzy-wuzzy特性</strong> </a> <strong class="jl hj">的信息。</strong></p><h2 id="6228" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">对上面提取的特征的分析</h2><ul class=""><li id="e59c" class="lm ln hi jl b jm lg jp lh js lo jw lp ka lq ke mv ls lt lu bi translated">类别1中的数据点或问题的数量(重复对):<em class="ma"> 298526 </em></li><li id="d58a" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">类别0中数据点或问题的数量(非重复对):<em class="ma"> 510054 </em></li></ul><p id="e6b2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从问题中删除停用词后:</p><ul class=""><li id="b24c" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">重复对问题的总字数:<em class="ma"> 16109861 </em></li><li id="86a5" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">非重复对问题的总字数:<em class="ma"> 33192952 </em></li></ul><p id="0df6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从重复配对问题的文本中生成的词云:</p><ul class=""><li id="6f3e" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">donald trump、best way、1k卢比是重复问题对中出现最多的词。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/c607d36dc8a64ea632432f4b5af45b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*6Z_I91O0U_XU_INwfETcNw.png"/></div></figure><p id="c599" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从非重复配对问题的文本中生成的词云:</p><ul class=""><li id="38b1" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">not、will、difference、India是非重复问题对中出现最多的词。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ng"><img src="../Images/efb1f63496a7caf45947ea2c557933b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*uR9yAUqYr0WOD47Fbati5w.png"/></div></div></figure><ul class=""><li id="3e6a" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">下面的相关图还显示，并非所有提取的特征都有助于区分问题是否重复。token_sort_ratio、token_set_ratio、fuzz_ratio、fuzz_partial_ratio、ctc_min、ctc_max、cwc_min和cwc_max等特征与类别标签的相关性稍高，因此有助于区分问题是否是重复对。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/c05d3724d1e55a6d4dbed427e10ea2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*haqD-RKLZV4fxTy8chk8kg.png"/></div></figure><h2 id="21d5" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">用于可视化的t-SNE(t-分布随机邻域嵌入)</h2><p id="4001" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">T-SNE是最常用的降维技术。这是一种统计方法，通过在二维或三维地图中给出每个数据点的位置来可视化高维数据。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/abbbc310fb490bd6b1eeb4851113be9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*Zx3W7NKoXbcJCZt4mOhfuQ.png"/></div></figure><p id="eb34" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在上面的二维图形中，请注意，有几个区域的最大数据点只有一个类0(非重复)或1(重复)重叠，但也有几个区域可以快速分离类区域0或1。因此，这给了我们一个提示，我们设计的多维特征在区分这两个类上肯定有一些价值，虽然不是完美的，但是在某种程度上！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nj"><img src="../Images/670c0aef3ecd046a2e0ba66ceb97b098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*nrtHdsaNRaa6oYXTNTYBiA.png"/></div></figure><h2 id="f15f" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">使用TF-IDF加权word2vec进行特征化</h2><p id="9882" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">我们注意到，一些单词在重复的问题对(即类别1)中出现得更频繁，而一些单词在非重复的问题对(即类别0)中出现得更频繁(参见单词云部分)。</p><ul class=""><li id="4409" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">在该方法中，我们将计算每个单词的TF-IDF值，然后对TF-IDF值与相应单词向量的乘积求和，并将该和除以TF-IDF值的和。</li></ul><blockquote class="nk nl nm"><p id="3668" class="jj jk ma jl b jm jn ij jo jp jq im jr nn jt ju jv no jx jy jz np kb kc kd ke hb bi translated">注意，我们将使用Glove(全局向量)来代替word2vec。</p></blockquote><ul class=""><li id="4d4f" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">word2vec和Glove都使我们能够以向量的形式表示一个单词(通常称为嵌入)。它们是两种最流行的单词嵌入算法，可以产生单词的语义相似性，从而捕捉单词含义的不同方面。Word2vec嵌入基于训练浅层前馈神经网络，而glove嵌入基于矩阵分解技术学习。</li><li id="851f" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">Word2Vec算法平等地对待每个单词，因为它们的目标是计算单词嵌入。当需要处理句子或文档嵌入时，这种区别变得很重要；并不是所有的单词都能代表一个特定句子的意思。这里应用了不同的加权策略，TF-IDF就是其中一个成功的策略。</li><li id="1269" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">有时，它确实提高了推断的质量，所以这种结合值得一试。</li></ul><h1 id="f347" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">训练和测试分割</h1><p id="8df3" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">当用于对未用于训练模型的数据进行预测时，训练-测试分离过程用于估计机器学习算法的性能。</p><p id="7ab5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">想想<strong class="jl hj"> <em class="ma">时间分割或者基于时间的分割</em></strong>……假设你的完整数据是按时间排序的，然后你把最老的数据分割成训练，把最新的数据分割成测试。时间分割在这里是有意义的，就好像问的问题是“谁是现任总理？”那么合并几年前问的同一个问题的答案会给出无用的答案或结果。因为数据集中没有时间戳相关的特征，所以我们不能使用时间分割技术。因此，我们通过以70:30或80:20或我们选择的任何比例随机分割来构建训练集和测试集，因为我们有足够的观察数据来处理！</p><h2 id="5e0d" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">机器学习模型</h2><p id="b0c5" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">数据集中总共有796个要素。问题1 w2v数据框中的要素数量与问题2 w2v数据框中的要素数量相同，即384。总共有794个训练特征，包括在数据清理之前提取的基本特征、在数据预处理之后提取的高级NLP和模糊特征、问题1的Tf-Idf向量和问题2的Tf-Idf向量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/d1a73d6f7d424403e514cf0d34b9e4e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*DmP9H-rdib4DZbUUB6dHmw.png"/></div></figure><p id="76b6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里，我们将以70:30的比例将我们的数据分成训练测试。列车数据中的数据点数:<em class="ma"> 283003 </em>和测试数据中的数据点数:<em class="ma"> 121287 </em>。让我们称之为测试数据，验证数据，因为我们已经有了完全看不见的测试数据。可以从<a class="ae ll" href="https://www.kaggle.com/c/quora-question-pairs/data" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj"><em class="ma">Kaggle</em></strong></a><strong class="jl hj"><em class="ma">下载测试数据集。</em>T19】</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nr"><img src="../Images/36bb8e41ae58dd855529986262bdd64e.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*ZkrR1U7dFtSfuPHjTlIJ3g.png"/></div></figure><h2 id="08e9" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">什么是日志丢失？</h2><p id="1cc0" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">对数损失是基于概率的最重要的分类度量。很难解释原始的对数损失值，但是对数损失仍然是比较模型的一个很好的度量。对数损失是一个介于[0，无穷大]之间的度量。对于任何给定的问题，较低的对数损失值意味着更好的预测。</p><p id="791d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对数损失是对所谓的<strong class="jl hj">似然函数</strong>的一个小小的扭曲。事实上，对数损失是-1 *似然函数的对数。因此，我们将从理解似然函数开始。</p><p id="99fd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">可能性函数回答了“模型认为实际观察到的结果有多可能”这个问题。如果这听起来令人困惑，举个例子应该会有帮助。</p><p id="c188" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">例如，一个模型预测三所房子的概率<code class="du ns nt nu nv b">[0.8, 0.4, 0.1]</code>。前两套房子卖了，最后一套没卖。所以实际结果可以用数字表示为<code class="du ns nt nu nv b">[1, 1, 0]</code>。</p><p id="0157" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们一次一个地遍历这些预测来迭代计算似然函数。</p><p id="6d85" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">第一栋房子卖出去了，模型显示有80%的可能性。所以，看一个预测后的似然函数是0.8。</p><p id="e1c4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">第二栋房子卖出去了，模型显示有40%的可能性。有一个概率规则，多个独立事件的概率是它们各自概率的乘积。因此，我们通过将前两个预测的相关概率相乘，得到组合的可能性。那就是<code class="du ns nt nu nv b">0.8 * 0.4</code>，恰好是0.32。</p><p id="9825" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在我们开始第三个预测。那套房子没有卖出去。模型说有10%的可能卖出去。这意味着有90%的可能卖不出去。因此，根据模型，观察到的<em class="ma">不出售</em>的结果有90%的可能性。所以，我们把之前的结果0.32乘以0.9。</p><p id="aca1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们可以逐步完成所有的预测。每次我们都会找到与实际发生的结果相关的概率，然后乘以之前的结果。这就是可能性！</p><h2 id="8860" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">基线随机模型</h2><p id="2612" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">基线预测算法提供了一组预测，您可以像对问题的任何预测一样对其进行评估，例如分类准确性或损失。随机预测算法预测在训练数据中观察到的随机结果。这意味着随机模型随机预测标签0或1。当评估所有其他机器学习算法时，这些算法的分数提供了所需的比较点。<a class="ae ll" href="https://machinelearningmastery.com/implement-baseline-machine-learning-algorithms-scratch-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj">更有</strong> </a>...</p><p id="e097" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用随机模型的验证数据的对数损失为0.89。减少损失的余地很大！</p><p id="bd7e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我试过逻辑回归，因为数据是相当高的维度和线性SVM。我使用了具有“对数”损失的SGDclassifier，它实际上是一种逻辑回归，使用了具有“铰链”损失的SGDclassifier，它实际上是一种线性SVM，并对alpha(SGD分类器的参数)执行了超参数调整。但是这两个模型都没有给出承诺的精确度，因此也就没有了对数损失。</p><h2 id="fa07" class="mh kp hi bd kq mi mj mk ku ml mm mn ky js mo mp la jw mq mr lc ka ms mt le mu bi translated">XGBoost模型</h2><p id="04b5" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">XGBoost是一种基于决策树的集成机器学习算法，使用梯度推进框架。XGBoost的性能不是开玩笑的——它已经成为赢得许多Kaggle比赛的首选库。<a class="ae ll" href="https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7" rel="noopener" target="_blank">更多</a> …</p><p id="72a7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对训练数据拟合XGBoost模型后，结果如下:</p><ul class=""><li id="f936" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">训练数据的对数损失为0.69，验证数据的对数损失也约为0.69。0.69.日志丢失减少得更好，但仍有很大的改进空间！</li><li id="faf7" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">在应用一些超参数调整之后，最终实现了训练数据的对数损失为0.345，验证的对数损失为0.357，这是一个显著的改进。</li><li id="b9fd" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">此外，两个损失都很接近，表明模型既不是过拟合也不是欠拟合。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nw"><img src="../Images/cfe466829ef4502caeb41c59209ef887.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*UBgwjinElI--h-z6CrPGwg.png"/></div></figure><ul class=""><li id="fc40" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">真阳性(17081)和真阴性(7600)数量很大，这表明正确分类的数量很大。检查<a class="ae ll" href="https://vocal.media/stories/machine-learning-model-performance-metrics" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj">此处</strong> </a>用于分类问题的评估指标及其解释。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nx"><img src="../Images/f6ff7e5da69120375283038f5c0fa67c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhByMjkdEgJiKMNww-NnGQ.png"/></div></div></figure><ul class=""><li id="1f86" class="lm ln hi jl b jm jn jp jq js mx jw my ka mz ke mv ls lt lu bi translated">等级0的精度为83%，等级1的精度为80%。</li><li id="68e6" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">同样，类别0的召回率是90%，类别1的召回率是69%。</li><li id="24fd" class="lm ln hi jl b jm lv jp lw js lx jw ly ka lz ke mv ls lt lu bi translated">两者都是对上述两个线性模型，逻辑回归和线性SVM的重大改进。</li></ul><p id="fb78" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，您的任务是将相同的预处理步骤应用于测试数据，并将上述训练好的模型应用于预测！</p><p id="8074" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi">_______________________________________________________________</p><p id="9301" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">感谢你阅读❤.</p><p id="fc83" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对于任何建议或疑问，请在下面留下您的评论，并关注更新。</p><p id="3a22" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果你喜欢这篇文章，请点击👏图标来支持它。这将有助于其他媒体用户找到它。分享一下，让别人也能看！</p><p id="da68" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">快乐学习！:)</p></div></div>    
</body>
</html>